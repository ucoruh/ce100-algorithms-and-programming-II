{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"license/","text":"License \u00b6 MIT License Copyright \u00a9 2019-2022 U\u011fur CORUH Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"MIT License Copyright \u00a9 2019-2022 U\u011fur CORUH Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"changelog/","text":"Changelog \u00b6 Material for Algorithm Lovers \u00b6 1.0.0 _ October 20, 2020 \u00b6 Initial release","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#material-for-algorithm-lovers","text":"","title":"Material for Algorithm Lovers"},{"location":"changelog/#1.0.0","text":"Initial release","title":"1.0.0 _ October 20, 2020"},{"location":"syllabus/syllabus/","text":"Recep Tayyip Erdogan University \u00b6 Faculty of Engineering and Architecture \u00b6 Computer Engineering \u00b6 CE100 Algorithms and Programming-II \u00b6 Syllabus \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Download WORD , PDF Instructor Asst. Prof. Dr. U\u011fur CORUH Contact Information ugur.coruh@erdogan.edu.tr Office No F-301 Google Classroom Code bafwwt6 Lecture Hours and Days TBD Lecture Classroom \u0130BBF 402 Level-4 Office Hours Meetings will be scheduled over Google Meet with your university account and email and performed via demand emails. Please send emails with the subject starting with [CE100] tag for the fast response and write formal, clear, and short emails Lecture and Communication Language English Theory/Laboratory Course Hour Per Week 3/2 Hours Credit 4 Prerequisite CE103- Algorithms and Programming I Corequisite TBD Requirement TBD *TBD: To Be Defined. A.Course Description \u00b6 This course continues the CE103 Algorithms and Programming I course. This course taught programming skills in Algorithms and Programming I course met. This course taught programming skills in Algorithms and Programming I with common problems and their solution algorithms. This lecture is about analyzing and understanding how algorithms work for common issues. The class will be based on expertise sharing and guiding students to find learning methods and practice for algorithm and programming topics. By making programming applications and projects in the courses, the learning process will be strengthened by practicing rather than theory. B.Course Learning Outcomes \u00b6 After completing this course satisfactorily, a student will be able to: Interpret a computational problem specification and algorithmic solution and implement a C/C++, Java or C# application to solve that problem. Argue the correctness of algorithms using inductive proofs and invariants. Understand algorithm design steps Argue algorithm cost calculation for time complexity and asymptotic notation Analyze recursive algorithms complexity Understand divide-and-conquer, dynamic programming and greedy approaches. Understand graphs and graph related algorithms. Understand hashing and encryption operations input and outputs. C.Course Topics \u00b6 Algorithms Basics, Pseudocode Algorithms Analysis for Time Complexity and Asymptotic Notation Sorting Problems (Insertion and Merge Sorts) Recursive Algorithms Divide-and-Conquer Analysis (Merge Sort, Binary Search) Matrix Multiplication Problem Quicksort Analysis Heaps, Heap Sort and Priority Queues Linked Lists, Radix Sort, You should have a laptop for programming practices during this course and Counting Sort. Convex Hull Dynamic Programming Greedy Algorithms Graphs and Graphs Search Algorithms Breadth-First Search Depth-First Search and Topological Sort Graph Structure Algorithms Strongly Connected Components Minimum Spanning Tree Disjoint Set Operations Single Source Shortest Path Algorithm Q-Learning Shortest Path Implementation Network Flow and Applications Hashing and Encryption D.Textbooks and Required Hardware or Equipment \u00b6 This course does not require a coursebook. If necessary, you can use the following books and open-source online resources. Paul Deitel and Harvey Deitel. 2012. C How to Program (7 th . ed.). Prentice Hall Press, USA. Intro to Java Programming, Comprehensive Version (10 th Edition) 10 th Edition by Y. Daniel Liang Introduction to Algorithms, Third Edition By Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein Problem Solving and Program Design in C, J.R. Hanly, and E.B. Koffman, 6 th Edition. Robert Sedgewick and Kevin Wayne. 2011. Algorithms (4 th . ed.). Addison-Wesley Professional. Harvey M. Deitel and Paul J. Deitel. 2001. Java How to Program (4 th . ed.). Prentice Hall PTR, USA. Paul Deitel and Harvey Deitel. 2016. Visual C# How to Program (6 th . ed.). Pearson. Additional Books TBD During this course, you should have a laptop for programming practices. You will have your development environment, and you will use this for examination and assignments also classroom practices. E.Grading System \u00b6 Midterm and Final grades will be calculated with the weighted average of the project or homework-based examinations. Midterm grades will be calculated between term beginning to the midterm week, and Final grades will be calculated between Midterm and Final week home works or projects as follows. taught Algorithms and Programming I programming skills \\[ a_n=\\text{Homework or Project Weight} \\] \\[ HW_n=\\text{Homework or Project Points} \\] \\[ n=\\text{Number of Homework or Project} \\] \\[ Grade=(a_1HW_1+a_2HW_2+...+a_nHW_n)/n \\] Homework Weight Midterm %40 Final %60 \\[ \\text{Passing Grade}=(40*Midterm_{Grade}+60*Final_{Grade})/100 \\] F. Instructional Strategies and Methods \u00b6 The basic teaching method of this course will be planned to be face-to-face in the classroom, and support resources, home works, and announcements will be shared over google classroom. Students are expected to be in the university. This responsibility is very important to complete this course with success. If pandemic situation changes and distance education is required during this course, this course will be done using synchronous and asynchronous distance education methods. In this scenario, students are expected to be in the online platform, zoom, or meet at the time specified in the course schedule. Attendance will be taken. G. Late Homework \u00b6 Throughout the semester, assignments must be submitted as specified by the announced deadline. Your grade will be reduced by 10% of the full points for each calendar day for overdue assignments. Overdue assignments will not be accepted after three (3) days. Unexpected situations must be reported to the instructor for late home works by students. H. Course Platform and Communication \u00b6 Google Classroom will be used as a course learning management system. All electronic resources and announcements about the course will be shared on this platform. It is very important to check the course page daily, access the necessary resources and announcements, and communicate with the instructor as you need Algorithms and Programming I programming skills to complete the course with success I. Academic Integrity, Plagiarism & Cheating \u00b6 Academic Integrity is one of the most important principles of RTE\u00dc University. Anyone who breaches the principles of academic honesty is severely punished. It is natural to interact with classmates and others t.\"study together\". It may also be the case where a student asks to help from someone else, paid or unpaid, better understand a difficult topic or a whole course. However, what is the borderline between \"studying together\" or \"taking private lessons\" and \"academic dishonesty\"? When is it plagiarism, when is it cheating? It is obvious that looking at another student's paper or any source other than what is allowed during the exam is cheating and will be punished. However, it is known that many students come to university with very little experience concerning what is acceptable and what counts as \"copying,\"\" especially for assignments. The following are attempted as guidelines for the Faculty of Engineering and Architecture students to highlight the philosophy of academic honesty for assignments for which the student will be graded. Should a situation arise which is not described below, the student is advised to ask the instructor or assistant of the course whether what they intend to do would remain within the framework of academic honesty or not. a. What is acceptable when preparing an assignment? \u00b6 Communicating with classmates about the assignment to understand it better Putting ideas, quotes, paragraphs, small pieces of code (snippets) that you find online or elsewhere into your assignment, provided that these are not themselves the whole solution to the assignment, you cite the origins of these Asking sources for help in guiding you for the English language content of your assignment. Sharing small pieces of your assignment in the classroom to create a class discussion on some controversial topics. Turning to the web or elsewhere for instructions, references, and solutions to technical difficulties, but not for direct answers to the assignment Discuss solutions to assignments with others using diagrams or summarized statements but not actual text or code. Working with (and even paying) a tutor to help you with the course, provided the tutor does not do your assignment for you. b. What is not acceptable? \u00b6 Ask a classmate to see their solution to a problem before submitting your own. Failing to cite the origins of any text (or code for programming courses) that you discover outside of the course's lessons and integrate into your work You are giving or showing a classmate your solution to a problem when the classmate is struggling to solve it. J. Expectations \u00b6 You are expected to attend classes on time by completing weekly course requirements (readings and assignments) during the semester. The main communication channel between the instructor and the students email emailed. Please send your questions to the instructor's email address about the course via the email address provided to you by the university. Ensure that you include the course name in the subject field of your message and your name in the text field . In addition, the instructor will contact you via email if necessary. For this reason, it is very important to check your email address every day for healthy communication. K. Lecture Content and Syllabus Updates \u00b6 If deemed necessary, changes in the lecture content or course schedule can be made. If any changes are made in the scope of this document, the instructor will inform you about this. Course Schedule Overview \u00b6 Weeks Dates Subjects Other Tasks Week 1 TBD Course Plan and Communication Grading System, Assignments and Exams. Algorithms Basics, Pseudocode,iv. RAM (Random Access Machine Model), Algorithm Cost Calculation for Time Complexity. Worst, Average and Best Case Summary Sorting Problem (Insertion and Merge Sort Analysis), 4. Asymptotic Notation(Big O, Big Teta,Big Omega, Small o, Small omega Notations) TBD Week 2 TBD Solving Recurrences (Recursion Tree, Master Method and Back-Substitution) Divide-and-Conquer Analysis (Merge Sort, Binary Search) Recurrence Solution TBD Week 3 TBD Matrix Multiplication(Traditional,Recursive,Strassen),Quicksort(Hoare and Lomuto Partitioning,Recursive Sorting),Quicksort Analysis,Randomized Quicksort, Randomized Selection(Recursive,Medians) TBD Week 4 TBD Heaps (Max / Min Heap, Heap Data Structure, Iterative and Recursive Heapify, Extract-Max, Build Heap) Heap Sort, Priority Queues, Linked Lists, Radix Sort,Counting Sort TBD Week 5 TBD Convex Hull (Divide & Conquer) Dynamic Programming (Fibonacci Numbers) Divide-and-Conquer (DAC) vs Dynamic Programming (DP) Development of a DP Algorithms Matrix-Chain Multiplication and Analysis TBD Week-6 TBD Elements of Dynamic Programming Recursive Matrix Chain Order Memoization (Top-Down Approach, RMC, MemoizedMatrixChain, LookupC) Dynamic Programming vs. Memoization Longest Common Subsequence (LCS) Most Common Dynamic Programming Interview Questions TBD Week-7 TBD Greedy Algorithms and Dynamic Programming Differences Greedy Algorithms (Activity Selection Problem, Knapsack Problems) TBD Week-8 TBD Midterm TBD Week-9 TBD Heap Data Structure, Heap Sort, Huffman Coding TBD Week-10 TBD Introduction to Graphs, Gr,aphs and Representation, BFS (Breath-First Search), DFS (Depth-First Search), Topological Order, SCC (Strongly Connected Components), MST, Prim, Kruskal TBD Week-11 TBD Disjoint Sets and Kruskal Relationships,Single-Source Shortest Path,(Bellman- Ford,Dijkstra),Q-Learning Shortest Path,Max-Flow Min-Cut (Ford-Fulkerson,Edmond\u2019s Karp,Dinic) TBD Week-12 TBD Crypto++ Library Usage, Hashing and Integrity Control, Cryptographic Hash Functions (SHA-1,SHA-256,SHA-512,H-MAC), Checksums(MD5,CRC32) TBD Week-13 TBD Symmetric Encryption Algorithms (AES, DES, TDES), Symmetric Encryption Modes (ECB, CBC), Asymmetric Encryption, Key Pairs (Public-Private Key Pairs), Signature Generation and Validation TBD Week-14 TBD OTP Calculation(Time-based, Counter-based),File Encryption and Decryption and Integrity Control Operations TBD Week-15 TBD Review TBD Week-16 TBD Final TBD Ended \u00b6","title":"Syllabus"},{"location":"syllabus/syllabus/#recep-tayyip-erdogan-university","text":"","title":"Recep Tayyip Erdogan University"},{"location":"syllabus/syllabus/#faculty-of-engineering-and-architecture","text":"","title":"Faculty of Engineering and Architecture"},{"location":"syllabus/syllabus/#computer-engineering","text":"","title":"Computer Engineering"},{"location":"syllabus/syllabus/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming-II"},{"location":"syllabus/syllabus/#syllabus","text":"","title":"Syllabus"},{"location":"syllabus/syllabus/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX Download WORD , PDF Instructor Asst. Prof. Dr. U\u011fur CORUH Contact Information ugur.coruh@erdogan.edu.tr Office No F-301 Google Classroom Code bafwwt6 Lecture Hours and Days TBD Lecture Classroom \u0130BBF 402 Level-4 Office Hours Meetings will be scheduled over Google Meet with your university account and email and performed via demand emails. Please send emails with the subject starting with [CE100] tag for the fast response and write formal, clear, and short emails Lecture and Communication Language English Theory/Laboratory Course Hour Per Week 3/2 Hours Credit 4 Prerequisite CE103- Algorithms and Programming I Corequisite TBD Requirement TBD *TBD: To Be Defined.","title":"Spring Semester, 2021-2022"},{"location":"syllabus/syllabus/#acourse-description","text":"This course continues the CE103 Algorithms and Programming I course. This course taught programming skills in Algorithms and Programming I course met. This course taught programming skills in Algorithms and Programming I with common problems and their solution algorithms. This lecture is about analyzing and understanding how algorithms work for common issues. The class will be based on expertise sharing and guiding students to find learning methods and practice for algorithm and programming topics. By making programming applications and projects in the courses, the learning process will be strengthened by practicing rather than theory.","title":"A.Course Description"},{"location":"syllabus/syllabus/#bcourse-learning-outcomes","text":"After completing this course satisfactorily, a student will be able to: Interpret a computational problem specification and algorithmic solution and implement a C/C++, Java or C# application to solve that problem. Argue the correctness of algorithms using inductive proofs and invariants. Understand algorithm design steps Argue algorithm cost calculation for time complexity and asymptotic notation Analyze recursive algorithms complexity Understand divide-and-conquer, dynamic programming and greedy approaches. Understand graphs and graph related algorithms. Understand hashing and encryption operations input and outputs.","title":"B.Course Learning Outcomes"},{"location":"syllabus/syllabus/#ccourse-topics","text":"Algorithms Basics, Pseudocode Algorithms Analysis for Time Complexity and Asymptotic Notation Sorting Problems (Insertion and Merge Sorts) Recursive Algorithms Divide-and-Conquer Analysis (Merge Sort, Binary Search) Matrix Multiplication Problem Quicksort Analysis Heaps, Heap Sort and Priority Queues Linked Lists, Radix Sort, You should have a laptop for programming practices during this course and Counting Sort. Convex Hull Dynamic Programming Greedy Algorithms Graphs and Graphs Search Algorithms Breadth-First Search Depth-First Search and Topological Sort Graph Structure Algorithms Strongly Connected Components Minimum Spanning Tree Disjoint Set Operations Single Source Shortest Path Algorithm Q-Learning Shortest Path Implementation Network Flow and Applications Hashing and Encryption","title":"C.Course Topics"},{"location":"syllabus/syllabus/#dtextbooks-and-required-hardware-or-equipment","text":"This course does not require a coursebook. If necessary, you can use the following books and open-source online resources. Paul Deitel and Harvey Deitel. 2012. C How to Program (7 th . ed.). Prentice Hall Press, USA. Intro to Java Programming, Comprehensive Version (10 th Edition) 10 th Edition by Y. Daniel Liang Introduction to Algorithms, Third Edition By Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein Problem Solving and Program Design in C, J.R. Hanly, and E.B. Koffman, 6 th Edition. Robert Sedgewick and Kevin Wayne. 2011. Algorithms (4 th . ed.). Addison-Wesley Professional. Harvey M. Deitel and Paul J. Deitel. 2001. Java How to Program (4 th . ed.). Prentice Hall PTR, USA. Paul Deitel and Harvey Deitel. 2016. Visual C# How to Program (6 th . ed.). Pearson. Additional Books TBD During this course, you should have a laptop for programming practices. You will have your development environment, and you will use this for examination and assignments also classroom practices.","title":"D.Textbooks and Required Hardware or Equipment"},{"location":"syllabus/syllabus/#egrading-system","text":"Midterm and Final grades will be calculated with the weighted average of the project or homework-based examinations. Midterm grades will be calculated between term beginning to the midterm week, and Final grades will be calculated between Midterm and Final week home works or projects as follows. taught Algorithms and Programming I programming skills \\[ a_n=\\text{Homework or Project Weight} \\] \\[ HW_n=\\text{Homework or Project Points} \\] \\[ n=\\text{Number of Homework or Project} \\] \\[ Grade=(a_1HW_1+a_2HW_2+...+a_nHW_n)/n \\] Homework Weight Midterm %40 Final %60 \\[ \\text{Passing Grade}=(40*Midterm_{Grade}+60*Final_{Grade})/100 \\]","title":"E.Grading System"},{"location":"syllabus/syllabus/#f-instructional-strategies-and-methods","text":"The basic teaching method of this course will be planned to be face-to-face in the classroom, and support resources, home works, and announcements will be shared over google classroom. Students are expected to be in the university. This responsibility is very important to complete this course with success. If pandemic situation changes and distance education is required during this course, this course will be done using synchronous and asynchronous distance education methods. In this scenario, students are expected to be in the online platform, zoom, or meet at the time specified in the course schedule. Attendance will be taken.","title":"F. Instructional Strategies and Methods"},{"location":"syllabus/syllabus/#g-late-homework","text":"Throughout the semester, assignments must be submitted as specified by the announced deadline. Your grade will be reduced by 10% of the full points for each calendar day for overdue assignments. Overdue assignments will not be accepted after three (3) days. Unexpected situations must be reported to the instructor for late home works by students.","title":"G. Late Homework"},{"location":"syllabus/syllabus/#h-course-platform-and-communication","text":"Google Classroom will be used as a course learning management system. All electronic resources and announcements about the course will be shared on this platform. It is very important to check the course page daily, access the necessary resources and announcements, and communicate with the instructor as you need Algorithms and Programming I programming skills to complete the course with success","title":"H. Course Platform and Communication"},{"location":"syllabus/syllabus/#i-academic-integrity-plagiarism-cheating","text":"Academic Integrity is one of the most important principles of RTE\u00dc University. Anyone who breaches the principles of academic honesty is severely punished. It is natural to interact with classmates and others t.\"study together\". It may also be the case where a student asks to help from someone else, paid or unpaid, better understand a difficult topic or a whole course. However, what is the borderline between \"studying together\" or \"taking private lessons\" and \"academic dishonesty\"? When is it plagiarism, when is it cheating? It is obvious that looking at another student's paper or any source other than what is allowed during the exam is cheating and will be punished. However, it is known that many students come to university with very little experience concerning what is acceptable and what counts as \"copying,\"\" especially for assignments. The following are attempted as guidelines for the Faculty of Engineering and Architecture students to highlight the philosophy of academic honesty for assignments for which the student will be graded. Should a situation arise which is not described below, the student is advised to ask the instructor or assistant of the course whether what they intend to do would remain within the framework of academic honesty or not.","title":"I. Academic Integrity, Plagiarism &amp; Cheating"},{"location":"syllabus/syllabus/#a-what-is-acceptable-when-preparing-an-assignment","text":"Communicating with classmates about the assignment to understand it better Putting ideas, quotes, paragraphs, small pieces of code (snippets) that you find online or elsewhere into your assignment, provided that these are not themselves the whole solution to the assignment, you cite the origins of these Asking sources for help in guiding you for the English language content of your assignment. Sharing small pieces of your assignment in the classroom to create a class discussion on some controversial topics. Turning to the web or elsewhere for instructions, references, and solutions to technical difficulties, but not for direct answers to the assignment Discuss solutions to assignments with others using diagrams or summarized statements but not actual text or code. Working with (and even paying) a tutor to help you with the course, provided the tutor does not do your assignment for you.","title":"a. What is acceptable when preparing an assignment?"},{"location":"syllabus/syllabus/#b-what-is-not-acceptable","text":"Ask a classmate to see their solution to a problem before submitting your own. Failing to cite the origins of any text (or code for programming courses) that you discover outside of the course's lessons and integrate into your work You are giving or showing a classmate your solution to a problem when the classmate is struggling to solve it.","title":"b. What is not acceptable?"},{"location":"syllabus/syllabus/#j-expectations","text":"You are expected to attend classes on time by completing weekly course requirements (readings and assignments) during the semester. The main communication channel between the instructor and the students email emailed. Please send your questions to the instructor's email address about the course via the email address provided to you by the university. Ensure that you include the course name in the subject field of your message and your name in the text field . In addition, the instructor will contact you via email if necessary. For this reason, it is very important to check your email address every day for healthy communication.","title":"J. Expectations"},{"location":"syllabus/syllabus/#k-lecture-content-and-syllabus-updates","text":"If deemed necessary, changes in the lecture content or course schedule can be made. If any changes are made in the scope of this document, the instructor will inform you about this.","title":"K. Lecture Content and Syllabus Updates"},{"location":"syllabus/syllabus/#course-schedule-overview","text":"Weeks Dates Subjects Other Tasks Week 1 TBD Course Plan and Communication Grading System, Assignments and Exams. Algorithms Basics, Pseudocode,iv. RAM (Random Access Machine Model), Algorithm Cost Calculation for Time Complexity. Worst, Average and Best Case Summary Sorting Problem (Insertion and Merge Sort Analysis), 4. Asymptotic Notation(Big O, Big Teta,Big Omega, Small o, Small omega Notations) TBD Week 2 TBD Solving Recurrences (Recursion Tree, Master Method and Back-Substitution) Divide-and-Conquer Analysis (Merge Sort, Binary Search) Recurrence Solution TBD Week 3 TBD Matrix Multiplication(Traditional,Recursive,Strassen),Quicksort(Hoare and Lomuto Partitioning,Recursive Sorting),Quicksort Analysis,Randomized Quicksort, Randomized Selection(Recursive,Medians) TBD Week 4 TBD Heaps (Max / Min Heap, Heap Data Structure, Iterative and Recursive Heapify, Extract-Max, Build Heap) Heap Sort, Priority Queues, Linked Lists, Radix Sort,Counting Sort TBD Week 5 TBD Convex Hull (Divide & Conquer) Dynamic Programming (Fibonacci Numbers) Divide-and-Conquer (DAC) vs Dynamic Programming (DP) Development of a DP Algorithms Matrix-Chain Multiplication and Analysis TBD Week-6 TBD Elements of Dynamic Programming Recursive Matrix Chain Order Memoization (Top-Down Approach, RMC, MemoizedMatrixChain, LookupC) Dynamic Programming vs. Memoization Longest Common Subsequence (LCS) Most Common Dynamic Programming Interview Questions TBD Week-7 TBD Greedy Algorithms and Dynamic Programming Differences Greedy Algorithms (Activity Selection Problem, Knapsack Problems) TBD Week-8 TBD Midterm TBD Week-9 TBD Heap Data Structure, Heap Sort, Huffman Coding TBD Week-10 TBD Introduction to Graphs, Gr,aphs and Representation, BFS (Breath-First Search), DFS (Depth-First Search), Topological Order, SCC (Strongly Connected Components), MST, Prim, Kruskal TBD Week-11 TBD Disjoint Sets and Kruskal Relationships,Single-Source Shortest Path,(Bellman- Ford,Dijkstra),Q-Learning Shortest Path,Max-Flow Min-Cut (Ford-Fulkerson,Edmond\u2019s Karp,Dinic) TBD Week-12 TBD Crypto++ Library Usage, Hashing and Integrity Control, Cryptographic Hash Functions (SHA-1,SHA-256,SHA-512,H-MAC), Checksums(MD5,CRC32) TBD Week-13 TBD Symmetric Encryption Algorithms (AES, DES, TDES), Symmetric Encryption Modes (ECB, CBC), Asymmetric Encryption, Key Pairs (Public-Private Key Pairs), Signature Generation and Validation TBD Week-14 TBD OTP Calculation(Time-based, Counter-based),File Encryption and Decryption and Integrity Control Operations TBD Week-15 TBD Review TBD Week-16 TBD Final TBD","title":"Course Schedule Overview"},{"location":"syllabus/syllabus/#ended","text":"","title":"Ended"},{"location":"week-1/ce100-week-1-intro/","text":"CE100 Algorithms and Programming II \u00b6 Week-1 (Introduction to Analysis of Algorithms) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Brief Description of Course and Rules \u00b6 We will first talk about, Course Plan and Communication Grading System, Homeworks, and Exams please read the syllabus carefully. Outline \u00b6 Introduction to Analysis of Algorithms Algorithm Basics Flowgorithm Pseudocode Outline \u00b6 RAM (Random Access Machine Model) Sorting Problem Insertion Sort Analysis Algorithm Cost Calculation for Time Complexity Worst, Average, and Best Case Summary Merge Sort Analysis Outline \u00b6 Asymptotic Notation Big O Notation Big Teta Notation Big Omega Notation Small o Notation Small omega Notation We Need Mathematical Proofs \u00b6 Direct proof Proof by mathematical induction Proof by contraposition Proof by contradiction Proof by construction Proof by exhaustion We Need Mathematical Proofs \u00b6 Probabilistic proof Combinatorial proof Nonconstructive proof Statistical proofs in pure mathematics Computer-assisted proofs Mathematical proof - Wikipedia Introduction to Analysis of Algorithms \u00b6 Outline \u00b6 Study two sorting algorithms as examples Insertion sort: Incremental algorithm Merge sort: Divide-and-conquer Introduction to runtime analysis Best vs. worst vs. average case Asymptotic analysis What is Algorithm \u00b6 Algorithm : A sequence of computational steps that transform the input to the desired output Procedure vs. algorithm An algorithm must halt within finite time with the right output We Need to Measure Performance Metrics Processing Time Allocated Memory Network Congestion Power Usage etc. Example Sorting Algorithms Input : a sequence of n numbers \\[ \\langle a_1,a_2,...,a_n \\rangle \\] Algorithm : Sorting / Permutation \\[ \\prod = \\langle \\prod_{(1)},\\prod_{(2)},...,\\prod_{(n)} \\rangle \\] Output : sorted permutation of the input sequence \\[ \\langle a_{\\prod_{(1)}} \\leqslant a_{\\prod_{(2)}} \\leqslant,...,a_{\\prod_{(n)}} \\rangle \\] Pseudo-code notation \u00b6 We can use Flowgorithm - Flowchart Programming Language Objective: Express algorithms to humans in a clear and concise way Liberal use of English Indentation for block structures Omission of error handling and other details (needed in real programs) Pseudocode Links to Visit \u00b6 Pseudocode - Wikipedia Pseudocode Examples How to write a Pseudo Code? - GeeksforGeeks Correctness \u00b6 We often use a loop invariant to help us to understand why an algorithm gives the correct answer. Example: (Insertion Sort) at the start of each iteration of the \"outer\" for loop - the loop indexed by \\(j\\) - the subarray \\(A[1 \\dots j-1]\\) consist of the elements originally in \\(A[1\\dots j-1]\\) but in sorted order. Correctness \u00b6 To use a loop invariant to prove correctness, we must show 3 things about it. Initialization: It is true to the first iteration of the loop. Maintaince: If it is true before an iteration of the loop, it remains true before the next iteration. Termination: When the loop terminates, the invariant - usually along with the reason that the loop terminated - gives us a usefull property that helps show that the algorithm is correct. RAM (Random Access Machine Model) \\(\\Longrightarrow \\Theta(1)\\) \u00b6 Operations Single Step Sequential No Concurrent Arithmetic add, subtract, multiply, divide, remainder, floor, ceiling, shift left/shift right (good by multiply/dividing \\(2^k\\) ) RAM (Random Access Machine Model) \\(\\Longrightarrow \\Theta(1)\\) \u00b6 Data Movement load, store, copy Control conditional / unconditional branch subroutine calls returns RAM (Random Access Machine Model) \\(\\Longrightarrow \\Theta(1)\\) \u00b6 Each instruction take a constant amount of time Integer will be represented by \\(clogn\\) \\(c \\geq 1\\) \\(T(n)\\) the running time of the algorithm: \\[ \\sum_{\\text{all statement}}(\\text{cost of statement})*(\\text{number of times statement is executed}) = T(n) \\] What is the processing time ? \u00b6 Insertion Sort \u00b6 Insertion sort is a simple sorting algorithm that works similar to the way you sort playing cards in your hands The array is virtually split into a sorted and an unsorted part Values from the unsorted part are picked and placed at the correct position in the sorted part. Assume input array : \\(A[1..n]\\) Iterate \\(j\\) from \\(2\\) to \\(n\\) Insertion Sort Algorithm \u00b6 Insertion Sort Algorithm (inline) \u00b6 Insertion - Sort ( A ) 1 . for j = 2 to A.length 2 . key = A [ j ] 3 . // insert A [ j ] into the sorted sequence A [ 1 ...j -1 ] 4 . i = j - 1 5 . while i > 0 and A [ i ] > key 6 . A [ i +1 ] = A [ i ] 7 . i = i - 1 8 . A [ i +1 ] = key Insertion Sort Step-By-Step Description (1) \u00b6 Insertion Sort Step-By-Step Description (2) \u00b6 Insertion Sort Step-By-Step Description (3) \u00b6 Insertion Sort Example \u00b6 Insertion Sort Step-1 (initial) \u00b6 Insertion Sort Step-2 (j=2) \u00b6 Insertion Sort Step-3 (j=3) \u00b6 Insertion Sort Step-4 (j=3) \u00b6 Insertion Sort Step-5 (j=4) \u00b6 Insertion Sort Step-6 (j=5) \u00b6 Insertion Sort Step-7 (j=5) \u00b6 Insertion Sort Step-8 (j=6) \u00b6 Insertion Sort Review (1) \u00b6 Items sorted in-place Elements are rearranged within the array. At a most constant number of items stored outside the array at any time (e.,g. the variable key) Input array \\(A\\) contains a sorted output sequence when the algorithm ends Insertion Sort Review (2) \u00b6 Incremental approach Having sorted \\(A[1..j-1]\\) , place \\(A[j]\\) correctly so that \\(A[1..j]\\) is sorted Running Time It depends on Input Size (5 elements or 5 billion elements) and Input Itself (partially sorted) Algorithm approach to upper bound of overall performance analysis Visualization of Insertion Sort \u00b6 Sorting (Bubble, Selection, Insertion, Merge, Quick, Counting, Radix) - VisuAlgo https://www.cs.usfca.edu/~galles/visualization/ComparisonSort.html https://algorithm-visualizer.org/ HMvHTs - Online C++ Compiler & Debugging Tool - Ideone.com Kinds of Running Time Analysis (Time Complexity) \u00b6 Worst Case (Big-O Notation) \\(T(n)\\) = maximum processing time of any input \\(n\\) Presentation of Big-O : \\(O(n)\\) Average Case (Teta Notation) \\(T(n)\\) = average time over all inputs of size \\(n\\) , inputs can have a uniform distribution Presentation of Big-Theta : \\(\\Theta(n)\\) Best Case (Omega Notation) \\(T(n)\\) = min time on any input of size \\(n\\) , for example sorted array Presentation of Big-Omega : \\(\\Omega(n)\\) Array Sorting Algorithms Time and Space Complexity \u00b6 Comparison of Time Analysis Cases \u00b6 For insertion sort, worst-case time depends on the speed of primitive operations such as Relative Speed (on the same machine) Absolute Speed (on different machines) Asymptotic Analysis Ignore machine-dependent constants Look at the growth of \\(T(n) | n\\rightarrow\\infty\\) Asymptotic Analysis (1) \u00b6 Asymptotic Analysis (2) \u00b6 Theta-Notation (Average-Case) \u00b6 Drop low order terms Ignore leading constants e.g \\[ 2n^2 + 5n + 3 = \\Theta(n^2) \\] \\[ 3n^3+90n^2-2n+5=\\Theta(n^3) \\] As \\(n\\) gets large, a \\(\\Theta(n^2)\\) algorithm runs faster than a \\(\\Theta(n^3)\\) algorithm Asymptotic Analysis (3) \u00b6 For both algorithms, we can see a minimum item size in the following chart. After this point, we can see performance differences. Some algorithms for small item size can be run faster than others but if you increase item size you will see a reference point that notation proof performance metrics. Insertion Sort - Runtime Analysis (1) \u00b6 Cost Times Insertion - Sort ( A ) ---- ----- --------------------- c1 n 1 . for j = 2 to A.length c2 n -1 2 . key = A [ j ] c3 n -1 3 . // insert A [ j ] into the sorted sequence A [ 1 ...j -1 ] c4 n -1 4 . i = j - 1 c5 k5 5 . while i > 0 and A [ i ] > key do c6 k6 6 . A [ i +1 ] = A [ i ] c7 k6 7 . i = i - 1 c8 n -1 8 . A [ i +1 ] = key we have two loops here, if we sum up costs as follow we can see big-O worst case notation. \\(k_5 = \\sum_{j=2}^n{t_j}\\) and \\(k_6 = \\sum_{j=2}^n{t_i-1}\\) for operation counts Insertion Sort - Runtime Analysis (2) \u00b6 cost function can be evaluated as follow; \\(T(n)=c_1n+c_2(n-1)+0(n-1)+c_4(n-1)+c_5\\sum_{j=2}^n{t_j}+c_6\\sum_{j=2}^n{t_i-1}+c_7\\sum_{j=2}^n{t_i-1}+c_8(n-1)\\) Insertion Sort - Runtime Analysis (3) \u00b6 \\(\\sum_{j=2}^n j = (n(n+1)/2)- 1\\) and \\(\\sum_{j=2}^n {j-1} = n(n-1)/2\\) Insertion Sort - Runtime Analysis (4) \u00b6 \\(T(n)=(c_5/2 + c_6/2 + c_7/2)n^2 + (c_1+c_2+c_4+c_5/2-c_6/2-c_7/2+c_8)n-(c_2 + c_4 + c_5 + c_6)\\) Insertion Sort - Runtime Analysis (5) \u00b6 \\(T(n)= an^2 + bn + c\\) Insertion Sort - Runtime Analysis (6) \u00b6 \\(O(n^2)\\) Best-Case Scenario (Sorted Array) (1) \u00b6 Problem-1, If \\(A[1...j]\\) is already sorted, what will be \\(t_j=?\\) \\(t_j=1\\) Best-Case Scenario (Sorted Array) (2) \u00b6 Parameters are taken from image \\(T(n)=c_1n+c_2(n-1)+c_3(n-1)+c_4\\sum_{j=2}^nt_j+c_5\\sum_{j=2}^n(t_j-1)+c_6\\sum_{j=2}^n(t_j-1)+c_7(n-1)\\) \\(t_j=1\\) for all \\(j\\) \\(T(n)=(c_1+c_2+c_3+c_4+c_7)n-(c_2+c_3+c_4+c_7)\\) \\(T(n)=an-b\\) \\(\\Omega(n)\\) Worst-Case Scenario (Reversed Array) (1) \u00b6 Problem-2 If \\(A[j]\\) is smaller than every entry in \\(A[1...j-1]\\) , what will be \\(t_j=?\\) \\(t_j=?\\) Worst-Case Scenario (Reversed Array) (2) \u00b6 The input array is reverse sorted \\(t_j=j\\) for all \\(j\\) after calculation worst case runtime will be \\(T(n)=1/2(c_4+c_5+c_6)n^2+(c_1+c_2+c_3+1/2(c_4-c_5-c_6)+c_7)n-(c_2+c_3+c_4+c_7)\\) \\(T(n)=1/2an^2+bn-c\\) \\(O(n^2)\\) Asymptotic Runtime Analysis of Insertion-Sort \u00b6 Insertion-Sort Worst-case (input reverse sorted) \u00b6 Inner Loop is \\(\\Theta(j)\\) \\(T(n)=\\sum_{j=2}^n\\Theta(j)=\\Theta(\\sum_{j=2}^nj)=\\Theta(n^2)\\) Insertion-Sort Average-case (all permutations uniformly distributed) \u00b6 Inner Loop is \\(\\Theta(j/2)\\) \\(T(n)=\\sum_{j=2}^n\\Theta(j/2)=\\sum_{j=2}^n\\Theta(j)=\\Theta(n^2)\\) Array Sorting Algorithms Time/Space Complexities \u00b6 To compare this sorting algorithm please check the following map again. Merge Sort : Divide / Conquer / Combine (1) \u00b6 Merge Sort : Divide / Conquer / Combine (2) \u00b6 Divide : we divide the problem into a number of subproblems Conquer : We solve the subproblems recursively Base-Case : Solve by Brute-Force Combine : Subproblem solutions to the original problem Merge Sort Example \u00b6 Merge Sort Algorithm (initial setup) \u00b6 Merge Sort is a recursive sorting algorithm, for initial case we need to call Merge-Sort(A,1,n) for sorting \\(A[1..n]\\) initial case A : Array p : 1 ( offset ) r : n ( length ) Merge - Sort ( A , 1 , n ) Merge Sort Algorithm (internal iterations) \u00b6 internal iterations A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif Merge Sort Algorithm (Combine-1) \u00b6 \\(p = start-point\\) \\(q = mid-point\\) \\(r = end-point\\) Merge Sort Algorithm (Combine-2) \u00b6 brute-force task, merging two sorted subarrays The pseudo-code in the textbook (Sec. 2.3.1) Merge Sort Combine Algorithm (1) \u00b6 Merge ( A , p , q , r ) n1 = q - p +1 n2 = r - q // allocate left and right arrays // increment will be from left to right // left part will be bigger than right part L [ 1 ...n1 +1 ] // left array R [ 1 ...n2 +1 ] // right array // copy left part of array for i = 1 to n1 L [ i ] = A [ p + i -1 ] // copy right part of array for j = 1 to n2 R [ j ] = A [ q + j ] // put end items maximum values for termination L [ n1 +1 ] = inf R [ n2 +1 ] = inf i = 1 , j = 1 for k = p to r if L [ i ] <= R [ j ] A [ k ] = L [ i ] i = i +1 else A [ k ] = R [ j ] j = j +1 What is the complexity of merge operation? \u00b6 You can find by counting loops will provide you base constant nested level will provide you exponent of this constant, if you drop constants you will have complexity we have 3 for loops it will look like \\(3n\\) and \\(\\Theta(n)\\) will be merge complexity Merge Sort Correctness \u00b6 Base case \\(p = r\\) (Trivially correct) Inductive hypothesis MERGE-SORT is correct for any subarray that is a strict (smaller) subset of \\(A[p, q]\\) . General Case MERGE-SORT is correct for \\(A[p, q]\\) . From inductive hypothesis and correctness of Merge. Merge Sort Algorithm (Pseudo-Code) \u00b6 A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif Merge Sort Algorithm Complexity \u00b6 A : Array p : offset r : length Merge - Sort ( A , p , r ) -------------> T ( n ) if p = r then ---------------> Theta ( 1 ) return else q = floor (( p + r ) / 2 ) ----> Theta ( 1 ) Merge - Sort ( A , p , q ) -----> T ( n / 2 ) Merge - Sort ( A , q +1 , r ) ---> T ( n / 2 ) Merge ( A , p , q , r ) --------> Theta ( n ) endif Merge Sort Algorithm Recurrence \u00b6 We can describe a function recursively in terms of itself, to analyze the performance of recursive algorithms \\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\] How To Solve Recurrence (1) \u00b6 \\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\] How To Solve Recurrence (2) \u00b6 We will assume \\(T(n)= \\Theta(1)\\) for sufficiently small \\(n\\) to rewrite equation as \\[ T(n)=2T(n/2)+\\Theta(n) \\] Solution for this equation will be \\(\\Theta(nlgn)\\) with following recursion tree. How To Solve Recurrence (3) \u00b6 Multiply by height \\(\\Theta(lgn)\\) with each level cost \\(\\Theta(n)\\) we can found \\(\\Theta(nlgn)\\) How To Solve Recurrence (4) \u00b6 This tree is binary-tree and binary-tree height is related with item size. How Height of a Binary Tree is Equal to \\(logn\\) ? (1) \u00b6 Merge-Sort recursion tree is a perfect binary tree, a binary tree is a tree which every node has at most two children, A perfect binary tree is binary tree in which all internal nodes have exactly two children and all leaves are at the same level. How Height of a Binary Tree is Equal to \\(logn\\) ? (2) \u00b6 Let \\(n\\) be the number of nodes in the tree and let \\(l_k\\) denote the number of nodes on level k. According to this; \\(l_k = 2l_{k-1}\\) i.e. each level has exactly twice as many nodes as the previous level \\(l_0 = 1\\) , i.e. on the first level we have only one node (the root node) The leaves are at the last level, \\(l_h\\) where \\(h\\) is the height of the tree. How Height of a Binary Tree is Equal to \\(logn\\) ? (3) \u00b6 The total number of nodes in the tree is equal to the sum of the nodes on all the levels: nodes \\(n\\) \\[ 1+2^1+2^2+2^3+...+2^h=n \\] \\[ 1+2^1+2^2+2^3+...+2^h=2^{h+1}-1 \\] \\[ 2^{h+1}-1=n \\] \\[ 2^{h+1}=n+1 \\] \\[ log_2{2^{h+1}} = log_2{(n+1)} \\] \\[ h+1 = log_2{(n+1)} \\] \\[ h = log_2{(n+1)}-1 \\] How Height of a Binary Tree is Equal to \\(logn\\) ? (3) \u00b6 If we write it as asymptotic approach, we will have the following result \\[ \\text{height of tree is }h = log_2{(n+1)}-1 = O(logn) \\] also \\[ \\text{number of leaves is } l_h = (n+1)/2 \\] nearly half of the nodes are at the leaves Review \u00b6 \\(\\Theta(nlgn)\\) grows more slowly than \\(\\Theta(n^2)\\) Therefore Merge-Sort beats Insertion-Sort in the worst case In practice Merge-Sort beats Insertion-Sort for \\(n>30\\) or so Asymptotic Notations \u00b6 Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (1) \u00b6 \\(f(n)=O(g(n))\\) if \\(\\exists\\) positive constants \\(c\\) , \\(n_0\\) such that \\[ 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\] Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (2) \u00b6 Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (3) \u00b6 Asymptotic running times of algorithms are usually defined by functions whose domain are \\(N={0, 1, 2, \u2026}\\) (natural numbers) Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (4) \u00b6 Example-1 \u00b6 Show that \\(2n^2 = O(n^3)\\) we need to find two positive constant \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq 2n^2 \\leq cn^3 \\text{ for all } n \\geq n_0 \\] Choose \\(c=2\\) and \\(n_0 = 1\\) \\[ 2n^2 \\leq 2n^3 \\text{ for all } n \\geq 1 \\] Or, choose \\(c=1\\) and \\(n_0=2\\) \\[ 2n^2 \\leq n^3 \\text{ for all } n \\geq 2 \\] Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (5) \u00b6 Example-2 \u00b6 Show that \\(2n^2 + n = O(n^2)\\) We need to find two positive constant \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq {2n^2+n} \\leq cn^2 \\text{ for all } n \\geq n_0 \\] \\[ 2 + (1/n) \\leq c \\text{ for all } n \\geq n_0 \\] Choose \\(c=3\\) and \\(n_0=1\\) \\[ 2n^2 + n \\leq 3n^2 \\text{ for all } n \\geq 1 \\] Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (6) \u00b6 We can say the followings about \\(f(n)=O(g(n))\\) equation The notation is a little sloppy One-way equation, e.q. \\(n^2 = O(n^3)\\) but we cannot say \\(O(n^3)=n^2\\) Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (7) \u00b6 \\(O(g(n))\\) is in fact a set of functions as follow \\(O(g(n)) = \\{ f(n) : \\exists \\text{ positive constant } c, n_0 \\text{ such that } 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\}\\) Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (8) \u00b6 In other words \\(O(g(n))\\) is in fact, the set of functions that have asymptotic upper bound \\(g(n)\\) e.q \\(2n^2 = O(n^3)\\) means \\(2n^2 \\in O(n^3)\\) Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (9) \u00b6 Example-1 \u00b6 \\(10^9n^2 = O(n^2)\\) \\(0 \\leq 10^9n^2 \\leq cn^2 \\text{ for } n \\geq n_0\\) choose \\(c=10^9\\) and \\(n_0=1\\) \\(0 \\leq 10^9n^2 \\leq 10^9n^2 \\text{ for } n \\geq 1\\) CORRECT Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (10) \u00b6 Example-2 \u00b6 \\(100n^{1.9999}=O(n^2)\\) \\(0 \\leq 100n^{1.9999} \\leq cn^2 \\text{ for } n \\geq n_0\\) choose \\(c=100\\) and \\(n_0=1\\) \\(0 \\leq 100n^{1.9999} \\leq 100n^2 \\text{ for } n \\geq 1\\) CORRECT Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (11) \u00b6 Example-3 \u00b6 \\(10^{-9}n^{2.0001} = O(n^2)\\) \\(0 \\leq 10^{-9}n^{2.0001} \\leq cn^2 \\text{ for } n \\geq n_0\\) \\(10^{-9}n^{0.0001} \\leq c \\text{ for } n \\geq n_0\\) INCORRECT (Contradiction) Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (12) \u00b6 If we analysis \\(O(n^2)\\) case, \\(O\\) -notation is an upper bound notation and the runtime \\(T(n)\\) of algorithm A is at least \\(O(n^2 )\\) . \\(O(n^2)\\) : The set of functions with asymptotic upper bound \\(n^2\\) \\(T(n) \\geq O(n^2)\\) means \\(T(n) \\geq h(n)\\) for some \\(h(n) \\in O(n^2)\\) \\(h(n)=0\\) function is also in \\(O(n^2)\\) . Hence : \\(T(n) \\geq 0\\) , runtime must be nonnegative. Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (1) \u00b6 \\(f(n)=\\Omega(g(n))\\) if \\(\\exists\\) positive constants \\(c,n_0\\) such that \\(0 \\leq cg(n) \\leq f(n) , \\forall n \\geq n_0\\) Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (2) \u00b6 Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (3) \u00b6 Example-1 \u00b6 Show that \\(2n^3 = \\Omega(n^2)\\) We need to find two positive constants \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq cn^2 \\leq 2n^3 \\text{ for all } n \\geq n_0 \\] Choose \\(c=1\\) and \\(n_0=1\\) \\[ n^2 \\leq 2n^3 \\text{ for all } n \\geq 1 \\] Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (4) \u00b6 Example-4 \u00b6 Show that \\(\\sqrt{n}=\\Omega(lgn)\\) We need to find two positive constants \\(c\\) and \\(n_0\\) such that: \\[ clgn \\leq \\sqrt{n} \\text{ for all } n \\geq n_0 \\] Choose \\(c=1\\) and \\(n_0=16\\) \\[ lgn \\leq \\sqrt{n} \\text{ for all } n \\geq 16 \\] Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (5) \u00b6 \\(\\Omega(g(n))\\) is the set of functions that have asymptotic lower bound \\(g(n)\\) \\[ \\Omega(g(n))=\\{ f(n):\\exists \\text{ positive constants } c,n_0 \\text{ such that } 0 \\leq cg(n) \\leq f(n), \\forall n \\geq n_0 \\} \\] Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (6) \u00b6 Example-1 \u00b6 \\(10^9n^2 = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 10^9n^2 \\text{ for } n\\geq n_0\\) Choose \\(c=10^9\\) and \\(n_0=1\\) \\(0 \\leq 10^9n^2 \\leq 10^9n^2 \\text{ for } n\\geq 1\\) CORRECT Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (7) \u00b6 Example-2 \u00b6 \\(100n^{1.9999} = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 100n^{1.9999} \\text{ for } n \\geq n_0\\) \\(n^{0.0001} \\leq (100/c) \\text{ for } n \\geq n_0\\) INCORRECT (Contradiction) Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (8) \u00b6 Example-3 \u00b6 \\(10^{-9}n^{2.0001} = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 10^{-9}n^{2.0001} \\text{ for } n \\geq n_0\\) Choose \\(c=10^{-9}\\) and \\(n_0=1\\) \\(0 \\leq 10^{-9}n^2 \\leq 10^{-9}n^{2.0001} \\text{ for } n \\geq 1\\) CORRECT Comparison of Notations (1) \u00b6 Comparison of Notations (2) \u00b6 Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (1) \u00b6 \\(f(n)=\\Theta(g(n))\\) if \\(\\exists\\) positive constants \\(c_1,c_2,n_0\\) such that \\(0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n), \\forall n \\geq n_0\\) Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (2) \u00b6 Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (3) \u00b6 Example-1 \u00b6 Show that \\(2n^2 + n = \\Theta(n^2)\\) We need to find 3 positive constants \\(c_1,c_2\\) and \\(n_0\\) such that: \\(0 \\leq c_1n^2 \\leq 2n^2+n \\leq c_2n^2\\) for all \\(n \\geq n_0\\) \\(c_1 \\leq 2 + (1/n) \\leq c_2\\) for all \\(n \\geq n_0\\) Choose \\(c_1=2, c_2=3\\) and \\(n_0=1\\) \\(2n^2 \\leq 2n^2+n \\leq 3n^2\\) for all \\(n \\geq 1\\) Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (4) \u00b6 Example-2.1 \u00b6 Show that \\(1/2n^2-2n=\\Theta(n^2)\\) We need to find 3 positive constants \\(c_1,c_2\\) and \\(n_0\\) such that: \\(0 \\leq c_1n^2 \\leq 1/2n^2-2n \\leq c_2n^2 \\text{ for all } n \\geq n_0\\) \\(c_1 \\leq 1/2 - 2 / n \\leq c_2 \\text{ for all } n \\geq n_0\\) Choose 3 positive constants \\(c_1,c_2, n_0\\) that satisfy \\(c_1 \\leq 1/2 - 2/n \\leq c_2\\) for all \\(n \\geq n_0\\) Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (5) \u00b6 Example-2.2 \u00b6 Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (6) \u00b6 Example-2.3 \u00b6 \\[ 1/10 \\leq 1/2 - 2/n \\text{ for } n \\geq 5 \\] \\[ 1/2 - 2/n \\leq 1/2 \\text{ for } n \\geq 0 \\] Therefore we can choose \\(c_1 = 1/10, c_2=1/2, n_0=5\\) Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (7) \u00b6 Theorem : leading constants & low-order terms don\u2019t matter Justification : can choose the leading constant large enough to make high-order term dominate other terms Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (8) \u00b6 Example-1 \u00b6 \\(10^9n^2 = \\Theta(n^2)\\) CORRECT \\(100n^{1.9999} = \\Theta(n^2)\\) INCORRECT \\(10^9n^{2.0001} = \\Theta(n^2)\\) INCORRECT Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (9) \u00b6 \\(\\Theta(g(n))\\) is the set of functions that have asymptotically tight bound \\(g(n)\\) \\(\\Theta(g(n))=\\{ f(n): \\exists \\text{ positive constants } c_1,c_2, n_0 \\text{ such that } 0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n), \\forall n \\geq n_0 \\}\\) Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (10) \u00b6 Theorem : \\(f(n)=\\Theta(g(n))\\) if and only if \\(f(n)=O(g(n))\\) and \\(f(n)=\\Omega(g(n))\\) \\(\\Theta\\) is stronger than both \\(O\\) and \\(\\Omega\\) \\(\\Theta(g(n)) \\subseteq O(g(n)) \\text{ and } \\Theta(g(n)) \\subseteq \\Omega(g(n))\\) Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (11) \u00b6 Example-1.1 \u00b6 Prove that \\(10^{-8}n^2 \\neq \\Theta(n)\\) We can check that \\(10^{-8}n^2 = \\Omega(n)\\) and \\(10^{-8}n^2 \\neq O(n)\\) Proof by contradiction for \\(O(n)\\) notation \\[ O(g(n)) = \\{ f(n) : \\exists \\text{ positive constant } c, n_0 \\text{ such that } 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\} \\] Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (12) \u00b6 Example-1.2 \u00b6 Suppose positive constants \\(c_2\\) and \\(n_0\\) exist such that: \\(10^{-8}n^2 \\leq c_2n, \\forall n \\geq n_0\\) \\(10^{-8}n \\leq c_2, \\forall n \\geq n_0\\) Contradiction : \\(c_2\\) is a constant Summary of \\(O,\\Omega\\) and \\(\\Theta\\) notations (1) \u00b6 \\(O(g(n))\\) : The set of functions with asymptotic upper bound \\(g(n)\\) \\(\\Omega(g(n))\\) : The set of functions with asymptotic lower bound \\(g(n)\\) \\(\\Theta(n)\\) : The set of functions with asymptotically tight bound \\(g(n)\\) \\(f(n)=\\Theta(g(n)) \\Leftrightarrow f(n)=O(g(n)) \\text{ and } f(n)=\\Omega(g(n))\\) Summary of \\(O,\\Omega\\) and \\(\\Theta\\) notations (2) \u00b6 Small-o / \\(o\\) -Notation : Asymptotic upper bound that is not tight (1) \u00b6 Remember, upper bound provided by big- \\(O\\) notation can be tight or not tight Tight mean values are close the original function e.g. followings are true \\(2n^2 = O(n^2)\\) is asymptotically tight \\(2n = O(n^2)\\) is not asymptotically tight According to this small- \\(o\\) notation is an upper bound that is not asymptotically tight Small-o / \\(o\\) -Notation : Asymptotic upper bound that is not tight (2) \u00b6 Note that in equations equality is removed in small notations \\(o(g(n))=\\{ f(n): \\text{ for any constant} c > 0, \\exists \\text{ a constant } n_0 > 0, \\text{ such that } 0 \\leq f(n) < cg(n), \\forall n \\geq n_0 \\}\\) \\[ \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0 \\] e.g \\(2n=o(n^2)\\) any positive \\(c\\) satisfies but \\(2n^2 \\neq o(n^2)\\) \\(c=2\\) does not satisfy Small-omega / \\(\\omega\\) -Notation: Asymptotic lower bound that is not tight (1) \u00b6 \\(\\omega(g(n))=\\{ f(n): \\text{ for any constant } c > 0, \\exists \\text{ a constant } n_0>0, \\text{ such that } 0 \\leq cg(n) < f(n), \\forall n \\geq n_0\\) \\[ \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = \\infty \\] e.g. \\(n^2/2=\\omega(n)\\) , any positive \\(c\\) satisfies but \\(n^2/2 \\neq \\omega(n^2)\\) , \\(c=1/2\\) does not satisfy (Important) Analogy to compare of two real numbers \u00b6 \\[ f(n)= O(g(n)) \\leftrightarrow a \\leq b \\] \\[ f(n)= \\Omega(g(n)) \\leftrightarrow a \\geq b \\] \\[ f(n)= \\Theta(g(n)) \\leftrightarrow a = b \\] \\[ f(n)= o(g(n)) \\leftrightarrow a < b \\] \\[ f(n)= \\omega(g(n)) \\leftrightarrow a > b \\] (Important) Analogy to compare of two real numbers \u00b6 \\(O \\approx \\leq\\) \\(\\Theta \\approx =\\) \\(\\Omega \\approx \\geq\\) \\(\\omega \\approx >\\) \\(o \\approx <\\) (Important) Trichotomy property for real numbers \u00b6 For any two real numbers \\(a\\) and \\(b\\) , we have either \\(a<b\\) , or \\(a=b\\) , or \\(a>b\\) Trichotomy property does not hold for asymptotic notation, for two functions \\(f(n)\\) and \\(g(n)\\) , it may be the case that neither \\(f(n)=O(g(n))\\) nor \\(f(n)=\\Omega(g(n))\\) holds. e.g. \\(n\\) and \\(n^{1+sin(n)}\\) cannot be compared asymptotically Examples \u00b6 \\(5n^2=O(n^2)\\) TRUE \\(n^2lgn = O(n^2)\\) FALSE \\(5n^2=\\Omega(n^2)\\) TRUE \\(n^2lgn = \\Omega(n^2)\\) TRUE \\(5n^2=\\Theta(n^2)\\) TRUE \\(n^2lgn = \\Theta(n^2)\\) FALSE \\(5n^2=o(n^2)\\) FALSE \\(n^2lgn = o(n^2)\\) FALSE \\(5n^2=\\omega(n^2)\\) FALSE \\(n^2lgn = \\omega(n^2)\\) TRUE \\(2^n = O(3^n)\\) TRUE \\(2^n = \\Omega(3^n)\\) FALSE \\(2^n=o(3^n)\\) TRUE \\(2^n = \\Theta(3^n)\\) FALSE \\(2^n = \\omega(3^n)\\) FALSE Asymptotic Function Properties \u00b6 Transitivity : holds for all e.g. \\(f(n) = \\Theta(g(n)) \\& g(n)=\\Theta(h(n)) \\Rightarrow f(n)=\\Theta(h(n))\\) Reflexivity : holds for \\(\\Theta,O,\\Omega\\) e.g. \\(f(n)=O(f(n))\\) Symmetry : hold only for \\(\\Theta\\) e.g. \\(f(n)=\\Theta(g(n)) \\Leftrightarrow g(n)=\\Theta(f(n))\\) Transpose Symmetry : holds for \\((O \\leftrightarrow \\Omega)\\) and \\((o \\leftrightarrow \\omega)\\) e.g. \\(f(n)=O(g(n))\\Leftrightarrow g(n)=\\Omega(f(n))\\) Using \\(O\\) -Notation to Describe Running Times (1) \u00b6 Used to bound worst-case running times, Implies an upper bound runtime for arbitrary inputs as well Example: Insertion sort has worst-case runtime of \\(O(n^2 )\\) Note: This \\(O(n^2)\\) upper bound also applies to its running time on every input Abuse to say \u201crunning time of insertion sort is \\(O(n^2)\\) \" For a given \\(n\\) , the actual running time depends on the particular input of size \\(n\\) i.e., running time is not only a function of \\(n\\) However, worst-case running time is only a function of \\(n\\) Using \\(O\\) -Notation to Describe Running Times (2) \u00b6 When we say: Running time of insertion sort is \\(O(n^2)\\) What we really mean is Worst-case running time of insertion sort is \\(O(n^2)\\) or equivalently No matter what particular input of size n is chosen, the running time on that set of inputs is \\(O(n^2)\\) Using \\(\\Omega\\) -Notation to Describe Running Times (1) \u00b6 Used to bound best-case running times, Implies a lower bound runtime for arbitrary inputs as well Example: Insertion sort has best-case runtime of \\(\\Omega(n)\\) Note : This \\(\\Omega(n)\\) lower bound also applies to its running time on every input Using \\(\\Omega\\) -Notation to Describe Running Times (2) \u00b6 When we say Running time of algorithm A is \\(\\Omega(g(n))\\) What we mean is For any input of size \\(n\\) , the runtime of A is at least a constant times \\(g(n)\\) for sufficiently large \\(n\\) It\u2019s not contradictory to say worst-case running time of insertion sort is \\(\\Omega(n^2)\\) Because there exists an input that causes the algorithm to take \\(\\Omega(n^2)\\) Using \\(\\Theta\\) -Notation to Describe Running Times (1) \u00b6 Consider 2 cases about the runtime of an algorithm Case 1: Worst-case and best-case not asymptotically equal Use \\(\\Theta\\) -notation to bound worst-case and best-case runtimes separately Case 2: Worst-case and best-case asymptotically equal Use \\(\\Theta\\) -notation to bound the runtime for any input Using \\(\\Theta\\) -Notation to Describe Running Times (2) \u00b6 Case 1: Worst-case and best-case not asymptotically equal Use \\(\\Theta\\) -notation to bound the worst-case and best-case runtimes separately We can say: \"The worst-case runtime of insertion sort is \\(\\Theta(n^2)\\) \" \"The best-case runtime of insertion sort is \\(\\Theta(n)\\) \" But, we can\u2019t say: \"The runtime of insertion sort is \\(\\Theta(n^2)\\) for every input\" A \\(\\Theta\\) -bound on worst/best-case running time does not apply to its running time on arbitrary inputs Worst-Case and Best-Case Equation for Merge-Sort \u00b6 e.g. for merge-sort, we have: \\[ T(n)=\\Theta(nlgn)\\begin{cases} T(n)=O(nlgn)\\\\ T(n)=\\Omega(nlgn)\\end{cases} \\] Using Asymptotic Notation to Describe Runtimes Summary (1) \u00b6 \"The worst case runtime of Insertion Sort is \\(O(n^2)\\) \" Also implies: \"The runtime of Insertion Sort is \\(O(n^2)\\) \" \"The best-case runtime of Insertion Sort is \\(\\Omega(n)\\) \" Also implies: \"The runtime of Insertion Sort is \\(\\Omega(n)\\) \" Using Asymptotic Notation to Describe Runtimes Summary (2) \u00b6 \"The worst case runtime of Insertion Sort is \\(\\Theta(n^2)\\) \" But: \"The runtime of Insertion Sort is not \\(\\Theta(n^2)\\) \" \"The best case runtime of Insertion Sort is \\(\\Theta(n)\\) \" But: \"The runtime of Insertion Sort is not \\(\\Theta(n)\\) \" Using Asymptotic Notation to Describe Runtimes Summary (3) \u00b6 Which one is true? \u00b6 FALSE \"The worst case runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" FALSE \"The best case runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" TRUE \"The runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" This is true, because the best and worst case runtimes have asymptotically the same tight bound \\(\\Theta(nlgn)\\) Asymptotic Notation in Equations (RHS) \u00b6 Asymptotic notation appears alone on the RHS of an equation: implies set membership e.g., \\(n = O(n^2)\\) means \\(n \\in O(n^2)\\) Asymptotic notation appears on the RHS of an equation stands for some anonymous function in the set e.g., \\(2n^2 + 3n + 1 = 2n^2 + \\Theta(n)\\) means: \\(2n^2 + 3n + 1 = 2n^2 + h(n)\\) , for some \\(h(n) \\in \\Theta(n)\\) i.e., \\(h(n) = 3n + 1\\) Asymptotic Notation in Equations (LHS) \u00b6 Asymptotic notation appears on the LHS of an equation: stands for any anonymous function in the set e.g., \\(2n^2 + \\Theta(n) = \\Theta(n^2)\\) means: for any function \\(g(n) \\in \\Theta(n)\\) \\(\\exists\\) some function \\(h(n)\\in \\Theta(n^2)\\) such that \\(2n^2+g(n) = h(n)\\) RHS provides coarser level of detail than LHS References (TODO: Update Missing References) \u00b6 Introduction to Algorithms, Third Edition | The MIT Press http://nabil.abubaker.bilkent.edu.tr/473/ Insertion Sort - GeeksforGeeks http://www.cs.gettysburg.edu/~ilinkin/courses/Fall-2012/cs216/notes/bintree.pdf Dictionary of Algorithms and Data Structures big-O notation Omega","title":"Week-1 (Intro, Asymptotic Notations)"},{"location":"week-1/ce100-week-1-intro/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-1/ce100-week-1-intro/#week-1-introduction-to-analysis-of-algorithms","text":"","title":"Week-1 (Introduction to Analysis of Algorithms)"},{"location":"week-1/ce100-week-1-intro/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-1/ce100-week-1-intro/#brief-description-of-course-and-rules","text":"We will first talk about, Course Plan and Communication Grading System, Homeworks, and Exams please read the syllabus carefully.","title":"Brief Description of Course and Rules"},{"location":"week-1/ce100-week-1-intro/#outline","text":"Introduction to Analysis of Algorithms Algorithm Basics Flowgorithm Pseudocode","title":"Outline"},{"location":"week-1/ce100-week-1-intro/#outline_1","text":"RAM (Random Access Machine Model) Sorting Problem Insertion Sort Analysis Algorithm Cost Calculation for Time Complexity Worst, Average, and Best Case Summary Merge Sort Analysis","title":"Outline"},{"location":"week-1/ce100-week-1-intro/#outline_2","text":"Asymptotic Notation Big O Notation Big Teta Notation Big Omega Notation Small o Notation Small omega Notation","title":"Outline"},{"location":"week-1/ce100-week-1-intro/#we-need-mathematical-proofs","text":"Direct proof Proof by mathematical induction Proof by contraposition Proof by contradiction Proof by construction Proof by exhaustion","title":"We Need Mathematical Proofs"},{"location":"week-1/ce100-week-1-intro/#we-need-mathematical-proofs_1","text":"Probabilistic proof Combinatorial proof Nonconstructive proof Statistical proofs in pure mathematics Computer-assisted proofs Mathematical proof - Wikipedia","title":"We Need Mathematical Proofs"},{"location":"week-1/ce100-week-1-intro/#introduction-to-analysis-of-algorithms","text":"","title":"Introduction to Analysis of Algorithms"},{"location":"week-1/ce100-week-1-intro/#outline_3","text":"Study two sorting algorithms as examples Insertion sort: Incremental algorithm Merge sort: Divide-and-conquer Introduction to runtime analysis Best vs. worst vs. average case Asymptotic analysis","title":"Outline"},{"location":"week-1/ce100-week-1-intro/#what-is-algorithm","text":"Algorithm : A sequence of computational steps that transform the input to the desired output Procedure vs. algorithm An algorithm must halt within finite time with the right output We Need to Measure Performance Metrics Processing Time Allocated Memory Network Congestion Power Usage etc. Example Sorting Algorithms Input : a sequence of n numbers \\[ \\langle a_1,a_2,...,a_n \\rangle \\] Algorithm : Sorting / Permutation \\[ \\prod = \\langle \\prod_{(1)},\\prod_{(2)},...,\\prod_{(n)} \\rangle \\] Output : sorted permutation of the input sequence \\[ \\langle a_{\\prod_{(1)}} \\leqslant a_{\\prod_{(2)}} \\leqslant,...,a_{\\prod_{(n)}} \\rangle \\]","title":"What is Algorithm"},{"location":"week-1/ce100-week-1-intro/#pseudo-code-notation","text":"We can use Flowgorithm - Flowchart Programming Language Objective: Express algorithms to humans in a clear and concise way Liberal use of English Indentation for block structures Omission of error handling and other details (needed in real programs)","title":"Pseudo-code notation"},{"location":"week-1/ce100-week-1-intro/#pseudocode-links-to-visit","text":"Pseudocode - Wikipedia Pseudocode Examples How to write a Pseudo Code? - GeeksforGeeks","title":"Pseudocode Links to Visit"},{"location":"week-1/ce100-week-1-intro/#correctness","text":"We often use a loop invariant to help us to understand why an algorithm gives the correct answer. Example: (Insertion Sort) at the start of each iteration of the \"outer\" for loop - the loop indexed by \\(j\\) - the subarray \\(A[1 \\dots j-1]\\) consist of the elements originally in \\(A[1\\dots j-1]\\) but in sorted order.","title":"Correctness"},{"location":"week-1/ce100-week-1-intro/#correctness_1","text":"To use a loop invariant to prove correctness, we must show 3 things about it. Initialization: It is true to the first iteration of the loop. Maintaince: If it is true before an iteration of the loop, it remains true before the next iteration. Termination: When the loop terminates, the invariant - usually along with the reason that the loop terminated - gives us a usefull property that helps show that the algorithm is correct.","title":"Correctness"},{"location":"week-1/ce100-week-1-intro/#ram-random-access-machine-model-longrightarrow-theta1","text":"Operations Single Step Sequential No Concurrent Arithmetic add, subtract, multiply, divide, remainder, floor, ceiling, shift left/shift right (good by multiply/dividing \\(2^k\\) )","title":"RAM (Random Access Machine Model)  \\(\\Longrightarrow \\Theta(1)\\)"},{"location":"week-1/ce100-week-1-intro/#ram-random-access-machine-model-longrightarrow-theta1_1","text":"Data Movement load, store, copy Control conditional / unconditional branch subroutine calls returns","title":"RAM (Random Access Machine Model)  \\(\\Longrightarrow \\Theta(1)\\)"},{"location":"week-1/ce100-week-1-intro/#ram-random-access-machine-model-longrightarrow-theta1_2","text":"Each instruction take a constant amount of time Integer will be represented by \\(clogn\\) \\(c \\geq 1\\) \\(T(n)\\) the running time of the algorithm: \\[ \\sum_{\\text{all statement}}(\\text{cost of statement})*(\\text{number of times statement is executed}) = T(n) \\]","title":"RAM (Random Access Machine Model)  \\(\\Longrightarrow \\Theta(1)\\)"},{"location":"week-1/ce100-week-1-intro/#what-is-the-processing-time","text":"","title":"What is the processing time ?"},{"location":"week-1/ce100-week-1-intro/#insertion-sort","text":"Insertion sort is a simple sorting algorithm that works similar to the way you sort playing cards in your hands The array is virtually split into a sorted and an unsorted part Values from the unsorted part are picked and placed at the correct position in the sorted part. Assume input array : \\(A[1..n]\\) Iterate \\(j\\) from \\(2\\) to \\(n\\)","title":"Insertion Sort"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-algorithm","text":"","title":"Insertion Sort Algorithm"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-algorithm-inline","text":"Insertion - Sort ( A ) 1 . for j = 2 to A.length 2 . key = A [ j ] 3 . // insert A [ j ] into the sorted sequence A [ 1 ...j -1 ] 4 . i = j - 1 5 . while i > 0 and A [ i ] > key 6 . A [ i +1 ] = A [ i ] 7 . i = i - 1 8 . A [ i +1 ] = key","title":"Insertion Sort Algorithm (inline)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-by-step-description-1","text":"","title":"Insertion Sort Step-By-Step Description (1)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-by-step-description-2","text":"","title":"Insertion Sort Step-By-Step Description (2)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-by-step-description-3","text":"","title":"Insertion Sort Step-By-Step Description (3)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-example","text":"","title":"Insertion Sort Example"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-1-initial","text":"","title":"Insertion Sort Step-1 (initial)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-2-j2","text":"","title":"Insertion Sort Step-2 (j=2)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-3-j3","text":"","title":"Insertion Sort Step-3 (j=3)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-4-j3","text":"","title":"Insertion Sort Step-4 (j=3)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-5-j4","text":"","title":"Insertion Sort Step-5 (j=4)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-6-j5","text":"","title":"Insertion Sort Step-6 (j=5)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-7-j5","text":"","title":"Insertion Sort Step-7 (j=5)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-8-j6","text":"","title":"Insertion Sort Step-8 (j=6)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-review-1","text":"Items sorted in-place Elements are rearranged within the array. At a most constant number of items stored outside the array at any time (e.,g. the variable key) Input array \\(A\\) contains a sorted output sequence when the algorithm ends","title":"Insertion Sort Review (1)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-review-2","text":"Incremental approach Having sorted \\(A[1..j-1]\\) , place \\(A[j]\\) correctly so that \\(A[1..j]\\) is sorted Running Time It depends on Input Size (5 elements or 5 billion elements) and Input Itself (partially sorted) Algorithm approach to upper bound of overall performance analysis","title":"Insertion Sort Review (2)"},{"location":"week-1/ce100-week-1-intro/#visualization-of-insertion-sort","text":"Sorting (Bubble, Selection, Insertion, Merge, Quick, Counting, Radix) - VisuAlgo https://www.cs.usfca.edu/~galles/visualization/ComparisonSort.html https://algorithm-visualizer.org/ HMvHTs - Online C++ Compiler & Debugging Tool - Ideone.com","title":"Visualization of Insertion Sort"},{"location":"week-1/ce100-week-1-intro/#kinds-of-running-time-analysis-time-complexity","text":"Worst Case (Big-O Notation) \\(T(n)\\) = maximum processing time of any input \\(n\\) Presentation of Big-O : \\(O(n)\\) Average Case (Teta Notation) \\(T(n)\\) = average time over all inputs of size \\(n\\) , inputs can have a uniform distribution Presentation of Big-Theta : \\(\\Theta(n)\\) Best Case (Omega Notation) \\(T(n)\\) = min time on any input of size \\(n\\) , for example sorted array Presentation of Big-Omega : \\(\\Omega(n)\\)","title":"Kinds of Running Time Analysis (Time Complexity)"},{"location":"week-1/ce100-week-1-intro/#array-sorting-algorithms-time-and-space-complexity","text":"","title":"Array Sorting Algorithms Time and Space Complexity"},{"location":"week-1/ce100-week-1-intro/#comparison-of-time-analysis-cases","text":"For insertion sort, worst-case time depends on the speed of primitive operations such as Relative Speed (on the same machine) Absolute Speed (on different machines) Asymptotic Analysis Ignore machine-dependent constants Look at the growth of \\(T(n) | n\\rightarrow\\infty\\)","title":"Comparison of Time Analysis Cases"},{"location":"week-1/ce100-week-1-intro/#asymptotic-analysis-1","text":"","title":"Asymptotic Analysis (1)"},{"location":"week-1/ce100-week-1-intro/#asymptotic-analysis-2","text":"","title":"Asymptotic Analysis (2)"},{"location":"week-1/ce100-week-1-intro/#theta-notation-average-case","text":"Drop low order terms Ignore leading constants e.g \\[ 2n^2 + 5n + 3 = \\Theta(n^2) \\] \\[ 3n^3+90n^2-2n+5=\\Theta(n^3) \\] As \\(n\\) gets large, a \\(\\Theta(n^2)\\) algorithm runs faster than a \\(\\Theta(n^3)\\) algorithm","title":"Theta-Notation (Average-Case)"},{"location":"week-1/ce100-week-1-intro/#asymptotic-analysis-3","text":"For both algorithms, we can see a minimum item size in the following chart. After this point, we can see performance differences. Some algorithms for small item size can be run faster than others but if you increase item size you will see a reference point that notation proof performance metrics.","title":"Asymptotic Analysis (3)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-runtime-analysis-1","text":"Cost Times Insertion - Sort ( A ) ---- ----- --------------------- c1 n 1 . for j = 2 to A.length c2 n -1 2 . key = A [ j ] c3 n -1 3 . // insert A [ j ] into the sorted sequence A [ 1 ...j -1 ] c4 n -1 4 . i = j - 1 c5 k5 5 . while i > 0 and A [ i ] > key do c6 k6 6 . A [ i +1 ] = A [ i ] c7 k6 7 . i = i - 1 c8 n -1 8 . A [ i +1 ] = key we have two loops here, if we sum up costs as follow we can see big-O worst case notation. \\(k_5 = \\sum_{j=2}^n{t_j}\\) and \\(k_6 = \\sum_{j=2}^n{t_i-1}\\) for operation counts","title":"Insertion Sort - Runtime Analysis (1)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-runtime-analysis-2","text":"cost function can be evaluated as follow; \\(T(n)=c_1n+c_2(n-1)+0(n-1)+c_4(n-1)+c_5\\sum_{j=2}^n{t_j}+c_6\\sum_{j=2}^n{t_i-1}+c_7\\sum_{j=2}^n{t_i-1}+c_8(n-1)\\)","title":"Insertion Sort - Runtime Analysis (2)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-runtime-analysis-3","text":"\\(\\sum_{j=2}^n j = (n(n+1)/2)- 1\\) and \\(\\sum_{j=2}^n {j-1} = n(n-1)/2\\)","title":"Insertion Sort - Runtime Analysis (3)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-runtime-analysis-4","text":"\\(T(n)=(c_5/2 + c_6/2 + c_7/2)n^2 + (c_1+c_2+c_4+c_5/2-c_6/2-c_7/2+c_8)n-(c_2 + c_4 + c_5 + c_6)\\)","title":"Insertion Sort - Runtime Analysis (4)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-runtime-analysis-5","text":"\\(T(n)= an^2 + bn + c\\)","title":"Insertion Sort - Runtime Analysis (5)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-runtime-analysis-6","text":"\\(O(n^2)\\)","title":"Insertion Sort - Runtime Analysis (6)"},{"location":"week-1/ce100-week-1-intro/#best-case-scenario-sorted-array-1","text":"Problem-1, If \\(A[1...j]\\) is already sorted, what will be \\(t_j=?\\) \\(t_j=1\\)","title":"Best-Case Scenario (Sorted Array) (1)"},{"location":"week-1/ce100-week-1-intro/#best-case-scenario-sorted-array-2","text":"Parameters are taken from image \\(T(n)=c_1n+c_2(n-1)+c_3(n-1)+c_4\\sum_{j=2}^nt_j+c_5\\sum_{j=2}^n(t_j-1)+c_6\\sum_{j=2}^n(t_j-1)+c_7(n-1)\\) \\(t_j=1\\) for all \\(j\\) \\(T(n)=(c_1+c_2+c_3+c_4+c_7)n-(c_2+c_3+c_4+c_7)\\) \\(T(n)=an-b\\) \\(\\Omega(n)\\)","title":"Best-Case Scenario (Sorted Array) (2)"},{"location":"week-1/ce100-week-1-intro/#worst-case-scenario-reversed-array-1","text":"Problem-2 If \\(A[j]\\) is smaller than every entry in \\(A[1...j-1]\\) , what will be \\(t_j=?\\) \\(t_j=?\\)","title":"Worst-Case Scenario (Reversed Array) (1)"},{"location":"week-1/ce100-week-1-intro/#worst-case-scenario-reversed-array-2","text":"The input array is reverse sorted \\(t_j=j\\) for all \\(j\\) after calculation worst case runtime will be \\(T(n)=1/2(c_4+c_5+c_6)n^2+(c_1+c_2+c_3+1/2(c_4-c_5-c_6)+c_7)n-(c_2+c_3+c_4+c_7)\\) \\(T(n)=1/2an^2+bn-c\\) \\(O(n^2)\\)","title":"Worst-Case Scenario (Reversed Array) (2)"},{"location":"week-1/ce100-week-1-intro/#asymptotic-runtime-analysis-of-insertion-sort","text":"","title":"Asymptotic Runtime Analysis of Insertion-Sort"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-worst-case-input-reverse-sorted","text":"Inner Loop is \\(\\Theta(j)\\) \\(T(n)=\\sum_{j=2}^n\\Theta(j)=\\Theta(\\sum_{j=2}^nj)=\\Theta(n^2)\\)","title":"Insertion-Sort Worst-case (input reverse sorted)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-average-case-all-permutations-uniformly-distributed","text":"Inner Loop is \\(\\Theta(j/2)\\) \\(T(n)=\\sum_{j=2}^n\\Theta(j/2)=\\sum_{j=2}^n\\Theta(j)=\\Theta(n^2)\\)","title":"Insertion-Sort Average-case (all permutations uniformly distributed)"},{"location":"week-1/ce100-week-1-intro/#array-sorting-algorithms-timespace-complexities","text":"To compare this sorting algorithm please check the following map again.","title":"Array Sorting Algorithms Time/Space Complexities"},{"location":"week-1/ce100-week-1-intro/#merge-sort-divide-conquer-combine-1","text":"","title":"Merge Sort : Divide / Conquer / Combine (1)"},{"location":"week-1/ce100-week-1-intro/#merge-sort-divide-conquer-combine-2","text":"Divide : we divide the problem into a number of subproblems Conquer : We solve the subproblems recursively Base-Case : Solve by Brute-Force Combine : Subproblem solutions to the original problem","title":"Merge Sort : Divide / Conquer / Combine (2)"},{"location":"week-1/ce100-week-1-intro/#merge-sort-example","text":"","title":"Merge Sort Example"},{"location":"week-1/ce100-week-1-intro/#merge-sort-algorithm-initial-setup","text":"Merge Sort is a recursive sorting algorithm, for initial case we need to call Merge-Sort(A,1,n) for sorting \\(A[1..n]\\) initial case A : Array p : 1 ( offset ) r : n ( length ) Merge - Sort ( A , 1 , n )","title":"Merge Sort Algorithm (initial setup)"},{"location":"week-1/ce100-week-1-intro/#merge-sort-algorithm-internal-iterations","text":"internal iterations A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif","title":"Merge Sort Algorithm (internal iterations)"},{"location":"week-1/ce100-week-1-intro/#merge-sort-algorithm-combine-1","text":"\\(p = start-point\\) \\(q = mid-point\\) \\(r = end-point\\)","title":"Merge Sort Algorithm (Combine-1)"},{"location":"week-1/ce100-week-1-intro/#merge-sort-algorithm-combine-2","text":"brute-force task, merging two sorted subarrays The pseudo-code in the textbook (Sec. 2.3.1)","title":"Merge Sort Algorithm (Combine-2)"},{"location":"week-1/ce100-week-1-intro/#merge-sort-combine-algorithm-1","text":"Merge ( A , p , q , r ) n1 = q - p +1 n2 = r - q // allocate left and right arrays // increment will be from left to right // left part will be bigger than right part L [ 1 ...n1 +1 ] // left array R [ 1 ...n2 +1 ] // right array // copy left part of array for i = 1 to n1 L [ i ] = A [ p + i -1 ] // copy right part of array for j = 1 to n2 R [ j ] = A [ q + j ] // put end items maximum values for termination L [ n1 +1 ] = inf R [ n2 +1 ] = inf i = 1 , j = 1 for k = p to r if L [ i ] <= R [ j ] A [ k ] = L [ i ] i = i +1 else A [ k ] = R [ j ] j = j +1","title":"Merge Sort Combine Algorithm (1)"},{"location":"week-1/ce100-week-1-intro/#what-is-the-complexity-of-merge-operation","text":"You can find by counting loops will provide you base constant nested level will provide you exponent of this constant, if you drop constants you will have complexity we have 3 for loops it will look like \\(3n\\) and \\(\\Theta(n)\\) will be merge complexity","title":"What is the complexity of merge operation?"},{"location":"week-1/ce100-week-1-intro/#merge-sort-correctness","text":"Base case \\(p = r\\) (Trivially correct) Inductive hypothesis MERGE-SORT is correct for any subarray that is a strict (smaller) subset of \\(A[p, q]\\) . General Case MERGE-SORT is correct for \\(A[p, q]\\) . From inductive hypothesis and correctness of Merge.","title":"Merge Sort Correctness"},{"location":"week-1/ce100-week-1-intro/#merge-sort-algorithm-pseudo-code","text":"A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif","title":"Merge Sort Algorithm (Pseudo-Code)"},{"location":"week-1/ce100-week-1-intro/#merge-sort-algorithm-complexity","text":"A : Array p : offset r : length Merge - Sort ( A , p , r ) -------------> T ( n ) if p = r then ---------------> Theta ( 1 ) return else q = floor (( p + r ) / 2 ) ----> Theta ( 1 ) Merge - Sort ( A , p , q ) -----> T ( n / 2 ) Merge - Sort ( A , q +1 , r ) ---> T ( n / 2 ) Merge ( A , p , q , r ) --------> Theta ( n ) endif","title":"Merge Sort Algorithm Complexity"},{"location":"week-1/ce100-week-1-intro/#merge-sort-algorithm-recurrence","text":"We can describe a function recursively in terms of itself, to analyze the performance of recursive algorithms \\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\]","title":"Merge Sort Algorithm Recurrence"},{"location":"week-1/ce100-week-1-intro/#how-to-solve-recurrence-1","text":"\\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\]","title":"How To Solve Recurrence (1)"},{"location":"week-1/ce100-week-1-intro/#how-to-solve-recurrence-2","text":"We will assume \\(T(n)= \\Theta(1)\\) for sufficiently small \\(n\\) to rewrite equation as \\[ T(n)=2T(n/2)+\\Theta(n) \\] Solution for this equation will be \\(\\Theta(nlgn)\\) with following recursion tree.","title":"How To Solve Recurrence (2)"},{"location":"week-1/ce100-week-1-intro/#how-to-solve-recurrence-3","text":"Multiply by height \\(\\Theta(lgn)\\) with each level cost \\(\\Theta(n)\\) we can found \\(\\Theta(nlgn)\\)","title":"How To Solve Recurrence (3)"},{"location":"week-1/ce100-week-1-intro/#how-to-solve-recurrence-4","text":"This tree is binary-tree and binary-tree height is related with item size.","title":"How To Solve Recurrence (4)"},{"location":"week-1/ce100-week-1-intro/#how-height-of-a-binary-tree-is-equal-to-logn-1","text":"Merge-Sort recursion tree is a perfect binary tree, a binary tree is a tree which every node has at most two children, A perfect binary tree is binary tree in which all internal nodes have exactly two children and all leaves are at the same level.","title":"How Height of a Binary Tree is Equal to \\(logn\\) ? (1)"},{"location":"week-1/ce100-week-1-intro/#how-height-of-a-binary-tree-is-equal-to-logn-2","text":"Let \\(n\\) be the number of nodes in the tree and let \\(l_k\\) denote the number of nodes on level k. According to this; \\(l_k = 2l_{k-1}\\) i.e. each level has exactly twice as many nodes as the previous level \\(l_0 = 1\\) , i.e. on the first level we have only one node (the root node) The leaves are at the last level, \\(l_h\\) where \\(h\\) is the height of the tree.","title":"How Height of a Binary Tree is Equal to \\(logn\\) ? (2)"},{"location":"week-1/ce100-week-1-intro/#how-height-of-a-binary-tree-is-equal-to-logn-3","text":"The total number of nodes in the tree is equal to the sum of the nodes on all the levels: nodes \\(n\\) \\[ 1+2^1+2^2+2^3+...+2^h=n \\] \\[ 1+2^1+2^2+2^3+...+2^h=2^{h+1}-1 \\] \\[ 2^{h+1}-1=n \\] \\[ 2^{h+1}=n+1 \\] \\[ log_2{2^{h+1}} = log_2{(n+1)} \\] \\[ h+1 = log_2{(n+1)} \\] \\[ h = log_2{(n+1)}-1 \\]","title":"How Height of a Binary Tree is Equal to \\(logn\\) ? (3)"},{"location":"week-1/ce100-week-1-intro/#how-height-of-a-binary-tree-is-equal-to-logn-3_1","text":"If we write it as asymptotic approach, we will have the following result \\[ \\text{height of tree is }h = log_2{(n+1)}-1 = O(logn) \\] also \\[ \\text{number of leaves is } l_h = (n+1)/2 \\] nearly half of the nodes are at the leaves","title":"How Height of a Binary Tree is Equal to \\(logn\\) ? (3)"},{"location":"week-1/ce100-week-1-intro/#review","text":"\\(\\Theta(nlgn)\\) grows more slowly than \\(\\Theta(n^2)\\) Therefore Merge-Sort beats Insertion-Sort in the worst case In practice Merge-Sort beats Insertion-Sort for \\(n>30\\) or so","title":"Review"},{"location":"week-1/ce100-week-1-intro/#asymptotic-notations","text":"","title":"Asymptotic Notations"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-1","text":"\\(f(n)=O(g(n))\\) if \\(\\exists\\) positive constants \\(c\\) , \\(n_0\\) such that \\[ 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\]","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (1)"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-2","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (2)"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-3","text":"Asymptotic running times of algorithms are usually defined by functions whose domain are \\(N={0, 1, 2, \u2026}\\) (natural numbers)","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (3)"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-4","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (4)"},{"location":"week-1/ce100-week-1-intro/#example-1","text":"Show that \\(2n^2 = O(n^3)\\) we need to find two positive constant \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq 2n^2 \\leq cn^3 \\text{ for all } n \\geq n_0 \\] Choose \\(c=2\\) and \\(n_0 = 1\\) \\[ 2n^2 \\leq 2n^3 \\text{ for all } n \\geq 1 \\] Or, choose \\(c=1\\) and \\(n_0=2\\) \\[ 2n^2 \\leq n^3 \\text{ for all } n \\geq 2 \\]","title":"Example-1"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-5","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (5)"},{"location":"week-1/ce100-week-1-intro/#example-2","text":"Show that \\(2n^2 + n = O(n^2)\\) We need to find two positive constant \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq {2n^2+n} \\leq cn^2 \\text{ for all } n \\geq n_0 \\] \\[ 2 + (1/n) \\leq c \\text{ for all } n \\geq n_0 \\] Choose \\(c=3\\) and \\(n_0=1\\) \\[ 2n^2 + n \\leq 3n^2 \\text{ for all } n \\geq 1 \\]","title":"Example-2"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-6","text":"We can say the followings about \\(f(n)=O(g(n))\\) equation The notation is a little sloppy One-way equation, e.q. \\(n^2 = O(n^3)\\) but we cannot say \\(O(n^3)=n^2\\)","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (6)"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-7","text":"\\(O(g(n))\\) is in fact a set of functions as follow \\(O(g(n)) = \\{ f(n) : \\exists \\text{ positive constant } c, n_0 \\text{ such that } 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\}\\)","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (7)"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-8","text":"In other words \\(O(g(n))\\) is in fact, the set of functions that have asymptotic upper bound \\(g(n)\\) e.q \\(2n^2 = O(n^3)\\) means \\(2n^2 \\in O(n^3)\\)","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (8)"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-9","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (9)"},{"location":"week-1/ce100-week-1-intro/#example-1_1","text":"\\(10^9n^2 = O(n^2)\\) \\(0 \\leq 10^9n^2 \\leq cn^2 \\text{ for } n \\geq n_0\\) choose \\(c=10^9\\) and \\(n_0=1\\) \\(0 \\leq 10^9n^2 \\leq 10^9n^2 \\text{ for } n \\geq 1\\) CORRECT","title":"Example-1"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-10","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (10)"},{"location":"week-1/ce100-week-1-intro/#example-2_1","text":"\\(100n^{1.9999}=O(n^2)\\) \\(0 \\leq 100n^{1.9999} \\leq cn^2 \\text{ for } n \\geq n_0\\) choose \\(c=100\\) and \\(n_0=1\\) \\(0 \\leq 100n^{1.9999} \\leq 100n^2 \\text{ for } n \\geq 1\\) CORRECT","title":"Example-2"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-11","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (11)"},{"location":"week-1/ce100-week-1-intro/#example-3","text":"\\(10^{-9}n^{2.0001} = O(n^2)\\) \\(0 \\leq 10^{-9}n^{2.0001} \\leq cn^2 \\text{ for } n \\geq n_0\\) \\(10^{-9}n^{0.0001} \\leq c \\text{ for } n \\geq n_0\\) INCORRECT (Contradiction)","title":"Example-3"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-12","text":"If we analysis \\(O(n^2)\\) case, \\(O\\) -notation is an upper bound notation and the runtime \\(T(n)\\) of algorithm A is at least \\(O(n^2 )\\) . \\(O(n^2)\\) : The set of functions with asymptotic upper bound \\(n^2\\) \\(T(n) \\geq O(n^2)\\) means \\(T(n) \\geq h(n)\\) for some \\(h(n) \\in O(n^2)\\) \\(h(n)=0\\) function is also in \\(O(n^2)\\) . Hence : \\(T(n) \\geq 0\\) , runtime must be nonnegative.","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (12)"},{"location":"week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-1","text":"\\(f(n)=\\Omega(g(n))\\) if \\(\\exists\\) positive constants \\(c,n_0\\) such that \\(0 \\leq cg(n) \\leq f(n) , \\forall n \\geq n_0\\)","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (1)"},{"location":"week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-2","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (2)"},{"location":"week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-3","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (3)"},{"location":"week-1/ce100-week-1-intro/#example-1_2","text":"Show that \\(2n^3 = \\Omega(n^2)\\) We need to find two positive constants \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq cn^2 \\leq 2n^3 \\text{ for all } n \\geq n_0 \\] Choose \\(c=1\\) and \\(n_0=1\\) \\[ n^2 \\leq 2n^3 \\text{ for all } n \\geq 1 \\]","title":"Example-1"},{"location":"week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-4","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (4)"},{"location":"week-1/ce100-week-1-intro/#example-4","text":"Show that \\(\\sqrt{n}=\\Omega(lgn)\\) We need to find two positive constants \\(c\\) and \\(n_0\\) such that: \\[ clgn \\leq \\sqrt{n} \\text{ for all } n \\geq n_0 \\] Choose \\(c=1\\) and \\(n_0=16\\) \\[ lgn \\leq \\sqrt{n} \\text{ for all } n \\geq 16 \\]","title":"Example-4"},{"location":"week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-5","text":"\\(\\Omega(g(n))\\) is the set of functions that have asymptotic lower bound \\(g(n)\\) \\[ \\Omega(g(n))=\\{ f(n):\\exists \\text{ positive constants } c,n_0 \\text{ such that } 0 \\leq cg(n) \\leq f(n), \\forall n \\geq n_0 \\} \\]","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (5)"},{"location":"week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-6","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (6)"},{"location":"week-1/ce100-week-1-intro/#example-1_3","text":"\\(10^9n^2 = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 10^9n^2 \\text{ for } n\\geq n_0\\) Choose \\(c=10^9\\) and \\(n_0=1\\) \\(0 \\leq 10^9n^2 \\leq 10^9n^2 \\text{ for } n\\geq 1\\) CORRECT","title":"Example-1"},{"location":"week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-7","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (7)"},{"location":"week-1/ce100-week-1-intro/#example-2_2","text":"\\(100n^{1.9999} = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 100n^{1.9999} \\text{ for } n \\geq n_0\\) \\(n^{0.0001} \\leq (100/c) \\text{ for } n \\geq n_0\\) INCORRECT (Contradiction)","title":"Example-2"},{"location":"week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-8","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (8)"},{"location":"week-1/ce100-week-1-intro/#example-3_1","text":"\\(10^{-9}n^{2.0001} = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 10^{-9}n^{2.0001} \\text{ for } n \\geq n_0\\) Choose \\(c=10^{-9}\\) and \\(n_0=1\\) \\(0 \\leq 10^{-9}n^2 \\leq 10^{-9}n^{2.0001} \\text{ for } n \\geq 1\\) CORRECT","title":"Example-3"},{"location":"week-1/ce100-week-1-intro/#comparison-of-notations-1","text":"","title":"Comparison of Notations (1)"},{"location":"week-1/ce100-week-1-intro/#comparison-of-notations-2","text":"","title":"Comparison of Notations (2)"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-1","text":"\\(f(n)=\\Theta(g(n))\\) if \\(\\exists\\) positive constants \\(c_1,c_2,n_0\\) such that \\(0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n), \\forall n \\geq n_0\\)","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (1)"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-2","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (2)"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-3","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (3)"},{"location":"week-1/ce100-week-1-intro/#example-1_4","text":"Show that \\(2n^2 + n = \\Theta(n^2)\\) We need to find 3 positive constants \\(c_1,c_2\\) and \\(n_0\\) such that: \\(0 \\leq c_1n^2 \\leq 2n^2+n \\leq c_2n^2\\) for all \\(n \\geq n_0\\) \\(c_1 \\leq 2 + (1/n) \\leq c_2\\) for all \\(n \\geq n_0\\) Choose \\(c_1=2, c_2=3\\) and \\(n_0=1\\) \\(2n^2 \\leq 2n^2+n \\leq 3n^2\\) for all \\(n \\geq 1\\)","title":"Example-1"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-4","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (4)"},{"location":"week-1/ce100-week-1-intro/#example-21","text":"Show that \\(1/2n^2-2n=\\Theta(n^2)\\) We need to find 3 positive constants \\(c_1,c_2\\) and \\(n_0\\) such that: \\(0 \\leq c_1n^2 \\leq 1/2n^2-2n \\leq c_2n^2 \\text{ for all } n \\geq n_0\\) \\(c_1 \\leq 1/2 - 2 / n \\leq c_2 \\text{ for all } n \\geq n_0\\) Choose 3 positive constants \\(c_1,c_2, n_0\\) that satisfy \\(c_1 \\leq 1/2 - 2/n \\leq c_2\\) for all \\(n \\geq n_0\\)","title":"Example-2.1"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-5","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (5)"},{"location":"week-1/ce100-week-1-intro/#example-22","text":"","title":"Example-2.2"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-6","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (6)"},{"location":"week-1/ce100-week-1-intro/#example-23","text":"\\[ 1/10 \\leq 1/2 - 2/n \\text{ for } n \\geq 5 \\] \\[ 1/2 - 2/n \\leq 1/2 \\text{ for } n \\geq 0 \\] Therefore we can choose \\(c_1 = 1/10, c_2=1/2, n_0=5\\)","title":"Example-2.3"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-7","text":"Theorem : leading constants & low-order terms don\u2019t matter Justification : can choose the leading constant large enough to make high-order term dominate other terms","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (7)"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-8","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (8)"},{"location":"week-1/ce100-week-1-intro/#example-1_5","text":"\\(10^9n^2 = \\Theta(n^2)\\) CORRECT \\(100n^{1.9999} = \\Theta(n^2)\\) INCORRECT \\(10^9n^{2.0001} = \\Theta(n^2)\\) INCORRECT","title":"Example-1"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-9","text":"\\(\\Theta(g(n))\\) is the set of functions that have asymptotically tight bound \\(g(n)\\) \\(\\Theta(g(n))=\\{ f(n): \\exists \\text{ positive constants } c_1,c_2, n_0 \\text{ such that } 0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n), \\forall n \\geq n_0 \\}\\)","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (9)"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-10","text":"Theorem : \\(f(n)=\\Theta(g(n))\\) if and only if \\(f(n)=O(g(n))\\) and \\(f(n)=\\Omega(g(n))\\) \\(\\Theta\\) is stronger than both \\(O\\) and \\(\\Omega\\) \\(\\Theta(g(n)) \\subseteq O(g(n)) \\text{ and } \\Theta(g(n)) \\subseteq \\Omega(g(n))\\)","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (10)"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-11","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (11)"},{"location":"week-1/ce100-week-1-intro/#example-11","text":"Prove that \\(10^{-8}n^2 \\neq \\Theta(n)\\) We can check that \\(10^{-8}n^2 = \\Omega(n)\\) and \\(10^{-8}n^2 \\neq O(n)\\) Proof by contradiction for \\(O(n)\\) notation \\[ O(g(n)) = \\{ f(n) : \\exists \\text{ positive constant } c, n_0 \\text{ such that } 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\} \\]","title":"Example-1.1"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-12","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (12)"},{"location":"week-1/ce100-week-1-intro/#example-12","text":"Suppose positive constants \\(c_2\\) and \\(n_0\\) exist such that: \\(10^{-8}n^2 \\leq c_2n, \\forall n \\geq n_0\\) \\(10^{-8}n \\leq c_2, \\forall n \\geq n_0\\) Contradiction : \\(c_2\\) is a constant","title":"Example-1.2"},{"location":"week-1/ce100-week-1-intro/#summary-of-oomega-and-theta-notations-1","text":"\\(O(g(n))\\) : The set of functions with asymptotic upper bound \\(g(n)\\) \\(\\Omega(g(n))\\) : The set of functions with asymptotic lower bound \\(g(n)\\) \\(\\Theta(n)\\) : The set of functions with asymptotically tight bound \\(g(n)\\) \\(f(n)=\\Theta(g(n)) \\Leftrightarrow f(n)=O(g(n)) \\text{ and } f(n)=\\Omega(g(n))\\)","title":"Summary of \\(O,\\Omega\\) and \\(\\Theta\\) notations (1)"},{"location":"week-1/ce100-week-1-intro/#summary-of-oomega-and-theta-notations-2","text":"","title":"Summary of \\(O,\\Omega\\) and \\(\\Theta\\) notations (2)"},{"location":"week-1/ce100-week-1-intro/#small-o-o-notation-asymptotic-upper-bound-that-is-not-tight-1","text":"Remember, upper bound provided by big- \\(O\\) notation can be tight or not tight Tight mean values are close the original function e.g. followings are true \\(2n^2 = O(n^2)\\) is asymptotically tight \\(2n = O(n^2)\\) is not asymptotically tight According to this small- \\(o\\) notation is an upper bound that is not asymptotically tight","title":"Small-o / \\(o\\)-Notation : Asymptotic upper bound that is not tight (1)"},{"location":"week-1/ce100-week-1-intro/#small-o-o-notation-asymptotic-upper-bound-that-is-not-tight-2","text":"Note that in equations equality is removed in small notations \\(o(g(n))=\\{ f(n): \\text{ for any constant} c > 0, \\exists \\text{ a constant } n_0 > 0, \\text{ such that } 0 \\leq f(n) < cg(n), \\forall n \\geq n_0 \\}\\) \\[ \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0 \\] e.g \\(2n=o(n^2)\\) any positive \\(c\\) satisfies but \\(2n^2 \\neq o(n^2)\\) \\(c=2\\) does not satisfy","title":"Small-o / \\(o\\)-Notation : Asymptotic upper bound that is not tight (2)"},{"location":"week-1/ce100-week-1-intro/#small-omega-omega-notation-asymptotic-lower-bound-that-is-not-tight-1","text":"\\(\\omega(g(n))=\\{ f(n): \\text{ for any constant } c > 0, \\exists \\text{ a constant } n_0>0, \\text{ such that } 0 \\leq cg(n) < f(n), \\forall n \\geq n_0\\) \\[ \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = \\infty \\] e.g. \\(n^2/2=\\omega(n)\\) , any positive \\(c\\) satisfies but \\(n^2/2 \\neq \\omega(n^2)\\) , \\(c=1/2\\) does not satisfy","title":"Small-omega / \\(\\omega\\)-Notation: Asymptotic lower bound that is not tight (1)"},{"location":"week-1/ce100-week-1-intro/#important-analogy-to-compare-of-two-real-numbers","text":"\\[ f(n)= O(g(n)) \\leftrightarrow a \\leq b \\] \\[ f(n)= \\Omega(g(n)) \\leftrightarrow a \\geq b \\] \\[ f(n)= \\Theta(g(n)) \\leftrightarrow a = b \\] \\[ f(n)= o(g(n)) \\leftrightarrow a < b \\] \\[ f(n)= \\omega(g(n)) \\leftrightarrow a > b \\]","title":"(Important) Analogy to compare of two real numbers"},{"location":"week-1/ce100-week-1-intro/#important-analogy-to-compare-of-two-real-numbers_1","text":"\\(O \\approx \\leq\\) \\(\\Theta \\approx =\\) \\(\\Omega \\approx \\geq\\) \\(\\omega \\approx >\\) \\(o \\approx <\\)","title":"(Important) Analogy to compare of two real numbers"},{"location":"week-1/ce100-week-1-intro/#important-trichotomy-property-for-real-numbers","text":"For any two real numbers \\(a\\) and \\(b\\) , we have either \\(a<b\\) , or \\(a=b\\) , or \\(a>b\\) Trichotomy property does not hold for asymptotic notation, for two functions \\(f(n)\\) and \\(g(n)\\) , it may be the case that neither \\(f(n)=O(g(n))\\) nor \\(f(n)=\\Omega(g(n))\\) holds. e.g. \\(n\\) and \\(n^{1+sin(n)}\\) cannot be compared asymptotically","title":"(Important) Trichotomy property for real numbers"},{"location":"week-1/ce100-week-1-intro/#examples","text":"\\(5n^2=O(n^2)\\) TRUE \\(n^2lgn = O(n^2)\\) FALSE \\(5n^2=\\Omega(n^2)\\) TRUE \\(n^2lgn = \\Omega(n^2)\\) TRUE \\(5n^2=\\Theta(n^2)\\) TRUE \\(n^2lgn = \\Theta(n^2)\\) FALSE \\(5n^2=o(n^2)\\) FALSE \\(n^2lgn = o(n^2)\\) FALSE \\(5n^2=\\omega(n^2)\\) FALSE \\(n^2lgn = \\omega(n^2)\\) TRUE \\(2^n = O(3^n)\\) TRUE \\(2^n = \\Omega(3^n)\\) FALSE \\(2^n=o(3^n)\\) TRUE \\(2^n = \\Theta(3^n)\\) FALSE \\(2^n = \\omega(3^n)\\) FALSE","title":"Examples"},{"location":"week-1/ce100-week-1-intro/#asymptotic-function-properties","text":"Transitivity : holds for all e.g. \\(f(n) = \\Theta(g(n)) \\& g(n)=\\Theta(h(n)) \\Rightarrow f(n)=\\Theta(h(n))\\) Reflexivity : holds for \\(\\Theta,O,\\Omega\\) e.g. \\(f(n)=O(f(n))\\) Symmetry : hold only for \\(\\Theta\\) e.g. \\(f(n)=\\Theta(g(n)) \\Leftrightarrow g(n)=\\Theta(f(n))\\) Transpose Symmetry : holds for \\((O \\leftrightarrow \\Omega)\\) and \\((o \\leftrightarrow \\omega)\\) e.g. \\(f(n)=O(g(n))\\Leftrightarrow g(n)=\\Omega(f(n))\\)","title":"Asymptotic Function Properties"},{"location":"week-1/ce100-week-1-intro/#using-o-notation-to-describe-running-times-1","text":"Used to bound worst-case running times, Implies an upper bound runtime for arbitrary inputs as well Example: Insertion sort has worst-case runtime of \\(O(n^2 )\\) Note: This \\(O(n^2)\\) upper bound also applies to its running time on every input Abuse to say \u201crunning time of insertion sort is \\(O(n^2)\\) \" For a given \\(n\\) , the actual running time depends on the particular input of size \\(n\\) i.e., running time is not only a function of \\(n\\) However, worst-case running time is only a function of \\(n\\)","title":"Using \\(O\\)-Notation to Describe Running Times (1)"},{"location":"week-1/ce100-week-1-intro/#using-o-notation-to-describe-running-times-2","text":"When we say: Running time of insertion sort is \\(O(n^2)\\) What we really mean is Worst-case running time of insertion sort is \\(O(n^2)\\) or equivalently No matter what particular input of size n is chosen, the running time on that set of inputs is \\(O(n^2)\\)","title":"Using \\(O\\)-Notation to Describe Running Times (2)"},{"location":"week-1/ce100-week-1-intro/#using-omega-notation-to-describe-running-times-1","text":"Used to bound best-case running times, Implies a lower bound runtime for arbitrary inputs as well Example: Insertion sort has best-case runtime of \\(\\Omega(n)\\) Note : This \\(\\Omega(n)\\) lower bound also applies to its running time on every input","title":"Using \\(\\Omega\\)-Notation to Describe Running Times (1)"},{"location":"week-1/ce100-week-1-intro/#using-omega-notation-to-describe-running-times-2","text":"When we say Running time of algorithm A is \\(\\Omega(g(n))\\) What we mean is For any input of size \\(n\\) , the runtime of A is at least a constant times \\(g(n)\\) for sufficiently large \\(n\\) It\u2019s not contradictory to say worst-case running time of insertion sort is \\(\\Omega(n^2)\\) Because there exists an input that causes the algorithm to take \\(\\Omega(n^2)\\)","title":"Using \\(\\Omega\\)-Notation to Describe Running Times (2)"},{"location":"week-1/ce100-week-1-intro/#using-theta-notation-to-describe-running-times-1","text":"Consider 2 cases about the runtime of an algorithm Case 1: Worst-case and best-case not asymptotically equal Use \\(\\Theta\\) -notation to bound worst-case and best-case runtimes separately Case 2: Worst-case and best-case asymptotically equal Use \\(\\Theta\\) -notation to bound the runtime for any input","title":"Using \\(\\Theta\\)-Notation to Describe Running Times (1)"},{"location":"week-1/ce100-week-1-intro/#using-theta-notation-to-describe-running-times-2","text":"Case 1: Worst-case and best-case not asymptotically equal Use \\(\\Theta\\) -notation to bound the worst-case and best-case runtimes separately We can say: \"The worst-case runtime of insertion sort is \\(\\Theta(n^2)\\) \" \"The best-case runtime of insertion sort is \\(\\Theta(n)\\) \" But, we can\u2019t say: \"The runtime of insertion sort is \\(\\Theta(n^2)\\) for every input\" A \\(\\Theta\\) -bound on worst/best-case running time does not apply to its running time on arbitrary inputs","title":"Using \\(\\Theta\\)-Notation to Describe Running Times (2)"},{"location":"week-1/ce100-week-1-intro/#worst-case-and-best-case-equation-for-merge-sort","text":"e.g. for merge-sort, we have: \\[ T(n)=\\Theta(nlgn)\\begin{cases} T(n)=O(nlgn)\\\\ T(n)=\\Omega(nlgn)\\end{cases} \\]","title":"Worst-Case and Best-Case Equation for Merge-Sort"},{"location":"week-1/ce100-week-1-intro/#using-asymptotic-notation-to-describe-runtimes-summary-1","text":"\"The worst case runtime of Insertion Sort is \\(O(n^2)\\) \" Also implies: \"The runtime of Insertion Sort is \\(O(n^2)\\) \" \"The best-case runtime of Insertion Sort is \\(\\Omega(n)\\) \" Also implies: \"The runtime of Insertion Sort is \\(\\Omega(n)\\) \"","title":"Using Asymptotic Notation to Describe Runtimes Summary (1)"},{"location":"week-1/ce100-week-1-intro/#using-asymptotic-notation-to-describe-runtimes-summary-2","text":"\"The worst case runtime of Insertion Sort is \\(\\Theta(n^2)\\) \" But: \"The runtime of Insertion Sort is not \\(\\Theta(n^2)\\) \" \"The best case runtime of Insertion Sort is \\(\\Theta(n)\\) \" But: \"The runtime of Insertion Sort is not \\(\\Theta(n)\\) \"","title":"Using Asymptotic Notation to Describe Runtimes Summary (2)"},{"location":"week-1/ce100-week-1-intro/#using-asymptotic-notation-to-describe-runtimes-summary-3","text":"","title":"Using Asymptotic Notation to Describe Runtimes Summary (3)"},{"location":"week-1/ce100-week-1-intro/#which-one-is-true","text":"FALSE \"The worst case runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" FALSE \"The best case runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" TRUE \"The runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" This is true, because the best and worst case runtimes have asymptotically the same tight bound \\(\\Theta(nlgn)\\)","title":"Which one is true?"},{"location":"week-1/ce100-week-1-intro/#asymptotic-notation-in-equations-rhs","text":"Asymptotic notation appears alone on the RHS of an equation: implies set membership e.g., \\(n = O(n^2)\\) means \\(n \\in O(n^2)\\) Asymptotic notation appears on the RHS of an equation stands for some anonymous function in the set e.g., \\(2n^2 + 3n + 1 = 2n^2 + \\Theta(n)\\) means: \\(2n^2 + 3n + 1 = 2n^2 + h(n)\\) , for some \\(h(n) \\in \\Theta(n)\\) i.e., \\(h(n) = 3n + 1\\)","title":"Asymptotic Notation in Equations (RHS)"},{"location":"week-1/ce100-week-1-intro/#asymptotic-notation-in-equations-lhs","text":"Asymptotic notation appears on the LHS of an equation: stands for any anonymous function in the set e.g., \\(2n^2 + \\Theta(n) = \\Theta(n^2)\\) means: for any function \\(g(n) \\in \\Theta(n)\\) \\(\\exists\\) some function \\(h(n)\\in \\Theta(n^2)\\) such that \\(2n^2+g(n) = h(n)\\) RHS provides coarser level of detail than LHS","title":"Asymptotic Notation in Equations (LHS)"},{"location":"week-1/ce100-week-1-intro/#references-todo-update-missing-references","text":"Introduction to Algorithms, Third Edition | The MIT Press http://nabil.abubaker.bilkent.edu.tr/473/ Insertion Sort - GeeksforGeeks http://www.cs.gettysburg.edu/~ilinkin/courses/Fall-2012/cs216/notes/bintree.pdf Dictionary of Algorithms and Data Structures big-O notation Omega","title":"References (TODO: Update Missing References)"},{"location":"week-10/ce100-week-10-graphs/","text":"CE100 Algorithms and Programming II \u00b6 Week-10 (Graphs) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Graphs \u00b6 Outline \u00b6 Introduction to Graphs Graphs and Representation BFS (Breath-First Search) DFS (Depth-First Search) in-order post-order pre-order Topological Order SCC (Strongly Connected Components) MST Prim Kruskal References \u00b6 TODO","title":"Week-10 (Graphs)"},{"location":"week-10/ce100-week-10-graphs/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-10/ce100-week-10-graphs/#week-10-graphs","text":"","title":"Week-10 (Graphs)"},{"location":"week-10/ce100-week-10-graphs/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-10/ce100-week-10-graphs/#graphs","text":"","title":"Graphs"},{"location":"week-10/ce100-week-10-graphs/#outline","text":"Introduction to Graphs Graphs and Representation BFS (Breath-First Search) DFS (Depth-First Search) in-order post-order pre-order Topological Order SCC (Strongly Connected Components) MST Prim Kruskal","title":"Outline"},{"location":"week-10/ce100-week-10-graphs/#references","text":"TODO","title":"References"},{"location":"week-11/ce100-week-11-shortestpath/","text":"CE100 Algorithms and Programming II \u00b6 Week-11 (Shortest Path) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Shortest Path \u00b6 Outline \u00b6 Disjoint Sets and Kruskal Relationships Single-Source Shortest Paths Bellman-Ford Dijkstra Q-Learning Shortest Path Max-Flow Min-Cut Ford-Fulkerson Edmond\u2019s Karp Dinic References \u00b6 TODO","title":"Week-11 (Shortest Path)"},{"location":"week-11/ce100-week-11-shortestpath/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-11/ce100-week-11-shortestpath/#week-11-shortest-path","text":"","title":"Week-11 (Shortest Path)"},{"location":"week-11/ce100-week-11-shortestpath/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-11/ce100-week-11-shortestpath/#shortest-path","text":"","title":"Shortest Path"},{"location":"week-11/ce100-week-11-shortestpath/#outline","text":"Disjoint Sets and Kruskal Relationships Single-Source Shortest Paths Bellman-Ford Dijkstra Q-Learning Shortest Path Max-Flow Min-Cut Ford-Fulkerson Edmond\u2019s Karp Dinic","title":"Outline"},{"location":"week-11/ce100-week-11-shortestpath/#references","text":"TODO","title":"References"},{"location":"week-12/ce100-week-12-crypto/","text":"CE100 Algorithms and Programming II \u00b6 Week-12 (Hashing and Encryption) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Hashing and Encryption \u00b6 Outline \u00b6 Crypto++ Library Usage Hashing and Encryption Integrity Control Hash Values Cryptographic Hash Functions SHA-1 SHA-256 SHA-512 Checksums MD5 CRC32 Hash Algorithms SHA-1 SHA-256 SHA-512 H-MAC References \u00b6 TODO","title":"Week-12 (Hashing)"},{"location":"week-12/ce100-week-12-crypto/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-12/ce100-week-12-crypto/#week-12-hashing-and-encryption","text":"","title":"Week-12 (Hashing and Encryption)"},{"location":"week-12/ce100-week-12-crypto/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-12/ce100-week-12-crypto/#hashing-and-encryption","text":"","title":"Hashing and Encryption"},{"location":"week-12/ce100-week-12-crypto/#outline","text":"Crypto++ Library Usage Hashing and Encryption Integrity Control Hash Values Cryptographic Hash Functions SHA-1 SHA-256 SHA-512 Checksums MD5 CRC32 Hash Algorithms SHA-1 SHA-256 SHA-512 H-MAC","title":"Outline"},{"location":"week-12/ce100-week-12-crypto/#references","text":"TODO","title":"References"},{"location":"week-13/ce100-week-13-symenc/","text":"CE100 Algorithms and Programming II \u00b6 Week-13 (Symmetric and Asymmetric Encryption) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Symmetric and Asymmetric Encryption \u00b6 Outline \u00b6 Symmetric Encryption Algorithms AES https://formaestudio.com/portfolio/aes-animation/ DES http://desalgorithm.yolasite.com/ TDES https://en.wikipedia.org/wiki/Triple_DES Symmetric Encryption Modes https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation ECB CBC Asymmetric Encryption Key Pairs (Public-Private Key Pairs) Signature Generation and Validation References \u00b6 TODO","title":"Week-13 (Encryption)"},{"location":"week-13/ce100-week-13-symenc/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-13/ce100-week-13-symenc/#week-13-symmetric-and-asymmetric-encryption","text":"","title":"Week-13 (Symmetric and  Asymmetric Encryption)"},{"location":"week-13/ce100-week-13-symenc/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-13/ce100-week-13-symenc/#symmetric-and-asymmetric-encryption","text":"","title":"Symmetric and Asymmetric Encryption"},{"location":"week-13/ce100-week-13-symenc/#outline","text":"Symmetric Encryption Algorithms AES https://formaestudio.com/portfolio/aes-animation/ DES http://desalgorithm.yolasite.com/ TDES https://en.wikipedia.org/wiki/Triple_DES Symmetric Encryption Modes https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation ECB CBC Asymmetric Encryption Key Pairs (Public-Private Key Pairs) Signature Generation and Validation","title":"Outline"},{"location":"week-13/ce100-week-13-symenc/#references","text":"TODO","title":"References"},{"location":"week-14/ce100-week-14-otp/","text":"CE100 Algorithms and Programming II \u00b6 Week-14 (OTP Calculation, File Encryption) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX OTP Calculation, File Encryption \u00b6 Outline \u00b6 1.OTP Calculation a.Time-based b.Counter-based File Encryption and Decryption and Integrity Control Operations References \u00b6 TODO","title":"Week-14 (One-Time-Password / File Enc.)"},{"location":"week-14/ce100-week-14-otp/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-14/ce100-week-14-otp/#week-14-otp-calculation-file-encryption","text":"","title":"Week-14 (OTP Calculation, File Encryption)"},{"location":"week-14/ce100-week-14-otp/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-14/ce100-week-14-otp/#otp-calculation-file-encryption","text":"","title":"OTP Calculation, File Encryption"},{"location":"week-14/ce100-week-14-otp/#outline","text":"1.OTP Calculation a.Time-based b.Counter-based File Encryption and Decryption and Integrity Control Operations","title":"Outline"},{"location":"week-14/ce100-week-14-otp/#references","text":"TODO","title":"References"},{"location":"week-15/ce100-week-15-review/","text":"CE100 Algorithms and Programming II \u00b6 Week-15 (Review) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Review \u00b6 Outline \u00b6 References \u00b6 TODO","title":"Week-15 (Review)"},{"location":"week-15/ce100-week-15-review/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-15/ce100-week-15-review/#week-15-review","text":"","title":"Week-15 (Review)"},{"location":"week-15/ce100-week-15-review/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-15/ce100-week-15-review/#review","text":"","title":"Review"},{"location":"week-15/ce100-week-15-review/#outline","text":"","title":"Outline"},{"location":"week-15/ce100-week-15-review/#references","text":"TODO","title":"References"},{"location":"week-16/ce100-week-16-final/","text":"CE100 Algorithms and Programming II \u00b6 Week-16 (Final) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Final \u00b6 Outline \u00b6 References \u00b6 TODO","title":"Week-16 (Final)"},{"location":"week-16/ce100-week-16-final/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-16/ce100-week-16-final/#week-16-final","text":"","title":"Week-16 (Final)"},{"location":"week-16/ce100-week-16-final/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-16/ce100-week-16-final/#final","text":"","title":"Final"},{"location":"week-16/ce100-week-16-final/#outline","text":"","title":"Outline"},{"location":"week-16/ce100-week-16-final/#references","text":"TODO","title":"References"},{"location":"week-2/ce100-week-2-recurrence/","text":"CE100 Algorithms and Programming II \u00b6 Week-2 (Solving Recurrences / The Divide-and-Conquer) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Solving Recurrences \u00b6 Outline \u00b6 Solving Recurrences Recursion Tree Master Method Back-Substitution Outline \u00b6 Divide-and-Conquer Analysis Merge Sort Binary Search Merge Sort Analysis Complexity Outline \u00b6 Recurrence Solution Solving Recurrences \u00b6 Reminder: Runtime \\((T(n))\\) of MergeSort was expressed as a recurrence \\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\] Solving recurrences is like solving differential equations, integrals, etc. Need to learn a few tricks Recurrences \u00b6 Recurrence : An equation or inequality that describes a function in terms of its value on smaller inputs. Example : \\[ T(n)=\\begin{cases} \\ 1 &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ 1 &\\text{if n>1}\\end{cases} \\] Recurrence Example \u00b6 \\[ T(n)=\\begin{cases} \\ 1 &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ 1 &\\text{if n>1}\\end{cases} \\] Simplification: Assume \\(n=2^k\\) Claimed answer : \\(T(n)=lgn+1\\) Substitute claimed answer in the recurrence: \\[ lgn+1=\\begin{cases} \\ 1 &\\text{if n=1} \\\\ lg(\\lceil{n/2}\\rceil)+ 2 &\\text{if n>1}\\end{cases} \\] True when \\(n=2^k\\) Technicalities: Floor / Ceiling \u00b6 Technically, should be careful about the floor and ceiling functions (as in the book). e.g. For merge sort, the recurrence should in fact be:, \\[ T(n)=\\begin{cases} \\ \\Theta(1) &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ T(\\lfloor{n/2}\\rfloor) +\\Theta(n) &\\text{if n>1} \\end{cases} \\] But, it's usually ok to: ignore floor/ceiling solve for the exact power of 2 (or another number) Technicalities: Boundary Conditions \u00b6 Usually assume: \\(T(n) = \\Theta(1)\\) for sufficiently small \\(n\\) Changes the exact solution, but usually the asymptotic solution is not affected (e.g. if polynomially bounded) For convenience, the boundary conditions generally implicitly stated in a recurrence \\(T(n) = 2T(n/2) + \\Theta(n)\\) assuming that \\(T(n)=\\Theta(1)\\) for sufficiently small \\(n\\) Example: When Boundary Conditions Matter \u00b6 Exponential function: \\(T(n) = (T(n/2))2\\) Assume \\(T(1) = c \\text{ (where c is a positive constant)}\\) \\(T(2) = (T(1))^2 = c^2\\) \\(T(4) = (T(2))^2 = c^4\\) \\(T(n) = \\Theta(c^n)\\) e.g. \\[ \\begin{rcases} T(1)= 2 &\\Rightarrow & T(n)= \\Theta(2^n) \\\\ T(1)= 3 &\\Rightarrow & T(n)= \\Theta(3^n) \\end{rcases} \\text{ However } \\Theta(2^n) \\neq \\Theta(3^n) \\] The difference in solution more dramatic when: \\[ T(1) = 1 \\Rightarrow T(n) = \\Theta(1^n) = \\Theta(1) \\] Solving Recurrences \u00b6 We will focus on 3 techniques Substitution method Recursion tree approach Master method Substitution Method \u00b6 The most general method: Guess Prove by induction Solve for constants Substitution Method: Example \u00b6 Solve \\(T(n)=4T(n/2)+n\\) (assume \\(T(1)= \\Theta(1)\\) ) Guess \\(T(n) = O(n^3)\\) (need to prove \\(O\\) and \\(\\Omega\\) separately) Prove by induction that \\(T(n) \\leq cn^3\\) for large \\(n\\) (i.e. \\(n \\geq n_0\\) ) Inductive hypothesis: \\(T(k) \\leq ck^3\\) for any \\(k < n\\) Assuming ind. hyp. holds, prove \\(T(n) \\leq cn^3\\) Substitution Method: Example \u2013 cont\u2019d \u00b6 Original recurrence: \\(T(n) = 4T(n/2) + n\\) From inductive hypothesis: \\(T(n/2) \\leq c(n/2)^3\\) Substitute this into the original recurrence: \\(T(n) \\leq 4c(n/2)^3 + n\\) \\(= (c/2)n^3 + n\\) \\(= cn^3 \u2013 ((c/2)n^3 \u2013 n)\\) \\(\\Longleftarrow\\) desired - residual \\(\\leq cn^3\\) when \\(((c/2)n^3 \u2013 n) \\geq 0\\) Substitution Method: Example \u2013 cont\u2019d \u00b6 So far, we have shown: \\[ T(n) \\leq cn^3 \\text{ when } ((c/2)n^3 \u2013 n) \\geq 0 \\] We can choose \\(c \\geq 2\\) and \\(n_0 \\geq 1\\) But, the proof is not complete yet. Reminder: Proof by induction: 1.Prove the base cases \\(\\Longleftarrow\\) haven\u2019t proved the base cases yet 2.Inductive hypothesis for smaller sizes 3.Prove the general case Substitution Method: Example \u2013 cont\u2019d \u00b6 We need to prove the base cases Base: \\(T(n) = \\Theta(1)\\) for small \\(n\\) (e.g. for \\(n = n_0\\) ) We should show that: \\(\\Theta(1) \\leq cn^3\\) for \\(n = n_0\\) , This holds if we pick \\(c\\) big enough So, the proof of \\(T(n) = O(n^3)\\) is complete But, is this a tight bound? Example: A tighter upper bound? \u00b6 Original recurrence: \\(T(n) = 4T(n/2) + n\\) Try to prove that \\(T(n) = O(n^2)\\) , i.e. \\(T(n) \\leq cn^2\\) for all \\(n \\geq n_0\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\) Example (cont\u2019d) \u00b6 Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\) \\[ T(n) = 4T(n/2) + n \\\\ \u2264 4c(n/2)^2 + n \\\\ = cn^2 + n \\\\ = O(n2) \\Longleftarrow \\text{ Wrong! We must prove exactly} \\] Example (cont\u2019d) \u00b6 Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\) So far, we have: \\(T(n) \\leq cn^2 + n\\) No matter which positive c value we choose, this does not show that \\(T(n) \\leq cn^2\\) Proof failed? Example (cont\u2019d) \u00b6 What was the problem? The inductive hypothesis was not strong enough Idea: Start with a stronger inductive hypothesis Subtract a low-order term Inductive hypothesis: \\(T(k) \\leq c_1k^2 \u2013 c_2k\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq c_1n^2 - c_2n\\) Example (cont\u2019d) \u00b6 Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq c_1k^2 \u2013 c_2k\\) for \\(k < n\\) Prove the general case: \\(T(n) \u2264 c_1n^2 \u2013 c_2n\\) \\(T(n) = 4T(n/2) + n\\) \\(\u2264 4 (c_1(n/2)^2 \u2013 c_2(n/2)) + n\\) \\(= c_1n^2 \u2013 2c_2n + n\\) \\(= c_1n^2 \u2013 c_2n \u2013 (c_2n \u2013 n)\\) \\(\u2264 c_1n^2 \u2013 c_2n\\) for \\(n(c_2 \u2013 1) \\geq 0\\) choose \\(c2 \\geq 1\\) Example (cont\u2019d) \u00b6 We now need to prove $$ T(n) \\leq c_1n^2 \u2013 c_2n $$ for the base cases. \\(T(n) = \\Theta(1) \\text{ for } 1 \\leq n \\leq n_0\\) (implicit assumption) \\(\\Theta(1) \\leq c_1n^2 \u2013 c_2n\\) for \\(n\\) small enough (e.g. \\(n = n_0\\) ) We can choose c1 large enough to make this hold We have proved that \\(T(n) = O(n^2)\\) Substitution Method: Example 2 \u00b6 For the recurrence \\(T(n) = 4T(n/2) + n\\) , prove that \\(T(n) = \\Omega(n^2)\\) i.e. \\(T(n) \u2265 cn^2\\) for any \\(n \\geq n_0\\) Ind. hyp: \\(T(k) \\geq ck^2\\) for any \\(k < n\\) Prove general case: \\(T(n) \\geq cn^2\\) \\(T(n) = 4T(n/2) + n\\) \\(\\geq 4c (n/2)^2 + n\\) \\(= cn^2 + n\\) \\(\\geq cn^2\\) since \\(n > 0\\) Proof succeeded \u2013 no need to strengthen the ind. hyp as in the last example Example 2 (cont\u2019d) \u00b6 We now need to prove that \\(T(n) \u2265 cn^2\\) for the base cases \\(T(n) = \\Theta(1)\\) for \\(1 \\leq n \\leq n_0\\) (implicit assumption) \\(\\Theta(1) \\geq cn^2\\) for \\(n = n_0\\) \\(n_0\\) is sufficiently small (i.e. constant) We can choose \\(c\\) small enough for this to hold We have proved that \\(T(n) = \\Omega(n^2)\\) Substitution Method - Summary \u00b6 Guess the asymptotic complexity Prove your guess using induction Assume inductive hypothesis holds for \\(k < n\\) Try to prove the general case for \\(n\\) Note: \\(MUST\\) prove the \\(EXACT\\) inequality \\(CANNOT\\) ignore lower order terms, If the proof fails, strengthen the ind. hyp. and try again Prove the base cases (usually straightforward) Recursion Tree Method \u00b6 A recursion tree models the runtime costs of a recursive execution of an algorithm. The recursion tree method is good for generating guesses for the substitution method. The recursion-tree method can be unreliable. Not suitable for formal proofs The recursion-tree method promotes intuition, however. Solve Recurrence : \\(T(n)=2T(n/2)+\\Theta(n)\\) \u00b6 Solve Recurrence : \\(T(n)=2T(n/2)+\\Theta(n)\\) \u00b6 Solve Recurrence : \\(T(n)=2T(n/2)+\\Theta(n)\\) \u00b6 Example of Recursion Tree \u00b6 Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\) Example of Recursion Tree \u00b6 Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\) Example of Recursion Tree \u00b6 Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\) The Master Method \u00b6 A powerful black-box method to solve recurrences. The master method applies to recurrences of the form \\(T(n) = aT(n/b) + f (n)\\) where \\(a \\geq 1, b > 1\\) , and \\(f\\) is asymptotically positive . The Master Method: 3 Cases \u00b6 (TODO : Add Notes ) Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Compare \\(f(n)\\) with \\(n^{log_b^a}\\) Intuitively: Case 1: \\(f(n)\\) grows polynomially slower than \\(n^{log_b^a}\\) Case 2: \\(f(n)\\) grows at the same rate as \\(n^{log_b^a}\\) Case 3: \\(f(n)\\) grows polynomially faster than \\(n^{log_b^a}\\) The Master Method: Case 1 (Bigger) \u00b6 Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon})\\) for some constant \\(\\varepsilon>0\\) i.e., \\(f(n)\\) grows polynomialy slower than \\(n^{log_b^a}\\) (by an \\(n^{\\varepsilon}\\) factor) Solution: \\(T(n)=\\Theta(n^{log_b^a})\\) The Master Method: Case 2 (Simple Version) (Equal) \u00b6 Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(1)\\) i.e., \\(f(n)\\) and \\(n^{log_b^a}\\) grow at similar rates Solution: \\(T(n)=\\Theta(n^{log_b^a}lgn)\\) The Master Method: Case 3 (Smaller) \u00b6 Case 3: \\(\\frac{f(n)}{n^{log_b^a}}=\\Omega(n^{\\varepsilon})\\) for some constant \\(\\varepsilon > 0\\) i.e., \\(f(n)\\) grows polynomialy faster than \\(n^{log_b^a}\\) (by an \\(n^{\\varepsilon}\\) factor) and the following regularity condition holds: \\(af(n/b) \\leq cf(n)\\) for some constant \\(c<1\\) Solution: \\(T(n)=\\Theta(f(n))\\) Example : \\(T(n)=4T(n/2)+n\\) \u00b6 \\(a=4\\) \\(b=2\\) \\(f(n)=n\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n\\) grows polynomially slower than \\(n^{log_b^a}=n^2\\) \\(\\frac{n^{log_b^a}}{f(n)}=\\frac{n^2}{n}=n=\\Omega(n^{\\varepsilon})\\) CASE-1: \\(T(n)=\\Theta(n^{log_b^a})=\\Theta(n^{log_2^4})=\\Theta(n^2)\\) Example : \\(T(n)=4T(n/2)+n^2\\) \u00b6 \\(a=4\\) \\(b=2\\) \\(f(n)=n^2\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^2\\) grows at similar rate as \\(n^{log_b^a}=n^2\\) \\(f(n)=\\Theta(n^{log_b^a})=n^2\\) CASE-2: \\(T(n)=\\Theta(n^{log_b^a}lgn)=\\Theta(n^{log_2^4}lgn)=\\Theta(n^2lgn)\\) Example : \\(T(n)=4T(n/2)+n^3\\) \u00b6 \\(a=4\\) \\(b=2\\) \\(f(n)=n^3\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^3\\) grows polynomially faster than \\(n^{log_b^a}=n^2\\) \\(\\frac{f(n)}{n^{log_b^a}}=\\frac{n^3}{n^2}=n=\\Omega(n^{\\varepsilon})\\) Example : \\(T(n)=4T(n/2)+n^3\\) (con't) \u00b6 Seems like CASE 3, but need to check the regularity condition Regularity condition \\(af(n/b) \\leq cf(n)\\) for some constant \\(c<1\\) \\(4(n/2)^3 \\leq cn^3\\) for \\(c=1/2\\) CASE-3: \\(T(n)=\\Theta(f(n))\\) \\(\\Longrightarrow\\) \\(T(n)=\\Theta(n^3)\\) Example : \\(T(n)=4T(n/2)+n^2lgn\\) \u00b6 \\(a=4\\) \\(b=2\\) \\(f(n)=n^2lgn\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^2lgn\\) grows slower than \\(n^{log_b^a}=n^2\\) but is it polynomially slower? \\(\\frac{n^{log_b^a}{f(n)}}=\\frac{n^2}{\\frac{n^2}{lgn}}=lgn \\neq \\Omega(n^{\\varepsilon})\\) for any \\(\\varepsilon>0\\) is not CASE-1 Master Method does not apply! The Master Method : Case 2 (General Version) \u00b6 Recurrence : \\(T(n) = aT(n/b) + f(n)\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn)\\) for some constant \\(k \\geq 0\\) Solution : \\(T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) General Method (Akra-Bazzi) \u00b6 \\(T(n)=\\sum_{i=1}^k{a_iT(n/b_i)}+f(n)\\) Let \\(p\\) be the unique solution to \\(\\sum_{i=1}^k{(a_i/b^p_i)}=1\\) Then, the answers are the same as for the master method, but with \\(n^p\\) instead of \\(n^{log_b^a}\\) (Akra and Bazzi also prove an even more general result.) Idea of Master Theorem \u00b6 Recursion Tree: Idea of Master Theorem \u00b6 CASE 1 : The weight increases geometrically from the root to the leaves. The leaves hold a constant fraction of the total weight. \\(n^{log_b^a}T(1)=\\Theta(n^{log_b^a})\\) Idea of Master Theorem \u00b6 CASE 2 : \\((k = 0)\\) The weight is approximately the same on each of the \\(log_bn\\) levels. \\(n^{log_b^a}T(1)=\\Theta(n^{log_b^a}lgn)\\) Idea of Master Theorem \u00b6 CASE 3 : The weight decreases geometrically from the root to the leaves. The root holds a constant fraction of the total weight. \\(n^{log_b^a}T(1)=\\Theta(f(n))\\) Proof of Master Theorem: Case 1 and Case 2 \u00b6 Recall from the recursion tree (note \\(h = lg_bn =\\text{tree height}\\) ) \\(\\text{Leaf Cost}=\\Theta(n^{log_b^a})\\) \\(\\text{Non-leaf Cost}=g(n)=\\sum_{i=0}^{h-1}a^if(n/{b^i})\\) \\(T(n)=\\text{Leaf Cost} + \\text{Non-leaf Cost}\\) \\(T(n)=\\Theta(n^{log_b^a}) + \\sum_{i=0}^{h-1}a^if(n/{b^i})\\) Proof Case 1 \u00b6 \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon})\\) for some \\(\\varepsilon>0\\) \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow O(n^{-\\varepsilon}) \\Longrightarrow f(n) = O(n^{log_b^{a-\\varepsilon}})\\) \\(g(n)=\\sum_{i=0}^{h-1}a^iO((n/{b^i})^{log_b^{a-\\varepsilon}})=O(\\sum_{i=0}^{h-1}a^i(n/{b^i})^{log_b^{a-\\varepsilon}})\\) \\(O(n^{log_b^{a-\\varepsilon}}\\sum_{i=0}^{h-1}a^ib^{i\\varepsilon}/b^{ilog_b^{a-\\varepsilon}})\\) Proof Case 1 (con't) \u00b6 \\(\\sum_{i=0}^{h-1} \\frac{a^ib^{i\\varepsilon}}{b^{ilog_b^a}} =\\sum_{i=0}^{h-1} a^i\\frac{(b^\\varepsilon)^i}{(b^{log_b^a})^i} =\\sum a^i\\frac{b^{i\\varepsilon}}{a^i}=\\sum_{i=0}^{h-1}(b^{\\varepsilon})^i\\) = An increasing geometric series since \\(b > 1\\) \\(\\frac{b^{h\\varepsilon}-1}{b^{\\varepsilon}-1}=\\frac{(b^h)^{\\varepsilon}-1}{b^{\\varepsilon}-1} = \\frac{(b^{log_b^n})^{\\varepsilon}-1}{b^{\\varepsilon}-1}=\\frac{n^{\\varepsilon}-1}{b^{\\varepsilon}-1} = O(n^{\\varepsilon})\\) Proof Case 1 (con't) \u00b6 \\(g(n)=O(n^{log_b{a-\\varepsilon}}O(n^{\\varepsilon}))=O(\\frac{n^{log_b^a}}{n^{\\varepsilon}}O(n^{\\varepsilon}))=O(n^{log_b^a})\\) \\(T(n)=\\Theta(n^{log_b^a})+g(n)=\\Theta(n^{log_b^a})+O(n^{log_b^a})=\\Theta(n^{log_b^a})\\) Q.E.D. (Quod Erat Demonstrandum) Proof of Case 2 (limited to k=0) \u00b6 \\(\\frac{f(n)}{n^log_b^a}=\\Theta(lg^0n)=\\Theta(1) \\Longrightarrow f(n)=\\Theta(n^{log_b^a}) \\Longrightarrow f(n/b^i)=\\Theta((n/b^i)^{log_b^a})\\) \\(g(n)=\\sum_{i=0}^{h-1}a^i\\Theta((n/b^i)^{log_b^a})\\) \\(= \\Theta(\\sum_{i=0}^{h-1}a^i\\frac{n^{log_b^a}}{b^{ilog_b^a}})\\) \\(=\\Theta(n^{log_b^a}\\sum_{i=0}^{h-1}a^i\\frac{1}{(b^{log_b^a})^i})\\) \\(=\\Theta(n^{log_b^a}\\sum_{i=0}^{h-1}a^i\\frac{1}{a^i})\\) \\(=\\Theta(n^{log_b^a}\\sum_{i=0}^{log_b^{n-1}}1) = \\Theta(n^{log_b^a}log_bn)=\\Theta(n^{log_b^a}lgn)\\) \\(T(n)=n^{log_b^a}+\\Theta(n^{log_b^a}lgn)\\) \\(=\\Theta(n^{log_b^a}lgn)\\) Q.E.D. The Divide-and-Conquer Design Paradigm \u00b6 The Divide-and-Conquer Design Paradigm \u00b6 Divide we divide the problem into a number of subproblems. Conquer we solve the subproblems recursively. BaseCase solve by Brute-Force Combine subproblem solutions to the original problem. The Divide-and-Conquer Design Paradigm \u00b6 \\(a=\\text{subproblem}\\) \\(1/b=\\text{each size of the problem}\\) \\[ T(n)=\\begin{cases} \\Theta(1) & \\text{if} & n \\leq c & (basecase) \\\\ aT(n/b)+D(n)+C(n) & \\text{otherwise} \\end{cases} \\] Merge-Sort \\[ T(n)=\\begin{cases} \\Theta(1) & & n = 1 \\\\ 2T(n/2)+\\Theta(n) & \\text{if} & n>1 \\end{cases} \\] \\(T(n)=\\Theta(nlgn)\\) Selection Sort \u00b6 SELECTION - SORT ( A ) n = A.length ; for j = 1 to n -1 smallest = j ; for i = j +1 to n if A [ i ] < A [ smallest ] smallest = i ; endfor exchange A [ j ] with A [ smallest ] endfor Selection Sort \u00b6 \\[ T(n)=\\begin{cases} \\Theta(1) & & n = 1 \\\\ T(n-1)+\\Theta(n) & \\text{if} & n>1 \\end{cases} \\] Sequential Series \\[ cost = n(n+1)/2 = {1/2}n^2 +{1/2}n \\] Drop low-order terms Ignore the constant coefficient in the leading term \\[ T(n)=\\Theta(n^2) \\] Merge Sort Algorithm (initial setup) \u00b6 Merge Sort is a recursive sorting algorithm, for initial case we need to call Merge-Sort(A,1,n) for sorting \\(A[1..n]\\) initial case A : Array p : 1 ( offset ) r : n ( length ) Merge - Sort ( A , 1 , n ) Merge Sort Algorithm (internal iterations) \u00b6 internal iterations \\(p = start-point\\) \\(q = mid-point\\) \\(r = end-point\\) A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif Merge Sort Combine Algorithm (1) \u00b6 Merge ( A , p , q , r ) n1 = q - p +1 n2 = r - q // allocate left and right arrays // increment will be from left to right // left part will be bigger than right part L [ 1 ...n1 +1 ] // left array R [ 1 ...n2 +1 ] // right array // copy left part of array for i = 1 to n1 L [ i ] = A [ p + i -1 ] // copy right part of array for j = 1 to n2 R [ j ] = A [ q + j ] // put end items maximum values for termination L [ n1 +1 ] = inf R [ n2 +1 ] = inf i = 1 , j = 1 for k = p to r if L [ i ] <= R [ j ] A [ k ] = L [ i ] i = i +1 else A [ k ] = R [ j ] j = j +1 Example : Merge Sort \u00b6 Divide: Trivial. Conquer: Recursively sort 2 subarrays. Combine: Linear- time merge. \\(T(n)=2T(n/2)+\\Theta(n)\\) Subproblems \\(\\Longrightarrow 2\\) Subproblemsize \\(\\Longrightarrow n/2\\) Work dividing and combining \\(\\Longrightarrow\\Theta(n)\\) Master Theorem: Reminder \u00b6 \\(T(n) = aT(n/b) + f(n)\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(n^{log_b^a})\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) Case 3: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(f(n))\\) and \\(af(n/b) \\leq cf(n)\\) for \\(c<1\\) Merge Sort: Solving the Recurrence \u00b6 \\(T(n)=2T(n/2)+\\Theta(n)\\) \\(a=2,b=2,f(n)=\\Theta(n),n^{log_b^a}=n\\) Case-2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(nlgn)\\) Binary Search \u00b6 Find an element in a sorted array: 1. Divide: Check middle element. 2. Conquer: Recursively search 1 subarray. 3. Combine: Trivial. Binary Search : \u00b6 \\[ \\text{PARENT} = \\lfloor i/2 \\rfloor \\] \\[ \\text{LEFT-CHILD} = 2i, \\text{ 2i>n} \\] \\[ \\text{RIGHT-CHILD} = 2i+1, \\text{ 2i>n} \\] Binary Search : Iterative \u00b6 ITERATIVE - BINARY - SEARCH ( A , V , low , high ) while low <= high mid = floor (( low + high ) / 2 ); if v == A [ mid ] return mid ; elseif v > A [ mid ] low = mid + 1 ; else high = mid - 1 ; endwhile return NIL Binary Search : Recursive \u00b6 RECURSIVE - BINARY - SEARCH ( A , V , low , high ) if low > high return NIL ; endif mid = floor (( low + high ) / 2 ); if v == A [ mid ] return mid ; elseif v > A [ mid ] return RECURSIVE - BINARY - SEARCH ( A , V , mid +1 , high ); else return RECURSIVE - BINARY - SEARCH ( A , V , low , mid -1 ); endif Binary Search : Recursive \u00b6 \\[ T(n)=T(n/2)+\\Theta(1) \\Longrightarrow T(n)=\\Theta(lgn) \\] Example: Find 9 \u00b6 Recurrence for Binary Search \u00b6 \\(T(n)=1T(n/2)+\\Theta(1)\\) Subproblems \\(\\Longrightarrow 1\\) Subproblemsize \\(\\Longrightarrow n/2\\) Work dividing and combining \\(\\Longrightarrow\\Theta(1)\\) Binary Search: Solving the Recurrence \u00b6 \\(T(n) = T(n/2) + \\Theta(1)\\) \\(a = 1,b = 2,f(n) = \\Theta(1) \\Longrightarrow n^{log_b^a} = n^0=1\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(lgn)\\) Powering a Number \u00b6 Problem : Compute an, where n is a natural number NAIVE - POWER ( a , n ) powerVal = 1 ; for i = 1 to n powerVal = powerVal * a ; endfor return powerVal ; What is the complexity? \\(\\Longrightarrow T(n)=\\Theta(n)\\) Powering a Number: Divide & Conquer \u00b6 Basic Idea: \\[ a^n=\\begin{cases} a^{n/2}*a^{n/2} & \\text{if n is even} \\\\ a^{(n-1)/2}*a^{(n-1)/2}*a & \\text{if n is odd} \\end{cases} \\] Powering a Number: Divide & Conquer \u00b6 POWER ( a , n ) if n = 0 then return 1 ; else if n is even then val = POWER ( a , n / 2 ); return val * val ; else if n is odd then val = POWER ( a ,( n -1 ) / 2 ) return val * val * a ; endif Powering a Number: Solving the Recurrence \u00b6 \\(T(n) = T(n/2) + \\Theta(1)\\) \\(a = 1,b = 2,f(n) = \\Theta(1) \\Longrightarrow n^{log_b^a} = n^0=1\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(lgn)\\) References \u00b6 TODO \u00b6","title":"Week-2 (Solving Recurrences)"},{"location":"week-2/ce100-week-2-recurrence/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-2/ce100-week-2-recurrence/#week-2-solving-recurrences-the-divide-and-conquer","text":"","title":"Week-2 (Solving Recurrences / The Divide-and-Conquer)"},{"location":"week-2/ce100-week-2-recurrence/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-2/ce100-week-2-recurrence/#solving-recurrences","text":"","title":"Solving Recurrences"},{"location":"week-2/ce100-week-2-recurrence/#outline","text":"Solving Recurrences Recursion Tree Master Method Back-Substitution","title":"Outline"},{"location":"week-2/ce100-week-2-recurrence/#outline_1","text":"Divide-and-Conquer Analysis Merge Sort Binary Search Merge Sort Analysis Complexity","title":"Outline"},{"location":"week-2/ce100-week-2-recurrence/#outline_2","text":"Recurrence Solution","title":"Outline"},{"location":"week-2/ce100-week-2-recurrence/#solving-recurrences_1","text":"Reminder: Runtime \\((T(n))\\) of MergeSort was expressed as a recurrence \\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\] Solving recurrences is like solving differential equations, integrals, etc. Need to learn a few tricks","title":"Solving Recurrences"},{"location":"week-2/ce100-week-2-recurrence/#recurrences","text":"Recurrence : An equation or inequality that describes a function in terms of its value on smaller inputs. Example : \\[ T(n)=\\begin{cases} \\ 1 &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ 1 &\\text{if n>1}\\end{cases} \\]","title":"Recurrences"},{"location":"week-2/ce100-week-2-recurrence/#recurrence-example","text":"\\[ T(n)=\\begin{cases} \\ 1 &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ 1 &\\text{if n>1}\\end{cases} \\] Simplification: Assume \\(n=2^k\\) Claimed answer : \\(T(n)=lgn+1\\) Substitute claimed answer in the recurrence: \\[ lgn+1=\\begin{cases} \\ 1 &\\text{if n=1} \\\\ lg(\\lceil{n/2}\\rceil)+ 2 &\\text{if n>1}\\end{cases} \\] True when \\(n=2^k\\)","title":"Recurrence Example"},{"location":"week-2/ce100-week-2-recurrence/#technicalities-floor-ceiling","text":"Technically, should be careful about the floor and ceiling functions (as in the book). e.g. For merge sort, the recurrence should in fact be:, \\[ T(n)=\\begin{cases} \\ \\Theta(1) &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ T(\\lfloor{n/2}\\rfloor) +\\Theta(n) &\\text{if n>1} \\end{cases} \\] But, it's usually ok to: ignore floor/ceiling solve for the exact power of 2 (or another number)","title":"Technicalities: Floor / Ceiling"},{"location":"week-2/ce100-week-2-recurrence/#technicalities-boundary-conditions","text":"Usually assume: \\(T(n) = \\Theta(1)\\) for sufficiently small \\(n\\) Changes the exact solution, but usually the asymptotic solution is not affected (e.g. if polynomially bounded) For convenience, the boundary conditions generally implicitly stated in a recurrence \\(T(n) = 2T(n/2) + \\Theta(n)\\) assuming that \\(T(n)=\\Theta(1)\\) for sufficiently small \\(n\\)","title":"Technicalities: Boundary Conditions"},{"location":"week-2/ce100-week-2-recurrence/#example-when-boundary-conditions-matter","text":"Exponential function: \\(T(n) = (T(n/2))2\\) Assume \\(T(1) = c \\text{ (where c is a positive constant)}\\) \\(T(2) = (T(1))^2 = c^2\\) \\(T(4) = (T(2))^2 = c^4\\) \\(T(n) = \\Theta(c^n)\\) e.g. \\[ \\begin{rcases} T(1)= 2 &\\Rightarrow & T(n)= \\Theta(2^n) \\\\ T(1)= 3 &\\Rightarrow & T(n)= \\Theta(3^n) \\end{rcases} \\text{ However } \\Theta(2^n) \\neq \\Theta(3^n) \\] The difference in solution more dramatic when: \\[ T(1) = 1 \\Rightarrow T(n) = \\Theta(1^n) = \\Theta(1) \\]","title":"Example: When Boundary Conditions Matter"},{"location":"week-2/ce100-week-2-recurrence/#solving-recurrences_2","text":"We will focus on 3 techniques Substitution method Recursion tree approach Master method","title":"Solving Recurrences"},{"location":"week-2/ce100-week-2-recurrence/#substitution-method","text":"The most general method: Guess Prove by induction Solve for constants","title":"Substitution Method"},{"location":"week-2/ce100-week-2-recurrence/#substitution-method-example","text":"Solve \\(T(n)=4T(n/2)+n\\) (assume \\(T(1)= \\Theta(1)\\) ) Guess \\(T(n) = O(n^3)\\) (need to prove \\(O\\) and \\(\\Omega\\) separately) Prove by induction that \\(T(n) \\leq cn^3\\) for large \\(n\\) (i.e. \\(n \\geq n_0\\) ) Inductive hypothesis: \\(T(k) \\leq ck^3\\) for any \\(k < n\\) Assuming ind. hyp. holds, prove \\(T(n) \\leq cn^3\\)","title":"Substitution Method: Example"},{"location":"week-2/ce100-week-2-recurrence/#substitution-method-example-contd","text":"Original recurrence: \\(T(n) = 4T(n/2) + n\\) From inductive hypothesis: \\(T(n/2) \\leq c(n/2)^3\\) Substitute this into the original recurrence: \\(T(n) \\leq 4c(n/2)^3 + n\\) \\(= (c/2)n^3 + n\\) \\(= cn^3 \u2013 ((c/2)n^3 \u2013 n)\\) \\(\\Longleftarrow\\) desired - residual \\(\\leq cn^3\\) when \\(((c/2)n^3 \u2013 n) \\geq 0\\)","title":"Substitution Method: Example \u2013 cont\u2019d"},{"location":"week-2/ce100-week-2-recurrence/#substitution-method-example-contd_1","text":"So far, we have shown: \\[ T(n) \\leq cn^3 \\text{ when } ((c/2)n^3 \u2013 n) \\geq 0 \\] We can choose \\(c \\geq 2\\) and \\(n_0 \\geq 1\\) But, the proof is not complete yet. Reminder: Proof by induction: 1.Prove the base cases \\(\\Longleftarrow\\) haven\u2019t proved the base cases yet 2.Inductive hypothesis for smaller sizes 3.Prove the general case","title":"Substitution Method: Example \u2013 cont\u2019d"},{"location":"week-2/ce100-week-2-recurrence/#substitution-method-example-contd_2","text":"We need to prove the base cases Base: \\(T(n) = \\Theta(1)\\) for small \\(n\\) (e.g. for \\(n = n_0\\) ) We should show that: \\(\\Theta(1) \\leq cn^3\\) for \\(n = n_0\\) , This holds if we pick \\(c\\) big enough So, the proof of \\(T(n) = O(n^3)\\) is complete But, is this a tight bound?","title":"Substitution Method: Example \u2013 cont\u2019d"},{"location":"week-2/ce100-week-2-recurrence/#example-a-tighter-upper-bound","text":"Original recurrence: \\(T(n) = 4T(n/2) + n\\) Try to prove that \\(T(n) = O(n^2)\\) , i.e. \\(T(n) \\leq cn^2\\) for all \\(n \\geq n_0\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\)","title":"Example: A tighter upper bound?"},{"location":"week-2/ce100-week-2-recurrence/#example-contd","text":"Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\) \\[ T(n) = 4T(n/2) + n \\\\ \u2264 4c(n/2)^2 + n \\\\ = cn^2 + n \\\\ = O(n2) \\Longleftarrow \\text{ Wrong! We must prove exactly} \\]","title":"Example (cont\u2019d)"},{"location":"week-2/ce100-week-2-recurrence/#example-contd_1","text":"Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\) So far, we have: \\(T(n) \\leq cn^2 + n\\) No matter which positive c value we choose, this does not show that \\(T(n) \\leq cn^2\\) Proof failed?","title":"Example (cont\u2019d)"},{"location":"week-2/ce100-week-2-recurrence/#example-contd_2","text":"What was the problem? The inductive hypothesis was not strong enough Idea: Start with a stronger inductive hypothesis Subtract a low-order term Inductive hypothesis: \\(T(k) \\leq c_1k^2 \u2013 c_2k\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq c_1n^2 - c_2n\\)","title":"Example (cont\u2019d)"},{"location":"week-2/ce100-week-2-recurrence/#example-contd_3","text":"Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq c_1k^2 \u2013 c_2k\\) for \\(k < n\\) Prove the general case: \\(T(n) \u2264 c_1n^2 \u2013 c_2n\\) \\(T(n) = 4T(n/2) + n\\) \\(\u2264 4 (c_1(n/2)^2 \u2013 c_2(n/2)) + n\\) \\(= c_1n^2 \u2013 2c_2n + n\\) \\(= c_1n^2 \u2013 c_2n \u2013 (c_2n \u2013 n)\\) \\(\u2264 c_1n^2 \u2013 c_2n\\) for \\(n(c_2 \u2013 1) \\geq 0\\) choose \\(c2 \\geq 1\\)","title":"Example (cont\u2019d)"},{"location":"week-2/ce100-week-2-recurrence/#example-contd_4","text":"We now need to prove $$ T(n) \\leq c_1n^2 \u2013 c_2n $$ for the base cases. \\(T(n) = \\Theta(1) \\text{ for } 1 \\leq n \\leq n_0\\) (implicit assumption) \\(\\Theta(1) \\leq c_1n^2 \u2013 c_2n\\) for \\(n\\) small enough (e.g. \\(n = n_0\\) ) We can choose c1 large enough to make this hold We have proved that \\(T(n) = O(n^2)\\)","title":"Example (cont\u2019d)"},{"location":"week-2/ce100-week-2-recurrence/#substitution-method-example-2","text":"For the recurrence \\(T(n) = 4T(n/2) + n\\) , prove that \\(T(n) = \\Omega(n^2)\\) i.e. \\(T(n) \u2265 cn^2\\) for any \\(n \\geq n_0\\) Ind. hyp: \\(T(k) \\geq ck^2\\) for any \\(k < n\\) Prove general case: \\(T(n) \\geq cn^2\\) \\(T(n) = 4T(n/2) + n\\) \\(\\geq 4c (n/2)^2 + n\\) \\(= cn^2 + n\\) \\(\\geq cn^2\\) since \\(n > 0\\) Proof succeeded \u2013 no need to strengthen the ind. hyp as in the last example","title":"Substitution Method: Example 2"},{"location":"week-2/ce100-week-2-recurrence/#example-2-contd","text":"We now need to prove that \\(T(n) \u2265 cn^2\\) for the base cases \\(T(n) = \\Theta(1)\\) for \\(1 \\leq n \\leq n_0\\) (implicit assumption) \\(\\Theta(1) \\geq cn^2\\) for \\(n = n_0\\) \\(n_0\\) is sufficiently small (i.e. constant) We can choose \\(c\\) small enough for this to hold We have proved that \\(T(n) = \\Omega(n^2)\\)","title":"Example 2 (cont\u2019d)"},{"location":"week-2/ce100-week-2-recurrence/#substitution-method-summary","text":"Guess the asymptotic complexity Prove your guess using induction Assume inductive hypothesis holds for \\(k < n\\) Try to prove the general case for \\(n\\) Note: \\(MUST\\) prove the \\(EXACT\\) inequality \\(CANNOT\\) ignore lower order terms, If the proof fails, strengthen the ind. hyp. and try again Prove the base cases (usually straightforward)","title":"Substitution Method - Summary"},{"location":"week-2/ce100-week-2-recurrence/#recursion-tree-method","text":"A recursion tree models the runtime costs of a recursive execution of an algorithm. The recursion tree method is good for generating guesses for the substitution method. The recursion-tree method can be unreliable. Not suitable for formal proofs The recursion-tree method promotes intuition, however.","title":"Recursion Tree Method"},{"location":"week-2/ce100-week-2-recurrence/#solve-recurrence-tn2tn2thetan","text":"","title":"Solve Recurrence : \\(T(n)=2T(n/2)+\\Theta(n)\\)"},{"location":"week-2/ce100-week-2-recurrence/#solve-recurrence-tn2tn2thetan_1","text":"","title":"Solve Recurrence : \\(T(n)=2T(n/2)+\\Theta(n)\\)"},{"location":"week-2/ce100-week-2-recurrence/#solve-recurrence-tn2tn2thetan_2","text":"","title":"Solve Recurrence : \\(T(n)=2T(n/2)+\\Theta(n)\\)"},{"location":"week-2/ce100-week-2-recurrence/#example-of-recursion-tree","text":"Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\)","title":"Example of Recursion Tree"},{"location":"week-2/ce100-week-2-recurrence/#example-of-recursion-tree_1","text":"Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\)","title":"Example of Recursion Tree"},{"location":"week-2/ce100-week-2-recurrence/#example-of-recursion-tree_2","text":"Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\)","title":"Example of Recursion Tree"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method","text":"A powerful black-box method to solve recurrences. The master method applies to recurrences of the form \\(T(n) = aT(n/b) + f (n)\\) where \\(a \\geq 1, b > 1\\) , and \\(f\\) is asymptotically positive .","title":"The Master Method"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method-3-cases","text":"(TODO : Add Notes ) Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Compare \\(f(n)\\) with \\(n^{log_b^a}\\) Intuitively: Case 1: \\(f(n)\\) grows polynomially slower than \\(n^{log_b^a}\\) Case 2: \\(f(n)\\) grows at the same rate as \\(n^{log_b^a}\\) Case 3: \\(f(n)\\) grows polynomially faster than \\(n^{log_b^a}\\)","title":"The Master Method: 3 Cases"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method-case-1-bigger","text":"Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon})\\) for some constant \\(\\varepsilon>0\\) i.e., \\(f(n)\\) grows polynomialy slower than \\(n^{log_b^a}\\) (by an \\(n^{\\varepsilon}\\) factor) Solution: \\(T(n)=\\Theta(n^{log_b^a})\\)","title":"The Master Method: Case 1 (Bigger)"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method-case-2-simple-version-equal","text":"Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(1)\\) i.e., \\(f(n)\\) and \\(n^{log_b^a}\\) grow at similar rates Solution: \\(T(n)=\\Theta(n^{log_b^a}lgn)\\)","title":"The Master Method: Case 2 (Simple Version) (Equal)"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method-case-3-smaller","text":"Case 3: \\(\\frac{f(n)}{n^{log_b^a}}=\\Omega(n^{\\varepsilon})\\) for some constant \\(\\varepsilon > 0\\) i.e., \\(f(n)\\) grows polynomialy faster than \\(n^{log_b^a}\\) (by an \\(n^{\\varepsilon}\\) factor) and the following regularity condition holds: \\(af(n/b) \\leq cf(n)\\) for some constant \\(c<1\\) Solution: \\(T(n)=\\Theta(f(n))\\)","title":"The Master Method: Case 3 (Smaller)"},{"location":"week-2/ce100-week-2-recurrence/#example-tn4tn2n","text":"\\(a=4\\) \\(b=2\\) \\(f(n)=n\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n\\) grows polynomially slower than \\(n^{log_b^a}=n^2\\) \\(\\frac{n^{log_b^a}}{f(n)}=\\frac{n^2}{n}=n=\\Omega(n^{\\varepsilon})\\) CASE-1: \\(T(n)=\\Theta(n^{log_b^a})=\\Theta(n^{log_2^4})=\\Theta(n^2)\\)","title":"Example : \\(T(n)=4T(n/2)+n\\)"},{"location":"week-2/ce100-week-2-recurrence/#example-tn4tn2n2","text":"\\(a=4\\) \\(b=2\\) \\(f(n)=n^2\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^2\\) grows at similar rate as \\(n^{log_b^a}=n^2\\) \\(f(n)=\\Theta(n^{log_b^a})=n^2\\) CASE-2: \\(T(n)=\\Theta(n^{log_b^a}lgn)=\\Theta(n^{log_2^4}lgn)=\\Theta(n^2lgn)\\)","title":"Example : \\(T(n)=4T(n/2)+n^2\\)"},{"location":"week-2/ce100-week-2-recurrence/#example-tn4tn2n3","text":"\\(a=4\\) \\(b=2\\) \\(f(n)=n^3\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^3\\) grows polynomially faster than \\(n^{log_b^a}=n^2\\) \\(\\frac{f(n)}{n^{log_b^a}}=\\frac{n^3}{n^2}=n=\\Omega(n^{\\varepsilon})\\)","title":"Example : \\(T(n)=4T(n/2)+n^3\\)"},{"location":"week-2/ce100-week-2-recurrence/#example-tn4tn2n3-cont","text":"Seems like CASE 3, but need to check the regularity condition Regularity condition \\(af(n/b) \\leq cf(n)\\) for some constant \\(c<1\\) \\(4(n/2)^3 \\leq cn^3\\) for \\(c=1/2\\) CASE-3: \\(T(n)=\\Theta(f(n))\\) \\(\\Longrightarrow\\) \\(T(n)=\\Theta(n^3)\\)","title":"Example : \\(T(n)=4T(n/2)+n^3\\) (con't)"},{"location":"week-2/ce100-week-2-recurrence/#example-tn4tn2n2lgn","text":"\\(a=4\\) \\(b=2\\) \\(f(n)=n^2lgn\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^2lgn\\) grows slower than \\(n^{log_b^a}=n^2\\) but is it polynomially slower? \\(\\frac{n^{log_b^a}{f(n)}}=\\frac{n^2}{\\frac{n^2}{lgn}}=lgn \\neq \\Omega(n^{\\varepsilon})\\) for any \\(\\varepsilon>0\\) is not CASE-1 Master Method does not apply!","title":"Example : \\(T(n)=4T(n/2)+n^2lgn\\)"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method-case-2-general-version","text":"Recurrence : \\(T(n) = aT(n/b) + f(n)\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn)\\) for some constant \\(k \\geq 0\\) Solution : \\(T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\)","title":"The Master Method : Case 2 (General Version)"},{"location":"week-2/ce100-week-2-recurrence/#general-method-akra-bazzi","text":"\\(T(n)=\\sum_{i=1}^k{a_iT(n/b_i)}+f(n)\\) Let \\(p\\) be the unique solution to \\(\\sum_{i=1}^k{(a_i/b^p_i)}=1\\) Then, the answers are the same as for the master method, but with \\(n^p\\) instead of \\(n^{log_b^a}\\) (Akra and Bazzi also prove an even more general result.)","title":"General Method (Akra-Bazzi)"},{"location":"week-2/ce100-week-2-recurrence/#idea-of-master-theorem","text":"Recursion Tree:","title":"Idea of Master Theorem"},{"location":"week-2/ce100-week-2-recurrence/#idea-of-master-theorem_1","text":"CASE 1 : The weight increases geometrically from the root to the leaves. The leaves hold a constant fraction of the total weight. \\(n^{log_b^a}T(1)=\\Theta(n^{log_b^a})\\)","title":"Idea of Master Theorem"},{"location":"week-2/ce100-week-2-recurrence/#idea-of-master-theorem_2","text":"CASE 2 : \\((k = 0)\\) The weight is approximately the same on each of the \\(log_bn\\) levels. \\(n^{log_b^a}T(1)=\\Theta(n^{log_b^a}lgn)\\)","title":"Idea of Master Theorem"},{"location":"week-2/ce100-week-2-recurrence/#idea-of-master-theorem_3","text":"CASE 3 : The weight decreases geometrically from the root to the leaves. The root holds a constant fraction of the total weight. \\(n^{log_b^a}T(1)=\\Theta(f(n))\\)","title":"Idea of Master Theorem"},{"location":"week-2/ce100-week-2-recurrence/#proof-of-master-theorem-case-1-and-case-2","text":"Recall from the recursion tree (note \\(h = lg_bn =\\text{tree height}\\) ) \\(\\text{Leaf Cost}=\\Theta(n^{log_b^a})\\) \\(\\text{Non-leaf Cost}=g(n)=\\sum_{i=0}^{h-1}a^if(n/{b^i})\\) \\(T(n)=\\text{Leaf Cost} + \\text{Non-leaf Cost}\\) \\(T(n)=\\Theta(n^{log_b^a}) + \\sum_{i=0}^{h-1}a^if(n/{b^i})\\)","title":"Proof of Master Theorem: Case 1 and Case 2"},{"location":"week-2/ce100-week-2-recurrence/#proof-case-1","text":"\\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon})\\) for some \\(\\varepsilon>0\\) \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow O(n^{-\\varepsilon}) \\Longrightarrow f(n) = O(n^{log_b^{a-\\varepsilon}})\\) \\(g(n)=\\sum_{i=0}^{h-1}a^iO((n/{b^i})^{log_b^{a-\\varepsilon}})=O(\\sum_{i=0}^{h-1}a^i(n/{b^i})^{log_b^{a-\\varepsilon}})\\) \\(O(n^{log_b^{a-\\varepsilon}}\\sum_{i=0}^{h-1}a^ib^{i\\varepsilon}/b^{ilog_b^{a-\\varepsilon}})\\)","title":"Proof Case 1"},{"location":"week-2/ce100-week-2-recurrence/#proof-case-1-cont","text":"\\(\\sum_{i=0}^{h-1} \\frac{a^ib^{i\\varepsilon}}{b^{ilog_b^a}} =\\sum_{i=0}^{h-1} a^i\\frac{(b^\\varepsilon)^i}{(b^{log_b^a})^i} =\\sum a^i\\frac{b^{i\\varepsilon}}{a^i}=\\sum_{i=0}^{h-1}(b^{\\varepsilon})^i\\) = An increasing geometric series since \\(b > 1\\) \\(\\frac{b^{h\\varepsilon}-1}{b^{\\varepsilon}-1}=\\frac{(b^h)^{\\varepsilon}-1}{b^{\\varepsilon}-1} = \\frac{(b^{log_b^n})^{\\varepsilon}-1}{b^{\\varepsilon}-1}=\\frac{n^{\\varepsilon}-1}{b^{\\varepsilon}-1} = O(n^{\\varepsilon})\\)","title":"Proof Case 1 (con't)"},{"location":"week-2/ce100-week-2-recurrence/#proof-case-1-cont_1","text":"\\(g(n)=O(n^{log_b{a-\\varepsilon}}O(n^{\\varepsilon}))=O(\\frac{n^{log_b^a}}{n^{\\varepsilon}}O(n^{\\varepsilon}))=O(n^{log_b^a})\\) \\(T(n)=\\Theta(n^{log_b^a})+g(n)=\\Theta(n^{log_b^a})+O(n^{log_b^a})=\\Theta(n^{log_b^a})\\) Q.E.D. (Quod Erat Demonstrandum)","title":"Proof Case 1 (con't)"},{"location":"week-2/ce100-week-2-recurrence/#proof-of-case-2-limited-to-k0","text":"\\(\\frac{f(n)}{n^log_b^a}=\\Theta(lg^0n)=\\Theta(1) \\Longrightarrow f(n)=\\Theta(n^{log_b^a}) \\Longrightarrow f(n/b^i)=\\Theta((n/b^i)^{log_b^a})\\) \\(g(n)=\\sum_{i=0}^{h-1}a^i\\Theta((n/b^i)^{log_b^a})\\) \\(= \\Theta(\\sum_{i=0}^{h-1}a^i\\frac{n^{log_b^a}}{b^{ilog_b^a}})\\) \\(=\\Theta(n^{log_b^a}\\sum_{i=0}^{h-1}a^i\\frac{1}{(b^{log_b^a})^i})\\) \\(=\\Theta(n^{log_b^a}\\sum_{i=0}^{h-1}a^i\\frac{1}{a^i})\\) \\(=\\Theta(n^{log_b^a}\\sum_{i=0}^{log_b^{n-1}}1) = \\Theta(n^{log_b^a}log_bn)=\\Theta(n^{log_b^a}lgn)\\) \\(T(n)=n^{log_b^a}+\\Theta(n^{log_b^a}lgn)\\) \\(=\\Theta(n^{log_b^a}lgn)\\) Q.E.D.","title":"Proof of Case 2 (limited to k=0)"},{"location":"week-2/ce100-week-2-recurrence/#the-divide-and-conquer-design-paradigm","text":"","title":"The Divide-and-Conquer Design Paradigm"},{"location":"week-2/ce100-week-2-recurrence/#the-divide-and-conquer-design-paradigm_1","text":"Divide we divide the problem into a number of subproblems. Conquer we solve the subproblems recursively. BaseCase solve by Brute-Force Combine subproblem solutions to the original problem.","title":"The Divide-and-Conquer Design Paradigm"},{"location":"week-2/ce100-week-2-recurrence/#the-divide-and-conquer-design-paradigm_2","text":"\\(a=\\text{subproblem}\\) \\(1/b=\\text{each size of the problem}\\) \\[ T(n)=\\begin{cases} \\Theta(1) & \\text{if} & n \\leq c & (basecase) \\\\ aT(n/b)+D(n)+C(n) & \\text{otherwise} \\end{cases} \\] Merge-Sort \\[ T(n)=\\begin{cases} \\Theta(1) & & n = 1 \\\\ 2T(n/2)+\\Theta(n) & \\text{if} & n>1 \\end{cases} \\] \\(T(n)=\\Theta(nlgn)\\)","title":"The Divide-and-Conquer Design Paradigm"},{"location":"week-2/ce100-week-2-recurrence/#selection-sort","text":"SELECTION - SORT ( A ) n = A.length ; for j = 1 to n -1 smallest = j ; for i = j +1 to n if A [ i ] < A [ smallest ] smallest = i ; endfor exchange A [ j ] with A [ smallest ] endfor","title":"Selection Sort"},{"location":"week-2/ce100-week-2-recurrence/#selection-sort_1","text":"\\[ T(n)=\\begin{cases} \\Theta(1) & & n = 1 \\\\ T(n-1)+\\Theta(n) & \\text{if} & n>1 \\end{cases} \\] Sequential Series \\[ cost = n(n+1)/2 = {1/2}n^2 +{1/2}n \\] Drop low-order terms Ignore the constant coefficient in the leading term \\[ T(n)=\\Theta(n^2) \\]","title":"Selection Sort"},{"location":"week-2/ce100-week-2-recurrence/#merge-sort-algorithm-initial-setup","text":"Merge Sort is a recursive sorting algorithm, for initial case we need to call Merge-Sort(A,1,n) for sorting \\(A[1..n]\\) initial case A : Array p : 1 ( offset ) r : n ( length ) Merge - Sort ( A , 1 , n )","title":"Merge Sort Algorithm (initial setup)"},{"location":"week-2/ce100-week-2-recurrence/#merge-sort-algorithm-internal-iterations","text":"internal iterations \\(p = start-point\\) \\(q = mid-point\\) \\(r = end-point\\) A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif","title":"Merge Sort Algorithm (internal iterations)"},{"location":"week-2/ce100-week-2-recurrence/#merge-sort-combine-algorithm-1","text":"Merge ( A , p , q , r ) n1 = q - p +1 n2 = r - q // allocate left and right arrays // increment will be from left to right // left part will be bigger than right part L [ 1 ...n1 +1 ] // left array R [ 1 ...n2 +1 ] // right array // copy left part of array for i = 1 to n1 L [ i ] = A [ p + i -1 ] // copy right part of array for j = 1 to n2 R [ j ] = A [ q + j ] // put end items maximum values for termination L [ n1 +1 ] = inf R [ n2 +1 ] = inf i = 1 , j = 1 for k = p to r if L [ i ] <= R [ j ] A [ k ] = L [ i ] i = i +1 else A [ k ] = R [ j ] j = j +1","title":"Merge Sort Combine Algorithm (1)"},{"location":"week-2/ce100-week-2-recurrence/#example-merge-sort","text":"Divide: Trivial. Conquer: Recursively sort 2 subarrays. Combine: Linear- time merge. \\(T(n)=2T(n/2)+\\Theta(n)\\) Subproblems \\(\\Longrightarrow 2\\) Subproblemsize \\(\\Longrightarrow n/2\\) Work dividing and combining \\(\\Longrightarrow\\Theta(n)\\)","title":"Example : Merge Sort"},{"location":"week-2/ce100-week-2-recurrence/#master-theorem-reminder","text":"\\(T(n) = aT(n/b) + f(n)\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(n^{log_b^a})\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) Case 3: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(f(n))\\) and \\(af(n/b) \\leq cf(n)\\) for \\(c<1\\)","title":"Master Theorem: Reminder"},{"location":"week-2/ce100-week-2-recurrence/#merge-sort-solving-the-recurrence","text":"\\(T(n)=2T(n/2)+\\Theta(n)\\) \\(a=2,b=2,f(n)=\\Theta(n),n^{log_b^a}=n\\) Case-2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(nlgn)\\)","title":"Merge Sort: Solving the Recurrence"},{"location":"week-2/ce100-week-2-recurrence/#binary-search","text":"Find an element in a sorted array: 1. Divide: Check middle element. 2. Conquer: Recursively search 1 subarray. 3. Combine: Trivial.","title":"Binary Search"},{"location":"week-2/ce100-week-2-recurrence/#binary-search_1","text":"\\[ \\text{PARENT} = \\lfloor i/2 \\rfloor \\] \\[ \\text{LEFT-CHILD} = 2i, \\text{ 2i>n} \\] \\[ \\text{RIGHT-CHILD} = 2i+1, \\text{ 2i>n} \\]","title":"Binary Search :"},{"location":"week-2/ce100-week-2-recurrence/#binary-search-iterative","text":"ITERATIVE - BINARY - SEARCH ( A , V , low , high ) while low <= high mid = floor (( low + high ) / 2 ); if v == A [ mid ] return mid ; elseif v > A [ mid ] low = mid + 1 ; else high = mid - 1 ; endwhile return NIL","title":"Binary Search : Iterative"},{"location":"week-2/ce100-week-2-recurrence/#binary-search-recursive","text":"RECURSIVE - BINARY - SEARCH ( A , V , low , high ) if low > high return NIL ; endif mid = floor (( low + high ) / 2 ); if v == A [ mid ] return mid ; elseif v > A [ mid ] return RECURSIVE - BINARY - SEARCH ( A , V , mid +1 , high ); else return RECURSIVE - BINARY - SEARCH ( A , V , low , mid -1 ); endif","title":"Binary Search : Recursive"},{"location":"week-2/ce100-week-2-recurrence/#binary-search-recursive_1","text":"\\[ T(n)=T(n/2)+\\Theta(1) \\Longrightarrow T(n)=\\Theta(lgn) \\]","title":"Binary Search : Recursive"},{"location":"week-2/ce100-week-2-recurrence/#example-find-9","text":"","title":"Example: Find 9"},{"location":"week-2/ce100-week-2-recurrence/#recurrence-for-binary-search","text":"\\(T(n)=1T(n/2)+\\Theta(1)\\) Subproblems \\(\\Longrightarrow 1\\) Subproblemsize \\(\\Longrightarrow n/2\\) Work dividing and combining \\(\\Longrightarrow\\Theta(1)\\)","title":"Recurrence for Binary Search"},{"location":"week-2/ce100-week-2-recurrence/#binary-search-solving-the-recurrence","text":"\\(T(n) = T(n/2) + \\Theta(1)\\) \\(a = 1,b = 2,f(n) = \\Theta(1) \\Longrightarrow n^{log_b^a} = n^0=1\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(lgn)\\)","title":"Binary Search: Solving the Recurrence"},{"location":"week-2/ce100-week-2-recurrence/#powering-a-number","text":"Problem : Compute an, where n is a natural number NAIVE - POWER ( a , n ) powerVal = 1 ; for i = 1 to n powerVal = powerVal * a ; endfor return powerVal ; What is the complexity? \\(\\Longrightarrow T(n)=\\Theta(n)\\)","title":"Powering a Number"},{"location":"week-2/ce100-week-2-recurrence/#powering-a-number-divide-conquer","text":"Basic Idea: \\[ a^n=\\begin{cases} a^{n/2}*a^{n/2} & \\text{if n is even} \\\\ a^{(n-1)/2}*a^{(n-1)/2}*a & \\text{if n is odd} \\end{cases} \\]","title":"Powering a Number: Divide &amp; Conquer"},{"location":"week-2/ce100-week-2-recurrence/#powering-a-number-divide-conquer_1","text":"POWER ( a , n ) if n = 0 then return 1 ; else if n is even then val = POWER ( a , n / 2 ); return val * val ; else if n is odd then val = POWER ( a ,( n -1 ) / 2 ) return val * val * a ; endif","title":"Powering a Number: Divide &amp; Conquer"},{"location":"week-2/ce100-week-2-recurrence/#powering-a-number-solving-the-recurrence","text":"\\(T(n) = T(n/2) + \\Theta(1)\\) \\(a = 1,b = 2,f(n) = \\Theta(1) \\Longrightarrow n^{log_b^a} = n^0=1\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(lgn)\\)","title":"Powering a Number: Solving the Recurrence"},{"location":"week-2/ce100-week-2-recurrence/#references","text":"","title":"References"},{"location":"week-2/ce100-week-2-recurrence/#todo","text":"","title":"TODO"},{"location":"week-3/ce100-week-3-matrix/","text":"CE100 Algorithms and Programming II \u00b6 Week-3 (Matrix Multiplication/ Quick Sort) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Solving Recurrences \u00b6 Outline \u00b6 Matrix Multiplication Traditional Recursive Strassen Outline \u00b6 Quicksort Hoare Partitioning Lomuto Partitioning Recursive Sorting Outline \u00b6 Quicksort Analysis Randomized Quicksort Randomized Selection Recursive Medians Matrix Multiplication \u00b6 Input: \\(A=[a_{ij}],B=[b_{ij}]\\) Output: \\(C=[c_{ij}]=A \\cdot B\\) \\(\\Longrightarrow i,j=1,2,3, \\dots, n\\) \\[ \\begin{bmatrix} c_{11} & c_{12} & \\dots & c_{1n} \\\\ c_{21} & c_{22} & \\dots & c_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ c_{n1} & c_{n2} & \\dots & c_{nn} \\\\ \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ a_{n1} & a_{n2} & \\dots & a_{nn} \\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} b_{11} & b_{12} & \\dots & b_{1n} \\\\ b_{21} & b_{22} & \\dots & b_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ b_{n1} & a_{n2} & \\dots & b_{nn} \\\\ \\end{bmatrix} \\] - \\(c_{ij}=\\sum_{1\\leq k \\leq n}a_{ik}.b_{kj}\\) \u00b6 References \u00b6 TODO","title":"Week-3 (Matrix Multiplication/Quick Sort)"},{"location":"week-3/ce100-week-3-matrix/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-3/ce100-week-3-matrix/#week-3-matrix-multiplication-quick-sort","text":"","title":"Week-3 (Matrix Multiplication/ Quick Sort)"},{"location":"week-3/ce100-week-3-matrix/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-3/ce100-week-3-matrix/#solving-recurrences","text":"","title":"Solving Recurrences"},{"location":"week-3/ce100-week-3-matrix/#outline","text":"Matrix Multiplication Traditional Recursive Strassen","title":"Outline"},{"location":"week-3/ce100-week-3-matrix/#outline_1","text":"Quicksort Hoare Partitioning Lomuto Partitioning Recursive Sorting","title":"Outline"},{"location":"week-3/ce100-week-3-matrix/#outline_2","text":"Quicksort Analysis Randomized Quicksort Randomized Selection Recursive Medians","title":"Outline"},{"location":"week-3/ce100-week-3-matrix/#matrix-multiplication","text":"Input: \\(A=[a_{ij}],B=[b_{ij}]\\) Output: \\(C=[c_{ij}]=A \\cdot B\\) \\(\\Longrightarrow i,j=1,2,3, \\dots, n\\) \\[ \\begin{bmatrix} c_{11} & c_{12} & \\dots & c_{1n} \\\\ c_{21} & c_{22} & \\dots & c_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ c_{n1} & c_{n2} & \\dots & c_{nn} \\\\ \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ a_{n1} & a_{n2} & \\dots & a_{nn} \\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} b_{11} & b_{12} & \\dots & b_{1n} \\\\ b_{21} & b_{22} & \\dots & b_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ b_{n1} & a_{n2} & \\dots & b_{nn} \\\\ \\end{bmatrix} \\]","title":"Matrix Multiplication"},{"location":"week-3/ce100-week-3-matrix/#-c_ijsum_1leq-k-leq-na_ikb_kj","text":"","title":"- \\(c_{ij}=\\sum_{1\\leq k \\leq n}a_{ik}.b_{kj}\\)"},{"location":"week-3/ce100-week-3-matrix/#references","text":"TODO","title":"References"},{"location":"week-4/ce100-week-4-heap/","text":"CE100 Algorithms and Programming II \u00b6 Week-4 (Heap/Heap Sort) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Heap/Heap Sort \u00b6 Outline \u00b6 Heaps Max / Min Heap Heap Data Structure Heapify Iterative Recursive Extract-Max Build Heap Outline \u00b6 Heap Sort Priority Queues Linked Lists Radix Sort Counting Sort References \u00b6 TODO","title":"Week-4 (Heap/Heap Sort)"},{"location":"week-4/ce100-week-4-heap/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-4/ce100-week-4-heap/#week-4-heapheap-sort","text":"","title":"Week-4 (Heap/Heap Sort)"},{"location":"week-4/ce100-week-4-heap/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-4/ce100-week-4-heap/#heapheap-sort","text":"","title":"Heap/Heap Sort"},{"location":"week-4/ce100-week-4-heap/#outline","text":"Heaps Max / Min Heap Heap Data Structure Heapify Iterative Recursive Extract-Max Build Heap","title":"Outline"},{"location":"week-4/ce100-week-4-heap/#outline_1","text":"Heap Sort Priority Queues Linked Lists Radix Sort Counting Sort","title":"Outline"},{"location":"week-4/ce100-week-4-heap/#references","text":"TODO","title":"References"},{"location":"week-5/ce100-week-5-dp/","text":"CE100 Algorithms and Programming II \u00b6 Week-5 (Dynamic Programming) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Quicksort Sort \u00b6 Outline \u00b6 Convex Hull (Divide & Conquer) Dynamic Programming Introduction Divide-and-Conquer (DAC) vs Dynamic Programming (DP) Fibonacci Numbers Recursive Solution Bottom-Up Solution Optimization Problems Development of a DP Algorithms Matrix-Chain Multiplication Matrix Multiplication and Row Columns Definitions Cost of Multiplication Operations (pxqxr) Counting the Number of Parenthesizations The Structure of Optimal Parenthesization Characterize the structure of an optimal solution A Recursive Solution Direct Recursion Inefficiency. Computing the optimal Cost of Matrix-Chain Multiplication Bottom-up Computation Algorithm for Computing the Optimal Costs MATRIX-CHAIN-ORDER Construction and Optimal Solution MATRIX-CHAIN-MULTIPLY Summary References \u00b6 TODO","title":"Week-5 (Dynamic Programming)"},{"location":"week-5/ce100-week-5-dp/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-5/ce100-week-5-dp/#week-5-dynamic-programming","text":"","title":"Week-5 (Dynamic Programming)"},{"location":"week-5/ce100-week-5-dp/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-5/ce100-week-5-dp/#quicksort-sort","text":"","title":"Quicksort Sort"},{"location":"week-5/ce100-week-5-dp/#outline","text":"Convex Hull (Divide & Conquer) Dynamic Programming Introduction Divide-and-Conquer (DAC) vs Dynamic Programming (DP) Fibonacci Numbers Recursive Solution Bottom-Up Solution Optimization Problems Development of a DP Algorithms Matrix-Chain Multiplication Matrix Multiplication and Row Columns Definitions Cost of Multiplication Operations (pxqxr) Counting the Number of Parenthesizations The Structure of Optimal Parenthesization Characterize the structure of an optimal solution A Recursive Solution Direct Recursion Inefficiency. Computing the optimal Cost of Matrix-Chain Multiplication Bottom-up Computation Algorithm for Computing the Optimal Costs MATRIX-CHAIN-ORDER Construction and Optimal Solution MATRIX-CHAIN-MULTIPLY Summary","title":"Outline"},{"location":"week-5/ce100-week-5-dp/#references","text":"TODO","title":"References"},{"location":"week-6/ce100-week-6-lcs/","text":"CE100 Algorithms and Programming II \u00b6 Week-6 (Matrix Chain Order / LCS) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Matrix Chain Order / Longest Common Subsequence \u00b6 Outline \u00b6 Elements of Dynamic Programming Optimal Substructure Overlapping Subproblems Recursive Matrix Chain Order Memoization Top-Down Approach RMC MemoizedMatrixChain LookupC Dynamic Programming vs Memoization Summary Dynamic Programming Problem-2 : Longest Common Subsequence Definitions LCS Problem Notations Optimal Substructure of LCS Proof Case-1 Proof Case-2 Proof Case-3 A recursive solution to subproblems (inefficient) Computing the length of and LCS LCS Data Structure for DP Bottom-Up Computation Constructing and LCS PRINT-LCS Back-pointer space optimization for LCS length Most Common Dynamic Programming Interview Questions Problem-1: Longest Increasing Subsequence https://www.geeksforgeeks.org/longest-increasing-subsequence-dp-3/ https://en.wikipedia.org/wiki/Longest_increasing_subsequence#:~:text=In%20computer%20science%2C%20the%20longest,not%20necessarily%20contiguous%2C%20or%20unique . https://www.youtube.com/watch?v=22s1xxRvy28&ab_channel=StableSort Problem-2: Edit Distance https://www.geeksforgeeks.org/edit-distance-dp-5/ https://www.youtube.com/watch?v=tU2f2JwHmfQ&feature=youtu.be&ab_channel=PrepForTech Recursive https://www.youtube.com/watch?v=8Q2IEIY2pDU&ab_channel=BenLangmead DP https://www.youtube.com/watch?v=0KzWq118UNI&ab_channel=BenLangmead https://www.youtube.com/watch?v=eAVGRWSryGo&ab_channel=BenLangmead Problem-3: Partition a set into two subsets such that the difference of subset sums is minimum https://www.geeksforgeeks.org/partition-a-set-into-two-subsets-such-that-the-difference-of-subset-sums-is-minimum/ Problem-4: Count number of ways to cover a distance https://www.geeksforgeeks.org/count-number-of-ways-to-cover-a-distance/ Problem-5: Find the longest path in a matrix with given constraints https://www.geeksforgeeks.org/find-the-longest-path-in-a-matrix-with-given-constraints/ Problem-6: Subset Sum Problem https://www.geeksforgeeks.org/subset-sum-problem-dp-25/ Problem-7: Optimal Strategy for a Game https://www.geeksforgeeks.org/optimal-strategy-for-a-game-dp-31/ Problem-8: 0-1 Knapsack Problem https://www.geeksforgeeks.org/0-1-knapsack-problem-dp-10/ Problem-9: Boolean Parenthesization Problem https://www.geeksforgeeks.org/boolean-parenthesization-problem-dp-37/ Problem-10: Shortest Common Supersequence https://www.geeksforgeeks.org/shortest-common-supersequence/ https://en.wikipedia.org/wiki/Shortest_common_supersequence_problem Problem-11: Partition Problem https://www.geeksforgeeks.org/partition-problem-dp-18/ Problem-12: Cutting a Rod https://www.geeksforgeeks.org/cutting-a-rod-dp-13/ Problem-13: Coin Change https://www.geeksforgeeks.org/coin-change-dp-7/ Problem-14: Word Break Problem https://www.geeksforgeeks.org/word-break-problem-dp-32/ Problem-15: Maximum Product Cutting https://www.geeksforgeeks.org/maximum-product-cutting-dp-36/ Problem-16: Dice Throw https://www.geeksforgeeks.org/dice-throw-dp-30/ Problem-17: Box Stacking Problem https://www.geeksforgeeks.org/box-stacking-problem-dp-22/ Problem-18: Egg Dropping Puzzle https://www.geeksforgeeks.org/egg-dropping-puzzle-dp-11/ References \u00b6 TODO","title":"Week-6 (Matrix Chain Order / LCS)"},{"location":"week-6/ce100-week-6-lcs/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-6/ce100-week-6-lcs/#week-6-matrix-chain-order-lcs","text":"","title":"Week-6 (Matrix Chain Order / LCS)"},{"location":"week-6/ce100-week-6-lcs/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-6/ce100-week-6-lcs/#matrix-chain-order-longest-common-subsequence","text":"","title":"Matrix Chain Order / Longest Common Subsequence"},{"location":"week-6/ce100-week-6-lcs/#outline","text":"Elements of Dynamic Programming Optimal Substructure Overlapping Subproblems Recursive Matrix Chain Order Memoization Top-Down Approach RMC MemoizedMatrixChain LookupC Dynamic Programming vs Memoization Summary Dynamic Programming Problem-2 : Longest Common Subsequence Definitions LCS Problem Notations Optimal Substructure of LCS Proof Case-1 Proof Case-2 Proof Case-3 A recursive solution to subproblems (inefficient) Computing the length of and LCS LCS Data Structure for DP Bottom-Up Computation Constructing and LCS PRINT-LCS Back-pointer space optimization for LCS length Most Common Dynamic Programming Interview Questions Problem-1: Longest Increasing Subsequence https://www.geeksforgeeks.org/longest-increasing-subsequence-dp-3/ https://en.wikipedia.org/wiki/Longest_increasing_subsequence#:~:text=In%20computer%20science%2C%20the%20longest,not%20necessarily%20contiguous%2C%20or%20unique . https://www.youtube.com/watch?v=22s1xxRvy28&ab_channel=StableSort Problem-2: Edit Distance https://www.geeksforgeeks.org/edit-distance-dp-5/ https://www.youtube.com/watch?v=tU2f2JwHmfQ&feature=youtu.be&ab_channel=PrepForTech Recursive https://www.youtube.com/watch?v=8Q2IEIY2pDU&ab_channel=BenLangmead DP https://www.youtube.com/watch?v=0KzWq118UNI&ab_channel=BenLangmead https://www.youtube.com/watch?v=eAVGRWSryGo&ab_channel=BenLangmead Problem-3: Partition a set into two subsets such that the difference of subset sums is minimum https://www.geeksforgeeks.org/partition-a-set-into-two-subsets-such-that-the-difference-of-subset-sums-is-minimum/ Problem-4: Count number of ways to cover a distance https://www.geeksforgeeks.org/count-number-of-ways-to-cover-a-distance/ Problem-5: Find the longest path in a matrix with given constraints https://www.geeksforgeeks.org/find-the-longest-path-in-a-matrix-with-given-constraints/ Problem-6: Subset Sum Problem https://www.geeksforgeeks.org/subset-sum-problem-dp-25/ Problem-7: Optimal Strategy for a Game https://www.geeksforgeeks.org/optimal-strategy-for-a-game-dp-31/ Problem-8: 0-1 Knapsack Problem https://www.geeksforgeeks.org/0-1-knapsack-problem-dp-10/ Problem-9: Boolean Parenthesization Problem https://www.geeksforgeeks.org/boolean-parenthesization-problem-dp-37/ Problem-10: Shortest Common Supersequence https://www.geeksforgeeks.org/shortest-common-supersequence/ https://en.wikipedia.org/wiki/Shortest_common_supersequence_problem Problem-11: Partition Problem https://www.geeksforgeeks.org/partition-problem-dp-18/ Problem-12: Cutting a Rod https://www.geeksforgeeks.org/cutting-a-rod-dp-13/ Problem-13: Coin Change https://www.geeksforgeeks.org/coin-change-dp-7/ Problem-14: Word Break Problem https://www.geeksforgeeks.org/word-break-problem-dp-32/ Problem-15: Maximum Product Cutting https://www.geeksforgeeks.org/maximum-product-cutting-dp-36/ Problem-16: Dice Throw https://www.geeksforgeeks.org/dice-throw-dp-30/ Problem-17: Box Stacking Problem https://www.geeksforgeeks.org/box-stacking-problem-dp-22/ Problem-18: Egg Dropping Puzzle https://www.geeksforgeeks.org/egg-dropping-puzzle-dp-11/","title":"Outline"},{"location":"week-6/ce100-week-6-lcs/#references","text":"TODO","title":"References"},{"location":"week-7/ce100-week-7-knapsack/","text":"CE100 Algorithms and Programming II \u00b6 Week-7 (Greedy Algorithms, Knapsack) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Greedy Algorithms, Knapsack \u00b6 Outline \u00b6 Greedy Algorithms and Dynamic Programming Differences Greedy Algorithms Activity Selection Problem Knapsack Problems The 0-1 knapsack problem The fractional knapsack problem References \u00b6 TODO","title":"Week-7 (Greedy Algorithms, Knapsack)"},{"location":"week-7/ce100-week-7-knapsack/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-7/ce100-week-7-knapsack/#week-7-greedy-algorithms-knapsack","text":"","title":"Week-7 (Greedy Algorithms, Knapsack)"},{"location":"week-7/ce100-week-7-knapsack/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-7/ce100-week-7-knapsack/#greedy-algorithms-knapsack","text":"","title":"Greedy Algorithms, Knapsack"},{"location":"week-7/ce100-week-7-knapsack/#outline","text":"Greedy Algorithms and Dynamic Programming Differences Greedy Algorithms Activity Selection Problem Knapsack Problems The 0-1 knapsack problem The fractional knapsack problem","title":"Outline"},{"location":"week-7/ce100-week-7-knapsack/#references","text":"TODO","title":"References"},{"location":"week-8/ce100-week-8-midterm/","text":"CE100 Algorithms and Programming II \u00b6 Week-8 (Midterm) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Midterm \u00b6 Outline \u00b6 References \u00b6 TODO","title":"Week-8 (Midterm)"},{"location":"week-8/ce100-week-8-midterm/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-8/ce100-week-8-midterm/#week-8-midterm","text":"","title":"Week-8 (Midterm)"},{"location":"week-8/ce100-week-8-midterm/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-8/ce100-week-8-midterm/#midterm","text":"","title":"Midterm"},{"location":"week-8/ce100-week-8-midterm/#outline","text":"","title":"Outline"},{"location":"week-8/ce100-week-8-midterm/#references","text":"TODO","title":"References"},{"location":"week-9/ce100-week-9-huffman/","text":"CE100 Algorithms and Programming II \u00b6 Week-9 (Huffman Coding) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Huffman Coding \u00b6 Outline \u00b6 Heap Data Structure Heap Sort Huffman Coding References \u00b6 TODO","title":"Week-9 (Huffman Coding)"},{"location":"week-9/ce100-week-9-huffman/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-9/ce100-week-9-huffman/#week-9-huffman-coding","text":"","title":"Week-9 (Huffman Coding)"},{"location":"week-9/ce100-week-9-huffman/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-9/ce100-week-9-huffman/#huffman-coding","text":"","title":"Huffman Coding"},{"location":"week-9/ce100-week-9-huffman/#outline","text":"Heap Data Structure Heap Sort Huffman Coding","title":"Outline"},{"location":"week-9/ce100-week-9-huffman/#references","text":"TODO","title":"References"}]}