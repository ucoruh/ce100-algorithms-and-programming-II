{"config":{"indexing":"full","lang":["en","tr"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"license/","text":"License \u00b6 MIT License Copyright \u00a9 2019-2022 U\u011fur CORUH Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"MIT License Copyright \u00a9 2019-2022 U\u011fur CORUH Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"changelog/","text":"Changelog \u00b6 Material for Algorithm Lovers \u00b6 1.0.0 _ October 20, 2020 \u00b6 Initial release","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#material-for-algorithm-lovers","text":"","title":"Material for Algorithm Lovers"},{"location":"changelog/#1.0.0","text":"Initial release","title":"1.0.0 _ October 20, 2020"},{"location":"resume/","text":"Resume \u00b6 Download Resume-English Resume-Turkish English Turkish","title":"Resume"},{"location":"resume/#resume","text":"Download Resume-English Resume-Turkish English Turkish","title":"Resume"},{"location":"syllabus/syllabus/","text":"Recep Tayyip Erdogan University \u00b6 Faculty of Engineering and Architecture \u00b6 Computer Engineering \u00b6 CE100 Algorithms and Programming-II \u00b6 Syllabus \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Download WORD , PDF Instructor Asst. Prof. Dr. U\u011fur CORUH Contact Information ugur.coruh@erdogan.edu.tr Office No F-301 Google Classroom Code bafwwt6 Lecture Hours and Days TBD Lecture Classroom \u0130BBF 402 Level-4 Office Hours Meetings will be scheduled over Google Meet with your university account and email and performed via demand emails. Please send emails with the subject starting with [CE100] tag for the fast response and write formal, clear, and short emails Lecture and Communication Language English Theory/Laboratory Course Hour Per Week 3/2 Hours Credit 4 Prerequisite CE103- Algorithms and Programming I Corequisite TBD Requirement TBD *TBD: To Be Defined. A.Course Description \u00b6 This course continues the CE103 Algorithms and Programming I course. This course taught programming skills in Algorithms and Programming I course met. This course taught programming skills in Algorithms and Programming I with common problems and their solution algorithms. This lecture is about analyzing and understanding how algorithms work for common issues. The class will be based on expertise sharing and guiding students to find learning methods and practice for algorithm and programming topics. By making programming applications and projects in the courses, the learning process will be strengthened by practicing rather than theory. B.Course Learning Outcomes \u00b6 After completing this course satisfactorily, a student will be able to: Interpret a computational problem specification and algorithmic solution and implement a C/C++, Java or C# application to solve that problem. Argue the correctness of algorithms using inductive proofs and invariants. Understand algorithm design steps Argue algorithm cost calculation for time complexity and asymptotic notation Analyze recursive algorithms complexity Understand divide-and-conquer, dynamic programming and greedy approaches. Understand graphs and graph related algorithms. Understand hashing and encryption operations input and outputs. C.Course Topics \u00b6 Algorithms Basics, Pseudocode Algorithms Analysis for Time Complexity and Asymptotic Notation Sorting Problems (Insertion and Merge Sorts) Recursive Algorithms Divide-and-Conquer Analysis (Merge Sort, Binary Search) Matrix Multiplication Problem Quicksort Analysis Heaps, Heap Sort and Priority Queues Linked Lists, Radix Sort, You should have a laptop for programming practices during this course and Counting Sort. Convex Hull Dynamic Programming Greedy Algorithms Graphs and Graphs Search Algorithms Breadth-First Search Depth-First Search and Topological Sort Graph Structure Algorithms Strongly Connected Components Minimum Spanning Tree Disjoint Set Operations Single Source Shortest Path Algorithm Q-Learning Shortest Path Implementation Network Flow and Applications Hashing and Encryption D.Textbooks and Required Hardware or Equipment \u00b6 This course does not require a coursebook. If necessary, you can use the following books and open-source online resources. Paul Deitel and Harvey Deitel. 2012. C How to Program (7 th . ed.). Prentice Hall Press, USA. Intro to Java Programming, Comprehensive Version (10 th Edition) 10 th Edition by Y. Daniel Liang Introduction to Algorithms, Third Edition By Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein Problem Solving and Program Design in C, J.R. Hanly, and E.B. Koffman, 6 th Edition. Robert Sedgewick and Kevin Wayne. 2011. Algorithms (4 th . ed.). Addison-Wesley Professional. Harvey M. Deitel and Paul J. Deitel. 2001. Java How to Program (4 th . ed.). Prentice Hall PTR, USA. Paul Deitel and Harvey Deitel. 2016. Visual C# How to Program (6 th . ed.). Pearson. Additional Books TBD During this course, you should have a laptop for programming practices. You will have your development environment, and you will use this for examination and assignments also classroom practices. E.Grading System \u00b6 Midterm and Final grades will be calculated with the weighted average of the project or homework-based examinations. Midterm grades will be calculated between term beginning to the midterm week, and Final grades will be calculated between Midterm and Final week home works or projects as follows. taught Algorithms and Programming I programming skills \\[ a_n=\\text{Homework or Project Weight} \\] \\[ HW_n=\\text{Homework or Project Points} \\] \\[ n=\\text{Number of Homework or Project} \\] \\[ Grade=(a_1HW_1+a_2HW_2+...+a_nHW_n)/n \\] Homework Weight Midterm %40 Final %60 \\[ \\text{Passing Grade}=(40*Midterm_{Grade}+60*Final_{Grade})/100 \\] F. Instructional Strategies and Methods \u00b6 The basic teaching method of this course will be planned to be face-to-face in the classroom, and support resources, home works, and announcements will be shared over google classroom. Students are expected to be in the university. This responsibility is very important to complete this course with success. If pandemic situation changes and distance education is required during this course, this course will be done using synchronous and asynchronous distance education methods. In this scenario, students are expected to be in the online platform, zoom, or meet at the time specified in the course schedule. Attendance will be taken. G. Late Homework \u00b6 Throughout the semester, assignments must be submitted as specified by the announced deadline. Your grade will be reduced by 10% of the full points for each calendar day for overdue assignments. Overdue assignments will not be accepted after three (3) days. Unexpected situations must be reported to the instructor for late home works by students. H. Course Platform and Communication \u00b6 Google Classroom will be used as a course learning management system. All electronic resources and announcements about the course will be shared on this platform. It is very important to check the course page daily, access the necessary resources and announcements, and communicate with the instructor as you need Algorithms and Programming I programming skills to complete the course with success I. Academic Integrity, Plagiarism & Cheating \u00b6 Academic Integrity is one of the most important principles of RTE\u00dc University. Anyone who breaches the principles of academic honesty is severely punished. It is natural to interact with classmates and others t.\"study together\". It may also be the case where a student asks to help from someone else, paid or unpaid, better understand a difficult topic or a whole course. However, what is the borderline between \"studying together\" or \"taking private lessons\" and \"academic dishonesty\"? When is it plagiarism, when is it cheating? It is obvious that looking at another student's paper or any source other than what is allowed during the exam is cheating and will be punished. However, it is known that many students come to university with very little experience concerning what is acceptable and what counts as \"copying,\"\" especially for assignments. The following are attempted as guidelines for the Faculty of Engineering and Architecture students to highlight the philosophy of academic honesty for assignments for which the student will be graded. Should a situation arise which is not described below, the student is advised to ask the instructor or assistant of the course whether what they intend to do would remain within the framework of academic honesty or not. a. What is acceptable when preparing an assignment? \u00b6 Communicating with classmates about the assignment to understand it better Putting ideas, quotes, paragraphs, small pieces of code (snippets) that you find online or elsewhere into your assignment, provided that these are not themselves the whole solution to the assignment, you cite the origins of these Asking sources for help in guiding you for the English language content of your assignment. Sharing small pieces of your assignment in the classroom to create a class discussion on some controversial topics. Turning to the web or elsewhere for instructions, references, and solutions to technical difficulties, but not for direct answers to the assignment Discuss solutions to assignments with others using diagrams or summarized statements but not actual text or code. Working with (and even paying) a tutor to help you with the course, provided the tutor does not do your assignment for you. b. What is not acceptable? \u00b6 Ask a classmate to see their solution to a problem before submitting your own. Failing to cite the origins of any text (or code for programming courses) that you discover outside of the course's lessons and integrate into your work You are giving or showing a classmate your solution to a problem when the classmate is struggling to solve it. J. Expectations \u00b6 You are expected to attend classes on time by completing weekly course requirements (readings and assignments) during the semester. The main communication channel between the instructor and the students email emailed. Please send your questions to the instructor's email address about the course via the email address provided to you by the university. Ensure that you include the course name in the subject field of your message and your name in the text field . In addition, the instructor will contact you via email if necessary. For this reason, it is very important to check your email address every day for healthy communication. K. Lecture Content and Syllabus Updates \u00b6 If deemed necessary, changes in the lecture content or course schedule can be made. If any changes are made in the scope of this document, the instructor will inform you about this. Course Schedule Overview \u00b6 Weeks Dates Subjects Other Tasks Week 1 TBD Course Plan and Communication Grading System, Assignments and Exams. Algorithms Basics, Pseudocode,iv. RAM (Random Access Machine Model), Algorithm Cost Calculation for Time Complexity. Worst, Average and Best Case Summary Sorting Problem (Insertion and Merge Sort Analysis), 4. Asymptotic Notation(Big O, Big Teta,Big Omega, Small o, Small omega Notations) TBD Week 2 TBD Solving Recurrences (Recursion Tree, Master Method and Back-Substitution) Divide-and-Conquer Analysis (Merge Sort, Binary Search) Recurrence Solution TBD Week 3 TBD Matrix Multiplication(Traditional,Recursive,Strassen),Quicksort(Hoare and Lomuto Partitioning,Recursive Sorting),Quicksort Analysis,Randomized Quicksort, Randomized Selection(Recursive,Medians) TBD Week 4 TBD Heaps (Max / Min Heap, Heap Data Structure, Iterative and Recursive Heapify, Extract-Max, Build Heap) Heap Sort, Priority Queues, Linked Lists, Radix Sort,Counting Sort TBD Week 5 TBD Convex Hull (Divide & Conquer) Dynamic Programming (Fibonacci Numbers) Divide-and-Conquer (DAC) vs Dynamic Programming (DP) Development of a DP Algorithms Matrix-Chain Multiplication and Analysis TBD Week-6 TBD Elements of Dynamic Programming Recursive Matrix Chain Order Memoization (Top-Down Approach, RMC, MemoizedMatrixChain, LookupC) Dynamic Programming vs. Memoization Longest Common Subsequence (LCS) Most Common Dynamic Programming Interview Questions TBD Week-7 TBD Greedy Algorithms and Dynamic Programming Differences Greedy Algorithms (Activity Selection Problem, Knapsack Problems) TBD Week-8 TBD Midterm TBD Week-9 TBD Heap Data Structure, Heap Sort, Huffman Coding TBD Week-10 TBD Introduction to Graphs, Gr,aphs and Representation, BFS (Breath-First Search), DFS (Depth-First Search), Topological Order, SCC (Strongly Connected Components), MST, Prim, Kruskal TBD Week-11 TBD Disjoint Sets and Kruskal Relationships,Single-Source Shortest Path,(Bellman- Ford,Dijkstra),Q-Learning Shortest Path,Max-Flow Min-Cut (Ford-Fulkerson,Edmond\u2019s Karp,Dinic) TBD Week-12 TBD Crypto++ Library Usage, Hashing and Integrity Control, Cryptographic Hash Functions (SHA-1,SHA-256,SHA-512,H-MAC), Checksums(MD5,CRC32) TBD Week-13 TBD Symmetric Encryption Algorithms (AES, DES, TDES), Symmetric Encryption Modes (ECB, CBC), Asymmetric Encryption, Key Pairs (Public-Private Key Pairs), Signature Generation and Validation TBD Week-14 TBD OTP Calculation(Time-based, Counter-based),File Encryption and Decryption and Integrity Control Operations TBD Week-15 TBD Review TBD Week-16 TBD Final TBD Bologna Information \u00b6 \\(End-Of-CE100-Syllabus\\)","title":"Syllabus"},{"location":"syllabus/syllabus/#recep-tayyip-erdogan-university","text":"","title":"Recep Tayyip Erdogan University"},{"location":"syllabus/syllabus/#faculty-of-engineering-and-architecture","text":"","title":"Faculty of Engineering and Architecture"},{"location":"syllabus/syllabus/#computer-engineering","text":"","title":"Computer Engineering"},{"location":"syllabus/syllabus/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming-II"},{"location":"syllabus/syllabus/#syllabus","text":"","title":"Syllabus"},{"location":"syllabus/syllabus/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX Download WORD , PDF Instructor Asst. Prof. Dr. U\u011fur CORUH Contact Information ugur.coruh@erdogan.edu.tr Office No F-301 Google Classroom Code bafwwt6 Lecture Hours and Days TBD Lecture Classroom \u0130BBF 402 Level-4 Office Hours Meetings will be scheduled over Google Meet with your university account and email and performed via demand emails. Please send emails with the subject starting with [CE100] tag for the fast response and write formal, clear, and short emails Lecture and Communication Language English Theory/Laboratory Course Hour Per Week 3/2 Hours Credit 4 Prerequisite CE103- Algorithms and Programming I Corequisite TBD Requirement TBD *TBD: To Be Defined.","title":"Spring Semester, 2021-2022"},{"location":"syllabus/syllabus/#acourse-description","text":"This course continues the CE103 Algorithms and Programming I course. This course taught programming skills in Algorithms and Programming I course met. This course taught programming skills in Algorithms and Programming I with common problems and their solution algorithms. This lecture is about analyzing and understanding how algorithms work for common issues. The class will be based on expertise sharing and guiding students to find learning methods and practice for algorithm and programming topics. By making programming applications and projects in the courses, the learning process will be strengthened by practicing rather than theory.","title":"A.Course Description"},{"location":"syllabus/syllabus/#bcourse-learning-outcomes","text":"After completing this course satisfactorily, a student will be able to: Interpret a computational problem specification and algorithmic solution and implement a C/C++, Java or C# application to solve that problem. Argue the correctness of algorithms using inductive proofs and invariants. Understand algorithm design steps Argue algorithm cost calculation for time complexity and asymptotic notation Analyze recursive algorithms complexity Understand divide-and-conquer, dynamic programming and greedy approaches. Understand graphs and graph related algorithms. Understand hashing and encryption operations input and outputs.","title":"B.Course Learning Outcomes"},{"location":"syllabus/syllabus/#ccourse-topics","text":"Algorithms Basics, Pseudocode Algorithms Analysis for Time Complexity and Asymptotic Notation Sorting Problems (Insertion and Merge Sorts) Recursive Algorithms Divide-and-Conquer Analysis (Merge Sort, Binary Search) Matrix Multiplication Problem Quicksort Analysis Heaps, Heap Sort and Priority Queues Linked Lists, Radix Sort, You should have a laptop for programming practices during this course and Counting Sort. Convex Hull Dynamic Programming Greedy Algorithms Graphs and Graphs Search Algorithms Breadth-First Search Depth-First Search and Topological Sort Graph Structure Algorithms Strongly Connected Components Minimum Spanning Tree Disjoint Set Operations Single Source Shortest Path Algorithm Q-Learning Shortest Path Implementation Network Flow and Applications Hashing and Encryption","title":"C.Course Topics"},{"location":"syllabus/syllabus/#dtextbooks-and-required-hardware-or-equipment","text":"This course does not require a coursebook. If necessary, you can use the following books and open-source online resources. Paul Deitel and Harvey Deitel. 2012. C How to Program (7 th . ed.). Prentice Hall Press, USA. Intro to Java Programming, Comprehensive Version (10 th Edition) 10 th Edition by Y. Daniel Liang Introduction to Algorithms, Third Edition By Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein Problem Solving and Program Design in C, J.R. Hanly, and E.B. Koffman, 6 th Edition. Robert Sedgewick and Kevin Wayne. 2011. Algorithms (4 th . ed.). Addison-Wesley Professional. Harvey M. Deitel and Paul J. Deitel. 2001. Java How to Program (4 th . ed.). Prentice Hall PTR, USA. Paul Deitel and Harvey Deitel. 2016. Visual C# How to Program (6 th . ed.). Pearson. Additional Books TBD During this course, you should have a laptop for programming practices. You will have your development environment, and you will use this for examination and assignments also classroom practices.","title":"D.Textbooks and Required Hardware or Equipment"},{"location":"syllabus/syllabus/#egrading-system","text":"Midterm and Final grades will be calculated with the weighted average of the project or homework-based examinations. Midterm grades will be calculated between term beginning to the midterm week, and Final grades will be calculated between Midterm and Final week home works or projects as follows. taught Algorithms and Programming I programming skills \\[ a_n=\\text{Homework or Project Weight} \\] \\[ HW_n=\\text{Homework or Project Points} \\] \\[ n=\\text{Number of Homework or Project} \\] \\[ Grade=(a_1HW_1+a_2HW_2+...+a_nHW_n)/n \\] Homework Weight Midterm %40 Final %60 \\[ \\text{Passing Grade}=(40*Midterm_{Grade}+60*Final_{Grade})/100 \\]","title":"E.Grading System"},{"location":"syllabus/syllabus/#f-instructional-strategies-and-methods","text":"The basic teaching method of this course will be planned to be face-to-face in the classroom, and support resources, home works, and announcements will be shared over google classroom. Students are expected to be in the university. This responsibility is very important to complete this course with success. If pandemic situation changes and distance education is required during this course, this course will be done using synchronous and asynchronous distance education methods. In this scenario, students are expected to be in the online platform, zoom, or meet at the time specified in the course schedule. Attendance will be taken.","title":"F. Instructional Strategies and Methods"},{"location":"syllabus/syllabus/#g-late-homework","text":"Throughout the semester, assignments must be submitted as specified by the announced deadline. Your grade will be reduced by 10% of the full points for each calendar day for overdue assignments. Overdue assignments will not be accepted after three (3) days. Unexpected situations must be reported to the instructor for late home works by students.","title":"G. Late Homework"},{"location":"syllabus/syllabus/#h-course-platform-and-communication","text":"Google Classroom will be used as a course learning management system. All electronic resources and announcements about the course will be shared on this platform. It is very important to check the course page daily, access the necessary resources and announcements, and communicate with the instructor as you need Algorithms and Programming I programming skills to complete the course with success","title":"H. Course Platform and Communication"},{"location":"syllabus/syllabus/#i-academic-integrity-plagiarism-cheating","text":"Academic Integrity is one of the most important principles of RTE\u00dc University. Anyone who breaches the principles of academic honesty is severely punished. It is natural to interact with classmates and others t.\"study together\". It may also be the case where a student asks to help from someone else, paid or unpaid, better understand a difficult topic or a whole course. However, what is the borderline between \"studying together\" or \"taking private lessons\" and \"academic dishonesty\"? When is it plagiarism, when is it cheating? It is obvious that looking at another student's paper or any source other than what is allowed during the exam is cheating and will be punished. However, it is known that many students come to university with very little experience concerning what is acceptable and what counts as \"copying,\"\" especially for assignments. The following are attempted as guidelines for the Faculty of Engineering and Architecture students to highlight the philosophy of academic honesty for assignments for which the student will be graded. Should a situation arise which is not described below, the student is advised to ask the instructor or assistant of the course whether what they intend to do would remain within the framework of academic honesty or not.","title":"I. Academic Integrity, Plagiarism &amp; Cheating"},{"location":"syllabus/syllabus/#a-what-is-acceptable-when-preparing-an-assignment","text":"Communicating with classmates about the assignment to understand it better Putting ideas, quotes, paragraphs, small pieces of code (snippets) that you find online or elsewhere into your assignment, provided that these are not themselves the whole solution to the assignment, you cite the origins of these Asking sources for help in guiding you for the English language content of your assignment. Sharing small pieces of your assignment in the classroom to create a class discussion on some controversial topics. Turning to the web or elsewhere for instructions, references, and solutions to technical difficulties, but not for direct answers to the assignment Discuss solutions to assignments with others using diagrams or summarized statements but not actual text or code. Working with (and even paying) a tutor to help you with the course, provided the tutor does not do your assignment for you.","title":"a. What is acceptable when preparing an assignment?"},{"location":"syllabus/syllabus/#b-what-is-not-acceptable","text":"Ask a classmate to see their solution to a problem before submitting your own. Failing to cite the origins of any text (or code for programming courses) that you discover outside of the course's lessons and integrate into your work You are giving or showing a classmate your solution to a problem when the classmate is struggling to solve it.","title":"b. What is not acceptable?"},{"location":"syllabus/syllabus/#j-expectations","text":"You are expected to attend classes on time by completing weekly course requirements (readings and assignments) during the semester. The main communication channel between the instructor and the students email emailed. Please send your questions to the instructor's email address about the course via the email address provided to you by the university. Ensure that you include the course name in the subject field of your message and your name in the text field . In addition, the instructor will contact you via email if necessary. For this reason, it is very important to check your email address every day for healthy communication.","title":"J. Expectations"},{"location":"syllabus/syllabus/#k-lecture-content-and-syllabus-updates","text":"If deemed necessary, changes in the lecture content or course schedule can be made. If any changes are made in the scope of this document, the instructor will inform you about this.","title":"K. Lecture Content and Syllabus Updates"},{"location":"syllabus/syllabus/#course-schedule-overview","text":"Weeks Dates Subjects Other Tasks Week 1 TBD Course Plan and Communication Grading System, Assignments and Exams. Algorithms Basics, Pseudocode,iv. RAM (Random Access Machine Model), Algorithm Cost Calculation for Time Complexity. Worst, Average and Best Case Summary Sorting Problem (Insertion and Merge Sort Analysis), 4. Asymptotic Notation(Big O, Big Teta,Big Omega, Small o, Small omega Notations) TBD Week 2 TBD Solving Recurrences (Recursion Tree, Master Method and Back-Substitution) Divide-and-Conquer Analysis (Merge Sort, Binary Search) Recurrence Solution TBD Week 3 TBD Matrix Multiplication(Traditional,Recursive,Strassen),Quicksort(Hoare and Lomuto Partitioning,Recursive Sorting),Quicksort Analysis,Randomized Quicksort, Randomized Selection(Recursive,Medians) TBD Week 4 TBD Heaps (Max / Min Heap, Heap Data Structure, Iterative and Recursive Heapify, Extract-Max, Build Heap) Heap Sort, Priority Queues, Linked Lists, Radix Sort,Counting Sort TBD Week 5 TBD Convex Hull (Divide & Conquer) Dynamic Programming (Fibonacci Numbers) Divide-and-Conquer (DAC) vs Dynamic Programming (DP) Development of a DP Algorithms Matrix-Chain Multiplication and Analysis TBD Week-6 TBD Elements of Dynamic Programming Recursive Matrix Chain Order Memoization (Top-Down Approach, RMC, MemoizedMatrixChain, LookupC) Dynamic Programming vs. Memoization Longest Common Subsequence (LCS) Most Common Dynamic Programming Interview Questions TBD Week-7 TBD Greedy Algorithms and Dynamic Programming Differences Greedy Algorithms (Activity Selection Problem, Knapsack Problems) TBD Week-8 TBD Midterm TBD Week-9 TBD Heap Data Structure, Heap Sort, Huffman Coding TBD Week-10 TBD Introduction to Graphs, Gr,aphs and Representation, BFS (Breath-First Search), DFS (Depth-First Search), Topological Order, SCC (Strongly Connected Components), MST, Prim, Kruskal TBD Week-11 TBD Disjoint Sets and Kruskal Relationships,Single-Source Shortest Path,(Bellman- Ford,Dijkstra),Q-Learning Shortest Path,Max-Flow Min-Cut (Ford-Fulkerson,Edmond\u2019s Karp,Dinic) TBD Week-12 TBD Crypto++ Library Usage, Hashing and Integrity Control, Cryptographic Hash Functions (SHA-1,SHA-256,SHA-512,H-MAC), Checksums(MD5,CRC32) TBD Week-13 TBD Symmetric Encryption Algorithms (AES, DES, TDES), Symmetric Encryption Modes (ECB, CBC), Asymmetric Encryption, Key Pairs (Public-Private Key Pairs), Signature Generation and Validation TBD Week-14 TBD OTP Calculation(Time-based, Counter-based),File Encryption and Decryption and Integrity Control Operations TBD Week-15 TBD Review TBD Week-16 TBD Final TBD","title":"Course Schedule Overview"},{"location":"syllabus/syllabus/#bologna-information","text":"\\(End-Of-CE100-Syllabus\\)","title":"Bologna Information"},{"location":"week-1/ce100-week-1-intro/","text":"CE100 Algorithms and Programming II \u00b6 Week-1 (Introduction to Analysis of Algorithms) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Brief Description of Course and Rules \u00b6 We will first talk about, Course Plan and Communication Grading System, Homeworks, and Exams please read the syllabus carefully. Outline (1) \u00b6 Introduction to Analysis of Algorithms Algorithm Basics Flowgorithm Pseudocode Outline (2) \u00b6 RAM (Random Access Machine Model) Sorting Problem Insertion Sort Analysis Algorithm Cost Calculation for Time Complexity Worst, Average, and Best Case Summary Merge Sort Analysis Outline (3) \u00b6 Asymptotic Notation Big O Notation Big Teta Notation Big Omega Notation Small o Notation Small omega Notation We Need Mathematical Proofs (1) \u00b6 Direct proof Proof by mathematical induction Proof by contraposition Proof by contradiction Proof by construction Proof by exhaustion We Need Mathematical Proofs (2) \u00b6 Probabilistic proof Combinatorial proof Nonconstructive proof Statistical proofs in pure mathematics Computer-assisted proofs Mathematical proof - Wikipedia Introduction to Analysis of Algorithms \u00b6 Study two sorting algorithms as examples Insertion sort: Incremental algorithm Merge sort: Divide-and-conquer Introduction to runtime analysis Best vs. worst vs. average case Asymptotic analysis What is Algorithm \u00b6 Algorithm : A sequence of computational steps that transform the input to the desired output Procedure vs. algorithm An algorithm must halt within finite time with the right output We Need to Measure Performance Metrics Processing Time Allocated Memory Network Congestion Power Usage etc. Example Sorting Algorithms Input : a sequence of n numbers \\[ \\langle a_1,a_2,...,a_n \\rangle \\] Algorithm : Sorting / Permutation \\[ \\prod = \\langle \\prod_{(1)},\\prod_{(2)},...,\\prod_{(n)} \\rangle \\] Output : sorted permutation of the input sequence \\[ \\langle a_{\\prod_{(1)}} \\leqslant a_{\\prod_{(2)}} \\leqslant,...,a_{\\prod_{(n)}} \\rangle \\] Pseudo-code notation (1) \u00b6 Objective: Express algorithms to humans in a clear and concise way Liberal use of English Indentation for block structures Omission of error handling and other details (needed in real programs) You can use Flowgorithm application to understand concept easily. Pseudo-code notation (2) \u00b6 Links and Examples \u00b6 Wikipedia CS50 University of North Florida GeeksforGeeks Correctness (1) \u00b6 We often use a loop invariant to help us to understand why an algorithm gives the correct answer. Example: (Insertion Sort) at the start of each iteration of the \"outer\" for loop - the loop indexed by \\(j\\) - the subarray \\(A[1 \\dots j-1]\\) consist of the elements originally in \\(A[1\\dots j-1]\\) but in sorted order. Correctness (2) \u00b6 To use a loop invariant to prove correctness, we must show 3 things about it. Initialization: It is true to the first iteration of the loop. Maintaince: If it is true before an iteration of the loop, it remains true before the next iteration. Termination: When the loop terminates, the invariant - usually along with the reason that the loop terminated - gives us a usefull property that helps show that the algorithm is correct. RAM (Random Access Machine Model) \\(\\Longrightarrow \\Theta(1)\\) (1) \u00b6 Operations Single Step Sequential No Concurrent Arithmetic add, subtract, multiply, divide, remainder, floor, ceiling, shift left/shift right (good by multiply/dividing \\(2^k\\) ) RAM (Random Access Machine Model) \\(\\Longrightarrow \\Theta(1)\\) (2) \u00b6 Data Movement load, store, copy Control conditional / unconditional branch subroutine calls returns RAM (Random Access Machine Model) \\(\\Longrightarrow \\Theta(1)\\) (3) \u00b6 Each instruction take a constant amount of time Integer will be represented by \\(clogn\\) \\(c \\geq 1\\) \\(T(n)\\) the running time of the algorithm: \\[ \\sum \\limits_{\\text{all statement}}^{}(\\text{cost of statement})*(\\text{number of times statement is executed}) = T(n) \\] What is the processing time ? \u00b6 section{ font-size: 25px; } Insertion Sort Algorithm (1) \u00b6 Insertion sort is a simple sorting algorithm that works similar to the way you sort playing cards in your hands The array is virtually split into a sorted and an unsorted part Values from the unsorted part are picked and placed at the correct position in the sorted part. Assume input array : \\(A[1..n]\\) Iterate \\(j\\) from \\(2\\) to \\(n\\) Insertion Sort Algorithm (2) \u00b6 Insertion Sort Algorithm (Pseudo-Code) (3) \u00b6 Insertion - Sort ( A ) 1 . for j = 2 to A.length 2 . key = A [ j ] 3 . // insert A [ j ] into the sorted sequence A [ 1 ...j -1 ] 4 . i = j - 1 5 . while i > 0 and A [ i ] > key 6 . A [ i +1 ] = A [ i ] 7 . i = i - 1 8 . A [ i +1 ] = key Insertion Sort Step-By-Step Description (1) \u00b6 Insertion Sort Step-By-Step Description (2) \u00b6 Insertion Sort Step-By-Step Description (3) \u00b6 Insertion Sort Example \u00b6 Insertion Sort Step-1 (initial) \u00b6 Insertion Sort Step-2 (j=2) \u00b6 Insertion Sort Step-3 (j=3) \u00b6 Insertion Sort Step-4 (j=3) \u00b6 Insertion Sort Step-5 (j=4) \u00b6 Insertion Sort Step-6 (j=5) \u00b6 Insertion Sort Step-7 (j=5) \u00b6 Insertion Sort Step-8 (j=6) \u00b6 Insertion Sort Review (1) \u00b6 Items sorted in-place Elements are rearranged within the array. At a most constant number of items stored outside the array at any time (e.,g. the variable key) Input array \\(A\\) contains a sorted output sequence when the algorithm ends Insertion Sort Review (2) \u00b6 Incremental approach Having sorted \\(A[1..j-1]\\) , place \\(A[j]\\) correctly so that \\(A[1..j]\\) is sorted Running Time It depends on Input Size (5 elements or 5 billion elements) and Input Itself (partially sorted) Algorithm approach to upper bound of overall performance analysis Visualization of Insertion Sort \u00b6 Sorting (Bubble, Selection, Insertion, Merge, Quick, Counting, Radix) - VisuAlgo https://www.cs.usfca.edu/~galles/visualization/ComparisonSort.html https://algorithm-visualizer.org/ HMvHTs - Online C++ Compiler & Debugging Tool - Ideone.com Kinds of Running Time Analysis (Time Complexity) \u00b6 Worst Case (Big-O Notation) \\(T(n)\\) = maximum processing time of any input \\(n\\) Presentation of Big-O : \\(O(n)\\) Average Case (Teta Notation) \\(T(n)\\) = average time over all inputs of size \\(n\\) , inputs can have a uniform distribution Presentation of Big-Theta : \\(\\Theta(n)\\) Best Case (Omega Notation) \\(T(n)\\) = min time on any input of size \\(n\\) , for example sorted array Presentation of Big-Omega : \\(\\Omega(n)\\) Array Sorting Algorithms Time and Space Complexity \u00b6 Comparison of Time Analysis Cases \u00b6 For insertion sort, worst-case time depends on the speed of primitive operations such as Relative Speed (on the same machine) Absolute Speed (on different machines) Asymptotic Analysis Ignore machine-dependent constants Look at the growth of \\(T(n) | n\\rightarrow\\infty\\) Asymptotic Analysis (1) \u00b6 Asymptotic Analysis (2) \u00b6 Theta-Notation (Average-Case) \u00b6 Drop low order terms Ignore leading constants e.g \\[ \\begin{align*} 2n^2+5n+3 &= \\Theta(n^2) \\\\ 3n^3+90n^2-2n+5 &= \\Theta(n^3) \\end{align*} \\] As \\(n\\) gets large, a \\(\\Theta(n^2)\\) algorithm runs faster than a \\(\\Theta(n^3)\\) algorithm Asymptotic Analysis (3) \u00b6 section{ font-size: 25px; } For both algorithms, we can see a minimum item size in the following chart. After this point, we can see performance differences. Some algorithms for small item size can be run faster than others but if you increase item size you will see a reference point that notation proof performance metrics. section{ font-size: 25px; } Insertion Sort - Runtime Analysis (1) \u00b6 Cost Times Insertion - Sort ( A ) ---- ----- --------------------- c1 n 1 . for j = 2 to A.length c2 n -1 2 . key = A [ j ] c3 n -1 3 . // insert A [ j ] into the sorted sequence A [ 1 ...j -1 ] c4 n -1 4 . i = j - 1 c5 k5 5 . while i > 0 and A [ i ] > key do c6 k6 6 . A [ i +1 ] = A [ i ] c7 k6 7 . i = i - 1 c8 n -1 8 . A [ i +1 ] = key we have two loops here, if we sum up costs as follow we can see big-O worst case notation. \\(k_5 = \\sum \\limits_{j=2}^n{t_j}\\) and \\(k_6 = \\sum \\limits_{j=2}^n{t_i-1}\\) for operation counts Insertion Sort - Runtime Analysis (2) \u00b6 cost function can be evaluated as follow; \\[ \\begin{align*} T(n) &=c_1n+c_2(n-1)+ 0(n-1)+c_4(n-1) \\\\ & +c_5\\sum \\limits_{j=2}^n{t_j}+c_6\\sum \\limits_{j=2}^n{t_i-1} \\\\ & +c_7\\sum \\limits_{j=2}^n{t_i-1}+c_8(n-1) \\end{align*} \\] Insertion Sort - Runtime Analysis (3) \u00b6 \\[ \\begin{align*} \\sum \\limits_{j=2}^n j &= (n(n+1)/2)- 1 \\\\ & \\text{ and } \\\\ \\sum \\limits_{j=2}^n {j-1} &= n(n-1)/2 \\end{align*} \\] Insertion Sort - Runtime Analysis (4) \u00b6 \\[ \\begin{align*} T(n) & =(c_5/2 + c_6/2 + c_7/2)n^2 \\\\ & + (c_1+c_2+c_4+c_5/2-c_6/2-c_7/2+c_8)n \\\\ & -(c_2 + c_4 + c_5 + c_6) \\end{align*} \\] Insertion Sort - Runtime Analysis (5) \u00b6 \\[ \\begin{align*} T(n) &= an^2 + bn + c \\\\ &=O(n^2) \\end{align*} \\] section{ font-size: 25px; } Best-Case Scenario (Sorted Array) (1) \u00b6 Problem-1, If \\(A[1...j]\\) is already sorted, what will be \\(t_j=?\\) \\(t_j=1\\) section{ font-size: 25px; } Best-Case Scenario (Sorted Array) (2) \u00b6 Parameters are taken from image \\[ \\begin{align*} T(n) &=c_1n+c_2(n-1)+c_3(n-1) \\\\ & +c_4\\sum \\limits_{j=2}^nt_j+c_5\\sum \\limits_{j=2}^n(t_j-1) \\\\ & +c_6\\sum \\limits_{j=2}^n(t_j-1)+c_7(n-1) \\end{align*} \\] \\(t_j=1\\) for all \\(j\\) \\[ \\begin{align*} T(n) &= (c_1+c_2+c_3+c_4+c_7)n \\\\ &-(c_2+c_3+c_4+c_7) \\\\ T(n) &=an-b \\\\ &=\\Omega(n) \\end{align*} \\] section{ font-size: 25px; } Worst-Case Scenario (Reversed Array) (1) \u00b6 Problem-2 If \\(A[j]\\) is smaller than every entry in \\(A[1...j-1]\\) , what will be \\(t_j=?\\) \\(t_j=?\\) Worst-Case Scenario (Reversed Array) (2) \u00b6 The input array is reverse sorted \\(t_j=j\\) for all \\(j\\) after calculation worst case runtime will be \\[ \\begin{align*} T(n) &=1/2(c_4+c_5+c_6)n^2 \\\\ & +(c_1+c_2+c_3+1/2(c_4-c_5-c_6)+c_7)n -(c_2+c_3+c_4+c_7) \\\\ T(n) &=1/2an^2+bn-c \\\\ &= O(n^2) \\end{align*} \\] Asymptotic Runtime Analysis of Insertion-Sort \u00b6 Insertion-Sort Worst-case (input reverse sorted) \u00b6 Inner Loop is \\(\\Theta(j)\\) \\[ \\begin{align*} T(n) &=\\sum \\limits_{j=2}^n\\Theta(j) \\\\ &=\\Theta(\\sum \\limits_{j=2}^nj) \\\\ &=\\Theta(n^2) \\end{align*} \\] Insertion-Sort Average-case (all permutations uniformly distributed) \u00b6 Inner Loop is \\(\\Theta(j/2)\\) \\[ \\begin{align*} T(n) &=\\sum \\limits_{j=2}^n\\Theta(j/2) \\\\ &=\\sum \\limits_{j=2}^n\\Theta(j) \\\\ &=\\Theta(n^2) \\end{align*} \\] Array Sorting Algorithms Time/Space Complexities \u00b6 To compare this sorting algorithm please check the following map again. Merge Sort : Divide / Conquer / Combine (1) \u00b6 Merge Sort : Divide / Conquer / Combine (2) \u00b6 Divide : we divide the problem into a number of subproblems Conquer : We solve the subproblems recursively Base-Case : Solve by Brute-Force Combine : Subproblem solutions to the original problem Merge Sort Example \u00b6 Merge Sort Algorithm (initial setup) \u00b6 Merge Sort is a recursive sorting algorithm, for initial case we need to call Merge-Sort(A,1,n) for sorting \\(A[1..n]\\) initial case A : Array p : 1 ( offset ) r : n ( length ) Merge - Sort ( A , 1 , n ) Merge Sort Algorithm (internal iterations) \u00b6 internal iterations A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif section{ font-size: 25px; } Merge Sort Algorithm (Combine-1) \u00b6 \\(p = start-point\\) \\(q = mid-point\\) \\(r = end-point\\) Merge Sort Algorithm (Combine-2) \u00b6 brute-force task, merging two sorted subarrays The pseudo-code in the textbook (Sec. 2.3.1) Merge Sort Combine Algorithm (1) \u00b6 Merge ( A , p , q , r ) n1 = q - p +1 n2 = r - q // allocate left and right arrays // increment will be from left to right // left part will be bigger than right part L [ 1 ...n1 +1 ] // left array R [ 1 ...n2 +1 ] // right array // copy left part of array for i = 1 to n1 L [ i ] = A [ p + i -1 ] // copy right part of array for j = 1 to n2 R [ j ] = A [ q + j ] // put end items maximum values for termination L [ n1 +1 ] = inf R [ n2 +1 ] = inf i = 1 , j = 1 for k = p to r if L [ i ] <= R [ j ] A [ k ] = L [ i ] i = i +1 else A [ k ] = R [ j ] j = j +1 What is the complexity of merge operation? \u00b6 You can find by counting loops will provide you base constant nested level will provide you exponent of this constant, if you drop constants you will have complexity we have 3 for loops it will look like \\(3n\\) and \\(\\Theta(n)\\) will be merge complexity Merge Sort Correctness \u00b6 Base case \\(p = r\\) (Trivially correct) Inductive hypothesis MERGE-SORT is correct for any subarray that is a strict (smaller) subset of \\(A[p, q]\\) . General Case MERGE-SORT is correct for \\(A[p, q]\\) . From inductive hypothesis and correctness of Merge. Merge Sort Algorithm (Pseudo-Code) \u00b6 A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif Merge Sort Algorithm Complexity \u00b6 A : Array p : offset r : length Merge - Sort ( A , p , r ) -------------> T ( n ) if p = r then ---------------> Theta ( 1 ) return else q = floor (( p + r ) / 2 ) ----> Theta ( 1 ) Merge - Sort ( A , p , q ) -----> T ( n / 2 ) Merge - Sort ( A , q +1 , r ) ---> T ( n / 2 ) Merge ( A , p , q , r ) --------> Theta ( n ) endif Merge Sort Algorithm Recurrence \u00b6 We can describe a function recursively in terms of itself, to analyze the performance of recursive algorithms \\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\] How To Solve Recurrence (1) \u00b6 \\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\] How To Solve Recurrence (2) \u00b6 We will assume \\(T(n)= \\Theta(1)\\) for sufficiently small \\(n\\) to rewrite equation as \\[ T(n)=2T(n/2)+\\Theta(n) \\] Solution for this equation will be \\(\\Theta(nlgn)\\) with following recursion tree. How To Solve Recurrence (3) \u00b6 Multiply by height \\(\\Theta(lgn)\\) with each level cost \\(\\Theta(n)\\) we can found \\(\\Theta(nlgn)\\) How To Solve Recurrence (4) \u00b6 This tree is binary-tree and binary-tree height is related with item size. How Height of a Binary Tree is Equal to \\(logn\\) ? (1) \u00b6 Merge-Sort recursion tree is a perfect binary tree, a binary tree is a tree which every node has at most two children, A perfect binary tree is binary tree in which all internal nodes have exactly two children and all leaves are at the same level. How Height of a Binary Tree is Equal to \\(logn\\) ? (2) \u00b6 Let \\(n\\) be the number of nodes in the tree and let \\(l_k\\) denote the number of nodes on level k. According to this; \\(l_k = 2l_{k-1}\\) i.e. each level has exactly twice as many nodes as the previous level \\(l_0 = 1\\) , i.e. on the first level we have only one node (the root node) The leaves are at the last level, \\(l_h\\) where \\(h\\) is the height of the tree. How Height of a Binary Tree is Equal to \\(logn\\) ? (3) \u00b6 The total number of nodes in the tree is equal to the sum of the nodes on all the levels: nodes \\(n\\) \\[ \\begin{align*} 1+2^1+2^2+2^3+...+2^h &= n \\\\ 1+2^1+2^2+2^3+...+2^h &= 2^{h+1}-1 \\\\ 2^{h+1}-1 &= n\\\\ 2^{h+1} &= n+1\\\\ log_2{2^{h+1}} &= log_2{(n+1)} \\\\ h+1 &= log_2{(n+1)} \\\\ h &= log_2{(n+1)}-1 \\end{align*} \\] How Height of a Binary Tree is Equal to \\(logn\\) ? (3) \u00b6 If we write it as asymptotic approach, we will have the following result \\[ \\text{height of tree is }h = log_2{(n+1)}-1 = O(logn) \\] also \\[ \\text{number of leaves is } l_h = (n+1)/2 \\] nearly half of the nodes are at the leaves Review \u00b6 \\(\\Theta(nlgn)\\) grows more slowly than \\(\\Theta(n^2)\\) Therefore Merge-Sort beats Insertion-Sort in the worst case In practice Merge-Sort beats Insertion-Sort for \\(n>30\\) or so Asymptotic Notations \u00b6 Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (1) \u00b6 \\(f(n)=O(g(n))\\) if \\(\\exists\\) positive constants \\(c\\) , \\(n_0\\) such that \\[ 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\] Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (2) \u00b6 Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (3) \u00b6 Asymptotic running times of algorithms are usually defined by functions whose domain are \\(N={0, 1, 2, \u2026}\\) (natural numbers) Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (4) \u00b6 Example-1 \u00b6 Show that \\(2n^2 = O(n^3)\\) we need to find two positive constant \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq 2n^2 \\leq cn^3 \\text{ for all } n \\geq n_0 \\] Choose \\(c=2\\) and \\(n_0 = 1\\) \\[ 2n^2 \\leq 2n^3 \\text{ for all } n \\geq 1 \\] Or, choose \\(c=1\\) and \\(n_0=2\\) \\[ 2n^2 \\leq n^3 \\text{ for all } n \\geq 2 \\] Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (5) \u00b6 Example-2 \u00b6 Show that \\(2n^2 + n = O(n^2)\\) We need to find two positive constant \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq {2n^2+n} \\leq cn^2 \\text{ for all } n \\geq n_0 \\] \\[ 2 + (1/n) \\leq c \\text{ for all } n \\geq n_0 \\] Choose \\(c=3\\) and \\(n_0=1\\) \\[ 2n^2 + n \\leq 3n^2 \\text{ for all } n \\geq 1 \\] Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (6) \u00b6 We can say the followings about \\(f(n)=O(g(n))\\) equation The notation is a little sloppy One-way equation, e.q. \\(n^2 = O(n^3)\\) but we cannot say \\(O(n^3)=n^2\\) Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (7) \u00b6 \\(O(g(n))\\) is in fact a set of functions as follow \\(O(g(n)) = \\{ f(n) : \\exists \\text{ positive constant } c, n_0 \\text{ such that } 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\}\\) Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (8) \u00b6 In other words \\(O(g(n))\\) is in fact, the set of functions that have asymptotic upper bound \\(g(n)\\) e.q \\(2n^2 = O(n^3)\\) means \\(2n^2 \\in O(n^3)\\) Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (9) \u00b6 Example-1 \u00b6 \\(10^9n^2 = O(n^2)\\) \\(0 \\leq 10^9n^2 \\leq cn^2 \\text{ for } n \\geq n_0\\) choose \\(c=10^9\\) and \\(n_0=1\\) \\(0 \\leq 10^9n^2 \\leq 10^9n^2 \\text{ for } n \\geq 1\\) CORRECT Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (10) \u00b6 Example-2 \u00b6 \\(100n^{1.9999}=O(n^2)\\) \\(0 \\leq 100n^{1.9999} \\leq cn^2 \\text{ for } n \\geq n_0\\) choose \\(c=100\\) and \\(n_0=1\\) \\(0 \\leq 100n^{1.9999} \\leq 100n^2 \\text{ for } n \\geq 1\\) CORRECT Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (11) \u00b6 Example-3 \u00b6 \\(10^{-9}n^{2.0001} = O(n^2)\\) \\(0 \\leq 10^{-9}n^{2.0001} \\leq cn^2 \\text{ for } n \\geq n_0\\) \\(10^{-9}n^{0.0001} \\leq c \\text{ for } n \\geq n_0\\) INCORRECT (Contradiction) Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (12) \u00b6 If we analysis \\(O(n^2)\\) case, \\(O\\) -notation is an upper bound notation and the runtime \\(T(n)\\) of algorithm A is at least \\(O(n^2 )\\) . \\(O(n^2)\\) : The set of functions with asymptotic upper bound \\(n^2\\) \\(T(n) \\geq O(n^2)\\) means \\(T(n) \\geq h(n)\\) for some \\(h(n) \\in O(n^2)\\) \\(h(n)=0\\) function is also in \\(O(n^2)\\) . Hence : \\(T(n) \\geq 0\\) , runtime must be nonnegative. Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (1) \u00b6 \\(f(n)=\\Omega(g(n))\\) if \\(\\exists\\) positive constants \\(c,n_0\\) such that \\(0 \\leq cg(n) \\leq f(n) , \\forall n \\geq n_0\\) Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (2) \u00b6 Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (3) \u00b6 Example-1 \u00b6 Show that \\(2n^3 = \\Omega(n^2)\\) We need to find two positive constants \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq cn^2 \\leq 2n^3 \\text{ for all } n \\geq n_0 \\] Choose \\(c=1\\) and \\(n_0=1\\) \\[ n^2 \\leq 2n^3 \\text{ for all } n \\geq 1 \\] Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (4) \u00b6 Example-4 \u00b6 Show that \\(\\sqrt{n}=\\Omega(lgn)\\) We need to find two positive constants \\(c\\) and \\(n_0\\) such that: \\[ clgn \\leq \\sqrt{n} \\text{ for all } n \\geq n_0 \\] Choose \\(c=1\\) and \\(n_0=16\\) \\[ lgn \\leq \\sqrt{n} \\text{ for all } n \\geq 16 \\] Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (5) \u00b6 \\(\\Omega(g(n))\\) is the set of functions that have asymptotic lower bound \\(g(n)\\) \\[ \\begin{align*} \\Omega(g(n)) &=\\{ f(n):\\exists \\text{ positive constants } c,n_0 \\text{ such that } \\\\ & 0 \\leq cg(n) \\leq f(n), \\forall n \\geq n_0 \\} \\end{align*} \\] Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (6) \u00b6 Example-1 \u00b6 \\(10^9n^2 = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 10^9n^2 \\text{ for } n\\geq n_0\\) Choose \\(c=10^9\\) and \\(n_0=1\\) \\(0 \\leq 10^9n^2 \\leq 10^9n^2 \\text{ for } n\\geq 1\\) CORRECT Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (7) \u00b6 Example-2 \u00b6 \\(100n^{1.9999} = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 100n^{1.9999} \\text{ for } n \\geq n_0\\) \\(n^{0.0001} \\leq (100/c) \\text{ for } n \\geq n_0\\) INCORRECT (Contradiction) Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (8) \u00b6 Example-3 \u00b6 \\(10^{-9}n^{2.0001} = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 10^{-9}n^{2.0001} \\text{ for } n \\geq n_0\\) Choose \\(c=10^{-9}\\) and \\(n_0=1\\) \\(0 \\leq 10^{-9}n^2 \\leq 10^{-9}n^{2.0001} \\text{ for } n \\geq 1\\) CORRECT Comparison of Notations (1) \u00b6 Comparison of Notations (2) \u00b6 Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (1) \u00b6 \\[ \\begin{align*} f(n) &=\\Theta(g(n)) \\ if \\ \\exists \\ \\text{positive constants} \\ c_1,c_2,n_0 \\text{such that} \\\\ & 0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n), \\forall n \\geq n_0 \\end{align*} \\] Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (2) \u00b6 Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (3) \u00b6 Example-1 \u00b6 Show that \\(2n^2 + n = \\Theta(n^2)\\) We need to find 3 positive constants \\(c_1,c_2\\) and \\(n_0\\) such that: \\(0 \\leq c_1n^2 \\leq 2n^2+n \\leq c_2n^2\\) for all \\(n \\geq n_0\\) \\(c_1 \\leq 2 + (1/n) \\leq c_2\\) for all \\(n \\geq n_0\\) Choose \\(c_1=2, c_2=3\\) and \\(n_0=1\\) \\(2n^2 \\leq 2n^2+n \\leq 3n^2\\) for all \\(n \\geq 1\\) Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (4) \u00b6 Example-2.1 \u00b6 Show that \\(1/2n^2-2n=\\Theta(n^2)\\) We need to find 3 positive constants \\(c_1,c_2\\) and \\(n_0\\) such that: \\(0 \\leq c_1n^2 \\leq 1/2n^2-2n \\leq c_2n^2 \\text{ for all } n \\geq n_0\\) \\(c_1 \\leq 1/2 - 2 / n \\leq c_2 \\text{ for all } n \\geq n_0\\) Choose 3 positive constants \\(c_1,c_2, n_0\\) that satisfy \\(c_1 \\leq 1/2 - 2/n \\leq c_2\\) for all \\(n \\geq n_0\\) Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (5) \u00b6 Example-2.2 \u00b6 Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (6) \u00b6 Example-2.3 \u00b6 \\[ 1/10 \\leq 1/2 - 2/n \\text{ for } n \\geq 5 \\] \\[ 1/2 - 2/n \\leq 1/2 \\text{ for } n \\geq 0 \\] Therefore we can choose \\(c_1 = 1/10, c_2=1/2, n_0=5\\) Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (7) \u00b6 Theorem : leading constants & low-order terms don\u2019t matter Justification : can choose the leading constant large enough to make high-order term dominate other terms Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (8) \u00b6 Example-1 \u00b6 \\(10^9n^2 = \\Theta(n^2)\\) CORRECT \\(100n^{1.9999} = \\Theta(n^2)\\) INCORRECT \\(10^9n^{2.0001} = \\Theta(n^2)\\) INCORRECT Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (9) \u00b6 \\(\\Theta(g(n))\\) is the set of functions that have asymptotically tight bound \\(g(n)\\) \\(\\Theta(g(n))=\\{ f(n):\\) \\(\\exists\\) positive constants \\(c_1,c_2, n_0\\) such that \\(0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n), \\forall n \\geq n_0 \\}\\) Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (10) \u00b6 Theorem : \\(f(n)=\\Theta(g(n))\\) if and only if \\(f(n)=O(g(n))\\) and \\(f(n)=\\Omega(g(n))\\) \\(\\Theta\\) is stronger than both \\(O\\) and \\(\\Omega\\) \\(\\Theta(g(n)) \\subseteq O(g(n)) \\text{ and } \\Theta(g(n)) \\subseteq \\Omega(g(n))\\) Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (11) \u00b6 Example-1.1 \u00b6 Prove that \\(10^{-8}n^2 \\neq \\Theta(n)\\) We can check that \\(10^{-8}n^2 = \\Omega(n)\\) and \\(10^{-8}n^2 \\neq O(n)\\) Proof by contradiction for \\(O(n)\\) notation \\[ \\begin{align*} O(g(n)) &= \\{ f(n) : \\exists \\text{ positive constant } c, n_0 \\text{ such that } \\\\ & 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\} \\end{align*} \\] Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (12) \u00b6 Example-1.2 \u00b6 Suppose positive constants \\(c_2\\) and \\(n_0\\) exist such that: \\(10^{-8}n^2 \\leq c_2n, \\forall n \\geq n_0\\) \\(10^{-8}n \\leq c_2, \\forall n \\geq n_0\\) Contradiction : \\(c_2\\) is a constant Summary of \\(O,\\Omega\\) and \\(\\Theta\\) notations (1) \u00b6 \\(O(g(n))\\) : The set of functions with asymptotic upper bound \\(g(n)\\) \\(\\Omega(g(n))\\) : The set of functions with asymptotic lower bound \\(g(n)\\) \\(\\Theta(n)\\) : The set of functions with asymptotically tight bound \\(g(n)\\) \\(f(n)=\\Theta(g(n)) \\Leftrightarrow f(n)=O(g(n)) \\text{ and } f(n)=\\Omega(g(n))\\) Summary of \\(O,\\Omega\\) and \\(\\Theta\\) notations (2) \u00b6 Small-o / \\(o\\) -Notation : Asymptotic upper bound that is not tight (1) \u00b6 Remember, upper bound provided by big- \\(O\\) notation can be tight or not tight Tight mean values are close the original function e.g. followings are true \\(2n^2 = O(n^2)\\) is asymptotically tight \\(2n = O(n^2)\\) is not asymptotically tight According to this small- \\(o\\) notation is an upper bound that is not asymptotically tight Small-o / \\(o\\) -Notation : Asymptotic upper bound that is not tight (2) \u00b6 Note that in equations equality is removed in small notations \\[ \\begin{align*} o(g(n)) &=\\{ f(n): \\text{ for any constant} c > 0, \\exists \\text{ a constant } n_0 > 0, \\\\ & \\text{ such that } 0 \\leq f(n) < cg(n), \\\\ & \\forall n \\geq n_0 \\} \\end{align*} \\] \\[ \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0 \\] e.g \\(2n=o(n^2)\\) any positive \\(c\\) satisfies but \\(2n^2 \\neq o(n^2)\\) \\(c=2\\) does not satisfy Small-omega / \\(\\omega\\) -Notation: Asymptotic lower bound that is not tight (1) \u00b6 \\[ \\begin{align*} \\omega(g(n)) &= \\{ f(n): \\text{ for any constant } c > 0, \\exists \\text{ a constant } n_0>0, \\\\ & \\text{ such that } 0 \\leq cg(n) < f(n), \\\\ & \\forall n \\geq n_0 \\end{align*} \\] \\[ \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = \\infty \\] e.g. \\(n^2/2=\\omega(n)\\) , any positive \\(c\\) satisfies but \\(n^2/2 \\neq \\omega(n^2)\\) , \\(c=1/2\\) does not satisfy (Important) Analogy to compare of two real numbers (1) \u00b6 \\[ \\begin{align*} f(n) &= O(g(n)) \\leftrightarrow a \\leq b \\\\ f(n) &= \\Omega(g(n)) \\leftrightarrow a \\geq b \\\\ f(n) &= \\Theta(g(n)) \\leftrightarrow a = b \\\\ f(n) &= o(g(n)) \\leftrightarrow a < b \\\\ f(n) &= \\omega(g(n)) \\leftrightarrow a > b \\\\ \\end{align*} \\] (Important) Analogy to compare of two real numbers (2) \u00b6 \\[ \\begin{align*} O \\approx \\leq \\\\ \\Theta \\approx = \\\\ \\Omega \\approx \\geq \\\\ \\omega \\approx > \\\\ o \\approx < \\end{align*} \\] (Important) Trichotomy property for real numbers \u00b6 For any two real numbers \\(a\\) and \\(b\\) , we have either \\(a<b\\) , or \\(a=b\\) , or \\(a>b\\) Trichotomy property does not hold for asymptotic notation, for two functions \\(f(n)\\) and \\(g(n)\\) , it may be the case that neither \\(f(n)=O(g(n))\\) nor \\(f(n)=\\Omega(g(n))\\) holds. e.g. \\(n\\) and \\(n^{1+sin(n)}\\) cannot be compared asymptotically Examples \u00b6 \\(5n^2=O(n^2)\\) TRUE \\(n^2lgn = O(n^2)\\) FALSE \\(5n^2=\\Omega(n^2)\\) TRUE \\(n^2lgn = \\Omega(n^2)\\) TRUE \\(5n^2=\\Theta(n^2)\\) TRUE \\(n^2lgn = \\Theta(n^2)\\) FALSE \\(5n^2=o(n^2)\\) FALSE \\(n^2lgn = o(n^2)\\) FALSE \\(5n^2=\\omega(n^2)\\) FALSE \\(n^2lgn = \\omega(n^2)\\) TRUE \\(2^n = O(3^n)\\) TRUE \\(2^n = \\Omega(3^n)\\) FALSE \\(2^n=o(3^n)\\) TRUE \\(2^n = \\Theta(3^n)\\) FALSE \\(2^n = \\omega(3^n)\\) FALSE section{ font-size: 25px; } Asymptotic Function Properties \u00b6 Transitivity : holds for all e.g. \\(f(n) = \\Theta(g(n)) \\& g(n)=\\Theta(h(n)) \\Rightarrow f(n)=\\Theta(h(n))\\) Reflexivity : holds for \\(\\Theta,O,\\Omega\\) e.g. \\(f(n)=O(f(n))\\) Symmetry : hold only for \\(\\Theta\\) e.g. \\(f(n)=\\Theta(g(n)) \\Leftrightarrow g(n)=\\Theta(f(n))\\) Transpose Symmetry : holds for \\((O \\leftrightarrow \\Omega)\\) and \\((o \\leftrightarrow \\omega)\\) e.g. \\(f(n)=O(g(n))\\Leftrightarrow g(n)=\\Omega(f(n))\\) section{ font-size: 25px; } Using \\(O\\) -Notation to Describe Running Times (1) \u00b6 Used to bound worst-case running times, Implies an upper bound runtime for arbitrary inputs as well Example: Insertion sort has worst-case runtime of \\(O(n^2 )\\) Note: This \\(O(n^2)\\) upper bound also applies to its running time on every input Abuse to say \u201crunning time of insertion sort is \\(O(n^2)\\) \" For a given \\(n\\) , the actual running time depends on the particular input of size \\(n\\) i.e., running time is not only a function of \\(n\\) However, worst-case running time is only a function of \\(n\\) Using \\(O\\) -Notation to Describe Running Times (2) \u00b6 When we say: Running time of insertion sort is \\(O(n^2)\\) What we really mean is Worst-case running time of insertion sort is \\(O(n^2)\\) or equivalently No matter what particular input of size n is chosen, the running time on that set of inputs is \\(O(n^2)\\) Using \\(\\Omega\\) -Notation to Describe Running Times (1) \u00b6 Used to bound best-case running times, Implies a lower bound runtime for arbitrary inputs as well Example: Insertion sort has best-case runtime of \\(\\Omega(n)\\) Note : This \\(\\Omega(n)\\) lower bound also applies to its running time on every input Using \\(\\Omega\\) -Notation to Describe Running Times (2) \u00b6 When we say Running time of algorithm A is \\(\\Omega(g(n))\\) What we mean is For any input of size \\(n\\) , the runtime of A is at least a constant times \\(g(n)\\) for sufficiently large \\(n\\) It\u2019s not contradictory to say worst-case running time of insertion sort is \\(\\Omega(n^2)\\) Because there exists an input that causes the algorithm to take \\(\\Omega(n^2)\\) Using \\(\\Theta\\) -Notation to Describe Running Times (1) \u00b6 Consider 2 cases about the runtime of an algorithm Case 1: Worst-case and best-case not asymptotically equal Use \\(\\Theta\\) -notation to bound worst-case and best-case runtimes separately Case 2: Worst-case and best-case asymptotically equal Use \\(\\Theta\\) -notation to bound the runtime for any input Using \\(\\Theta\\) -Notation to Describe Running Times (2) \u00b6 Case 1: Worst-case and best-case not asymptotically equal Use \\(\\Theta\\) -notation to bound the worst-case and best-case runtimes separately We can say: \"The worst-case runtime of insertion sort is \\(\\Theta(n^2)\\) \" \"The best-case runtime of insertion sort is \\(\\Theta(n)\\) \" But, we can\u2019t say: \"The runtime of insertion sort is \\(\\Theta(n^2)\\) for every input\" A \\(\\Theta\\) -bound on worst/best-case running time does not apply to its running time on arbitrary inputs Worst-Case and Best-Case Equation for Merge-Sort \u00b6 e.g. for merge-sort, we have: \\[ T(n)=\\Theta(nlgn)\\begin{cases} T(n)=O(nlgn)\\\\ T(n)=\\Omega(nlgn)\\end{cases} \\] Using Asymptotic Notation to Describe Runtimes Summary (1) \u00b6 \"The worst case runtime of Insertion Sort is \\(O(n^2)\\) \" Also implies: \"The runtime of Insertion Sort is \\(O(n^2)\\) \" \"The best-case runtime of Insertion Sort is \\(\\Omega(n)\\) \" Also implies: \"The runtime of Insertion Sort is \\(\\Omega(n)\\) \" Using Asymptotic Notation to Describe Runtimes Summary (2) \u00b6 \"The worst case runtime of Insertion Sort is \\(\\Theta(n^2)\\) \" But: \"The runtime of Insertion Sort is not \\(\\Theta(n^2)\\) \" \"The best case runtime of Insertion Sort is \\(\\Theta(n)\\) \" But: \"The runtime of Insertion Sort is not \\(\\Theta(n)\\) \" Using Asymptotic Notation to Describe Runtimes Summary (3) \u00b6 Which one is true? \u00b6 FALSE \"The worst case runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" FALSE \"The best case runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" TRUE \"The runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" This is true, because the best and worst case runtimes have asymptotically the same tight bound \\(\\Theta(nlgn)\\) Asymptotic Notation in Equations (RHS) \u00b6 Asymptotic notation appears alone on the RHS of an equation: implies set membership e.g., \\(n = O(n^2)\\) means \\(n \\in O(n^2)\\) Asymptotic notation appears on the RHS of an equation stands for some anonymous function in the set e.g., \\(2n^2 + 3n + 1 = 2n^2 + \\Theta(n)\\) means: \\(2n^2 + 3n + 1 = 2n^2 + h(n)\\) , for some \\(h(n) \\in \\Theta(n)\\) i.e., \\(h(n) = 3n + 1\\) Asymptotic Notation in Equations (LHS) \u00b6 Asymptotic notation appears on the LHS of an equation: stands for any anonymous function in the set e.g., \\(2n^2 + \\Theta(n) = \\Theta(n^2)\\) means: for any function \\(g(n) \\in \\Theta(n)\\) \\(\\exists\\) some function \\(h(n)\\in \\Theta(n^2)\\) such that \\(2n^2+g(n) = h(n)\\) RHS provides coarser level of detail than LHS References \u00b6 Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) Insertion Sort - GeeksforGeeks NIST Dictionary of Algorithms and Data Structures NIST - Dictionary of Algorithms and Data Structures NIST - big-O notation NIST - big-Omega notation \\(-End-Of-Week-1-Course-Module-\\)","title":"Week-1 (Intro, Asymptotic Notations)"},{"location":"week-1/ce100-week-1-intro/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-1/ce100-week-1-intro/#week-1-introduction-to-analysis-of-algorithms","text":"","title":"Week-1 (Introduction to Analysis of Algorithms)"},{"location":"week-1/ce100-week-1-intro/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-1/ce100-week-1-intro/#brief-description-of-course-and-rules","text":"We will first talk about, Course Plan and Communication Grading System, Homeworks, and Exams please read the syllabus carefully.","title":"Brief Description of Course and Rules"},{"location":"week-1/ce100-week-1-intro/#outline-1","text":"Introduction to Analysis of Algorithms Algorithm Basics Flowgorithm Pseudocode","title":"Outline (1)"},{"location":"week-1/ce100-week-1-intro/#outline-2","text":"RAM (Random Access Machine Model) Sorting Problem Insertion Sort Analysis Algorithm Cost Calculation for Time Complexity Worst, Average, and Best Case Summary Merge Sort Analysis","title":"Outline (2)"},{"location":"week-1/ce100-week-1-intro/#outline-3","text":"Asymptotic Notation Big O Notation Big Teta Notation Big Omega Notation Small o Notation Small omega Notation","title":"Outline (3)"},{"location":"week-1/ce100-week-1-intro/#we-need-mathematical-proofs-1","text":"Direct proof Proof by mathematical induction Proof by contraposition Proof by contradiction Proof by construction Proof by exhaustion","title":"We Need Mathematical Proofs (1)"},{"location":"week-1/ce100-week-1-intro/#we-need-mathematical-proofs-2","text":"Probabilistic proof Combinatorial proof Nonconstructive proof Statistical proofs in pure mathematics Computer-assisted proofs Mathematical proof - Wikipedia","title":"We Need Mathematical Proofs (2)"},{"location":"week-1/ce100-week-1-intro/#introduction-to-analysis-of-algorithms","text":"Study two sorting algorithms as examples Insertion sort: Incremental algorithm Merge sort: Divide-and-conquer Introduction to runtime analysis Best vs. worst vs. average case Asymptotic analysis","title":"Introduction to Analysis of Algorithms"},{"location":"week-1/ce100-week-1-intro/#what-is-algorithm","text":"Algorithm : A sequence of computational steps that transform the input to the desired output Procedure vs. algorithm An algorithm must halt within finite time with the right output We Need to Measure Performance Metrics Processing Time Allocated Memory Network Congestion Power Usage etc. Example Sorting Algorithms Input : a sequence of n numbers \\[ \\langle a_1,a_2,...,a_n \\rangle \\] Algorithm : Sorting / Permutation \\[ \\prod = \\langle \\prod_{(1)},\\prod_{(2)},...,\\prod_{(n)} \\rangle \\] Output : sorted permutation of the input sequence \\[ \\langle a_{\\prod_{(1)}} \\leqslant a_{\\prod_{(2)}} \\leqslant,...,a_{\\prod_{(n)}} \\rangle \\]","title":"What is Algorithm"},{"location":"week-1/ce100-week-1-intro/#pseudo-code-notation-1","text":"Objective: Express algorithms to humans in a clear and concise way Liberal use of English Indentation for block structures Omission of error handling and other details (needed in real programs) You can use Flowgorithm application to understand concept easily.","title":"Pseudo-code notation (1)"},{"location":"week-1/ce100-week-1-intro/#pseudo-code-notation-2","text":"","title":"Pseudo-code notation (2)"},{"location":"week-1/ce100-week-1-intro/#links-and-examples","text":"Wikipedia CS50 University of North Florida GeeksforGeeks","title":"Links and Examples"},{"location":"week-1/ce100-week-1-intro/#correctness-1","text":"We often use a loop invariant to help us to understand why an algorithm gives the correct answer. Example: (Insertion Sort) at the start of each iteration of the \"outer\" for loop - the loop indexed by \\(j\\) - the subarray \\(A[1 \\dots j-1]\\) consist of the elements originally in \\(A[1\\dots j-1]\\) but in sorted order.","title":"Correctness (1)"},{"location":"week-1/ce100-week-1-intro/#correctness-2","text":"To use a loop invariant to prove correctness, we must show 3 things about it. Initialization: It is true to the first iteration of the loop. Maintaince: If it is true before an iteration of the loop, it remains true before the next iteration. Termination: When the loop terminates, the invariant - usually along with the reason that the loop terminated - gives us a usefull property that helps show that the algorithm is correct.","title":"Correctness (2)"},{"location":"week-1/ce100-week-1-intro/#ram-random-access-machine-model-longrightarrow-theta1-1","text":"Operations Single Step Sequential No Concurrent Arithmetic add, subtract, multiply, divide, remainder, floor, ceiling, shift left/shift right (good by multiply/dividing \\(2^k\\) )","title":"RAM (Random Access Machine Model)  \\(\\Longrightarrow \\Theta(1)\\) (1)"},{"location":"week-1/ce100-week-1-intro/#ram-random-access-machine-model-longrightarrow-theta1-2","text":"Data Movement load, store, copy Control conditional / unconditional branch subroutine calls returns","title":"RAM (Random Access Machine Model)  \\(\\Longrightarrow \\Theta(1)\\) (2)"},{"location":"week-1/ce100-week-1-intro/#ram-random-access-machine-model-longrightarrow-theta1-3","text":"Each instruction take a constant amount of time Integer will be represented by \\(clogn\\) \\(c \\geq 1\\) \\(T(n)\\) the running time of the algorithm: \\[ \\sum \\limits_{\\text{all statement}}^{}(\\text{cost of statement})*(\\text{number of times statement is executed}) = T(n) \\]","title":"RAM (Random Access Machine Model)  \\(\\Longrightarrow \\Theta(1)\\) (3)"},{"location":"week-1/ce100-week-1-intro/#what-is-the-processing-time","text":"section{ font-size: 25px; }","title":"What is the processing time ?"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-algorithm-1","text":"Insertion sort is a simple sorting algorithm that works similar to the way you sort playing cards in your hands The array is virtually split into a sorted and an unsorted part Values from the unsorted part are picked and placed at the correct position in the sorted part. Assume input array : \\(A[1..n]\\) Iterate \\(j\\) from \\(2\\) to \\(n\\)","title":"Insertion Sort Algorithm (1)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-algorithm-2","text":"","title":"Insertion Sort Algorithm (2)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-algorithm-pseudo-code-3","text":"Insertion - Sort ( A ) 1 . for j = 2 to A.length 2 . key = A [ j ] 3 . // insert A [ j ] into the sorted sequence A [ 1 ...j -1 ] 4 . i = j - 1 5 . while i > 0 and A [ i ] > key 6 . A [ i +1 ] = A [ i ] 7 . i = i - 1 8 . A [ i +1 ] = key","title":"Insertion Sort Algorithm (Pseudo-Code) (3)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-by-step-description-1","text":"","title":"Insertion Sort Step-By-Step Description (1)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-by-step-description-2","text":"","title":"Insertion Sort Step-By-Step Description (2)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-by-step-description-3","text":"","title":"Insertion Sort Step-By-Step Description (3)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-example","text":"","title":"Insertion Sort Example"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-1-initial","text":"","title":"Insertion Sort Step-1 (initial)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-2-j2","text":"","title":"Insertion Sort Step-2 (j=2)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-3-j3","text":"","title":"Insertion Sort Step-3 (j=3)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-4-j3","text":"","title":"Insertion Sort Step-4 (j=3)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-5-j4","text":"","title":"Insertion Sort Step-5 (j=4)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-6-j5","text":"","title":"Insertion Sort Step-6 (j=5)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-7-j5","text":"","title":"Insertion Sort Step-7 (j=5)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-step-8-j6","text":"","title":"Insertion Sort Step-8 (j=6)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-review-1","text":"Items sorted in-place Elements are rearranged within the array. At a most constant number of items stored outside the array at any time (e.,g. the variable key) Input array \\(A\\) contains a sorted output sequence when the algorithm ends","title":"Insertion Sort Review (1)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-review-2","text":"Incremental approach Having sorted \\(A[1..j-1]\\) , place \\(A[j]\\) correctly so that \\(A[1..j]\\) is sorted Running Time It depends on Input Size (5 elements or 5 billion elements) and Input Itself (partially sorted) Algorithm approach to upper bound of overall performance analysis","title":"Insertion Sort Review (2)"},{"location":"week-1/ce100-week-1-intro/#visualization-of-insertion-sort","text":"Sorting (Bubble, Selection, Insertion, Merge, Quick, Counting, Radix) - VisuAlgo https://www.cs.usfca.edu/~galles/visualization/ComparisonSort.html https://algorithm-visualizer.org/ HMvHTs - Online C++ Compiler & Debugging Tool - Ideone.com","title":"Visualization of Insertion Sort"},{"location":"week-1/ce100-week-1-intro/#kinds-of-running-time-analysis-time-complexity","text":"Worst Case (Big-O Notation) \\(T(n)\\) = maximum processing time of any input \\(n\\) Presentation of Big-O : \\(O(n)\\) Average Case (Teta Notation) \\(T(n)\\) = average time over all inputs of size \\(n\\) , inputs can have a uniform distribution Presentation of Big-Theta : \\(\\Theta(n)\\) Best Case (Omega Notation) \\(T(n)\\) = min time on any input of size \\(n\\) , for example sorted array Presentation of Big-Omega : \\(\\Omega(n)\\)","title":"Kinds of Running Time Analysis (Time Complexity)"},{"location":"week-1/ce100-week-1-intro/#array-sorting-algorithms-time-and-space-complexity","text":"","title":"Array Sorting Algorithms Time and Space Complexity"},{"location":"week-1/ce100-week-1-intro/#comparison-of-time-analysis-cases","text":"For insertion sort, worst-case time depends on the speed of primitive operations such as Relative Speed (on the same machine) Absolute Speed (on different machines) Asymptotic Analysis Ignore machine-dependent constants Look at the growth of \\(T(n) | n\\rightarrow\\infty\\)","title":"Comparison of Time Analysis Cases"},{"location":"week-1/ce100-week-1-intro/#asymptotic-analysis-1","text":"","title":"Asymptotic Analysis (1)"},{"location":"week-1/ce100-week-1-intro/#asymptotic-analysis-2","text":"","title":"Asymptotic Analysis (2)"},{"location":"week-1/ce100-week-1-intro/#theta-notation-average-case","text":"Drop low order terms Ignore leading constants e.g \\[ \\begin{align*} 2n^2+5n+3 &= \\Theta(n^2) \\\\ 3n^3+90n^2-2n+5 &= \\Theta(n^3) \\end{align*} \\] As \\(n\\) gets large, a \\(\\Theta(n^2)\\) algorithm runs faster than a \\(\\Theta(n^3)\\) algorithm","title":"Theta-Notation (Average-Case)"},{"location":"week-1/ce100-week-1-intro/#asymptotic-analysis-3","text":"section{ font-size: 25px; } For both algorithms, we can see a minimum item size in the following chart. After this point, we can see performance differences. Some algorithms for small item size can be run faster than others but if you increase item size you will see a reference point that notation proof performance metrics. section{ font-size: 25px; }","title":"Asymptotic Analysis (3)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-runtime-analysis-1","text":"Cost Times Insertion - Sort ( A ) ---- ----- --------------------- c1 n 1 . for j = 2 to A.length c2 n -1 2 . key = A [ j ] c3 n -1 3 . // insert A [ j ] into the sorted sequence A [ 1 ...j -1 ] c4 n -1 4 . i = j - 1 c5 k5 5 . while i > 0 and A [ i ] > key do c6 k6 6 . A [ i +1 ] = A [ i ] c7 k6 7 . i = i - 1 c8 n -1 8 . A [ i +1 ] = key we have two loops here, if we sum up costs as follow we can see big-O worst case notation. \\(k_5 = \\sum \\limits_{j=2}^n{t_j}\\) and \\(k_6 = \\sum \\limits_{j=2}^n{t_i-1}\\) for operation counts","title":"Insertion Sort - Runtime Analysis (1)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-runtime-analysis-2","text":"cost function can be evaluated as follow; \\[ \\begin{align*} T(n) &=c_1n+c_2(n-1)+ 0(n-1)+c_4(n-1) \\\\ & +c_5\\sum \\limits_{j=2}^n{t_j}+c_6\\sum \\limits_{j=2}^n{t_i-1} \\\\ & +c_7\\sum \\limits_{j=2}^n{t_i-1}+c_8(n-1) \\end{align*} \\]","title":"Insertion Sort - Runtime Analysis (2)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-runtime-analysis-3","text":"\\[ \\begin{align*} \\sum \\limits_{j=2}^n j &= (n(n+1)/2)- 1 \\\\ & \\text{ and } \\\\ \\sum \\limits_{j=2}^n {j-1} &= n(n-1)/2 \\end{align*} \\]","title":"Insertion Sort - Runtime Analysis (3)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-runtime-analysis-4","text":"\\[ \\begin{align*} T(n) & =(c_5/2 + c_6/2 + c_7/2)n^2 \\\\ & + (c_1+c_2+c_4+c_5/2-c_6/2-c_7/2+c_8)n \\\\ & -(c_2 + c_4 + c_5 + c_6) \\end{align*} \\]","title":"Insertion Sort - Runtime Analysis (4)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-runtime-analysis-5","text":"\\[ \\begin{align*} T(n) &= an^2 + bn + c \\\\ &=O(n^2) \\end{align*} \\] section{ font-size: 25px; }","title":"Insertion Sort - Runtime Analysis (5)"},{"location":"week-1/ce100-week-1-intro/#best-case-scenario-sorted-array-1","text":"Problem-1, If \\(A[1...j]\\) is already sorted, what will be \\(t_j=?\\) \\(t_j=1\\) section{ font-size: 25px; }","title":"Best-Case Scenario (Sorted Array) (1)"},{"location":"week-1/ce100-week-1-intro/#best-case-scenario-sorted-array-2","text":"Parameters are taken from image \\[ \\begin{align*} T(n) &=c_1n+c_2(n-1)+c_3(n-1) \\\\ & +c_4\\sum \\limits_{j=2}^nt_j+c_5\\sum \\limits_{j=2}^n(t_j-1) \\\\ & +c_6\\sum \\limits_{j=2}^n(t_j-1)+c_7(n-1) \\end{align*} \\] \\(t_j=1\\) for all \\(j\\) \\[ \\begin{align*} T(n) &= (c_1+c_2+c_3+c_4+c_7)n \\\\ &-(c_2+c_3+c_4+c_7) \\\\ T(n) &=an-b \\\\ &=\\Omega(n) \\end{align*} \\] section{ font-size: 25px; }","title":"Best-Case Scenario (Sorted Array) (2)"},{"location":"week-1/ce100-week-1-intro/#worst-case-scenario-reversed-array-1","text":"Problem-2 If \\(A[j]\\) is smaller than every entry in \\(A[1...j-1]\\) , what will be \\(t_j=?\\) \\(t_j=?\\)","title":"Worst-Case Scenario (Reversed Array) (1)"},{"location":"week-1/ce100-week-1-intro/#worst-case-scenario-reversed-array-2","text":"The input array is reverse sorted \\(t_j=j\\) for all \\(j\\) after calculation worst case runtime will be \\[ \\begin{align*} T(n) &=1/2(c_4+c_5+c_6)n^2 \\\\ & +(c_1+c_2+c_3+1/2(c_4-c_5-c_6)+c_7)n -(c_2+c_3+c_4+c_7) \\\\ T(n) &=1/2an^2+bn-c \\\\ &= O(n^2) \\end{align*} \\]","title":"Worst-Case Scenario (Reversed Array) (2)"},{"location":"week-1/ce100-week-1-intro/#asymptotic-runtime-analysis-of-insertion-sort","text":"","title":"Asymptotic Runtime Analysis of Insertion-Sort"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-worst-case-input-reverse-sorted","text":"Inner Loop is \\(\\Theta(j)\\) \\[ \\begin{align*} T(n) &=\\sum \\limits_{j=2}^n\\Theta(j) \\\\ &=\\Theta(\\sum \\limits_{j=2}^nj) \\\\ &=\\Theta(n^2) \\end{align*} \\]","title":"Insertion-Sort Worst-case (input reverse sorted)"},{"location":"week-1/ce100-week-1-intro/#insertion-sort-average-case-all-permutations-uniformly-distributed","text":"Inner Loop is \\(\\Theta(j/2)\\) \\[ \\begin{align*} T(n) &=\\sum \\limits_{j=2}^n\\Theta(j/2) \\\\ &=\\sum \\limits_{j=2}^n\\Theta(j) \\\\ &=\\Theta(n^2) \\end{align*} \\]","title":"Insertion-Sort Average-case (all permutations uniformly distributed)"},{"location":"week-1/ce100-week-1-intro/#array-sorting-algorithms-timespace-complexities","text":"To compare this sorting algorithm please check the following map again.","title":"Array Sorting Algorithms Time/Space Complexities"},{"location":"week-1/ce100-week-1-intro/#merge-sort-divide-conquer-combine-1","text":"","title":"Merge Sort : Divide / Conquer / Combine (1)"},{"location":"week-1/ce100-week-1-intro/#merge-sort-divide-conquer-combine-2","text":"Divide : we divide the problem into a number of subproblems Conquer : We solve the subproblems recursively Base-Case : Solve by Brute-Force Combine : Subproblem solutions to the original problem","title":"Merge Sort : Divide / Conquer / Combine (2)"},{"location":"week-1/ce100-week-1-intro/#merge-sort-example","text":"","title":"Merge Sort Example"},{"location":"week-1/ce100-week-1-intro/#merge-sort-algorithm-initial-setup","text":"Merge Sort is a recursive sorting algorithm, for initial case we need to call Merge-Sort(A,1,n) for sorting \\(A[1..n]\\) initial case A : Array p : 1 ( offset ) r : n ( length ) Merge - Sort ( A , 1 , n )","title":"Merge Sort Algorithm (initial setup)"},{"location":"week-1/ce100-week-1-intro/#merge-sort-algorithm-internal-iterations","text":"internal iterations A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif section{ font-size: 25px; }","title":"Merge Sort Algorithm (internal iterations)"},{"location":"week-1/ce100-week-1-intro/#merge-sort-algorithm-combine-1","text":"\\(p = start-point\\) \\(q = mid-point\\) \\(r = end-point\\)","title":"Merge Sort Algorithm (Combine-1)"},{"location":"week-1/ce100-week-1-intro/#merge-sort-algorithm-combine-2","text":"brute-force task, merging two sorted subarrays The pseudo-code in the textbook (Sec. 2.3.1)","title":"Merge Sort Algorithm (Combine-2)"},{"location":"week-1/ce100-week-1-intro/#merge-sort-combine-algorithm-1","text":"Merge ( A , p , q , r ) n1 = q - p +1 n2 = r - q // allocate left and right arrays // increment will be from left to right // left part will be bigger than right part L [ 1 ...n1 +1 ] // left array R [ 1 ...n2 +1 ] // right array // copy left part of array for i = 1 to n1 L [ i ] = A [ p + i -1 ] // copy right part of array for j = 1 to n2 R [ j ] = A [ q + j ] // put end items maximum values for termination L [ n1 +1 ] = inf R [ n2 +1 ] = inf i = 1 , j = 1 for k = p to r if L [ i ] <= R [ j ] A [ k ] = L [ i ] i = i +1 else A [ k ] = R [ j ] j = j +1","title":"Merge Sort Combine Algorithm (1)"},{"location":"week-1/ce100-week-1-intro/#what-is-the-complexity-of-merge-operation","text":"You can find by counting loops will provide you base constant nested level will provide you exponent of this constant, if you drop constants you will have complexity we have 3 for loops it will look like \\(3n\\) and \\(\\Theta(n)\\) will be merge complexity","title":"What is the complexity of merge operation?"},{"location":"week-1/ce100-week-1-intro/#merge-sort-correctness","text":"Base case \\(p = r\\) (Trivially correct) Inductive hypothesis MERGE-SORT is correct for any subarray that is a strict (smaller) subset of \\(A[p, q]\\) . General Case MERGE-SORT is correct for \\(A[p, q]\\) . From inductive hypothesis and correctness of Merge.","title":"Merge Sort Correctness"},{"location":"week-1/ce100-week-1-intro/#merge-sort-algorithm-pseudo-code","text":"A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif","title":"Merge Sort Algorithm (Pseudo-Code)"},{"location":"week-1/ce100-week-1-intro/#merge-sort-algorithm-complexity","text":"A : Array p : offset r : length Merge - Sort ( A , p , r ) -------------> T ( n ) if p = r then ---------------> Theta ( 1 ) return else q = floor (( p + r ) / 2 ) ----> Theta ( 1 ) Merge - Sort ( A , p , q ) -----> T ( n / 2 ) Merge - Sort ( A , q +1 , r ) ---> T ( n / 2 ) Merge ( A , p , q , r ) --------> Theta ( n ) endif","title":"Merge Sort Algorithm Complexity"},{"location":"week-1/ce100-week-1-intro/#merge-sort-algorithm-recurrence","text":"We can describe a function recursively in terms of itself, to analyze the performance of recursive algorithms \\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\]","title":"Merge Sort Algorithm Recurrence"},{"location":"week-1/ce100-week-1-intro/#how-to-solve-recurrence-1","text":"\\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\]","title":"How To Solve Recurrence (1)"},{"location":"week-1/ce100-week-1-intro/#how-to-solve-recurrence-2","text":"We will assume \\(T(n)= \\Theta(1)\\) for sufficiently small \\(n\\) to rewrite equation as \\[ T(n)=2T(n/2)+\\Theta(n) \\] Solution for this equation will be \\(\\Theta(nlgn)\\) with following recursion tree.","title":"How To Solve Recurrence (2)"},{"location":"week-1/ce100-week-1-intro/#how-to-solve-recurrence-3","text":"Multiply by height \\(\\Theta(lgn)\\) with each level cost \\(\\Theta(n)\\) we can found \\(\\Theta(nlgn)\\)","title":"How To Solve Recurrence (3)"},{"location":"week-1/ce100-week-1-intro/#how-to-solve-recurrence-4","text":"This tree is binary-tree and binary-tree height is related with item size.","title":"How To Solve Recurrence (4)"},{"location":"week-1/ce100-week-1-intro/#how-height-of-a-binary-tree-is-equal-to-logn-1","text":"Merge-Sort recursion tree is a perfect binary tree, a binary tree is a tree which every node has at most two children, A perfect binary tree is binary tree in which all internal nodes have exactly two children and all leaves are at the same level.","title":"How Height of a Binary Tree is Equal to \\(logn\\) ? (1)"},{"location":"week-1/ce100-week-1-intro/#how-height-of-a-binary-tree-is-equal-to-logn-2","text":"Let \\(n\\) be the number of nodes in the tree and let \\(l_k\\) denote the number of nodes on level k. According to this; \\(l_k = 2l_{k-1}\\) i.e. each level has exactly twice as many nodes as the previous level \\(l_0 = 1\\) , i.e. on the first level we have only one node (the root node) The leaves are at the last level, \\(l_h\\) where \\(h\\) is the height of the tree.","title":"How Height of a Binary Tree is Equal to \\(logn\\) ? (2)"},{"location":"week-1/ce100-week-1-intro/#how-height-of-a-binary-tree-is-equal-to-logn-3","text":"The total number of nodes in the tree is equal to the sum of the nodes on all the levels: nodes \\(n\\) \\[ \\begin{align*} 1+2^1+2^2+2^3+...+2^h &= n \\\\ 1+2^1+2^2+2^3+...+2^h &= 2^{h+1}-1 \\\\ 2^{h+1}-1 &= n\\\\ 2^{h+1} &= n+1\\\\ log_2{2^{h+1}} &= log_2{(n+1)} \\\\ h+1 &= log_2{(n+1)} \\\\ h &= log_2{(n+1)}-1 \\end{align*} \\]","title":"How Height of a Binary Tree is Equal to \\(logn\\) ? (3)"},{"location":"week-1/ce100-week-1-intro/#how-height-of-a-binary-tree-is-equal-to-logn-3_1","text":"If we write it as asymptotic approach, we will have the following result \\[ \\text{height of tree is }h = log_2{(n+1)}-1 = O(logn) \\] also \\[ \\text{number of leaves is } l_h = (n+1)/2 \\] nearly half of the nodes are at the leaves","title":"How Height of a Binary Tree is Equal to \\(logn\\) ? (3)"},{"location":"week-1/ce100-week-1-intro/#review","text":"\\(\\Theta(nlgn)\\) grows more slowly than \\(\\Theta(n^2)\\) Therefore Merge-Sort beats Insertion-Sort in the worst case In practice Merge-Sort beats Insertion-Sort for \\(n>30\\) or so","title":"Review"},{"location":"week-1/ce100-week-1-intro/#asymptotic-notations","text":"","title":"Asymptotic Notations"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-1","text":"\\(f(n)=O(g(n))\\) if \\(\\exists\\) positive constants \\(c\\) , \\(n_0\\) such that \\[ 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\]","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (1)"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-2","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (2)"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-3","text":"Asymptotic running times of algorithms are usually defined by functions whose domain are \\(N={0, 1, 2, \u2026}\\) (natural numbers)","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (3)"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-4","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (4)"},{"location":"week-1/ce100-week-1-intro/#example-1","text":"Show that \\(2n^2 = O(n^3)\\) we need to find two positive constant \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq 2n^2 \\leq cn^3 \\text{ for all } n \\geq n_0 \\] Choose \\(c=2\\) and \\(n_0 = 1\\) \\[ 2n^2 \\leq 2n^3 \\text{ for all } n \\geq 1 \\] Or, choose \\(c=1\\) and \\(n_0=2\\) \\[ 2n^2 \\leq n^3 \\text{ for all } n \\geq 2 \\]","title":"Example-1"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-5","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (5)"},{"location":"week-1/ce100-week-1-intro/#example-2","text":"Show that \\(2n^2 + n = O(n^2)\\) We need to find two positive constant \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq {2n^2+n} \\leq cn^2 \\text{ for all } n \\geq n_0 \\] \\[ 2 + (1/n) \\leq c \\text{ for all } n \\geq n_0 \\] Choose \\(c=3\\) and \\(n_0=1\\) \\[ 2n^2 + n \\leq 3n^2 \\text{ for all } n \\geq 1 \\]","title":"Example-2"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-6","text":"We can say the followings about \\(f(n)=O(g(n))\\) equation The notation is a little sloppy One-way equation, e.q. \\(n^2 = O(n^3)\\) but we cannot say \\(O(n^3)=n^2\\)","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (6)"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-7","text":"\\(O(g(n))\\) is in fact a set of functions as follow \\(O(g(n)) = \\{ f(n) : \\exists \\text{ positive constant } c, n_0 \\text{ such that } 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\}\\)","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (7)"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-8","text":"In other words \\(O(g(n))\\) is in fact, the set of functions that have asymptotic upper bound \\(g(n)\\) e.q \\(2n^2 = O(n^3)\\) means \\(2n^2 \\in O(n^3)\\)","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (8)"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-9","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (9)"},{"location":"week-1/ce100-week-1-intro/#example-1_1","text":"\\(10^9n^2 = O(n^2)\\) \\(0 \\leq 10^9n^2 \\leq cn^2 \\text{ for } n \\geq n_0\\) choose \\(c=10^9\\) and \\(n_0=1\\) \\(0 \\leq 10^9n^2 \\leq 10^9n^2 \\text{ for } n \\geq 1\\) CORRECT","title":"Example-1"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-10","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (10)"},{"location":"week-1/ce100-week-1-intro/#example-2_1","text":"\\(100n^{1.9999}=O(n^2)\\) \\(0 \\leq 100n^{1.9999} \\leq cn^2 \\text{ for } n \\geq n_0\\) choose \\(c=100\\) and \\(n_0=1\\) \\(0 \\leq 100n^{1.9999} \\leq 100n^2 \\text{ for } n \\geq 1\\) CORRECT","title":"Example-2"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-11","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (11)"},{"location":"week-1/ce100-week-1-intro/#example-3","text":"\\(10^{-9}n^{2.0001} = O(n^2)\\) \\(0 \\leq 10^{-9}n^{2.0001} \\leq cn^2 \\text{ for } n \\geq n_0\\) \\(10^{-9}n^{0.0001} \\leq c \\text{ for } n \\geq n_0\\) INCORRECT (Contradiction)","title":"Example-3"},{"location":"week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-12","text":"If we analysis \\(O(n^2)\\) case, \\(O\\) -notation is an upper bound notation and the runtime \\(T(n)\\) of algorithm A is at least \\(O(n^2 )\\) . \\(O(n^2)\\) : The set of functions with asymptotic upper bound \\(n^2\\) \\(T(n) \\geq O(n^2)\\) means \\(T(n) \\geq h(n)\\) for some \\(h(n) \\in O(n^2)\\) \\(h(n)=0\\) function is also in \\(O(n^2)\\) . Hence : \\(T(n) \\geq 0\\) , runtime must be nonnegative.","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (12)"},{"location":"week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-1","text":"\\(f(n)=\\Omega(g(n))\\) if \\(\\exists\\) positive constants \\(c,n_0\\) such that \\(0 \\leq cg(n) \\leq f(n) , \\forall n \\geq n_0\\)","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (1)"},{"location":"week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-2","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (2)"},{"location":"week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-3","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (3)"},{"location":"week-1/ce100-week-1-intro/#example-1_2","text":"Show that \\(2n^3 = \\Omega(n^2)\\) We need to find two positive constants \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq cn^2 \\leq 2n^3 \\text{ for all } n \\geq n_0 \\] Choose \\(c=1\\) and \\(n_0=1\\) \\[ n^2 \\leq 2n^3 \\text{ for all } n \\geq 1 \\]","title":"Example-1"},{"location":"week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-4","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (4)"},{"location":"week-1/ce100-week-1-intro/#example-4","text":"Show that \\(\\sqrt{n}=\\Omega(lgn)\\) We need to find two positive constants \\(c\\) and \\(n_0\\) such that: \\[ clgn \\leq \\sqrt{n} \\text{ for all } n \\geq n_0 \\] Choose \\(c=1\\) and \\(n_0=16\\) \\[ lgn \\leq \\sqrt{n} \\text{ for all } n \\geq 16 \\]","title":"Example-4"},{"location":"week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-5","text":"\\(\\Omega(g(n))\\) is the set of functions that have asymptotic lower bound \\(g(n)\\) \\[ \\begin{align*} \\Omega(g(n)) &=\\{ f(n):\\exists \\text{ positive constants } c,n_0 \\text{ such that } \\\\ & 0 \\leq cg(n) \\leq f(n), \\forall n \\geq n_0 \\} \\end{align*} \\]","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (5)"},{"location":"week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-6","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (6)"},{"location":"week-1/ce100-week-1-intro/#example-1_3","text":"\\(10^9n^2 = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 10^9n^2 \\text{ for } n\\geq n_0\\) Choose \\(c=10^9\\) and \\(n_0=1\\) \\(0 \\leq 10^9n^2 \\leq 10^9n^2 \\text{ for } n\\geq 1\\) CORRECT","title":"Example-1"},{"location":"week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-7","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (7)"},{"location":"week-1/ce100-week-1-intro/#example-2_2","text":"\\(100n^{1.9999} = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 100n^{1.9999} \\text{ for } n \\geq n_0\\) \\(n^{0.0001} \\leq (100/c) \\text{ for } n \\geq n_0\\) INCORRECT (Contradiction)","title":"Example-2"},{"location":"week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-8","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (8)"},{"location":"week-1/ce100-week-1-intro/#example-3_1","text":"\\(10^{-9}n^{2.0001} = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 10^{-9}n^{2.0001} \\text{ for } n \\geq n_0\\) Choose \\(c=10^{-9}\\) and \\(n_0=1\\) \\(0 \\leq 10^{-9}n^2 \\leq 10^{-9}n^{2.0001} \\text{ for } n \\geq 1\\) CORRECT","title":"Example-3"},{"location":"week-1/ce100-week-1-intro/#comparison-of-notations-1","text":"","title":"Comparison of Notations (1)"},{"location":"week-1/ce100-week-1-intro/#comparison-of-notations-2","text":"","title":"Comparison of Notations (2)"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-1","text":"\\[ \\begin{align*} f(n) &=\\Theta(g(n)) \\ if \\ \\exists \\ \\text{positive constants} \\ c_1,c_2,n_0 \\text{such that} \\\\ & 0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n), \\forall n \\geq n_0 \\end{align*} \\]","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (1)"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-2","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (2)"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-3","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (3)"},{"location":"week-1/ce100-week-1-intro/#example-1_4","text":"Show that \\(2n^2 + n = \\Theta(n^2)\\) We need to find 3 positive constants \\(c_1,c_2\\) and \\(n_0\\) such that: \\(0 \\leq c_1n^2 \\leq 2n^2+n \\leq c_2n^2\\) for all \\(n \\geq n_0\\) \\(c_1 \\leq 2 + (1/n) \\leq c_2\\) for all \\(n \\geq n_0\\) Choose \\(c_1=2, c_2=3\\) and \\(n_0=1\\) \\(2n^2 \\leq 2n^2+n \\leq 3n^2\\) for all \\(n \\geq 1\\)","title":"Example-1"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-4","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (4)"},{"location":"week-1/ce100-week-1-intro/#example-21","text":"Show that \\(1/2n^2-2n=\\Theta(n^2)\\) We need to find 3 positive constants \\(c_1,c_2\\) and \\(n_0\\) such that: \\(0 \\leq c_1n^2 \\leq 1/2n^2-2n \\leq c_2n^2 \\text{ for all } n \\geq n_0\\) \\(c_1 \\leq 1/2 - 2 / n \\leq c_2 \\text{ for all } n \\geq n_0\\) Choose 3 positive constants \\(c_1,c_2, n_0\\) that satisfy \\(c_1 \\leq 1/2 - 2/n \\leq c_2\\) for all \\(n \\geq n_0\\)","title":"Example-2.1"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-5","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (5)"},{"location":"week-1/ce100-week-1-intro/#example-22","text":"","title":"Example-2.2"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-6","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (6)"},{"location":"week-1/ce100-week-1-intro/#example-23","text":"\\[ 1/10 \\leq 1/2 - 2/n \\text{ for } n \\geq 5 \\] \\[ 1/2 - 2/n \\leq 1/2 \\text{ for } n \\geq 0 \\] Therefore we can choose \\(c_1 = 1/10, c_2=1/2, n_0=5\\)","title":"Example-2.3"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-7","text":"Theorem : leading constants & low-order terms don\u2019t matter Justification : can choose the leading constant large enough to make high-order term dominate other terms","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (7)"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-8","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (8)"},{"location":"week-1/ce100-week-1-intro/#example-1_5","text":"\\(10^9n^2 = \\Theta(n^2)\\) CORRECT \\(100n^{1.9999} = \\Theta(n^2)\\) INCORRECT \\(10^9n^{2.0001} = \\Theta(n^2)\\) INCORRECT","title":"Example-1"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-9","text":"\\(\\Theta(g(n))\\) is the set of functions that have asymptotically tight bound \\(g(n)\\) \\(\\Theta(g(n))=\\{ f(n):\\) \\(\\exists\\) positive constants \\(c_1,c_2, n_0\\) such that \\(0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n), \\forall n \\geq n_0 \\}\\)","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (9)"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-10","text":"Theorem : \\(f(n)=\\Theta(g(n))\\) if and only if \\(f(n)=O(g(n))\\) and \\(f(n)=\\Omega(g(n))\\) \\(\\Theta\\) is stronger than both \\(O\\) and \\(\\Omega\\) \\(\\Theta(g(n)) \\subseteq O(g(n)) \\text{ and } \\Theta(g(n)) \\subseteq \\Omega(g(n))\\)","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (10)"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-11","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (11)"},{"location":"week-1/ce100-week-1-intro/#example-11","text":"Prove that \\(10^{-8}n^2 \\neq \\Theta(n)\\) We can check that \\(10^{-8}n^2 = \\Omega(n)\\) and \\(10^{-8}n^2 \\neq O(n)\\) Proof by contradiction for \\(O(n)\\) notation \\[ \\begin{align*} O(g(n)) &= \\{ f(n) : \\exists \\text{ positive constant } c, n_0 \\text{ such that } \\\\ & 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\} \\end{align*} \\]","title":"Example-1.1"},{"location":"week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-12","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (12)"},{"location":"week-1/ce100-week-1-intro/#example-12","text":"Suppose positive constants \\(c_2\\) and \\(n_0\\) exist such that: \\(10^{-8}n^2 \\leq c_2n, \\forall n \\geq n_0\\) \\(10^{-8}n \\leq c_2, \\forall n \\geq n_0\\) Contradiction : \\(c_2\\) is a constant","title":"Example-1.2"},{"location":"week-1/ce100-week-1-intro/#summary-of-oomega-and-theta-notations-1","text":"\\(O(g(n))\\) : The set of functions with asymptotic upper bound \\(g(n)\\) \\(\\Omega(g(n))\\) : The set of functions with asymptotic lower bound \\(g(n)\\) \\(\\Theta(n)\\) : The set of functions with asymptotically tight bound \\(g(n)\\) \\(f(n)=\\Theta(g(n)) \\Leftrightarrow f(n)=O(g(n)) \\text{ and } f(n)=\\Omega(g(n))\\)","title":"Summary of \\(O,\\Omega\\) and \\(\\Theta\\) notations (1)"},{"location":"week-1/ce100-week-1-intro/#summary-of-oomega-and-theta-notations-2","text":"","title":"Summary of \\(O,\\Omega\\) and \\(\\Theta\\) notations (2)"},{"location":"week-1/ce100-week-1-intro/#small-o-o-notation-asymptotic-upper-bound-that-is-not-tight-1","text":"Remember, upper bound provided by big- \\(O\\) notation can be tight or not tight Tight mean values are close the original function e.g. followings are true \\(2n^2 = O(n^2)\\) is asymptotically tight \\(2n = O(n^2)\\) is not asymptotically tight According to this small- \\(o\\) notation is an upper bound that is not asymptotically tight","title":"Small-o / \\(o\\)-Notation : Asymptotic upper bound that is not tight (1)"},{"location":"week-1/ce100-week-1-intro/#small-o-o-notation-asymptotic-upper-bound-that-is-not-tight-2","text":"Note that in equations equality is removed in small notations \\[ \\begin{align*} o(g(n)) &=\\{ f(n): \\text{ for any constant} c > 0, \\exists \\text{ a constant } n_0 > 0, \\\\ & \\text{ such that } 0 \\leq f(n) < cg(n), \\\\ & \\forall n \\geq n_0 \\} \\end{align*} \\] \\[ \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0 \\] e.g \\(2n=o(n^2)\\) any positive \\(c\\) satisfies but \\(2n^2 \\neq o(n^2)\\) \\(c=2\\) does not satisfy","title":"Small-o / \\(o\\)-Notation : Asymptotic upper bound that is not tight (2)"},{"location":"week-1/ce100-week-1-intro/#small-omega-omega-notation-asymptotic-lower-bound-that-is-not-tight-1","text":"\\[ \\begin{align*} \\omega(g(n)) &= \\{ f(n): \\text{ for any constant } c > 0, \\exists \\text{ a constant } n_0>0, \\\\ & \\text{ such that } 0 \\leq cg(n) < f(n), \\\\ & \\forall n \\geq n_0 \\end{align*} \\] \\[ \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = \\infty \\] e.g. \\(n^2/2=\\omega(n)\\) , any positive \\(c\\) satisfies but \\(n^2/2 \\neq \\omega(n^2)\\) , \\(c=1/2\\) does not satisfy","title":"Small-omega / \\(\\omega\\)-Notation: Asymptotic lower bound that is not tight (1)"},{"location":"week-1/ce100-week-1-intro/#important-analogy-to-compare-of-two-real-numbers-1","text":"\\[ \\begin{align*} f(n) &= O(g(n)) \\leftrightarrow a \\leq b \\\\ f(n) &= \\Omega(g(n)) \\leftrightarrow a \\geq b \\\\ f(n) &= \\Theta(g(n)) \\leftrightarrow a = b \\\\ f(n) &= o(g(n)) \\leftrightarrow a < b \\\\ f(n) &= \\omega(g(n)) \\leftrightarrow a > b \\\\ \\end{align*} \\]","title":"(Important) Analogy to compare of two real numbers (1)"},{"location":"week-1/ce100-week-1-intro/#important-analogy-to-compare-of-two-real-numbers-2","text":"\\[ \\begin{align*} O \\approx \\leq \\\\ \\Theta \\approx = \\\\ \\Omega \\approx \\geq \\\\ \\omega \\approx > \\\\ o \\approx < \\end{align*} \\]","title":"(Important) Analogy to compare of two real numbers (2)"},{"location":"week-1/ce100-week-1-intro/#important-trichotomy-property-for-real-numbers","text":"For any two real numbers \\(a\\) and \\(b\\) , we have either \\(a<b\\) , or \\(a=b\\) , or \\(a>b\\) Trichotomy property does not hold for asymptotic notation, for two functions \\(f(n)\\) and \\(g(n)\\) , it may be the case that neither \\(f(n)=O(g(n))\\) nor \\(f(n)=\\Omega(g(n))\\) holds. e.g. \\(n\\) and \\(n^{1+sin(n)}\\) cannot be compared asymptotically","title":"(Important) Trichotomy property for real numbers"},{"location":"week-1/ce100-week-1-intro/#examples","text":"\\(5n^2=O(n^2)\\) TRUE \\(n^2lgn = O(n^2)\\) FALSE \\(5n^2=\\Omega(n^2)\\) TRUE \\(n^2lgn = \\Omega(n^2)\\) TRUE \\(5n^2=\\Theta(n^2)\\) TRUE \\(n^2lgn = \\Theta(n^2)\\) FALSE \\(5n^2=o(n^2)\\) FALSE \\(n^2lgn = o(n^2)\\) FALSE \\(5n^2=\\omega(n^2)\\) FALSE \\(n^2lgn = \\omega(n^2)\\) TRUE \\(2^n = O(3^n)\\) TRUE \\(2^n = \\Omega(3^n)\\) FALSE \\(2^n=o(3^n)\\) TRUE \\(2^n = \\Theta(3^n)\\) FALSE \\(2^n = \\omega(3^n)\\) FALSE section{ font-size: 25px; }","title":"Examples"},{"location":"week-1/ce100-week-1-intro/#asymptotic-function-properties","text":"Transitivity : holds for all e.g. \\(f(n) = \\Theta(g(n)) \\& g(n)=\\Theta(h(n)) \\Rightarrow f(n)=\\Theta(h(n))\\) Reflexivity : holds for \\(\\Theta,O,\\Omega\\) e.g. \\(f(n)=O(f(n))\\) Symmetry : hold only for \\(\\Theta\\) e.g. \\(f(n)=\\Theta(g(n)) \\Leftrightarrow g(n)=\\Theta(f(n))\\) Transpose Symmetry : holds for \\((O \\leftrightarrow \\Omega)\\) and \\((o \\leftrightarrow \\omega)\\) e.g. \\(f(n)=O(g(n))\\Leftrightarrow g(n)=\\Omega(f(n))\\) section{ font-size: 25px; }","title":"Asymptotic Function Properties"},{"location":"week-1/ce100-week-1-intro/#using-o-notation-to-describe-running-times-1","text":"Used to bound worst-case running times, Implies an upper bound runtime for arbitrary inputs as well Example: Insertion sort has worst-case runtime of \\(O(n^2 )\\) Note: This \\(O(n^2)\\) upper bound also applies to its running time on every input Abuse to say \u201crunning time of insertion sort is \\(O(n^2)\\) \" For a given \\(n\\) , the actual running time depends on the particular input of size \\(n\\) i.e., running time is not only a function of \\(n\\) However, worst-case running time is only a function of \\(n\\)","title":"Using \\(O\\)-Notation to Describe Running Times (1)"},{"location":"week-1/ce100-week-1-intro/#using-o-notation-to-describe-running-times-2","text":"When we say: Running time of insertion sort is \\(O(n^2)\\) What we really mean is Worst-case running time of insertion sort is \\(O(n^2)\\) or equivalently No matter what particular input of size n is chosen, the running time on that set of inputs is \\(O(n^2)\\)","title":"Using \\(O\\)-Notation to Describe Running Times (2)"},{"location":"week-1/ce100-week-1-intro/#using-omega-notation-to-describe-running-times-1","text":"Used to bound best-case running times, Implies a lower bound runtime for arbitrary inputs as well Example: Insertion sort has best-case runtime of \\(\\Omega(n)\\) Note : This \\(\\Omega(n)\\) lower bound also applies to its running time on every input","title":"Using \\(\\Omega\\)-Notation to Describe Running Times (1)"},{"location":"week-1/ce100-week-1-intro/#using-omega-notation-to-describe-running-times-2","text":"When we say Running time of algorithm A is \\(\\Omega(g(n))\\) What we mean is For any input of size \\(n\\) , the runtime of A is at least a constant times \\(g(n)\\) for sufficiently large \\(n\\) It\u2019s not contradictory to say worst-case running time of insertion sort is \\(\\Omega(n^2)\\) Because there exists an input that causes the algorithm to take \\(\\Omega(n^2)\\)","title":"Using \\(\\Omega\\)-Notation to Describe Running Times (2)"},{"location":"week-1/ce100-week-1-intro/#using-theta-notation-to-describe-running-times-1","text":"Consider 2 cases about the runtime of an algorithm Case 1: Worst-case and best-case not asymptotically equal Use \\(\\Theta\\) -notation to bound worst-case and best-case runtimes separately Case 2: Worst-case and best-case asymptotically equal Use \\(\\Theta\\) -notation to bound the runtime for any input","title":"Using \\(\\Theta\\)-Notation to Describe Running Times (1)"},{"location":"week-1/ce100-week-1-intro/#using-theta-notation-to-describe-running-times-2","text":"Case 1: Worst-case and best-case not asymptotically equal Use \\(\\Theta\\) -notation to bound the worst-case and best-case runtimes separately We can say: \"The worst-case runtime of insertion sort is \\(\\Theta(n^2)\\) \" \"The best-case runtime of insertion sort is \\(\\Theta(n)\\) \" But, we can\u2019t say: \"The runtime of insertion sort is \\(\\Theta(n^2)\\) for every input\" A \\(\\Theta\\) -bound on worst/best-case running time does not apply to its running time on arbitrary inputs","title":"Using \\(\\Theta\\)-Notation to Describe Running Times (2)"},{"location":"week-1/ce100-week-1-intro/#worst-case-and-best-case-equation-for-merge-sort","text":"e.g. for merge-sort, we have: \\[ T(n)=\\Theta(nlgn)\\begin{cases} T(n)=O(nlgn)\\\\ T(n)=\\Omega(nlgn)\\end{cases} \\]","title":"Worst-Case and Best-Case Equation for Merge-Sort"},{"location":"week-1/ce100-week-1-intro/#using-asymptotic-notation-to-describe-runtimes-summary-1","text":"\"The worst case runtime of Insertion Sort is \\(O(n^2)\\) \" Also implies: \"The runtime of Insertion Sort is \\(O(n^2)\\) \" \"The best-case runtime of Insertion Sort is \\(\\Omega(n)\\) \" Also implies: \"The runtime of Insertion Sort is \\(\\Omega(n)\\) \"","title":"Using Asymptotic Notation to Describe Runtimes Summary (1)"},{"location":"week-1/ce100-week-1-intro/#using-asymptotic-notation-to-describe-runtimes-summary-2","text":"\"The worst case runtime of Insertion Sort is \\(\\Theta(n^2)\\) \" But: \"The runtime of Insertion Sort is not \\(\\Theta(n^2)\\) \" \"The best case runtime of Insertion Sort is \\(\\Theta(n)\\) \" But: \"The runtime of Insertion Sort is not \\(\\Theta(n)\\) \"","title":"Using Asymptotic Notation to Describe Runtimes Summary (2)"},{"location":"week-1/ce100-week-1-intro/#using-asymptotic-notation-to-describe-runtimes-summary-3","text":"","title":"Using Asymptotic Notation to Describe Runtimes Summary (3)"},{"location":"week-1/ce100-week-1-intro/#which-one-is-true","text":"FALSE \"The worst case runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" FALSE \"The best case runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" TRUE \"The runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" This is true, because the best and worst case runtimes have asymptotically the same tight bound \\(\\Theta(nlgn)\\)","title":"Which one is true?"},{"location":"week-1/ce100-week-1-intro/#asymptotic-notation-in-equations-rhs","text":"Asymptotic notation appears alone on the RHS of an equation: implies set membership e.g., \\(n = O(n^2)\\) means \\(n \\in O(n^2)\\) Asymptotic notation appears on the RHS of an equation stands for some anonymous function in the set e.g., \\(2n^2 + 3n + 1 = 2n^2 + \\Theta(n)\\) means: \\(2n^2 + 3n + 1 = 2n^2 + h(n)\\) , for some \\(h(n) \\in \\Theta(n)\\) i.e., \\(h(n) = 3n + 1\\)","title":"Asymptotic Notation in Equations (RHS)"},{"location":"week-1/ce100-week-1-intro/#asymptotic-notation-in-equations-lhs","text":"Asymptotic notation appears on the LHS of an equation: stands for any anonymous function in the set e.g., \\(2n^2 + \\Theta(n) = \\Theta(n^2)\\) means: for any function \\(g(n) \\in \\Theta(n)\\) \\(\\exists\\) some function \\(h(n)\\in \\Theta(n^2)\\) such that \\(2n^2+g(n) = h(n)\\) RHS provides coarser level of detail than LHS","title":"Asymptotic Notation in Equations (LHS)"},{"location":"week-1/ce100-week-1-intro/#references","text":"Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) Insertion Sort - GeeksforGeeks NIST Dictionary of Algorithms and Data Structures NIST - Dictionary of Algorithms and Data Structures NIST - big-O notation NIST - big-Omega notation \\(-End-Of-Week-1-Course-Module-\\)","title":"References"},{"location":"week-10/ce100-week-10-graphs/","text":"CE100 Algorithms and Programming II \u00b6 Week-10 (Graphs) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Graphs \u00b6 Outline \u00b6 Introduction to Graphs Graphs and Representation BFS (Breath-First Search) DFS (Depth-First Search) in-order post-order pre-order Topological Order SCC (Strongly Connected Components) MST Prim Kruskal References \u00b6 TODO","title":"Week-10 (Graphs)"},{"location":"week-10/ce100-week-10-graphs/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-10/ce100-week-10-graphs/#week-10-graphs","text":"","title":"Week-10 (Graphs)"},{"location":"week-10/ce100-week-10-graphs/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-10/ce100-week-10-graphs/#graphs","text":"","title":"Graphs"},{"location":"week-10/ce100-week-10-graphs/#outline","text":"Introduction to Graphs Graphs and Representation BFS (Breath-First Search) DFS (Depth-First Search) in-order post-order pre-order Topological Order SCC (Strongly Connected Components) MST Prim Kruskal","title":"Outline"},{"location":"week-10/ce100-week-10-graphs/#references","text":"TODO","title":"References"},{"location":"week-11/ce100-week-11-shortestpath/","text":"CE100 Algorithms and Programming II \u00b6 Week-11 (Shortest Path) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Shortest Path \u00b6 Outline \u00b6 Disjoint Sets and Kruskal Relationships Single-Source Shortest Paths Bellman-Ford Dijkstra Q-Learning Shortest Path Max-Flow Min-Cut Ford-Fulkerson Edmond\u2019s Karp Dinic References \u00b6 TODO","title":"Week-11 (Shortest Path)"},{"location":"week-11/ce100-week-11-shortestpath/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-11/ce100-week-11-shortestpath/#week-11-shortest-path","text":"","title":"Week-11 (Shortest Path)"},{"location":"week-11/ce100-week-11-shortestpath/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-11/ce100-week-11-shortestpath/#shortest-path","text":"","title":"Shortest Path"},{"location":"week-11/ce100-week-11-shortestpath/#outline","text":"Disjoint Sets and Kruskal Relationships Single-Source Shortest Paths Bellman-Ford Dijkstra Q-Learning Shortest Path Max-Flow Min-Cut Ford-Fulkerson Edmond\u2019s Karp Dinic","title":"Outline"},{"location":"week-11/ce100-week-11-shortestpath/#references","text":"TODO","title":"References"},{"location":"week-12/ce100-week-12-crypto/","text":"CE100 Algorithms and Programming II \u00b6 Week-12 (Hashing and Encryption) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Hashing and Encryption \u00b6 Outline \u00b6 Crypto++ Library Usage Hashing and Encryption Integrity Control Hash Values Cryptographic Hash Functions SHA-1 SHA-256 SHA-512 Checksums MD5 CRC32 Hash Algorithms SHA-1 SHA-256 SHA-512 H-MAC References \u00b6 TODO","title":"Week-12 (Hashing)"},{"location":"week-12/ce100-week-12-crypto/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-12/ce100-week-12-crypto/#week-12-hashing-and-encryption","text":"","title":"Week-12 (Hashing and Encryption)"},{"location":"week-12/ce100-week-12-crypto/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-12/ce100-week-12-crypto/#hashing-and-encryption","text":"","title":"Hashing and Encryption"},{"location":"week-12/ce100-week-12-crypto/#outline","text":"Crypto++ Library Usage Hashing and Encryption Integrity Control Hash Values Cryptographic Hash Functions SHA-1 SHA-256 SHA-512 Checksums MD5 CRC32 Hash Algorithms SHA-1 SHA-256 SHA-512 H-MAC","title":"Outline"},{"location":"week-12/ce100-week-12-crypto/#references","text":"TODO","title":"References"},{"location":"week-13/ce100-week-13-symenc/","text":"CE100 Algorithms and Programming II \u00b6 Week-13 (Symmetric and Asymmetric Encryption) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Symmetric and Asymmetric Encryption \u00b6 Outline \u00b6 Symmetric Encryption Algorithms AES https://formaestudio.com/portfolio/aes-animation/ DES http://desalgorithm.yolasite.com/ TDES https://en.wikipedia.org/wiki/Triple_DES Symmetric Encryption Modes https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation ECB CBC Asymmetric Encryption Key Pairs (Public-Private Key Pairs) Signature Generation and Validation References \u00b6 TODO","title":"Week-13 (Encryption)"},{"location":"week-13/ce100-week-13-symenc/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-13/ce100-week-13-symenc/#week-13-symmetric-and-asymmetric-encryption","text":"","title":"Week-13 (Symmetric and  Asymmetric Encryption)"},{"location":"week-13/ce100-week-13-symenc/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-13/ce100-week-13-symenc/#symmetric-and-asymmetric-encryption","text":"","title":"Symmetric and Asymmetric Encryption"},{"location":"week-13/ce100-week-13-symenc/#outline","text":"Symmetric Encryption Algorithms AES https://formaestudio.com/portfolio/aes-animation/ DES http://desalgorithm.yolasite.com/ TDES https://en.wikipedia.org/wiki/Triple_DES Symmetric Encryption Modes https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation ECB CBC Asymmetric Encryption Key Pairs (Public-Private Key Pairs) Signature Generation and Validation","title":"Outline"},{"location":"week-13/ce100-week-13-symenc/#references","text":"TODO","title":"References"},{"location":"week-14/ce100-week-14-otp/","text":"CE100 Algorithms and Programming II \u00b6 Week-14 (OTP Calculation, File Encryption) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX OTP Calculation, File Encryption \u00b6 Outline \u00b6 1.OTP Calculation a.Time-based b.Counter-based File Encryption and Decryption and Integrity Control Operations References \u00b6 TODO","title":"Week-14 (One-Time-Password / File Enc.)"},{"location":"week-14/ce100-week-14-otp/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-14/ce100-week-14-otp/#week-14-otp-calculation-file-encryption","text":"","title":"Week-14 (OTP Calculation, File Encryption)"},{"location":"week-14/ce100-week-14-otp/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-14/ce100-week-14-otp/#otp-calculation-file-encryption","text":"","title":"OTP Calculation, File Encryption"},{"location":"week-14/ce100-week-14-otp/#outline","text":"1.OTP Calculation a.Time-based b.Counter-based File Encryption and Decryption and Integrity Control Operations","title":"Outline"},{"location":"week-14/ce100-week-14-otp/#references","text":"TODO","title":"References"},{"location":"week-15/ce100-week-15-review/","text":"CE100 Algorithms and Programming II \u00b6 Week-15 (Review) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Review \u00b6 Outline \u00b6 References \u00b6 TODO","title":"Week-15 (Review)"},{"location":"week-15/ce100-week-15-review/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-15/ce100-week-15-review/#week-15-review","text":"","title":"Week-15 (Review)"},{"location":"week-15/ce100-week-15-review/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-15/ce100-week-15-review/#review","text":"","title":"Review"},{"location":"week-15/ce100-week-15-review/#outline","text":"","title":"Outline"},{"location":"week-15/ce100-week-15-review/#references","text":"TODO","title":"References"},{"location":"week-16/ce100-week-16-final/","text":"CE100 Algorithms and Programming II \u00b6 Week-16 (Final) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Final \u00b6 Outline \u00b6 References \u00b6 TODO","title":"Week-16 (Final)"},{"location":"week-16/ce100-week-16-final/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-16/ce100-week-16-final/#week-16-final","text":"","title":"Week-16 (Final)"},{"location":"week-16/ce100-week-16-final/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-16/ce100-week-16-final/#final","text":"","title":"Final"},{"location":"week-16/ce100-week-16-final/#outline","text":"","title":"Outline"},{"location":"week-16/ce100-week-16-final/#references","text":"TODO","title":"References"},{"location":"week-2/ce100-week-2-recurrence/","text":"CE100 Algorithms and Programming II \u00b6 Week-2 (Solving Recurrences / The Divide-and-Conquer) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Solving Recurrences \u00b6 Outline (1) \u00b6 Solving Recurrences Recursion Tree Master Method Back-Substitution Outline (2) \u00b6 Divide-and-Conquer Analysis Merge Sort Binary Search Merge Sort Analysis Complexity Outline (3) \u00b6 Recurrence Solution Solving Recurrences (1) \u00b6 Reminder: Runtime \\((T(n))\\) of MergeSort was expressed as a recurrence \\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\] Solving recurrences is like solving differential equations, integrals, etc. Need to learn a few tricks Solving Recurrences (2) \u00b6 Recurrence: An equation or inequality that describes a function in terms of its value on smaller inputs. Example : \\[ T(n)= \\begin{cases} 1 &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ 1 &\\text{if n>1} \\end{cases} \\] section{ font-size: 25px; } Recurrence Example \u00b6 \\[ T(n)= \\begin{cases} 1 &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ 1 &\\text{if n>1} \\end{cases} \\] Simplification: Assume \\(n=2^k\\) Claimed answer : \\(T(n)=lgn+1\\) Substitute claimed answer in the recurrence: \\[ lgn+1= \\begin{cases} 1 &\\text{if n=1} \\\\ lg(\\lceil{n/2}\\rceil)+ 2 &\\text{if n>1} \\end{cases} \\] True when \\(n=2^k\\) Technicalities: Floor / Ceiling \u00b6 Technically, should be careful about the floor and ceiling functions (as in the book). e.g. For merge sort, the recurrence should in fact be:, \\[ T(n)= \\begin{cases} \\Theta(1) &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ T(\\lfloor{n/2}\\rfloor) +\\Theta(n) &\\text{if n>1} \\end{cases} \\] But, it's usually ok to: ignore floor/ceiling solve for the exact power of 2 (or another number) Technicalities: Boundary Conditions \u00b6 Usually assume: \\(T(n) = \\Theta(1)\\) for sufficiently small \\(n\\) Changes the exact solution, but usually the asymptotic solution is not affected (e.g. if polynomially bounded) For convenience, the boundary conditions generally implicitly stated in a recurrence \\(T(n) = 2T(n/2) + \\Theta(n)\\) assuming that \\(T(n)=\\Theta(1)\\) for sufficiently small \\(n\\) section{ font-size: 25px; } Example: When Boundary Conditions Matter \u00b6 Exponential function: \\(T(n) = (T(n/2))2\\) Assume \\(T(1) = c \\text{ (where c is a positive constant)}\\) \\(T(2) = (T(1))^2 = c^2\\) \\(T(4) = (T(2))^2 = c^4\\) \\(T(n) = \\Theta(c^n)\\) e.g. \\[ \\text{ However } \\Theta(2^n) \\neq \\Theta(3^n) \\begin{cases} T(1)= 2 &\\Rightarrow & T(n)= \\Theta(2^n) \\\\ T(1)= 3 &\\Rightarrow & T(n)= \\Theta(3^n) \\end{cases} \\] The difference in solution more dramatic when: \\[ T(1) = 1 \\Rightarrow T(n) = \\Theta(1^n) = \\Theta(1) \\] Solving Recurrences Methods \u00b6 We will focus on 3 techniques Substitution method Recursion tree approach Master method Substitution Method \u00b6 The most general method: Guess Prove by induction Solve for constants Substitution Method: Example (1) \u00b6 Solve \\(T(n)=4T(n/2)+n\\) (assume \\(T(1)= \\Theta(1)\\) ) Guess \\(T(n) = O(n^3)\\) (need to prove \\(O\\) and \\(\\Omega\\) separately) Prove by induction that \\(T(n) \\leq cn^3\\) for large \\(n\\) (i.e. \\(n \\geq n_0\\) ) Inductive hypothesis: \\(T(k) \\leq ck^3\\) for any \\(k < n\\) Assuming ind. hyp. holds, prove \\(T(n) \\leq cn^3\\) Substitution Method: Example (2) \u00b6 Original recurrence: \\(T(n) = 4T(n/2) + n\\) From inductive hypothesis: \\(T(n/2) \\leq c(n/2)^3\\) Substitute this into the original recurrence: \\(T(n) \\leq 4c(n/2)^3 + n\\) \\(= (c/2)n^3 + n\\) \\(= cn^3 \u2013 ((c/2)n^3 \u2013 n)\\) \\(\\Longleftarrow\\) desired - residual \\(\\leq cn^3\\) when \\(((c/2)n^3 \u2013 n) \\geq 0\\) Substitution Method: Example (3) \u00b6 So far, we have shown: \\[ T(n) \\leq cn^3 \\text{ when } ((c/2)n^3 \u2013 n) \\geq 0 \\] We can choose \\(c \\geq 2\\) and \\(n_0 \\geq 1\\) But, the proof is not complete yet. Reminder: Proof by induction: 1.Prove the base cases \\(\\Longleftarrow\\) haven\u2019t proved the base cases yet 2.Inductive hypothesis for smaller sizes 3.Prove the general case Substitution Method: Example (4) \u00b6 We need to prove the base cases Base: \\(T(n) = \\Theta(1)\\) for small \\(n\\) (e.g. for \\(n = n_0\\) ) We should show that: \\(\\Theta(1) \\leq cn^3\\) for \\(n = n_0\\) , This holds if we pick \\(c\\) big enough So, the proof of \\(T(n) = O(n^3)\\) is complete But, is this a tight bound? Example: A tighter upper bound? (1) \u00b6 Original recurrence: \\(T(n) = 4T(n/2) + n\\) Try to prove that \\(T(n) = O(n^2)\\) , i.e. \\(T(n) \\leq cn^2\\) for all \\(n \\geq n_0\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\) Example: A tighter upper bound? (2) \u00b6 Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\) \\[ \\begin{align*} T(n) & = 4T(n/2) + n \\\\ & \\leq 4c(n/2)^2 + n \\\\ & = cn^2 + n \\\\ & = O(n2) \\Longleftarrow \\text{ Wrong! We must prove exactly} \\end{align*} \\] Example: A tighter upper bound? (3) \u00b6 Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\) So far, we have: \\(T(n) \\leq cn^2 + n\\) No matter which positive c value we choose, this does not show that \\(T(n) \\leq cn^2\\) Proof failed? Example: A tighter upper bound? (4) \u00b6 What was the problem? The inductive hypothesis was not strong enough Idea: Start with a stronger inductive hypothesis Subtract a low-order term Inductive hypothesis: \\(T(k) \\leq c_1k^2 \u2013 c_2k\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq c_1n^2 - c_2n\\) Example: A tighter upper bound? (5) \u00b6 Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq c_1k^2 \u2013 c_2k\\) for \\(k < n\\) Prove the general case: \\(T(n) \u2264 c_1n^2 \u2013 c_2n\\) \\[ \\begin{align*} T(n) & = 4T(n/2) + n \\\\ & \\leq 4 (c_1(n/2)^2 \u2013 c_2(n/2)) + n \\\\ & = c_1n^2 \u2013 2c_2n + n \\\\ & = c_1n^2 \u2013 c_2n \u2013 (c_2n \u2013 n) \\\\ & \\leq c_1n^2 \u2013 c_2n \\text{ for } n(c_2 \u2013 1) \\geq 0 \\\\ & \\text{choose } c2 \\geq 1 \\end{align*} \\] Example: A tighter upper bound? (6) \u00b6 We now need to prove $$ T(n) \\leq c_1n^2 \u2013 c_2n $$ for the base cases. \\(T(n) = \\Theta(1) \\text{ for } 1 \\leq n \\leq n_0\\) (implicit assumption) \\(\\Theta(1) \\leq c_1n^2 \u2013 c_2n\\) for \\(n\\) small enough (e.g. \\(n = n_0\\) ) We can choose c1 large enough to make this hold We have proved that \\(T(n) = O(n^2)\\) section{ font-size: 25px; } Substitution Method: Example 2 (1) \u00b6 For the recurrence \\(T(n) = 4T(n/2) + n\\) , prove that \\(T(n) = \\Omega(n^2)\\) i.e. \\(T(n) \u2265 cn^2\\) for any \\(n \\geq n_0\\) Ind. hyp: \\(T(k) \\geq ck^2\\) for any \\(k < n\\) Prove general case: \\(T(n) \\geq cn^2\\) \\(T(n) = 4T(n/2) + n\\) \\(\\geq 4c (n/2)^2 + n\\) \\(= cn^2 + n\\) \\(\\geq cn^2\\) since \\(n > 0\\) Proof succeeded \u2013 no need to strengthen the ind. hyp as in the last example Substitution Method: Example 2 (2) \u00b6 We now need to prove that \\(T(n) \u2265 cn^2\\) for the base cases \\(T(n) = \\Theta(1)\\) for \\(1 \\leq n \\leq n_0\\) (implicit assumption) \\(\\Theta(1) \\geq cn^2\\) for \\(n = n_0\\) \\(n_0\\) is sufficiently small (i.e. constant) We can choose \\(c\\) small enough for this to hold We have proved that \\(T(n) = \\Omega(n^2)\\) Substitution Method - Summary \u00b6 Guess the asymptotic complexity Prove your guess using induction Assume inductive hypothesis holds for \\(k < n\\) Try to prove the general case for \\(n\\) Note: \\(MUST\\) prove the \\(EXACT\\) inequality \\(CANNOT\\) ignore lower order terms, If the proof fails, strengthen the ind. hyp. and try again Prove the base cases (usually straightforward) Recursion Tree Method \u00b6 A recursion tree models the runtime costs of a recursive execution of an algorithm. The recursion tree method is good for generating guesses for the substitution method. The recursion-tree method can be unreliable. Not suitable for formal proofs The recursion-tree method promotes intuition, however. Solve Recurrence (1) : \\(T(n)=2T(n/2)+\\Theta(n)\\) \u00b6 Solve Recurrence (2) : \\(T(n)=2T(n/2)+\\Theta(n)\\) \u00b6 Solve Recurrence (3) : \\(T(n)=2T(n/2)+\\Theta(n)\\) \u00b6 Example of Recursion Tree (1) \u00b6 Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\) Example of Recursion Tree (2) \u00b6 Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\) Example of Recursion Tree (3) \u00b6 Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\) The Master Method \u00b6 A powerful black-box method to solve recurrences. The master method applies to recurrences of the form \\(T(n) = aT(n/b) + f (n)\\) where \\(a \\geq 1, b > 1\\) , and \\(f\\) is asymptotically positive . The Master Method: 3 Cases \u00b6 (TODO : Add Notes ) Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Compare \\(f(n)\\) with \\(n^{log_b^a}\\) Intuitively: Case 1: \\(f(n)\\) grows polynomially slower than \\(n^{log_b^a}\\) Case 2: \\(f(n)\\) grows at the same rate as \\(n^{log_b^a}\\) Case 3: \\(f(n)\\) grows polynomially faster than \\(n^{log_b^a}\\) The Master Method: Case 1 (Bigger) \u00b6 Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon})\\) for some constant \\(\\varepsilon>0\\) i.e., \\(f(n)\\) grows polynomialy slower than \\(n^{log_b^a}\\) (by an \\(n^{\\varepsilon}\\) factor) Solution: \\(T(n)=\\Theta(n^{log_b^a})\\) The Master Method: Case 2 (Simple Version) (Equal) \u00b6 Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(1)\\) i.e., \\(f(n)\\) and \\(n^{log_b^a}\\) grow at similar rates Solution: \\(T(n)=\\Theta(n^{log_b^a}lgn)\\) The Master Method: Case 3 (Smaller) \u00b6 Case 3: \\(\\frac{f(n)}{n^{log_b^a}}=\\Omega(n^{\\varepsilon})\\) for some constant \\(\\varepsilon > 0\\) i.e., \\(f(n)\\) grows polynomialy faster than \\(n^{log_b^a}\\) (by an \\(n^{\\varepsilon}\\) factor) and the following regularity condition holds: \\(af(n/b) \\leq cf(n)\\) for some constant \\(c<1\\) Solution: \\(T(n)=\\Theta(f(n))\\) The Master Method Example (case-1) : \\(T(n)=4T(n/2)+n\\) \u00b6 \\(a=4\\) \\(b=2\\) \\(f(n)=n\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n\\) grows polynomially slower than \\(n^{log_b^a}=n^2\\) \\(\\frac{n^{log_b^a}}{f(n)}=\\frac{n^2}{n}=n=\\Omega(n^{\\varepsilon})\\) CASE-1: \\(T(n)=\\Theta(n^{log_b^a})=\\Theta(n^{log_2^4})=\\Theta(n^2)\\) The Master Method Example (case-2) : \\(T(n)=4T(n/2)+n^2\\) \u00b6 \\(a=4\\) \\(b=2\\) \\(f(n)=n^2\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^2\\) grows at similar rate as \\(n^{log_b^a}=n^2\\) \\(f(n)=\\Theta(n^{log_b^a})=n^2\\) CASE-2: \\(T(n)=\\Theta(n^{log_b^a}lgn)=\\Theta(n^{log_2^4}lgn)=\\Theta(n^2lgn)\\) The Master Method Example (case-3) (1) : \\(T(n)=4T(n/2)+n^3\\) \u00b6 \\(a=4\\) \\(b=2\\) \\(f(n)=n^3\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^3\\) grows polynomially faster than \\(n^{log_b^a}=n^2\\) \\(\\frac{f(n)}{n^{log_b^a}}=\\frac{n^3}{n^2}=n=\\Omega(n^{\\varepsilon})\\) The Master Method Example (case-3) (2) : \\(T(n)=4T(n/2)+n^3\\) (con't) \u00b6 Seems like CASE 3, but need to check the regularity condition Regularity condition \\(af(n/b) \\leq cf(n)\\) for some constant \\(c<1\\) \\(4(n/2)^3 \\leq cn^3\\) for \\(c=1/2\\) CASE-3: \\(T(n)=\\Theta(f(n))\\) \\(\\Longrightarrow\\) \\(T(n)=\\Theta(n^3)\\) The Master Method Example (N/A case) : \\(T(n)=4T(n/2)+n^2lgn\\) \u00b6 \\(a=4\\) \\(b=2\\) \\(f(n)=n^2lgn\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^2lgn\\) grows slower than \\(n^{log_b^a}=n^2\\) but is it polynomially slower? \\(\\frac{n^{log_b^a}{f(n)}}=\\frac{n^2}{\\frac{n^2}{lgn}}=lgn \\neq \\Omega(n^{\\varepsilon})\\) for any \\(\\varepsilon>0\\) is not CASE-1 Master Method does not apply! The Master Method : Case 2 (General Version) \u00b6 Recurrence : \\(T(n) = aT(n/b) + f(n)\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn)\\) for some constant \\(k \\geq 0\\) Solution : \\(T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) General Method (Akra-Bazzi) \u00b6 \\(T(n)=\\sum \\limits_{i=1}^k{a_iT(n/b_i)}+f(n)\\) Let \\(p\\) be the unique solution to \\(\\sum \\limits_{i=1}^k{(a_i/b^p_i)}=1\\) Then, the answers are the same as for the master method, but with \\(n^p\\) instead of \\(n^{log_b^a}\\) (Akra and Bazzi also prove an even more general result.) Idea of Master Theorem (1) \u00b6 Recursion Tree: Idea of Master Theorem (2) \u00b6 CASE 1 : The weight increases geometrically from the root to the leaves. The leaves hold a constant fraction of the total weight. \\(n^{log_b^a}T(1)=\\Theta(n^{log_b^a})\\) Idea of Master Theorem (3) \u00b6 CASE 2 : \\((k = 0)\\) The weight is approximately the same on each of the \\(log_bn\\) levels. \\(n^{log_b^a}T(1)=\\Theta(n^{log_b^a}lgn)\\) Idea of Master Theorem (4) \u00b6 CASE 3 : The weight decreases geometrically from the root to the leaves. The root holds a constant fraction of the total weight. \\(n^{log_b^a}T(1)=\\Theta(f(n))\\) Proof of Master Theorem: Case 1 and Case 2 \u00b6 Recall from the recursion tree (note \\(h = lg_bn =\\text{tree height}\\) ) \\(\\text{Leaf Cost}=\\Theta(n^{log_b^a})\\) \\(\\text{Non-leaf Cost}=g(n)=\\sum \\limits_{i=0}^{h-1}a^if(n/{b^i})\\) \\(T(n)=\\text{Leaf Cost} + \\text{Non-leaf Cost}\\) \\(T(n)=\\Theta(n^{log_b^a}) + \\sum \\limits_{i=0}^{h-1}a^if(n/{b^i})\\) Proof of Master Theorem Case 1 (1) \u00b6 \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon})\\) for some \\(\\varepsilon>0\\) \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow O(n^{-\\varepsilon}) \\Longrightarrow f(n) = O(n^{log_b^{a-\\varepsilon}})\\) \\(g(n)=\\sum \\limits_{i=0}^{h-1}a^iO((n/{b^i})^{log_b^{a-\\varepsilon}})=O(\\sum \\limits_{i=0}^{h-1}a^i(n/{b^i})^{log_b^{a-\\varepsilon}})\\) \\(O(n^{log_b^{a-\\varepsilon}}\\sum \\limits_{i=0}^{h-1}a^ib^{i\\varepsilon}/b^{ilog_b^{a-\\varepsilon}})\\) Proof of Master Theorem Case 1 (2) \u00b6 \\(\\sum \\limits_{i=0}^{h-1} \\frac{a^ib^{i\\varepsilon}}{b^{ilog_b^a}} =\\sum \\limits_{i=0}^{h-1} a^i\\frac{(b^\\varepsilon)^i}{(b^{log_b^a})^i} =\\sum a^i\\frac{b^{i\\varepsilon}}{a^i}=\\sum \\limits_{i=0}^{h-1}(b^{\\varepsilon})^i\\) = An increasing geometric series since \\(b > 1\\) \\(\\frac{b^{h\\varepsilon}-1}{b^{\\varepsilon}-1}=\\frac{(b^h)^{\\varepsilon}-1}{b^{\\varepsilon}-1} = \\frac{(b^{log_b^n})^{\\varepsilon}-1}{b^{\\varepsilon}-1}=\\frac{n^{\\varepsilon}-1}{b^{\\varepsilon}-1} = O(n^{\\varepsilon})\\) Proof of Master Theorem Case 1 (3) \u00b6 \\(g(n)=O(n^{log_b{a-\\varepsilon}}O(n^{\\varepsilon}))=O(\\frac{n^{log_b^a}}{n^{\\varepsilon}}O(n^{\\varepsilon}))=O(n^{log_b^a})\\) \\(T(n)=\\Theta(n^{log_b^a})+g(n)=\\Theta(n^{log_b^a})+O(n^{log_b^a})=\\Theta(n^{log_b^a})\\) Q.E.D. (Quod Erat Demonstrandum) section{ font-size: 22px; } Proof of Master Theorem Case 2 (limited to k=0) \u00b6 \\(\\frac{f(n)}{n^log_b^a}=\\Theta(lg^0n)=\\Theta(1) \\Longrightarrow f(n)=\\Theta(n^{log_b^a}) \\Longrightarrow f(n/b^i)=\\Theta((n/b^i)^{log_b^a})\\) \\(g(n)=\\sum \\limits_{i=0}^{h-1}a^i\\Theta((n/b^i)^{log_b^a})\\) \\(= \\Theta(\\sum \\limits_{i=0}^{h-1}a^i\\frac{n^{log_b^a}}{b^{ilog_b^a}})\\) \\(=\\Theta(n^{log_b^a}\\sum \\limits_{i=0}^{h-1}a^i\\frac{1}{(b^{log_b^a})^i})\\) \\(=\\Theta(n^{log_b^a}\\sum \\limits_{i=0}^{h-1}a^i\\frac{1}{a^i})\\) \\(=\\Theta(n^{log_b^a}\\sum \\limits_{i=0}^{log_b^{n-1}}1) = \\Theta(n^{log_b^a}log_bn)=\\Theta(n^{log_b^a}lgn)\\) \\(T(n)=n^{log_b^a}+\\Theta(n^{log_b^a}lgn)\\) \\(=\\Theta(n^{log_b^a}lgn)\\) Q.E.D. The Divide-and-Conquer Design Paradigm (1) \u00b6 The Divide-and-Conquer Design Paradigm (2) \u00b6 Divide we divide the problem into a number of subproblems. Conquer we solve the subproblems recursively. BaseCase solve by Brute-Force Combine subproblem solutions to the original problem. The Divide-and-Conquer Design Paradigm (3) \u00b6 \\(a=\\text{subproblem}\\) \\(1/b=\\text{each size of the problem}\\) \\[ T(n)=\\begin{cases} \\Theta(1) & \\text{if} & n \\leq c & (basecase) \\\\ aT(n/b)+D(n)+C(n) & \\text{otherwise} \\end{cases} \\] Merge-Sort \\[ T(n)=\\begin{cases} \\Theta(1) & & n = 1 \\\\ 2T(n/2)+\\Theta(n) & \\text{if} & n>1 \\end{cases} \\] \\(T(n)=\\Theta(nlgn)\\) Selection Sort Algorithm \u00b6 SELECTION - SORT ( A ) n = A.length ; for j = 1 to n -1 smallest = j ; for i = j +1 to n if A [ i ] < A [ smallest ] smallest = i ; endfor exchange A [ j ] with A [ smallest ] endfor Selection Sort Algorithm \u00b6 \\[ T(n)=\\begin{cases} \\Theta(1) & & n = 1 \\\\ T(n-1)+\\Theta(n) & \\text{if} & n>1 \\end{cases} \\] Sequential Series \\[ cost = n(n+1)/2 = {1/2}n^2 +{1/2}n \\] Drop low-order terms Ignore the constant coefficient in the leading term \\[ T(n)=\\Theta(n^2) \\] Merge Sort Algorithm (initial setup) \u00b6 Merge Sort is a recursive sorting algorithm, for initial case we need to call Merge-Sort(A,1,n) for sorting \\(A[1..n]\\) initial case A : Array p : 1 ( offset ) r : n ( length ) Merge - Sort ( A , 1 , n ) section{ font-size: 25px; } Merge Sort Algorithm (internal iterations) \u00b6 internal iterations \\(p = start-point\\) \\(q = mid-point\\) \\(r = end-point\\) A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif Merge Sort Combine Algorithm (1) \u00b6 Merge ( A , p , q , r ) n1 = q - p +1 n2 = r - q // allocate left and right arrays // increment will be from left to right // left part will be bigger than right part L [ 1 ...n1 +1 ] // left array R [ 1 ...n2 +1 ] // right array // copy left part of array for i = 1 to n1 L [ i ] = A [ p + i -1 ] // copy right part of array for j = 1 to n2 R [ j ] = A [ q + j ] // put end items maximum values for termination L [ n1 +1 ] = inf R [ n2 +1 ] = inf i = 1 , j = 1 for k = p to r if L [ i ] <= R [ j ] A [ k ] = L [ i ] i = i +1 else A [ k ] = R [ j ] j = j +1 Example : Merge Sort \u00b6 Divide: Trivial. Conquer: Recursively sort 2 subarrays. Combine: Linear- time merge. \\(T(n)=2T(n/2)+\\Theta(n)\\) Subproblems \\(\\Longrightarrow 2\\) Subproblemsize \\(\\Longrightarrow n/2\\) Work dividing and combining \\(\\Longrightarrow\\Theta(n)\\) Master Theorem: Reminder \u00b6 \\(T(n) = aT(n/b) + f(n)\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(n^{log_b^a})\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) Case 3: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(f(n))\\) and \\(af(n/b) \\leq cf(n)\\) for \\(c<1\\) Merge Sort: Solving the Recurrence \u00b6 \\(T(n)=2T(n/2)+\\Theta(n)\\) \\(a=2,b=2,f(n)=\\Theta(n),n^{log_b^a}=n\\) Case-2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(nlgn)\\) Binary Search (1) \u00b6 Find an element in a sorted array: 1. Divide: Check middle element. 2. Conquer: Recursively search 1 subarray. 3. Combine: Trivial. Binary Search (2) \u00b6 \\[ \\text{PARENT} = \\lfloor i/2 \\rfloor \\] \\[ \\text{LEFT-CHILD} = 2i, \\text{ 2i>n} \\] \\[ \\text{RIGHT-CHILD} = 2i+1, \\text{ 2i>n} \\] Binary Search (3) : Iterative \u00b6 ITERATIVE - BINARY - SEARCH ( A , V , low , high ) while low <= high mid = floor (( low + high ) / 2 ); if v == A [ mid ] return mid ; elseif v > A [ mid ] low = mid + 1 ; else high = mid - 1 ; endwhile return NIL Binary Search (4): Recursive \u00b6 RECURSIVE - BINARY - SEARCH ( A , V , low , high ) if low > high return NIL ; endif mid = floor (( low + high ) / 2 ); if v == A [ mid ] return mid ; elseif v > A [ mid ] return RECURSIVE - BINARY - SEARCH ( A , V , mid +1 , high ); else return RECURSIVE - BINARY - SEARCH ( A , V , low , mid -1 ); endif Binary Search (5): Recursive \u00b6 \\[ T(n)=T(n/2)+\\Theta(1) \\Longrightarrow T(n)=\\Theta(lgn) \\] Binary Search (6): Example (Find 9) \u00b6 Recurrence for Binary Search (7) \u00b6 \\(T(n)=1T(n/2)+\\Theta(1)\\) Subproblems \\(\\Longrightarrow 1\\) Subproblemsize \\(\\Longrightarrow n/2\\) Work dividing and combining \\(\\Longrightarrow\\Theta(1)\\) Binary Search: Solving the Recurrence (8) \u00b6 \\(T(n) = T(n/2) + \\Theta(1)\\) \\(a = 1,b = 2,f(n) = \\Theta(1) \\Longrightarrow n^{log_b^a} = n^0=1\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(lgn)\\) Powering a Number: Divide & Conquer (1) \u00b6 Problem : Compute an, where n is a natural number NAIVE - POWER ( a , n ) powerVal = 1 ; for i = 1 to n powerVal = powerVal * a ; endfor return powerVal ; What is the complexity? \\(\\Longrightarrow T(n)=\\Theta(n)\\) Powering a Number: Divide & Conquer (2) \u00b6 Basic Idea: \\[ a^n=\\begin{cases} a^{n/2}*a^{n/2} & \\text{if n is even} \\\\ a^{(n-1)/2}*a^{(n-1)/2}*a & \\text{if n is odd} \\end{cases} \\] Powering a Number: Divide & Conquer (3) \u00b6 POWER ( a , n ) if n = 0 then return 1 ; else if n is even then val = POWER ( a , n / 2 ); return val * val ; else if n is odd then val = POWER ( a ,( n -1 ) / 2 ) return val * val * a ; endif Powering a Number: Solving the Recurrence (4) \u00b6 \\(T(n) = T(n/2) + \\Theta(1)\\) \\(a = 1,b = 2,f(n) = \\Theta(1) \\Longrightarrow n^{log_b^a} = n^0=1\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(lgn)\\) Correctness Proofs for Divide and Conquer Algorithms \u00b6 Proof by induction commonly used for Divide and Conquer Algorithms Base case: Show that the algorithm is correct when the recursion bottoms out (i.e., for sufficiently small n) Inductive hypothesis: Assume the alg. is correct for any recursive call on any smaller subproblem of size \\(k\\) , \\((k < n)\\) General case: Based on the inductive hypothesis, prove that the alg. is correct for any input of size n section{ font-size: 25px; } Example Correctness Proof: Powering a Number \u00b6 Base Case: \\(POWER(a, 0)\\) is correct, because it returns \\(1\\) Ind. Hyp: Assume \\(POWER(a, k)\\) is correct for any \\(k<n\\) General Case: In \\(POWER(a,n)\\) function: If \\(n\\) is \\(even\\) : \\(val = a^{n/2}\\) (due to ind. hyp.) it returns \\(val*val = a^n\\) If \\(n\\) is \\(odd\\) : \\(val = a^{(n-1)/2}\\) (due to ind. hyp.) it returns \\(val*val*a = a^n\\) The correctness proof is complete References \u00b6 Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) Insertion Sort - GeeksforGeeks NIST Dictionary of Algorithms and Data Structures NIST - Dictionary of Algorithms and Data Structures NIST - big-O notation NIST - big-Omega notation \\(-End-Of-Week-2-Course-Module-\\)","title":"Week-2 (Solving Recurrences)"},{"location":"week-2/ce100-week-2-recurrence/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-2/ce100-week-2-recurrence/#week-2-solving-recurrences-the-divide-and-conquer","text":"","title":"Week-2 (Solving Recurrences / The Divide-and-Conquer)"},{"location":"week-2/ce100-week-2-recurrence/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-2/ce100-week-2-recurrence/#solving-recurrences","text":"","title":"Solving Recurrences"},{"location":"week-2/ce100-week-2-recurrence/#outline-1","text":"Solving Recurrences Recursion Tree Master Method Back-Substitution","title":"Outline (1)"},{"location":"week-2/ce100-week-2-recurrence/#outline-2","text":"Divide-and-Conquer Analysis Merge Sort Binary Search Merge Sort Analysis Complexity","title":"Outline (2)"},{"location":"week-2/ce100-week-2-recurrence/#outline-3","text":"Recurrence Solution","title":"Outline (3)"},{"location":"week-2/ce100-week-2-recurrence/#solving-recurrences-1","text":"Reminder: Runtime \\((T(n))\\) of MergeSort was expressed as a recurrence \\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\] Solving recurrences is like solving differential equations, integrals, etc. Need to learn a few tricks","title":"Solving Recurrences (1)"},{"location":"week-2/ce100-week-2-recurrence/#solving-recurrences-2","text":"Recurrence: An equation or inequality that describes a function in terms of its value on smaller inputs. Example : \\[ T(n)= \\begin{cases} 1 &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ 1 &\\text{if n>1} \\end{cases} \\] section{ font-size: 25px; }","title":"Solving Recurrences (2)"},{"location":"week-2/ce100-week-2-recurrence/#recurrence-example","text":"\\[ T(n)= \\begin{cases} 1 &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ 1 &\\text{if n>1} \\end{cases} \\] Simplification: Assume \\(n=2^k\\) Claimed answer : \\(T(n)=lgn+1\\) Substitute claimed answer in the recurrence: \\[ lgn+1= \\begin{cases} 1 &\\text{if n=1} \\\\ lg(\\lceil{n/2}\\rceil)+ 2 &\\text{if n>1} \\end{cases} \\] True when \\(n=2^k\\)","title":"Recurrence Example"},{"location":"week-2/ce100-week-2-recurrence/#technicalities-floor-ceiling","text":"Technically, should be careful about the floor and ceiling functions (as in the book). e.g. For merge sort, the recurrence should in fact be:, \\[ T(n)= \\begin{cases} \\Theta(1) &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ T(\\lfloor{n/2}\\rfloor) +\\Theta(n) &\\text{if n>1} \\end{cases} \\] But, it's usually ok to: ignore floor/ceiling solve for the exact power of 2 (or another number)","title":"Technicalities: Floor / Ceiling"},{"location":"week-2/ce100-week-2-recurrence/#technicalities-boundary-conditions","text":"Usually assume: \\(T(n) = \\Theta(1)\\) for sufficiently small \\(n\\) Changes the exact solution, but usually the asymptotic solution is not affected (e.g. if polynomially bounded) For convenience, the boundary conditions generally implicitly stated in a recurrence \\(T(n) = 2T(n/2) + \\Theta(n)\\) assuming that \\(T(n)=\\Theta(1)\\) for sufficiently small \\(n\\) section{ font-size: 25px; }","title":"Technicalities: Boundary Conditions"},{"location":"week-2/ce100-week-2-recurrence/#example-when-boundary-conditions-matter","text":"Exponential function: \\(T(n) = (T(n/2))2\\) Assume \\(T(1) = c \\text{ (where c is a positive constant)}\\) \\(T(2) = (T(1))^2 = c^2\\) \\(T(4) = (T(2))^2 = c^4\\) \\(T(n) = \\Theta(c^n)\\) e.g. \\[ \\text{ However } \\Theta(2^n) \\neq \\Theta(3^n) \\begin{cases} T(1)= 2 &\\Rightarrow & T(n)= \\Theta(2^n) \\\\ T(1)= 3 &\\Rightarrow & T(n)= \\Theta(3^n) \\end{cases} \\] The difference in solution more dramatic when: \\[ T(1) = 1 \\Rightarrow T(n) = \\Theta(1^n) = \\Theta(1) \\]","title":"Example: When Boundary Conditions Matter"},{"location":"week-2/ce100-week-2-recurrence/#solving-recurrences-methods","text":"We will focus on 3 techniques Substitution method Recursion tree approach Master method","title":"Solving Recurrences Methods"},{"location":"week-2/ce100-week-2-recurrence/#substitution-method","text":"The most general method: Guess Prove by induction Solve for constants","title":"Substitution Method"},{"location":"week-2/ce100-week-2-recurrence/#substitution-method-example-1","text":"Solve \\(T(n)=4T(n/2)+n\\) (assume \\(T(1)= \\Theta(1)\\) ) Guess \\(T(n) = O(n^3)\\) (need to prove \\(O\\) and \\(\\Omega\\) separately) Prove by induction that \\(T(n) \\leq cn^3\\) for large \\(n\\) (i.e. \\(n \\geq n_0\\) ) Inductive hypothesis: \\(T(k) \\leq ck^3\\) for any \\(k < n\\) Assuming ind. hyp. holds, prove \\(T(n) \\leq cn^3\\)","title":"Substitution Method: Example (1)"},{"location":"week-2/ce100-week-2-recurrence/#substitution-method-example-2","text":"Original recurrence: \\(T(n) = 4T(n/2) + n\\) From inductive hypothesis: \\(T(n/2) \\leq c(n/2)^3\\) Substitute this into the original recurrence: \\(T(n) \\leq 4c(n/2)^3 + n\\) \\(= (c/2)n^3 + n\\) \\(= cn^3 \u2013 ((c/2)n^3 \u2013 n)\\) \\(\\Longleftarrow\\) desired - residual \\(\\leq cn^3\\) when \\(((c/2)n^3 \u2013 n) \\geq 0\\)","title":"Substitution Method: Example (2)"},{"location":"week-2/ce100-week-2-recurrence/#substitution-method-example-3","text":"So far, we have shown: \\[ T(n) \\leq cn^3 \\text{ when } ((c/2)n^3 \u2013 n) \\geq 0 \\] We can choose \\(c \\geq 2\\) and \\(n_0 \\geq 1\\) But, the proof is not complete yet. Reminder: Proof by induction: 1.Prove the base cases \\(\\Longleftarrow\\) haven\u2019t proved the base cases yet 2.Inductive hypothesis for smaller sizes 3.Prove the general case","title":"Substitution Method: Example (3)"},{"location":"week-2/ce100-week-2-recurrence/#substitution-method-example-4","text":"We need to prove the base cases Base: \\(T(n) = \\Theta(1)\\) for small \\(n\\) (e.g. for \\(n = n_0\\) ) We should show that: \\(\\Theta(1) \\leq cn^3\\) for \\(n = n_0\\) , This holds if we pick \\(c\\) big enough So, the proof of \\(T(n) = O(n^3)\\) is complete But, is this a tight bound?","title":"Substitution Method: Example (4)"},{"location":"week-2/ce100-week-2-recurrence/#example-a-tighter-upper-bound-1","text":"Original recurrence: \\(T(n) = 4T(n/2) + n\\) Try to prove that \\(T(n) = O(n^2)\\) , i.e. \\(T(n) \\leq cn^2\\) for all \\(n \\geq n_0\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\)","title":"Example: A tighter upper bound? (1)"},{"location":"week-2/ce100-week-2-recurrence/#example-a-tighter-upper-bound-2","text":"Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\) \\[ \\begin{align*} T(n) & = 4T(n/2) + n \\\\ & \\leq 4c(n/2)^2 + n \\\\ & = cn^2 + n \\\\ & = O(n2) \\Longleftarrow \\text{ Wrong! We must prove exactly} \\end{align*} \\]","title":"Example: A tighter upper bound? (2)"},{"location":"week-2/ce100-week-2-recurrence/#example-a-tighter-upper-bound-3","text":"Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\) So far, we have: \\(T(n) \\leq cn^2 + n\\) No matter which positive c value we choose, this does not show that \\(T(n) \\leq cn^2\\) Proof failed?","title":"Example: A tighter upper bound? (3)"},{"location":"week-2/ce100-week-2-recurrence/#example-a-tighter-upper-bound-4","text":"What was the problem? The inductive hypothesis was not strong enough Idea: Start with a stronger inductive hypothesis Subtract a low-order term Inductive hypothesis: \\(T(k) \\leq c_1k^2 \u2013 c_2k\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq c_1n^2 - c_2n\\)","title":"Example: A tighter upper bound? (4)"},{"location":"week-2/ce100-week-2-recurrence/#example-a-tighter-upper-bound-5","text":"Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq c_1k^2 \u2013 c_2k\\) for \\(k < n\\) Prove the general case: \\(T(n) \u2264 c_1n^2 \u2013 c_2n\\) \\[ \\begin{align*} T(n) & = 4T(n/2) + n \\\\ & \\leq 4 (c_1(n/2)^2 \u2013 c_2(n/2)) + n \\\\ & = c_1n^2 \u2013 2c_2n + n \\\\ & = c_1n^2 \u2013 c_2n \u2013 (c_2n \u2013 n) \\\\ & \\leq c_1n^2 \u2013 c_2n \\text{ for } n(c_2 \u2013 1) \\geq 0 \\\\ & \\text{choose } c2 \\geq 1 \\end{align*} \\]","title":"Example: A tighter upper bound? (5)"},{"location":"week-2/ce100-week-2-recurrence/#example-a-tighter-upper-bound-6","text":"We now need to prove $$ T(n) \\leq c_1n^2 \u2013 c_2n $$ for the base cases. \\(T(n) = \\Theta(1) \\text{ for } 1 \\leq n \\leq n_0\\) (implicit assumption) \\(\\Theta(1) \\leq c_1n^2 \u2013 c_2n\\) for \\(n\\) small enough (e.g. \\(n = n_0\\) ) We can choose c1 large enough to make this hold We have proved that \\(T(n) = O(n^2)\\) section{ font-size: 25px; }","title":"Example: A tighter upper bound? (6)"},{"location":"week-2/ce100-week-2-recurrence/#substitution-method-example-2-1","text":"For the recurrence \\(T(n) = 4T(n/2) + n\\) , prove that \\(T(n) = \\Omega(n^2)\\) i.e. \\(T(n) \u2265 cn^2\\) for any \\(n \\geq n_0\\) Ind. hyp: \\(T(k) \\geq ck^2\\) for any \\(k < n\\) Prove general case: \\(T(n) \\geq cn^2\\) \\(T(n) = 4T(n/2) + n\\) \\(\\geq 4c (n/2)^2 + n\\) \\(= cn^2 + n\\) \\(\\geq cn^2\\) since \\(n > 0\\) Proof succeeded \u2013 no need to strengthen the ind. hyp as in the last example","title":"Substitution Method: Example 2 (1)"},{"location":"week-2/ce100-week-2-recurrence/#substitution-method-example-2-2","text":"We now need to prove that \\(T(n) \u2265 cn^2\\) for the base cases \\(T(n) = \\Theta(1)\\) for \\(1 \\leq n \\leq n_0\\) (implicit assumption) \\(\\Theta(1) \\geq cn^2\\) for \\(n = n_0\\) \\(n_0\\) is sufficiently small (i.e. constant) We can choose \\(c\\) small enough for this to hold We have proved that \\(T(n) = \\Omega(n^2)\\)","title":"Substitution Method: Example 2 (2)"},{"location":"week-2/ce100-week-2-recurrence/#substitution-method-summary","text":"Guess the asymptotic complexity Prove your guess using induction Assume inductive hypothesis holds for \\(k < n\\) Try to prove the general case for \\(n\\) Note: \\(MUST\\) prove the \\(EXACT\\) inequality \\(CANNOT\\) ignore lower order terms, If the proof fails, strengthen the ind. hyp. and try again Prove the base cases (usually straightforward)","title":"Substitution Method - Summary"},{"location":"week-2/ce100-week-2-recurrence/#recursion-tree-method","text":"A recursion tree models the runtime costs of a recursive execution of an algorithm. The recursion tree method is good for generating guesses for the substitution method. The recursion-tree method can be unreliable. Not suitable for formal proofs The recursion-tree method promotes intuition, however.","title":"Recursion Tree Method"},{"location":"week-2/ce100-week-2-recurrence/#solve-recurrence-1-tn2tn2thetan","text":"","title":"Solve Recurrence (1) : \\(T(n)=2T(n/2)+\\Theta(n)\\)"},{"location":"week-2/ce100-week-2-recurrence/#solve-recurrence-2-tn2tn2thetan","text":"","title":"Solve Recurrence (2) : \\(T(n)=2T(n/2)+\\Theta(n)\\)"},{"location":"week-2/ce100-week-2-recurrence/#solve-recurrence-3-tn2tn2thetan","text":"","title":"Solve Recurrence (3) : \\(T(n)=2T(n/2)+\\Theta(n)\\)"},{"location":"week-2/ce100-week-2-recurrence/#example-of-recursion-tree-1","text":"Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\)","title":"Example of Recursion Tree (1)"},{"location":"week-2/ce100-week-2-recurrence/#example-of-recursion-tree-2","text":"Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\)","title":"Example of Recursion Tree (2)"},{"location":"week-2/ce100-week-2-recurrence/#example-of-recursion-tree-3","text":"Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\)","title":"Example of Recursion Tree (3)"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method","text":"A powerful black-box method to solve recurrences. The master method applies to recurrences of the form \\(T(n) = aT(n/b) + f (n)\\) where \\(a \\geq 1, b > 1\\) , and \\(f\\) is asymptotically positive .","title":"The Master Method"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method-3-cases","text":"(TODO : Add Notes ) Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Compare \\(f(n)\\) with \\(n^{log_b^a}\\) Intuitively: Case 1: \\(f(n)\\) grows polynomially slower than \\(n^{log_b^a}\\) Case 2: \\(f(n)\\) grows at the same rate as \\(n^{log_b^a}\\) Case 3: \\(f(n)\\) grows polynomially faster than \\(n^{log_b^a}\\)","title":"The Master Method: 3 Cases"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method-case-1-bigger","text":"Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon})\\) for some constant \\(\\varepsilon>0\\) i.e., \\(f(n)\\) grows polynomialy slower than \\(n^{log_b^a}\\) (by an \\(n^{\\varepsilon}\\) factor) Solution: \\(T(n)=\\Theta(n^{log_b^a})\\)","title":"The Master Method: Case 1 (Bigger)"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method-case-2-simple-version-equal","text":"Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(1)\\) i.e., \\(f(n)\\) and \\(n^{log_b^a}\\) grow at similar rates Solution: \\(T(n)=\\Theta(n^{log_b^a}lgn)\\)","title":"The Master Method: Case 2 (Simple Version) (Equal)"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method-case-3-smaller","text":"Case 3: \\(\\frac{f(n)}{n^{log_b^a}}=\\Omega(n^{\\varepsilon})\\) for some constant \\(\\varepsilon > 0\\) i.e., \\(f(n)\\) grows polynomialy faster than \\(n^{log_b^a}\\) (by an \\(n^{\\varepsilon}\\) factor) and the following regularity condition holds: \\(af(n/b) \\leq cf(n)\\) for some constant \\(c<1\\) Solution: \\(T(n)=\\Theta(f(n))\\)","title":"The Master Method: Case 3 (Smaller)"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method-example-case-1-tn4tn2n","text":"\\(a=4\\) \\(b=2\\) \\(f(n)=n\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n\\) grows polynomially slower than \\(n^{log_b^a}=n^2\\) \\(\\frac{n^{log_b^a}}{f(n)}=\\frac{n^2}{n}=n=\\Omega(n^{\\varepsilon})\\) CASE-1: \\(T(n)=\\Theta(n^{log_b^a})=\\Theta(n^{log_2^4})=\\Theta(n^2)\\)","title":"The Master Method Example (case-1) : \\(T(n)=4T(n/2)+n\\)"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method-example-case-2-tn4tn2n2","text":"\\(a=4\\) \\(b=2\\) \\(f(n)=n^2\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^2\\) grows at similar rate as \\(n^{log_b^a}=n^2\\) \\(f(n)=\\Theta(n^{log_b^a})=n^2\\) CASE-2: \\(T(n)=\\Theta(n^{log_b^a}lgn)=\\Theta(n^{log_2^4}lgn)=\\Theta(n^2lgn)\\)","title":"The Master Method Example (case-2) : \\(T(n)=4T(n/2)+n^2\\)"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method-example-case-3-1-tn4tn2n3","text":"\\(a=4\\) \\(b=2\\) \\(f(n)=n^3\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^3\\) grows polynomially faster than \\(n^{log_b^a}=n^2\\) \\(\\frac{f(n)}{n^{log_b^a}}=\\frac{n^3}{n^2}=n=\\Omega(n^{\\varepsilon})\\)","title":"The Master Method Example (case-3) (1) : \\(T(n)=4T(n/2)+n^3\\)"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method-example-case-3-2-tn4tn2n3-cont","text":"Seems like CASE 3, but need to check the regularity condition Regularity condition \\(af(n/b) \\leq cf(n)\\) for some constant \\(c<1\\) \\(4(n/2)^3 \\leq cn^3\\) for \\(c=1/2\\) CASE-3: \\(T(n)=\\Theta(f(n))\\) \\(\\Longrightarrow\\) \\(T(n)=\\Theta(n^3)\\)","title":"The Master Method Example (case-3) (2) : \\(T(n)=4T(n/2)+n^3\\) (con't)"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method-example-na-case-tn4tn2n2lgn","text":"\\(a=4\\) \\(b=2\\) \\(f(n)=n^2lgn\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^2lgn\\) grows slower than \\(n^{log_b^a}=n^2\\) but is it polynomially slower? \\(\\frac{n^{log_b^a}{f(n)}}=\\frac{n^2}{\\frac{n^2}{lgn}}=lgn \\neq \\Omega(n^{\\varepsilon})\\) for any \\(\\varepsilon>0\\) is not CASE-1 Master Method does not apply!","title":"The Master Method Example (N/A case) : \\(T(n)=4T(n/2)+n^2lgn\\)"},{"location":"week-2/ce100-week-2-recurrence/#the-master-method-case-2-general-version","text":"Recurrence : \\(T(n) = aT(n/b) + f(n)\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn)\\) for some constant \\(k \\geq 0\\) Solution : \\(T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\)","title":"The Master Method : Case 2 (General Version)"},{"location":"week-2/ce100-week-2-recurrence/#general-method-akra-bazzi","text":"\\(T(n)=\\sum \\limits_{i=1}^k{a_iT(n/b_i)}+f(n)\\) Let \\(p\\) be the unique solution to \\(\\sum \\limits_{i=1}^k{(a_i/b^p_i)}=1\\) Then, the answers are the same as for the master method, but with \\(n^p\\) instead of \\(n^{log_b^a}\\) (Akra and Bazzi also prove an even more general result.)","title":"General Method (Akra-Bazzi)"},{"location":"week-2/ce100-week-2-recurrence/#idea-of-master-theorem-1","text":"Recursion Tree:","title":"Idea of Master Theorem (1)"},{"location":"week-2/ce100-week-2-recurrence/#idea-of-master-theorem-2","text":"CASE 1 : The weight increases geometrically from the root to the leaves. The leaves hold a constant fraction of the total weight. \\(n^{log_b^a}T(1)=\\Theta(n^{log_b^a})\\)","title":"Idea of Master Theorem (2)"},{"location":"week-2/ce100-week-2-recurrence/#idea-of-master-theorem-3","text":"CASE 2 : \\((k = 0)\\) The weight is approximately the same on each of the \\(log_bn\\) levels. \\(n^{log_b^a}T(1)=\\Theta(n^{log_b^a}lgn)\\)","title":"Idea of Master Theorem (3)"},{"location":"week-2/ce100-week-2-recurrence/#idea-of-master-theorem-4","text":"CASE 3 : The weight decreases geometrically from the root to the leaves. The root holds a constant fraction of the total weight. \\(n^{log_b^a}T(1)=\\Theta(f(n))\\)","title":"Idea of Master Theorem (4)"},{"location":"week-2/ce100-week-2-recurrence/#proof-of-master-theorem-case-1-and-case-2","text":"Recall from the recursion tree (note \\(h = lg_bn =\\text{tree height}\\) ) \\(\\text{Leaf Cost}=\\Theta(n^{log_b^a})\\) \\(\\text{Non-leaf Cost}=g(n)=\\sum \\limits_{i=0}^{h-1}a^if(n/{b^i})\\) \\(T(n)=\\text{Leaf Cost} + \\text{Non-leaf Cost}\\) \\(T(n)=\\Theta(n^{log_b^a}) + \\sum \\limits_{i=0}^{h-1}a^if(n/{b^i})\\)","title":"Proof of Master Theorem: Case 1 and Case 2"},{"location":"week-2/ce100-week-2-recurrence/#proof-of-master-theorem-case-1-1","text":"\\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon})\\) for some \\(\\varepsilon>0\\) \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow O(n^{-\\varepsilon}) \\Longrightarrow f(n) = O(n^{log_b^{a-\\varepsilon}})\\) \\(g(n)=\\sum \\limits_{i=0}^{h-1}a^iO((n/{b^i})^{log_b^{a-\\varepsilon}})=O(\\sum \\limits_{i=0}^{h-1}a^i(n/{b^i})^{log_b^{a-\\varepsilon}})\\) \\(O(n^{log_b^{a-\\varepsilon}}\\sum \\limits_{i=0}^{h-1}a^ib^{i\\varepsilon}/b^{ilog_b^{a-\\varepsilon}})\\)","title":"Proof of Master Theorem Case 1 (1)"},{"location":"week-2/ce100-week-2-recurrence/#proof-of-master-theorem-case-1-2","text":"\\(\\sum \\limits_{i=0}^{h-1} \\frac{a^ib^{i\\varepsilon}}{b^{ilog_b^a}} =\\sum \\limits_{i=0}^{h-1} a^i\\frac{(b^\\varepsilon)^i}{(b^{log_b^a})^i} =\\sum a^i\\frac{b^{i\\varepsilon}}{a^i}=\\sum \\limits_{i=0}^{h-1}(b^{\\varepsilon})^i\\) = An increasing geometric series since \\(b > 1\\) \\(\\frac{b^{h\\varepsilon}-1}{b^{\\varepsilon}-1}=\\frac{(b^h)^{\\varepsilon}-1}{b^{\\varepsilon}-1} = \\frac{(b^{log_b^n})^{\\varepsilon}-1}{b^{\\varepsilon}-1}=\\frac{n^{\\varepsilon}-1}{b^{\\varepsilon}-1} = O(n^{\\varepsilon})\\)","title":"Proof of Master Theorem Case 1 (2)"},{"location":"week-2/ce100-week-2-recurrence/#proof-of-master-theorem-case-1-3","text":"\\(g(n)=O(n^{log_b{a-\\varepsilon}}O(n^{\\varepsilon}))=O(\\frac{n^{log_b^a}}{n^{\\varepsilon}}O(n^{\\varepsilon}))=O(n^{log_b^a})\\) \\(T(n)=\\Theta(n^{log_b^a})+g(n)=\\Theta(n^{log_b^a})+O(n^{log_b^a})=\\Theta(n^{log_b^a})\\) Q.E.D. (Quod Erat Demonstrandum) section{ font-size: 22px; }","title":"Proof of Master Theorem Case 1 (3)"},{"location":"week-2/ce100-week-2-recurrence/#proof-of-master-theorem-case-2-limited-to-k0","text":"\\(\\frac{f(n)}{n^log_b^a}=\\Theta(lg^0n)=\\Theta(1) \\Longrightarrow f(n)=\\Theta(n^{log_b^a}) \\Longrightarrow f(n/b^i)=\\Theta((n/b^i)^{log_b^a})\\) \\(g(n)=\\sum \\limits_{i=0}^{h-1}a^i\\Theta((n/b^i)^{log_b^a})\\) \\(= \\Theta(\\sum \\limits_{i=0}^{h-1}a^i\\frac{n^{log_b^a}}{b^{ilog_b^a}})\\) \\(=\\Theta(n^{log_b^a}\\sum \\limits_{i=0}^{h-1}a^i\\frac{1}{(b^{log_b^a})^i})\\) \\(=\\Theta(n^{log_b^a}\\sum \\limits_{i=0}^{h-1}a^i\\frac{1}{a^i})\\) \\(=\\Theta(n^{log_b^a}\\sum \\limits_{i=0}^{log_b^{n-1}}1) = \\Theta(n^{log_b^a}log_bn)=\\Theta(n^{log_b^a}lgn)\\) \\(T(n)=n^{log_b^a}+\\Theta(n^{log_b^a}lgn)\\) \\(=\\Theta(n^{log_b^a}lgn)\\) Q.E.D.","title":"Proof of Master Theorem Case 2 (limited to k=0)"},{"location":"week-2/ce100-week-2-recurrence/#the-divide-and-conquer-design-paradigm-1","text":"","title":"The Divide-and-Conquer Design Paradigm (1)"},{"location":"week-2/ce100-week-2-recurrence/#the-divide-and-conquer-design-paradigm-2","text":"Divide we divide the problem into a number of subproblems. Conquer we solve the subproblems recursively. BaseCase solve by Brute-Force Combine subproblem solutions to the original problem.","title":"The Divide-and-Conquer Design Paradigm (2)"},{"location":"week-2/ce100-week-2-recurrence/#the-divide-and-conquer-design-paradigm-3","text":"\\(a=\\text{subproblem}\\) \\(1/b=\\text{each size of the problem}\\) \\[ T(n)=\\begin{cases} \\Theta(1) & \\text{if} & n \\leq c & (basecase) \\\\ aT(n/b)+D(n)+C(n) & \\text{otherwise} \\end{cases} \\] Merge-Sort \\[ T(n)=\\begin{cases} \\Theta(1) & & n = 1 \\\\ 2T(n/2)+\\Theta(n) & \\text{if} & n>1 \\end{cases} \\] \\(T(n)=\\Theta(nlgn)\\)","title":"The Divide-and-Conquer Design Paradigm (3)"},{"location":"week-2/ce100-week-2-recurrence/#selection-sort-algorithm","text":"SELECTION - SORT ( A ) n = A.length ; for j = 1 to n -1 smallest = j ; for i = j +1 to n if A [ i ] < A [ smallest ] smallest = i ; endfor exchange A [ j ] with A [ smallest ] endfor","title":"Selection Sort Algorithm"},{"location":"week-2/ce100-week-2-recurrence/#selection-sort-algorithm_1","text":"\\[ T(n)=\\begin{cases} \\Theta(1) & & n = 1 \\\\ T(n-1)+\\Theta(n) & \\text{if} & n>1 \\end{cases} \\] Sequential Series \\[ cost = n(n+1)/2 = {1/2}n^2 +{1/2}n \\] Drop low-order terms Ignore the constant coefficient in the leading term \\[ T(n)=\\Theta(n^2) \\]","title":"Selection Sort Algorithm"},{"location":"week-2/ce100-week-2-recurrence/#merge-sort-algorithm-initial-setup","text":"Merge Sort is a recursive sorting algorithm, for initial case we need to call Merge-Sort(A,1,n) for sorting \\(A[1..n]\\) initial case A : Array p : 1 ( offset ) r : n ( length ) Merge - Sort ( A , 1 , n ) section{ font-size: 25px; }","title":"Merge Sort Algorithm (initial setup)"},{"location":"week-2/ce100-week-2-recurrence/#merge-sort-algorithm-internal-iterations","text":"internal iterations \\(p = start-point\\) \\(q = mid-point\\) \\(r = end-point\\) A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif","title":"Merge Sort Algorithm (internal iterations)"},{"location":"week-2/ce100-week-2-recurrence/#merge-sort-combine-algorithm-1","text":"Merge ( A , p , q , r ) n1 = q - p +1 n2 = r - q // allocate left and right arrays // increment will be from left to right // left part will be bigger than right part L [ 1 ...n1 +1 ] // left array R [ 1 ...n2 +1 ] // right array // copy left part of array for i = 1 to n1 L [ i ] = A [ p + i -1 ] // copy right part of array for j = 1 to n2 R [ j ] = A [ q + j ] // put end items maximum values for termination L [ n1 +1 ] = inf R [ n2 +1 ] = inf i = 1 , j = 1 for k = p to r if L [ i ] <= R [ j ] A [ k ] = L [ i ] i = i +1 else A [ k ] = R [ j ] j = j +1","title":"Merge Sort Combine Algorithm (1)"},{"location":"week-2/ce100-week-2-recurrence/#example-merge-sort","text":"Divide: Trivial. Conquer: Recursively sort 2 subarrays. Combine: Linear- time merge. \\(T(n)=2T(n/2)+\\Theta(n)\\) Subproblems \\(\\Longrightarrow 2\\) Subproblemsize \\(\\Longrightarrow n/2\\) Work dividing and combining \\(\\Longrightarrow\\Theta(n)\\)","title":"Example : Merge Sort"},{"location":"week-2/ce100-week-2-recurrence/#master-theorem-reminder","text":"\\(T(n) = aT(n/b) + f(n)\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(n^{log_b^a})\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) Case 3: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(f(n))\\) and \\(af(n/b) \\leq cf(n)\\) for \\(c<1\\)","title":"Master Theorem: Reminder"},{"location":"week-2/ce100-week-2-recurrence/#merge-sort-solving-the-recurrence","text":"\\(T(n)=2T(n/2)+\\Theta(n)\\) \\(a=2,b=2,f(n)=\\Theta(n),n^{log_b^a}=n\\) Case-2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(nlgn)\\)","title":"Merge Sort: Solving the Recurrence"},{"location":"week-2/ce100-week-2-recurrence/#binary-search-1","text":"Find an element in a sorted array: 1. Divide: Check middle element. 2. Conquer: Recursively search 1 subarray. 3. Combine: Trivial.","title":"Binary Search (1)"},{"location":"week-2/ce100-week-2-recurrence/#binary-search-2","text":"\\[ \\text{PARENT} = \\lfloor i/2 \\rfloor \\] \\[ \\text{LEFT-CHILD} = 2i, \\text{ 2i>n} \\] \\[ \\text{RIGHT-CHILD} = 2i+1, \\text{ 2i>n} \\]","title":"Binary Search (2)"},{"location":"week-2/ce100-week-2-recurrence/#binary-search-3-iterative","text":"ITERATIVE - BINARY - SEARCH ( A , V , low , high ) while low <= high mid = floor (( low + high ) / 2 ); if v == A [ mid ] return mid ; elseif v > A [ mid ] low = mid + 1 ; else high = mid - 1 ; endwhile return NIL","title":"Binary Search (3) : Iterative"},{"location":"week-2/ce100-week-2-recurrence/#binary-search-4-recursive","text":"RECURSIVE - BINARY - SEARCH ( A , V , low , high ) if low > high return NIL ; endif mid = floor (( low + high ) / 2 ); if v == A [ mid ] return mid ; elseif v > A [ mid ] return RECURSIVE - BINARY - SEARCH ( A , V , mid +1 , high ); else return RECURSIVE - BINARY - SEARCH ( A , V , low , mid -1 ); endif","title":"Binary Search (4): Recursive"},{"location":"week-2/ce100-week-2-recurrence/#binary-search-5-recursive","text":"\\[ T(n)=T(n/2)+\\Theta(1) \\Longrightarrow T(n)=\\Theta(lgn) \\]","title":"Binary Search (5): Recursive"},{"location":"week-2/ce100-week-2-recurrence/#binary-search-6-example-find-9","text":"","title":"Binary Search (6): Example (Find 9)"},{"location":"week-2/ce100-week-2-recurrence/#recurrence-for-binary-search-7","text":"\\(T(n)=1T(n/2)+\\Theta(1)\\) Subproblems \\(\\Longrightarrow 1\\) Subproblemsize \\(\\Longrightarrow n/2\\) Work dividing and combining \\(\\Longrightarrow\\Theta(1)\\)","title":"Recurrence for Binary Search (7)"},{"location":"week-2/ce100-week-2-recurrence/#binary-search-solving-the-recurrence-8","text":"\\(T(n) = T(n/2) + \\Theta(1)\\) \\(a = 1,b = 2,f(n) = \\Theta(1) \\Longrightarrow n^{log_b^a} = n^0=1\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(lgn)\\)","title":"Binary Search: Solving the Recurrence (8)"},{"location":"week-2/ce100-week-2-recurrence/#powering-a-number-divide-conquer-1","text":"Problem : Compute an, where n is a natural number NAIVE - POWER ( a , n ) powerVal = 1 ; for i = 1 to n powerVal = powerVal * a ; endfor return powerVal ; What is the complexity? \\(\\Longrightarrow T(n)=\\Theta(n)\\)","title":"Powering a Number: Divide &amp; Conquer (1)"},{"location":"week-2/ce100-week-2-recurrence/#powering-a-number-divide-conquer-2","text":"Basic Idea: \\[ a^n=\\begin{cases} a^{n/2}*a^{n/2} & \\text{if n is even} \\\\ a^{(n-1)/2}*a^{(n-1)/2}*a & \\text{if n is odd} \\end{cases} \\]","title":"Powering a Number: Divide &amp; Conquer (2)"},{"location":"week-2/ce100-week-2-recurrence/#powering-a-number-divide-conquer-3","text":"POWER ( a , n ) if n = 0 then return 1 ; else if n is even then val = POWER ( a , n / 2 ); return val * val ; else if n is odd then val = POWER ( a ,( n -1 ) / 2 ) return val * val * a ; endif","title":"Powering a Number: Divide &amp; Conquer (3)"},{"location":"week-2/ce100-week-2-recurrence/#powering-a-number-solving-the-recurrence-4","text":"\\(T(n) = T(n/2) + \\Theta(1)\\) \\(a = 1,b = 2,f(n) = \\Theta(1) \\Longrightarrow n^{log_b^a} = n^0=1\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(lgn)\\)","title":"Powering a Number: Solving the Recurrence (4)"},{"location":"week-2/ce100-week-2-recurrence/#correctness-proofs-for-divide-and-conquer-algorithms","text":"Proof by induction commonly used for Divide and Conquer Algorithms Base case: Show that the algorithm is correct when the recursion bottoms out (i.e., for sufficiently small n) Inductive hypothesis: Assume the alg. is correct for any recursive call on any smaller subproblem of size \\(k\\) , \\((k < n)\\) General case: Based on the inductive hypothesis, prove that the alg. is correct for any input of size n section{ font-size: 25px; }","title":"Correctness Proofs for Divide and Conquer Algorithms"},{"location":"week-2/ce100-week-2-recurrence/#example-correctness-proof-powering-a-number","text":"Base Case: \\(POWER(a, 0)\\) is correct, because it returns \\(1\\) Ind. Hyp: Assume \\(POWER(a, k)\\) is correct for any \\(k<n\\) General Case: In \\(POWER(a,n)\\) function: If \\(n\\) is \\(even\\) : \\(val = a^{n/2}\\) (due to ind. hyp.) it returns \\(val*val = a^n\\) If \\(n\\) is \\(odd\\) : \\(val = a^{(n-1)/2}\\) (due to ind. hyp.) it returns \\(val*val*a = a^n\\) The correctness proof is complete","title":"Example Correctness Proof: Powering a Number"},{"location":"week-2/ce100-week-2-recurrence/#references","text":"Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) Insertion Sort - GeeksforGeeks NIST Dictionary of Algorithms and Data Structures NIST - Dictionary of Algorithms and Data Structures NIST - big-O notation NIST - big-Omega notation \\(-End-Of-Week-2-Course-Module-\\)","title":"References"},{"location":"week-3/ce100-week-3-matrix/","text":"CE100 Algorithms and Programming II \u00b6 Week-3 (Matrix Multiplication/ Quick Sort) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Matrix Multiplication / Quick Sort \u00b6 Outline (1) \u00b6 Matrix Multiplication Traditional Recursive Strassen Outline (2) \u00b6 Quicksort Hoare Partitioning Lomuto Partitioning Recursive Sorting Outline (3) \u00b6 Quicksort Analysis Randomized Quicksort Randomized Selection Recursive Medians Matrix Multiplication (1) \u00b6 Input: \\(A=[a_{ij}],B=[b_{ij}]\\) Output: \\(C=[c_{ij}]=A \\cdot B\\) \\(\\Longrightarrow i,j=1,2,3, \\dots, n\\) \\[ \\begin{bmatrix} c_{11} & c_{12} & \\dots & c_{1n} \\\\ c_{21} & c_{22} & \\dots & c_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ c_{n1} & c_{n2} & \\dots & c_{nn} \\\\ \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ a_{n1} & a_{n2} & \\dots & a_{nn} \\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} b_{11} & b_{12} & \\dots & b_{1n} \\\\ b_{21} & b_{22} & \\dots & b_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ b_{n1} & a_{n2} & \\dots & b_{nn} \\\\ \\end{bmatrix} \\] Matrix Multiplication (2) \u00b6 \\(c_{ij}=\\sum \\limits_{1\\leq k \\leq n}^{}a_{ik}.b_{kj}\\) Matrix Multiplication: Standard Algorithm \u00b6 Running Time: \\(\\Theta(n^3)\\) for i = 1 to n do for j = 1 to n do C [ i , j ] = 0 for k = 1 to n do C [ i , j ] = C [ i , j ] + A [ i , k ] + B [ k , j ] endfor endfor endfor Matrix Multiplication: Divide & Conquer (1) \u00b6 IDEA: Divide the \\(nxn\\) matrix into \\(2x2\\) matrix of \\((n/2)x(n/2)\\) submatrices. Matrix Multiplication: Divide & Conquer (2) \u00b6 \\[ \\begin{bmatrix} c_{11} & c_{12} \\\\ c_{21} & c_{22} \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} \\cdot \\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{bmatrix} \\] \\[ \\text{8 mults and 4 adds of (n/2)*(n/2) submatrices}= \\begin{cases} c_{11}=a_{11}b_{11}+a_{12}b_{21} \\\\ c_{21}=a_{21}b_{11}+a_{22}b_{21} \\\\ c_{12}=a_{11}b_{12}+a_{12}b_{22} \\\\ c_{22}=a_{21}b_{12}+a_{22}b_{22} \\end{cases} \\] Matrix Multiplication: Divide & Conquer (3) \u00b6 MATRIX - MULTIPLY ( A , B ) // Assuming that both A and B are nxn matrices if n == 1 then return A * B else // partition A , B , and C as shown before C [ 1 , 1 ] = MATRIX - MULTIPLY ( A [ 1 , 1 ], B [ 1 , 1 ]) + MATRIX - MULTIPLY ( A [ 1 , 2 ], B [ 2 , 1 ]); C [ 1 , 2 ] = MATRIX - MULTIPLY ( A [ 1 , 1 ], B [ 1 , 2 ]) + MATRIX - MULTIPLY ( A [ 1 , 2 ], B [ 2 , 2 ]); C [ 2 , 1 ] = MATRIX - MULTIPLY ( A [ 2 , 1 ], B [ 1 , 1 ]) + MATRIX - MULTIPLY ( A [ 2 , 2 ], B [ 2 , 1 ]); C [ 2 , 2 ] = MATRIX - MULTIPLY ( A [ 2 , 1 ], B [ 1 , 2 ]) + MATRIX - MULTIPLY ( A [ 2 , 2 ], B [ 2 , 2 ]); endif return C Matrix Multiplication: Divide & Conquer Analysis \u00b6 \\(T(n) = 8T(n/2) + \\Theta(n^2)\\) \\(8\\) recursive calls \\(\\Longrightarrow 8T(\\cdots)\\) each problem has size \\(n/2\\) \\(\\Longrightarrow \\cdots T(n/2)\\) Submatrix addition \\(\\Longrightarrow \\Theta(n^2)\\) Matrix Multiplication: Solving the Recurrence \u00b6 \\(T(n) = 8T(n/2) + \\Theta(n^2)\\) \\(a=8\\) , \\(b=2\\) \\(f(n)=\\Theta(n^2)\\) \\(n^{log_b^a}=n^3\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(n^{log_b^a})\\) Similar with ordinary (iterative) algorithm. Matrix Multiplication: Strassen\u2019s Idea (1) \u00b6 Compute \\(c_{11},c_{12},c_{21},c_{22}\\) using \\(7\\) recursive multiplications. In normal case we need \\(8\\) as below. \\[ \\begin{bmatrix} c_{11} & c_{12} \\\\ c_{21} & c_{22} \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} \\cdot \\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{bmatrix} \\] \\[ \\text{8 mults and 4 adds of (n/2)*(n/2) submatrices}= \\begin{cases} c_{11}=a_{11}b_{11}+a_{12}b_{21} \\\\ c_{21}=a_{21}b_{11}+a_{22}b_{21} \\\\ c_{12}=a_{11}b_{12}+a_{12}b_{22} \\\\ c_{22}=a_{21}b_{12}+a_{22}b_{22} \\end{cases} \\] section{ font-size: 25px; } Matrix Multiplication: Strassen\u2019s Idea (2) \u00b6 Reminder: Each submatrix is of size \\((n/2)*(n/2)\\) Each add/sub operation takes \\(\\Theta(n^2)\\) time Compute \\(P1 \\dots P7\\) using \\(7\\) recursive calls to matrix-multiply \\[ \\begin{align*} P_1 & = a_{11} * (b_{12} - b_{22} ) \\\\ P_2 & = (a_{11} + a_{12} ) * b_{22} \\\\ P_3 & = (a_{21} + a_{22} ) * b_{11} \\\\ P_4 & = a_{22} * (b_{21} - b_{11} ) \\\\ P_5 & = (a_{11} + a_{22} ) * (b_{11} + b_{22} ) \\\\ P_6 & = (a_{12} - a_{22} ) * (b_{21} + b_{22} ) \\\\ P_7 & = ( a_{11} - a_{21} ) * (b_{11} + b_{12} ) \\end{align*} \\] section{ font-size: 25px; } Matrix Multiplication: Strassen\u2019s Idea (3) \u00b6 \\[ \\begin{align*} P_1 &= a_{11} * (b_{12} - b_{22} ) \\\\ P_2 &= (a_{11} + a_{12} ) * b_{22} \\\\ P_3 &= (a_{21} + a_{22} ) * b_{11} \\\\ P_4 &= a_{22} * (b_{21} - b_{11} ) \\\\ P_5 &= (a_{11} + a_{22} ) * (b_{11} + b_{22} ) \\\\ P_6 &= (a_{12} - a_{22} ) * (b_{21} + b_{22} ) \\\\ P_7 &= ( a_{11} - a_{21} ) * (b_{11} + b_{12} ) \\end{align*} \\] How to compute \\(c_{ij}\\) using \\(P1 \\dots P7\\) ? \\[ \\begin{align*} c_{11} & = P_5 + P_4 \u2013 P_2 + P_6 \\\\ c_{12} & = P_1 + P_2 \\\\ c_{21} & = P_3 + P_4 \\\\ c_{22} & = P_5 + P_1 \u2013 P_3 \u2013 P_7 \\end{align*} \\] Matrix Multiplication: Strassen\u2019s Idea (4) \u00b6 \\(7\\) recursive multiply calls \\(18\\) add/sub operations Matrix Multiplication: Strassen\u2019s Idea (5) \u00b6 e.g. Show that \\(c_{12} = P_1+P_2\\) : \\[ \\begin{align*} c_{12} & = P_1 + P_2 \\\\ &= a_{11}(b_{12}\u2013b_{22})+(a_{11}+a_{12})b_{22} \\\\ &= a_{11}b_{12}-a_{11}b_{22}+a_{11}b_{22}+a_{12}b_{22} \\\\ &= a_{11}b_{12}+a_{12}b_{22} \\end{align*} \\] Strassen\u2019s Algorithm \u00b6 Divide: Partition \\(A\\) and \\(B\\) into \\((n/2)*(n/2)\\) submatrices. Form terms to be multiplied using \\(+\\) and \\(-\\) . Conquer: Perform \\(7\\) multiplications of \\((n/2)*(n/2)\\) submatrices recursively. Combine: Form \\(C\\) using \\(+\\) and \\(\u2013\\) on \\((n/2)*(n/2)\\) submatrices. Recurrence: \\(T(n) = 7T(n/2) + \\Theta(n^2)\\) Strassen\u2019s Algorithm: Solving the Recurrence (1) \u00b6 \\(T(n) = 7T(n/2) + \\Theta(n^2)\\) \\(a=7\\) , \\(b=2\\) \\(f(n)=\\Theta(n^2)\\) \\(n^{log_b^a}=n^{lg7}\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(n^{log_b^a})\\) \\(T(n)=\\Theta(n^{log_2^7})\\) \\(2^3 = 8, 2^2=4\\) so \\(\\Longrightarrow log_2^7 \\approx 2.81\\) or use https://www.omnicalculator.com/math/log Strassen\u2019s Algorithm: Solving the Recurrence (2) \u00b6 The number \\(2.81\\) may not seem much smaller than \\(3\\) But, it is significant because the difference is in the exponent. Strassen\u2019s algorithm beats the ordinary algorithm on today\u2019s machines for \\(n \\geq 30\\) or so. Best to date: \\(\\Theta(n^{2.376 \\dots})\\) (of theoretical interest only) Maximum Subarray Problem \u00b6 Input: An array of values Output: The contiguous subarray that has the largest sum of elements Input array: \\([13][-3][-25][20][-3][-16][-23]\\overbrace{[18][20][-7][12]}^{\\textrm{max. contiguous subarray}}[-22][-4][7]\\) Maximum Subarray Problem: Divide & Conquer (1) \u00b6 Basic idea: Divide the input array into 2 from the middle Pick the best solution among the following: The max subarray of the left half The max subarray of the right half The max subarray crossing the mid-point Maximum Subarray Problem: Divide & Conquer (2) \u00b6 Maximum Subarray Problem: Divide & Conquer (3) \u00b6 Divide: Trivial (divide the array from the middle) Conquer: Recursively compute the max subarrays of the left and right halves Combine: Compute the max-subarray crossing the \\(mid-point\\) (can be done in \\(\\Theta(n)\\) time). Return the max among the following: the max subarray of the \\(\\text{left-subarray}\\) the max subarray of the \\(\\text{rightsubarray}\\) the max subarray crossing the \\(\\text{mid-point}\\) TODO : detailed solution in textbook... Conclusion : Divide & Conquer \u00b6 Divide and conquer is just one of several powerful techniques for algorithm design. Divide-and-conquer algorithms can be analyzed using recurrences and the master method (so practice this math). Can lead to more efficient algorithms Quicksort (1) \u00b6 One of the most-used algorithms in practice Proposed by C.A.R. Hoare in 1962. Divide-and-conquer algorithm In-place algorithm The additional space needed is O(1) The sorted array is returned in the input array Reminder: Insertion-sort is also an in-place algorithm, but Merge-Sort is not in-place. Very practical Quicksort (2) \u00b6 Divide: Partition the array into 2 subarrays such that elements in the lower part \\(\\leq\\) elements in the higher part Conquer: Recursively sort 2 subarrays Combine: Trivial (because in-place) Key: Linear-time \\((\\Theta(n))\\) partitioning algorithm section{ font-size: 25px; } Divide: Partition the array around a pivot element \u00b6 Choose a pivot element \\(x\\) Rearrange the array such that: Left subarray: All elements \\(\\leq x\\) Right subarray: All elements \\(\\geq x\\) Conquer: Recursively Sort the Subarrays \u00b6 Note: Everything in the left subarray \u2264 everything in the right subarray Note: Combine is trivial after conquer. Array already sorted. section{ font-size: 25px; } Two partitioning algorithms \u00b6 Hoare\u2019s algorithm: Partitions around the first element of subarray \\((pivot = x = A[p])\\) Lomuto\u2019s algorithm: Partitions around the last element of subarray \\((pivot = x = A[r])\\) Hoare\u2019s Partitioning Algorithm (1) \u00b6 Choose a pivot element: \\(pivot = x = A[p]\\) Grow two regions: from left to right: \\(A[p \\dots i]\\) from right to left: \\(A[j \\dots r]\\) such that: every element in \\(A[p \\dots i] \\leq\\) pivot every element in \\(A[p \\dots i] \\geq\\) pivot Hoare\u2019s Partitioning Algorithm (2) \u00b6 section{ font-size: 25px; } Hoare\u2019s Partitioning Algorithm (3) \u00b6 Elements are exchanged when \\(A[i]\\) is too large to belong to the left region \\(A[j]\\) is too small to belong to the right region assuming that the inequality is strict The two regions \\(A[p \\dots i]\\) and \\(A[j \\dots r]\\) grow until \\(A[i] \\geq pivot \\geq A[j]\\) H - PARTITION ( A , p , r ) pivot = A [ p ] i = p - 1 j = r - 1 while true do repeat j = j - 1 until A [ j ] <= pivot repeat i = i - 1 until A [ i ] <= pivot if i < j then exchange A [ i ] with A [ j ] else return j Hoare\u2019s Partitioning Algorithm Example (Step-1) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-2) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-3) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-4) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-5) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-6) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-7) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-8) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-9) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-10) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-11) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-12) \u00b6 section{ font-size: 25px; } Hoare\u2019s Partitioning Algorithm - Notes \u00b6 Elements are exchanged when \\(A[i]\\) is too large to belong to the left region \\(A[j]\\) is too small to belong to the right region assuming that the inequality is strict The two regions \\(A[p \\dots i]\\) and \\(A[j \\dots r]\\) grow until \\(A[i] \\geq pivot \\geq A[j]\\) The asymptotic runtime of Hoare\u2019s partitioning algorithm \\(\\Theta(n)\\) H - PARTITION ( A , p , r ) pivot = A [ p ] i = p - 1 j = r - 1 while true do repeat j = j - 1 until A [ j ] <= pivot repeat i = i - 1 until A [ i ] <= pivot if i < j then exchange A [ i ] with A [ j ] else return j section{ font-size: 25px; } Quicksort with Hoare\u2019s Partitioning Algorithm \u00b6 QUICKSORT ( A , p , r ) if p < r then q = H - PARTITION ( A , p , r ) QUICKSORT ( A , p , q ) QUICKSORT ( A , q + 1 , r ) endif Initial invocation: QUICKSORT(A,1,n) section{ font-size: 25px; } Hoare\u2019s Partitioning Algorithm: Pivot Selection \u00b6 if we select pivot to be \\(A[r]\\) instead of \\(A[p]\\) in H-PARTITION Consider the example where \\(A[r]\\) is the largest element in the array: End of H-PARTITION: \\(i = j = r\\) In QUICKSORT: \\(q = r\\) So, recursive call to: QUICKSORT(A, p, q=r) infinite loop Correctness of Hoare\u2019s Algorithm (1) \u00b6 We need to prove \\(3\\) claims to show correctness: Indices \\(i\\) and \\(j\\) never reference \\(A\\) outside the interval \\(A[p \\dots r]\\) Split is always non-trivial; i.e., \\(j \\neq r\\) at termination Every element in \\(A[p \\dots j] \\leq\\) every element in \\(A[j+1 \\dots r]\\) at termination Correctness of Hoare\u2019s Algorithm (2) \u00b6 Notations: \\(k\\) : \\(\\#\\) of times the while-loop iterates until termination \\(i_m\\) : the value of index i at the end of iteration \\(m\\) \\(j_m\\) : the value of index j at the end of iteration \\(m\\) \\(x\\) : the value of the pivot element Note : We always have \\(i_1= p\\) and \\(p \\leq j_1 \\leq r\\) because \\(x = A[p]\\) Correctness of Hoare\u2019s Algorithm (3) \u00b6 Lemma 1: Either \\(i_k = j_k\\) or \\(i_k = j_k+1\\) at termination Proof of Lemma 1: The algorithm terminates when \\(i \\geq j\\) (the else condition). So, it is sufficient to prove that \\(i_k \u2013 j_k \\leq 1\\) There are \\(2\\) cases to consider: Case 1: \\(k = 1\\) , i.e. the algorithm terminates in a single iteration Case 2: \\(k > 1\\) , i.e. the alg. does not terminate in a single iter. By contradiction , assume there is a run with \\(i_k \u2013 j_k > 1\\) section{ font-size: 25px; } Correctness of Hoare\u2019s Algorithm (4) \u00b6 Original correctness claims: Indices \\(i\\) and \\(j\\) never reference A outside the interval \\(A[p \\dots r]\\) Split is always non-trivial; i.e., \\(j \\neq r\\) at termination Proof: For \\(k = 1\\) : Trivial because \\(i_1 = j_1 = p\\) ( see Case 1 in proof of Lemma 2 ) For \\(k > 1\\) : \\(i_k > p\\) and \\(j_k < r\\) ( due to the repeat-until loops moving indices ) \\(i_k \\leq r\\) and \\(j_k \\geq p\\) ( due to Lemma 1 and the statement above ) The proof of claims (a) and (b) complete section{ font-size: 25px; } Correctness of Hoare\u2019s Algorithm (5) \u00b6 Lemma 2: At the end of iteration \\(m\\) , where \\(m<k\\) ( i.e. m is not the last iteration ), we must have: \\(A[p \\dots i_m] \\leq x\\) and \\(A[j_m \\dots r] \\geq x\\) Proof of Lemma 2: Base case: \\(m=1\\) and \\(k>1\\) ( i.e. the alg. does not terminate in the first iter. ) Ind. Hyp.: At the end of iteration \\(m-1\\) , where \\(m<k\\) ( i.e. m is not the last iteration ), we must have: \\(A[p \\dots i_m-1] \\leq x\\) and \\(A[j_m-1 \\dots r] \\geq x\\) General case: The lemma holds for \\(m\\) , where \\(m < k\\) Proof of base case complete! Correctness of Hoare\u2019s Algorithm (6) \u00b6 Original correctness claim: \u00a9 Every element in \\(A[ \\dots j] \\leq\\) every element in \\(A[j+ \\dots r]\\) at termination Proof of claim \u00a9 There are \\(3\\) cases to consider: Case 1: \\(k=1\\) , i.e. the algorithm terminates in a single iteration Case 2: \\(k>1\\) and \\(i_k = j_k\\) Case 3: \\(k>1\\) and \\(i_k = j_k + 1\\) Lomuto\u2019s Partitioning Algorithm (1) \u00b6 Choose a pivot element: \\(pivot = x = A[r]\\) Grow two regions: from left to right: \\(A[p \\dots i]\\) from left to right: \\(A[i+1 \\dots j]\\) such that: every element in \\(A[p \\dots i] \\leq pivot\\) every element in \\(A[i+1 \\dots j] > pivot\\) Lomuto\u2019s Partitioning Algorithm (2) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-1) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-2) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-3) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-4) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-5) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-6) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-7) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-8) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-9) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-10) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-11) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-12) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-13) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-14) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-15) \u00b6 Quicksort with Lomuto\u2019s Partitioning Algorithm \u00b6 QUICKSORT ( A , p , r ) if p < r then q = L - PARTITION ( A , p , r ) QUICKSORT ( A , p , q - 1 ) QUICKSORT ( A , q + 1 , r ) endif Initial invocation: QUICKSORT(A,1,n) Comparison of Hoare\u2019s & Lomuto\u2019s Algorithms (1) \u00b6 Notation: \\(n=r-p+1\\) \\(pivot=A[p]\\) ( Hoare ) \\(pivot=A[r]\\) ( Lomuto ) \\(\\#\\) of element exchanges: \\(e(n)\\) Hoare: \\(0 \\geq e(n) \\geq \\lfloor \\frac{n}{2} \\rfloor\\) Best : \\(k=1\\) with \\(i_1=j_1=p\\) (i.e., \\(A[p+1 \\dots r]>pivot\\) ) Worst : \\(A[p+1 \\dots p+ \\lfloor \\frac{n}{2} \\rfloor - 1] \\geq pivot \\geq A[p+ \\lceil \\frac{n}{2} \\rceil \\dots r]\\) Lomuto : \\(1 \\leq e(n) \\leq n\\) Best : \\(A[p \\dots r -1]>pivot\\) Worst : \\(A[p \\dots r-1] \\leq pivot\\) Comparison of Hoare\u2019s & Lomuto\u2019s Algorithms (2) \u00b6 \\(\\#\\) of element comparisons: \\(c_e(n)\\) Hoare : \\(n+1 \\leq c_e(n) \\leq n+2\\) Best : \\(i_k=j_k\\) Worst : \\(i_k=j_k+1\\) Lomuto : \\(c_e(n)=n-1\\) \\(\\#\\) of index comparisons: \\(c_i(n)\\) Hoare : \\(1 \\leq c_i(n) \\leq \\lfloor \\frac{n}{2} \\rfloor + 1 | (c_i(n)=e(n)+1)\\) Lomuto : \\(c_i(n)=n-1\\) Comparison of Hoare\u2019s & Lomuto\u2019s Algorithms (3) \u00b6 \\(\\#\\) of index increment/decrement operations: \\(a(n)\\) Hoare : \\(n+1 \\leq a(n) \\leq n+2 | (a(n)=c_e(n))\\) Lomuto : \\(n \\leq a(n) \\leq 2n-1 | (a(n)=e(n)+(n-1))\\) Hoare\u2019s algorithm is in general faster Hoare behaves better when pivot is repeated in \\(A[p \\dots r]\\) Hoare : Evenly distributes them between left & right regions Lomuto : Puts all of them to the left region Analysis of Quicksort (1) \u00b6 QUICKSORT ( A , p , r ) if p < r then q = H - PARTITION ( A , p , r ) QUICKSORT ( A , p , q ) QUICKSORT ( A , q + 1 , r ) endif Initial invocation: QUICKSORT(A,1,n) Assume all elements are distinct in the following analysis Analysis of Quicksort (2) \u00b6 H-PARTITION always chooses \\(A[p]\\) (the first element) as the pivot. The runtime of QUICKSORT on an already-sorted array is \\(\\Theta(n^2)\\) Example: An Already Sorted Array \u00b6 Partitioning always leads to \\(2\\) parts of size \\(1\\) and \\(n-1\\) Worst Case Analysis of Quicksort \u00b6 Worst case is when the PARTITION algorithm always returns imbalanced partitions (of size \\(1\\) and \\(n-1\\) ) in every recursive call. This happens when the pivot is selected to be either the min or max element. This happens for H-PARTITION when the input array is already sorted or reverse sorted \\[ \\begin{align*} T(n) &= T(1) + T(n-1) + \u0398(n) \\\\ &= T(n-1) + \u0398(n) \\\\ &= \u0398(n2) \\end{align*} \\] Worst Case Recursion Tree \u00b6 \\[ T(n) = T(1) + T(n-1) + cn \\] Best Case Analysis (for intuition only) \u00b6 If we\u2019re extremely lucky, H-PARTITION splits the array evenly at every recursive call \\[ \\begin{align*} T(n) &= 2T(n/2) + \\Theta(n) \\\\ &= \\Theta(nlgn) \\end{align*} \\] (same as merge sort) Instead of splitting \\(0.5:0.5\\) , if we split \\(0.1:0.9\\) then we need solve following equation. \\[ \\begin{align*} T(n) &= T(n/10) + T(9n/10) + \\Theta(n) \\\\ &= \\Theta(nlgn) \\end{align*} \\] \u201cAlmost-Best\u201d Case Analysis \u00b6 Balanced Partitioning (1) \u00b6 We have seen that if H-PARTITION always splits the array with \\(0.1-to-0.9\\) ratio, the runtime will be \\(\\Theta(nlgn)\\) . Same is true with a split ratio of \\(0.01-to-0.99\\) , etc. Possible to show that if the split has always constant \\((\\Theta(1))\\) proportionality, then the runtime will be \\(\\Theta(nlgn)\\) . In other words, for a constant \\(\\alpha | (0 < \\alpha \u2264 0.5)\\) : \\(\\alpha\u2013to\u2013(1-\\alpha)\\) proportional split yields \\(\\Theta(nlgn)\\) total runtime Balanced Partitioning (2) \u00b6 In the rest of the analysis, assume that all input permutations are equally likely. This is only to gain some intuition We cannot make this assumption for average case analysis We will revisit this assumption later Also, assume that all input elements are distinct. Balanced Partitioning (3) \u00b6 Question: What is the probability that H-PARTITION returns a split that is more balanced than \\(0.1-to-0.9\\) ? Balanced Partitioning (4) \u00b6 Reminder: H-PARTITION will place the pivot in the right partition unless the pivot is the smallest element in the arrays. Question: If the pivot selected is the mth smallest value \\((1 < m \u2264 n)\\) in the input array, what is the size of the left region after partitioning? Balanced Partitioning (5) \u00b6 Question: What is the probability that the pivot selected is the \\(m^{th}\\) smallest value in the array of size \\(n\\) ? \\(1/n\\) ( since all input permutations are equally likely ) Question: What is the probability that the left partition returned by H-PARTITION has size \\(m\\) , where \\(1<m<n\\) ? \\(1/n\\) ( due to the answers to the previous 2 questions ) Balanced Partitioning (6) \u00b6 section{ font-size: 25px; } Question: What is the probability that H-PARTITION returns a split that is more balanced than \\(0.1-to-0.9\\) ? \\[ \\begin{align*} Probability &=\\sum \\limits_{q=0.1n+1}^{0.9n-1}\\frac{1}{n} \\\\ &=\\frac{1}{n}(0.9n-1-0.1n-1+1) \\\\ &= 0.8-\\frac{1}{n} \\\\ & \\approx 0.8 \\text{ for large n} \\end{align*} \\] Balanced Partitioning (7) \u00b6 The probability that H-PARTITION yields a split that is more balanced than \\(0.1-to-0.9\\) is \\(80\\%\\) on a random array. Let \\(P_{\\alpha>}\\) be the probability that H-PARTITION yields a split more balanced than \\(\\alpha-to-(1-\\alpha)\\) , where \\(0 < \\alpha \\leq 0.5\\) Repeat the analysis to generalize the previous result section{ font-size: 25px; } Balanced Partitioning (8) \u00b6 Question: What is the probability that H-PARTITION returns a split that is more balanced than \\(\\alpha-to-(1-\\alpha)\\) ? \\[ \\begin{align*} Probability & =\\sum \\limits_{q=\\alpha n+1}^{(1-\\alpha)n-1}\\frac{1}{n} \\\\ & =\\frac{1}{n}((1-\\alpha)n-1- \\alpha n-1+1) \\\\ & = (1-2\\alpha)-\\frac{1}{n} \\\\ & \\approx (1-2\\alpha) \\text{ for large n} \\end{align*} \\] Balanced Partitioning (9) \u00b6 We found \\(P_{\\alpha >}=1-2\\alpha\\) Ex: \\(P_{0.1>}=0.8\\) and \\(P_{0.01>}=0.98\\) Hence, H-PARTITION produces a split more balanced than a \\(0.1-to-0.9\\) split \\(80\\%\\) of the time \\(0.01-to-0.99\\) split \\(98\\%\\) of the time less balanced than a \\(0.1-to-0.9\\) split \\(20\\%\\) of the time \\(0.01-to-0.99\\) split \\(2\\%\\) of the time Intuition for the Average Case (1) \u00b6 Assumption: All permutations are equally likely Only for intuition; we\u2019ll revisit this assumption later Unlikely: Splits always the same way at every level Expectation: Some splits will be reasonably balanced Some splits will be fairly unbalanced Average case: A mix of good and bad splits Good and bad splits distributed randomly thru the tree Intuition for the Average Case (2) \u00b6 Assume for intuition: Good and bad splits occur in the alternate levels of the tree Good split: Best case split Bad split: Worst case split Intuition for the Average Case (3) \u00b6 Compare 2-successive levels of avg case vs. 1 level of best case Intuition for the Average Case (4) \u00b6 In terms of the remaining subproblems, two levels of avg case is slightly better than the single level of the best case The avg case has extra divide cost of \\(\\Theta(n)\\) at alternate levels The extra divide cost \\(\\Theta(n)\\) of bad splits absorbed into the \\(\\Theta(n)\\) of good splits. Running time is still \\(\\Theta(nlgn)\\) But, slightly larger hidden constants, because the height of the recursion tree is about twice of that of best case. Intuition for the Average Case (5) \u00b6 Another way of looking at it: Suppose we alternate lucky, unlucky, lucky, unlucky, \\(\\dots\\) We can write the recurrence as: \\(L(n) = 2U(n/2) + \\Theta(n)\\) lucky split (best) \\(U(n) = L(n-1) + \\Theta(n)\\) unlucky split (worst) Solving: $$ \\begin{align*} L(n) & = 2(L(n/2-1) + \\Theta(n/2)) + \\Theta(n) \\ & = 2L(n/2-1) + \\Theta(n) \\ & = \u0398(nlgn) \\end{align*} $$ How can we make sure we are usually lucky for all inputs? Summary: Quicksort Runtime Analysis (1) \u00b6 Worst case: Unbalanced split at every recursive call \\[ \\begin{align*} T(n) & = T(1) + T(n-1) + \\Theta(n) \\\\ T(n) & = \\Theta(n2) \\end{align*} \\] Best case: Balanced split at every recursive call ( extremely lucky ) \\[ \\begin{align*} T(n) & = 2T(n/2) + \\Theta(n) \\\\ T(n) & = \\Theta(nlgn) \\end{align*} \\] Summary: Quicksort Runtime Analysis (2) \u00b6 Almost-best case: Almost-balanced split at every recursive call \\[ \\begin{align*} T(n) &=T(n/10)+T(9n/10)+ \\Theta(n) \\\\ \\text{or } T(n) &= T(n/100) + T(99n/100) + \u0398(n) \\\\ \\text{or } T(n) &= T(\\alpha n) + T((1-\\alpha n)+ \\Theta(n) \\end{align*} \\] for any constant \\(\\alpha, 0 < \\alpha \\leq 0.5\\) Summary: Quicksort Runtime Analysis (3) \u00b6 For a random input array, the probability of having a split more balanced than \\(0.1 \u2013 to \u2013 0.9 : 80\\%\\) more balanced than \\(0.01 \u2013 to \u2013 0.99 : 98\\%\\) more balanced than \\(\\alpha \u2013 to \u2013 (1-\\alpha) : 1 \u2013 2 \\alpha\\) for any constant \\(\\alpha, 0 < \\alpha \\leq 0.5\\) Summary: Quicksort Runtime Analysis (4) \u00b6 Avg case intuition: Different splits expected at different levels some balanced (good), some unbalanced (bad) Avg case intuition: Assume the good and bad splits alternate i.e. good split -> bad split -> good split -> \u2026 \\(T(n) = \\Theta(nlgn)\\) (informal analysis for intuition) Randomized Quicksort \u00b6 In the avg-case analysis, we assumed that all permutations of the input array are equally likely. But, this assumption does not always hold e.g. What if all the input arrays are reverse sorted ? Always worst-case behavior Ideally, the avg-case runtime should be independent of the input permutation . Randomness should be within the algorithm , not based on the distribution of the inputs. i.e. The avg case should hold for all possible inputs Randomized Algorithms (1) \u00b6 Alternative to assuming a uniform distribution: Impose a uniform distribution e.g. Choose a random pivot rather than the first element Typically useful when: there are many ways that an algorithm can proceed but, it\u2019s difficult to determine a way that is always guaranteed to be good . If there are many good alternatives ; simply choose one randomly . Randomized Algorithms (1) \u00b6 Ideally: Runtime should be independent of the specific inputs No specific input should cause worst-case behavior Worst-case should be determined only by output of a random number generator. section{ font-size: 25px; } Randomized Quicksort (1) \u00b6 Using Hoare\u2019s partitioning algorithm: R - QUICKSORT ( A , p , r ) if p < r then q = R - PARTITION ( A , p , r ) R - QUICKSORT ( A , p , q ) R - QUICKSORT ( A , q +1 , r ) R - PARTITION ( A , p , r ) s = RANDOM ( p , r ) exchange A [ p ] with A [ s ] return H - PARTITION ( A , p , r ) Alternatively, permuting the whole array would also work but, would be more difficult to analyze section{ font-size: 25px; } Randomized Quicksort (2) \u00b6 Using Lomuto\u2019s partitioning algorithm: R - QUICKSORT ( A , p , r ) if p < r then q = R - PARTITION ( A , p , r ) R - QUICKSORT ( A , p , q -1 ) R - QUICKSORT ( A , q +1 , r ) R - PARTITION ( A , p , r ) s = RANDOM ( p , r ) exchange A [ r ] with A [ s ] return L - PARTITION ( A , p , r ) Alternatively, permuting the whole array would also work but, would be more difficult to analyze Notations for Formal Analysis \u00b6 Assume all elements in \\(A[p \\dots r]\\) are distinct Let \\(n = r \u2013 p + 1\\) Let \\(rank(x) = |{A[i]: p \\leq i \\leq r \\text{ and } A[i] \\leq x}|\\) i.e. \\(rank(x)\\) is the number of array elements with value less than or equal to \\(x\\) \\(A=\\{5,9,7,6,8,1,4\\}\\) \\(p=5,r=4\\) \\(rank(5)=3\\) i.e. it is the \\(3^{rd}\\) smallest element in the array Formal Analysis for Average Case \u00b6 The following analysis will be for Quicksort using Hoare\u2019s partitioning algorithm. Reminder: The pivot is selected randomly and exchanged with \\(A[p]\\) before calling H-PARTITION Let \\(x\\) be the random pivot chosen. What is the probability that \\(rank(x) = i\\) for \\(i = 1, 2, \\dots n\\) ? \\(P(rank(x) = i) = 1/n\\) section{ font-size: 25px; } Various Outcomes of H-PARTITION (1) \u00b6 Assume that \\(rank(x)=1\\) i.e. the random pivot chosen is the smallest element What will be the size of the left partition \\((|L|)\\) ? Reminder: Only the elements less than or equal to \\(x\\) will be in the left partition. \\(A=\\{\\overbrace{2}^{p=x=pivot}\\underbrace{,}_{\\Longrightarrow|L|=1 } 9,7,6,8,5,\\overbrace{4}^r\\}\\) \\(p=2,r=4\\) \\(pivot=x=2\\) TODO: convert to image...S6_P9 section{ font-size: 25px; } Various Outcomes of H-PARTITION (2) \u00b6 Assume that \\(rank(x)>1\\) i.e. the random pivot chosen is not the smallest element What will be the size of the left partition \\((|L|)\\) ? Reminder: Only the elements less than or equal to \\(x\\) will be in the left partition. Reminder: The pivot will stay in the right region after H-PARTITION if \\(rank(x)>1\\) \\(A=\\{\\overbrace{2}^{p}, 4 \\underbrace{,}_{\\Longrightarrow|L|=rank(x)-1}7,6,8,\\overbrace{5,}^{pivot}\\overbrace{9}^r\\}\\) \\(p=2,r=4\\) \\(pivot=x=5\\) TODO: convert to image...S6_P10 Various Outcomes of H-PARTITION - Summary (1) \u00b6 \\(x: pivot\\) \\(|L|: \\text{size of left region}\\) \\(P(rank(x) = i) = 1/n \\text{ for } 1 \\leq i \\leq n\\) \\(\\text{if } rank(x) = 1 \\text{ then } |L| = 1\\) \\(\\text{if } rank(x) > 1 \\text{ then } |L| = rank(x) - 1\\) \\(P(|L| = 1) = P(rank(x) = 1) + P(rank(x) = 2)\\) \\(P(|L| = 1) = 2/n\\) \\(P(|L| = i) = P(rank(x) = i+1) \\text{ for } 1< i < n\\) \\(P(|L| = i) = 1/n \\text{ for } 1< i < n\\) Various Outcomes of H-PARTITION - Summary (2) \u00b6 section{ font-size: 25px; } Average - Case Analysis: Recurrence (1) \u00b6 \\(x=pivot\\) \\[ \\begin{align*} T(n) & = \\frac{1}{n}(T(1)+t(n-1) ) & rank:1 \\\\ & + \\frac{1}{n}(T(1)+t(n-1) ) & rank:2 \\\\ & + \\frac{1}{n}(T(2)+t(n-2) ) & rank:3 \\\\ & \\vdots & \\vdots \\\\ & + \\frac{1}{n}(T(i)+t(n-i) ) & rank:i+1 \\\\ & \\vdots & \\vdots \\\\ & + \\frac{1}{n}(T(n-1)+t(1) ) & rank:n \\\\ & + \\Theta(n) \\end{align*} \\] section{ font-size: 25px; } Average - Case Analysis: Recurrence (2) \u00b6 \\[ \\begin{align*} T(n) &= \\frac{1}{n}\\sum \\limits_{q=1}^{n-1}(T(q)+T(n-q))+\\frac{1}{n}(T(1)+T(n-1))+\\Theta(n)\\\\ & \\text{Note: } \\frac{1}{n}(T(1)+T(n-1))=\\frac{1}{n}(\\Theta(1)+O(n^2))=O(n) \\\\ T(n) &= \\frac{1}{n}\\sum \\limits_{q=1}^{n-1}(T(q)+T(n-q))+\\Theta(n) \\end{align*} \\] for \\(k=1,2,\\dots,n-1\\) each term \\(T(k)\\) appears twice once for \\(q = k\\) and once for \\(q = n\u2212k\\) \\[ T(n) = \\frac{2}{n}\\sum \\limits_{k=1}^{n-1} T(k)+\\Theta(n) \\] Average - Case Analysis -Solving Recurrence: Substitution \u00b6 Guess: \\(T(n)=O(nlgn)\\) \\(T(k) \u2264 aklgk\\) for \\(k<n\\) , for some constant \\(a > 0\\) \\[ \\begin{align*} T(n) &= \\frac{2}{n} \\sum \\limits_{k=1}^{n-1} T(k)+\\Theta(n) \\\\ & \\leq \\frac{2}{n} \\sum \\limits_{k=1}^{n-1} aklgk+\\Theta(n) \\\\ & \\leq \\frac{2a}{n} \\sum \\limits_{k=1}^{n-1} klgk+\\Theta(n) \\end{align*} \\] Need a tight bound for \\(\\sum klgk\\) Tight bound for \\(\\sum klgk\\) (1) \u00b6 Bounding the terms \\(\\ \\sum \\limits_{k=1}^{n-1}klgk \\leq \\sum \\limits_{k=1}^{n-1}nlgn = n(n-1)lgn \\leq n^2lgn\\) This bound is not strong enough because \\(T(n) \\leq \\frac{2a}{n}n^2lgn+\\Theta(n)\\) \\(=2anlgn+\\Theta(n)\\) \\(\\Longrightarrow\\) couldn\u2019t prove \\(T(n) \\leq anlgn\\) Tight bound for \\(\\sum klgk\\) (2) \u00b6 Splitting summations: ignore ceilings for simplicity $$ \\sum \\limits_{k=1}^{n-1}klgk \\leq \\sum \\limits_{k=1}^{n/2-1}klgk + \\sum \\limits_{k=n/2}^{n-1}klgk $$ First summation : \\(lgk < lg(n/2)=lgn-1\\) Second summation : \\(lgk < lgn\\) section{ font-size: 25px; } Splitting: \\(\\sum \\limits_{k=1}^{n-1}klgk \\leq \\sum \\limits_{k=1}^{n/2-1}klgk + \\sum \\limits_{k=n/2}^{n-1}klgk\\) (3) \u00b6 \\[ \\begin{align*} & \\sum \\limits_{k=1}^{n-1}klgk \\leq (lg(n-1))\\sum \\limits_{k=1}^{n/2-1}k + lgn \\sum \\limits_{k=n/2}^{n-1}k \\\\ &= lgn \\sum \\limits_{k=1}^{n-1}k- \\sum \\limits_{k=1}^{n/2-1}k \\\\ &= \\frac{1}{2}n(n-1)lgn - \\frac{1}{2} \\frac{n}{2}(\\frac{n}{2}-1) \\\\ &= \\frac{1}{2}n^2lgn - \\frac{1}{8}n^2 - \\frac{1}{2}n(lgn-1/2) \\\\ \\end{align*} \\] \\[ \\begin{align*} & \\sum \\limits_{k=1}^{n-1} klgk \\leq \\frac{1}{2}n^2lgn-\\frac{1}{8}n^2 \\ for \\ lgn \\geq 1/2 \\Longrightarrow n \\geq \\sqrt{2} \\end{align*} \\] Substituting: - \\(\\sum \\limits_{k=1}^{n-1}klgk \\leq \\frac{1}{2}n^2lgn-\\frac{1}{8}n^2\\) (4) \u00b6 \\[ \\begin{align*} T(n) & \\leq \\frac{2a}{n}\\sum \\limits_{k=1}^{n-1}klgk+\\Theta(n)\\\\ & \\leq \\frac{2a}{n}(\\frac{1}{2}n^2lgn-\\frac{1}{8}n^2)+\\Theta(n) \\\\ & = anlgn - (\\frac{a}{4}n-\\Theta(n)) \\end{align*} \\] We can choose a large enough so that \\(\\frac{a}{4}n \\geq \\Theta(n)\\) \\[ \\begin{align*} T(n) & \\leq anlgn \\\\ T(n) & = O(nlgn) \\end{align*} \\] Q.E.D. Medians and Order Statistics \u00b6 ith order statistic : \\(i^{th}\\) smallest element of a set of \\(n\\) elements minimum: first order statistic maximum: \\(n^{th}\\) order statistic median: \u201chalfway point\u201d of the set \\[ \\begin{align*} i & = \\lfloor \\frac{(n+1)}{2} \\rfloor \\\\ \\text{ or } \\\\ i & = \\lceil \\frac{(n+1)}{2} \\rceil \\end{align*} \\] Selection Problem \u00b6 Selection problem: Select the \\(i^{th}\\) smallest of \\(n\\) elements Na\u00efve algorithm: Sort the input array \\(A\\) ; then return \\(A[i]\\) \\(T(n) = \\theta(nlgn)\\) using e.g. merge sort (but not quicksort) Can we do any better? Selection in Expected Linear Time \u00b6 Randomized algorithm using divide and conquer Similar to randomized quicksort Like quicksort: Partitions input array recursively Unlike quicksort: Makes a single recursive call Reminder: Quicksort makes two recursive calls Expected runtime: \\(\\Theta(n)\\) Reminder: Expected runtime of quicksort: \\(\\Theta(nlgn)\\) Selection in Expected Linear Time: Example 1 \u00b6 Select the \\(2^{nd}\\) smallest element: \\[ \\begin{align*} A & = \\{6,10,13,5,8,3,2,11\\} \\\\ i & = 2 \\\\ \\end{align*} \\] Partition the input array: \\[ \\begin{align*} A & = \\{\\underbrace{2,3,5,}_{\\text{left subarray} }\\underbrace{13,8,10,6,11}_{\\text{right subarray}}\\} \\end{align*} \\] make a recursive call to select the \\(2^{nd}\\) smallest element in left subarray Selection in Expected Linear Time: Example 2 \u00b6 Select the \\(7^{th}\\) smallest element: \\[ \\begin{align*} A & = \\{6,10,13,5,8,3,2,11\\} \\\\ i & = 7 \\\\ \\end{align*} \\] Partition the input array: \\[ \\begin{align*} A & = \\{\\underbrace{2,3,5,}_{\\text{left subarray} }\\underbrace{13,8,10,6,11}_{\\text{right subarray}}\\} \\end{align*} \\] make a recursive call to select the \\(4^{th}\\) smallest element in right subarray Selection in Expected Linear Time (1) \u00b6 R - SELECT ( A , p , r , i ) if p == r then return A [ p ]; q = R - PARTITION ( A , p , r ) k = q \u2013 p +1 ; if i <= k then return R - SELECT ( A , p , q , i ); else return R - SELECT ( A , q +1 , r , i - k ); \\[ \\begin{align*} A & = \\{ \\underbrace{ | }_{p} \\dots \\leq x \\text{(k smallest elements)} \\dots \\underbrace{ | }_{q} \\dots \\geq x \\dots \\underbrace{ | }_{r} \\} \\\\ x & = pivot \\end{align*} \\] Selection in Expected Linear Time (2) \u00b6 \\[ \\begin{align*} A & = \\{ \\overbrace{ | }^{p} \\underbrace{ \\dots \\leq x \\dots }_{L} \\overbrace{ | }^{q} \\underbrace{ \\dots \\geq x \\dots }_{R} \\overbrace{ | }^{r} \\} \\\\ x & = pivot \\end{align*} \\] All elements in \\(L \\leq\\) all elements in \\(R\\) \\(L\\) contains: \\(|L| = q\u2013p+1\\) \\(=\\) k smallest elements of \\(A[p...r]\\) if \\(i \\leq |L| = k\\) then search \\(L\\) recursively for its \\(i^{th}\\) smallest element else search \\(R\\) recursively for its \\((i-k)^{th}\\) smallest element Runtime Analysis (1) \u00b6 Worst case: Imbalanced partitioning at every level and the recursive call always to the larger partition \\[ \\begin{align*} & = \\{1,\\underbrace{2,3,4,5,6,7,8}_{\\text{recursive call}} \\} & i & = 8 \\\\ & = \\{2,\\underbrace{3,4,5,6,7,8}_{\\text{recursive call}} \\} & i & = 7 \\end{align*} \\] Runtime Analysis (2) \u00b6 Worst case: Worse than the na\u00efve method (based on sorting) \\[ \\begin{align*} T(n) &= T(n-1) + \\Theta(n) \\\\ T(n) &= \\Theta(n^2) \\end{align*} \\] Best case: Balanced partitioning at every recursive level \\[ \\begin{align*} T(n) &= T(n/2) + \\Theta(n) \\\\ T(n) &= \\Theta(n) \\end{align*} \\] Avg case: Expected runtime \u2013 need analysis T.B.D. section{ font-size: 25px; } Reminder: Various Outcomes of H-PARTITION \u00b6 \\(x: pivot\\) \\(|L|: \\text{size of left region}\\) \\(P(rank(x) = i) = 1/n \\text{ for } 1 \\leq i \\leq n\\) \\(\\text{if } rank(x) = 1 \\text{ then } |L| = 1\\) \\(\\text{if } rank(x) > 1 \\text{ then } |L| = rank(x) - 1\\) \\(P(|L| = 1) = P(rank(x) = 1) + P(rank(x) = 2)\\) \\(P(|L| = 1) = 2/n\\) \\(P(|L| = i) = P(rank(x) = i+1) \\text{ for } 1< i < n\\) \\(P(|L| = i) = 1/n \\text{ for } 1< i < n\\) Average Case Analysis of Randomized Select \u00b6 To compute the upper bound for the avg case , assume that the \\(i^{th}\\) element always falls into the larger partition . \\[ \\begin{align*} A & = \\{ \\overbrace{ | }^{p} \\underbrace{ \\dots \\leq x \\dots }_{Left Partition} \\overbrace{ | }^{q} \\underbrace{ \\dots \\geq x \\dots }_{Right Partition} \\overbrace{ | }^{r} \\} \\\\ x & = pivot \\end{align*} \\] We will analyze the case where the recursive call is always made to the larger partition This will give us an upper bound for the avg case Various Outcomes of H-PARTITION \u00b6 Average-Case Analysis of Randomized Select (1) \u00b6 \\[ \\text{Recall:} P(|L|=i) = \\begin{cases} 2/n & \\text{for } i=1 \\\\ 1/n & \\text{for } i=2,3,\\dots,n-1 \\end{cases} \\] Upper bound: Assume \\(i^{th}\\) element always falls into the larger part. \\[ \\begin{align*} T(n) &\\leq \\frac{1}{n}T(max(1,n-1))+\\frac{1}{n}\\sum \\limits_{q=1}^{n-1}T(max(q,n-q))+O(n) \\\\ Note: & \\frac{1}{n}T(max(1,n-1)) = \\frac{1}{n}T(n-1)=\\frac{1}{n}O(n^2) = O(n) \\\\ \\therefore \\text{(3 dot mean therefore) } & T(n) \\leq \\frac{1}{n}\\sum \\limits_{q=1}^{n-1}T(max(q,n-q))+O(n) \\end{align*} \\] Average-Case Analysis of Randomized Select (2) \u00b6 \\[ \\begin{align*} \\therefore T(n) \\leq \\frac{1}{n}\\sum \\limits_{q=1}^{n-1}T(max(q,n-q))+O(n) \\end{align*} \\] \\[ max(q, n\u2013q) = \\begin{cases} q & \\text{ if } q \\geq \\lceil n/2 \\rceil \\\\ n-q & \\text{ if } q < \\lceil n/2 \\rceil \\\\ \\end{cases} \\] \\(n\\) is odd: \\(T(k)\\) appears twice for \\(k=\\lceil n/2 \\rceil+1,\\lceil n/2 \\rceil+2,\\dots,n\u20131\\) \\(n\\) is even: \\(T(\\lceil n/2 \\rceil)\\) appears once \\(T(k)\\) appears twice for \\(k = \\lceil n/2 \\rceil +1, \\lceil n/2 \\rceil+2,\\dots,n\u20131\\) Average-Case Analysis of Randomized Select (3) \u00b6 Hence, in both cases: \\[ \\begin{align*} \\sum \\limits_{q=1}^{n-1} T(max(q,n-q))+O(n) & \\leq 2\\sum \\limits_{q=\\lceil n/2 \\rceil}^{n-1} T(q)+O(n) \\\\ \\therefore T(n) & \\leq \\frac{2}{n} \\sum \\limits_{q=\\lceil n/2 \\rceil}^{n-1}T(q)+O(n) \\end{align*} \\] section{ font-size: 25px; } Average-Case Analysis of Randomized Select (4) \u00b6 \\[ \\begin{align*} T(n) & \\leq \\frac{2}{n} \\sum \\limits_{q=\\lceil n/2 \\rceil}^{n-1}T(q)+O(n) \\end{align*} \\] By substitution guess \\(T(n) = O(n)\\) Inductive hypothesis: \\(T(k) \\leq ck, \\forall k<n\\) \\[ \\begin{align*} T(n) & \\leq \\frac{2}{n} \\sum \\limits_{q=\\lceil n/2 \\rceil}^{n-1}ck+O(n) \\\\ & = \\frac{2c}{n} \\Bigg(\\sum \\limits_{k=1}^{n-1}k-\\sum \\limits_{k=1}^{\\lceil n/2 \\rceil-1}k \\Bigg)+ O(n) \\\\ & = \\frac{2c}{n} \\Bigg(\\frac{1}{2}n(n-1)-\\frac{1}{2} \\lceil \\frac{n}{2} \\rceil \\bigg( \\frac{n}{2}-1 \\bigg) \\Bigg)+ O(n) \\end{align*} \\] Average-Case Analysis of Randomized Select (5) \u00b6 \\[ \\begin{align*} T(n)& \\leq \\frac{2c}{n} \\Bigg(\\frac{1}{2}n(n-1)-\\frac{1}{2} \\lceil \\frac{n}{2} \\rceil \\bigg( \\frac{n}{2}-1 \\bigg) \\Bigg)+ O(n) \\\\ & \\leq c(n-1)-\\frac{c}{4}n+\\frac{c}{2}+O(n) \\\\ & = cn - \\frac{c}{4}n - \\frac{c}{2} + O(n) \\\\ & = cn - \\Bigg( \\bigg( \\frac{c}{4}n+\\frac{c}{2}\\bigg) + O(n) \\Bigg) \\\\ & \\leq cn \\end{align*} \\] since we can choose c large enough so that \\((cn/4+c/2 )\\) dominates \\(O(n)\\) Summary of Randomized Order-Statistic Selection \u00b6 Works fast: linear expected time Excellent algorithm in practise But, the worst case is very bad: \\(\\Theta(n^2)\\) Blum, Floyd, Pratt, Rivest & Tarjan[1973] algorithms are runs in linear time in the worst case . Generate a good pivot recursively section{ font-size: 25px; } Selection in Worst Case Linear Time \u00b6 // return i - th element in set S with n elements SELECT ( S , n , i ) if n <= 5 then SORT S and return the i - th element DIVIDE S into ceil ( n / 5 ) groups // first ceil ( n / 5 ) groups are of size 5 , last group is of size n mod 5 FIND median set M = { m , \u2026 , m_ceil ( n / 5 )} // m_j : median of j - th group x = SELECT ( M , ceil ( n / 5 ), floor (( ceil ( n / 5 ) +1 ) / 2 )) PARTITION set S around the pivot x into L and R if i <= | L | then return SELECT ( L , | L | , i ) else return SELECT ( R , n \u2013 | L | , i \u2013 | L | ) Selection in Worst Case Linear Time - Example (1) \u00b6 Input: Array \\(S\\) and index \\(i\\) Output: The \\(i^{th}\\) smallest value \\[ \\begin{array}{ccc} 25 & 9 & 16 & 8 & 11 & 27 & 39 & 42 & 15 & 6 32 & 14 & 36 & 20 & 33 & 22 & 31 & 4 & 17 & 3 & 30 & 41 \\\\ 2 & 13 & 19 & 7 & 21 & 10 & 34 & 1 & 37 & 23 & 40 & 5 & 29 & 18 & 24 & 12 & 38 & 28 & 26 & 35 & 43 \\end{array} \\] Selection in Worst Case Linear Time - Example (2) \u00b6 Step 1: Divide the input array into groups of size \\(5\\) \\[ \\overbrace{ \\begin{array}{ccc} 25 & 9 & 16 & 8 & 11 \\\\ 27 & 39 & 42 & 15 & 6 \\\\ 32 & 14 & 36 & 20 & 33 \\\\ 22 & 31 & 4 & 17 & 3 \\\\ 30 & 41 & 2 & 13 & 19 \\\\ 7 & 21 & 10 & 34 & 1 \\\\ 37 & 23 & 40 & 5 & 29 \\\\ 18 & 24 & 12 & 38 & 28 \\\\ 26 & 35 & 43 \\end{array} }^{\\text{group size}=5} \\] section{ font-size: 25px; } Selection in Worst Case Linear Time - Example (3) \u00b6 Step 2: Compute the median of each group ( \\(\\Theta(n)\\) ) \\[ \\begin{array}{ccc} 25 & 16 & \\overbrace{11}^{Medians} & 8 & 9 \\\\ 39 & 42 & 27 & 6 & 15 \\\\ 36 & 33 & 32 & 20 & 14 \\\\ 22 & 31 & 17 & 3 & 4 \\\\ 41 & 30 & 19 & 13 & 2 \\\\ 21 & 34 & 10 & 1 & 7 \\\\ 37 & 40 & 29 & 23 & 5 \\\\ 38 & 28 & 24 & 12 & 18 \\\\ & 26 & 35 & 43 \\end{array} \\] Let \\(M\\) be the set of the medians computed: \\(M = \\{11, 27, 32, 17, 19, 10, 29, 24, 35\\}\\) Selection in Worst Case Linear Time - Example (4) \u00b6 Step 3: Compute the median of the median group \\(M\\) \\(x \\leftarrow SELECT(M,|M|,\\lfloor (|M|+1)/2 \\rfloor)\\) where \\(|M|=\\lceil n/5 \\rceil\\) Let \\(M\\) be the set of the medians computed: \\(M = \\{11, 27, 32, 17, 19, 10, 29, \\overbrace{24}^{Median}, 35\\}\\) \\(Median = 24\\) The runtime of the recursive call: \\(T(|M|)=T(\\lceil n/5 \\rceil)\\) Selection in Worst Case Linear Time - Example (5) \u00b6 Step 4: Partition the input array \\(S\\) around the median-of-medians \\(x\\) \\[ \\begin{array}{ccc} 25 & 9 & 16 & 8 & 11 & 27 & 39 & 42 & 15 & 6 32 & 14 & 36 & 20 & 33 & 22 & 31 & 4 & 17 & 3 & 30 & 41 \\\\ 2 & 13 & 19 & 7 & 21 & 10 & 34 & 1 & 37 & 23 & 40 & 5 & 29 & 18 & 24 & 12 & 38 & 28 & 26 & 35 & 43 \\end{array} \\] Partition \\(S\\) around \\(x = 24\\) Claim: Partitioning around x is guaranteed to be well-balanced. section{ font-size: 25px; } Selection in Worst Case Linear Time - Example (6) \u00b6 \\(M\\) : Median, \\(M^*\\) : Median of Medians \\[ \\begin{array}{ccc} 41 & 30 & \\overbrace{19}^{M} & 13 & 2 \\\\ 21 & 34 & 10 & 1 & 7 \\\\ 22 & 31 & 17 & 3 & 4 \\\\ 25 & 16 & 11 & 8 & 9 \\\\ 38 & 28 & \\overbrace{24}^{M^*} & 12 & 18 \\\\ 36 & 33 & 32 & 20 & 14 \\\\ 37 & 40 & 29 & 23 & 5 \\\\ 39 & 42 & 27 & 6 & 15 \\\\ & 26 & 35 & 43 \\end{array} \\] About half of the medians greater than \\(x=24\\) (about \\(n/10\\) ) Selection in Worst Case Linear Time - Example (7) \u00b6 Selection in Worst Case Linear Time - Example (8) \u00b6 Selection in Worst Case Linear Time - Example (9) \u00b6 \\[ S = \\begin{array}{ccc} \\{ 25 & 9 & 16 & 8 & 11 & 27 & 39 & 42 & 15 & 6 32 & 14 & 36 & 20 & 33 & 22 & 31 & 4 & 17 & 3 & 30 & 41 \\\\ 2 & 13 & 19 & 7 & 21 & 10 & 34 & 1 & 37 & 23 & 40 & 5 & 29 & 18 & 24 & 12 & 38 & 28 & 26 & 35 & 43 \\} \\end{array} \\] Partitioning \\(S\\) around \\(x = 24\\) will lead to partitions of sizes \\(\\sim 3n/10\\) and \\(\\sim 7n/10\\) in the worst case . Step 5: Make a recursive call to one of the partitions if i <= | L | then return SELECT ( L , | L | , i ) else return SELECT ( R , n -| L | , i -| L | ) section{ font-size: 25px; } Selection in Worst Case Linear Time \u00b6 // return i - th element in set S with n elements SELECT ( S , n , i ) if n <= 5 then SORT S and return the i - th element DIVIDE S into ceil ( n / 5 ) groups // first ceil ( n / 5 ) groups are of size 5 , last group is of size n mod 5 FIND median set M = { m , \u2026 , m_ceil ( n / 5 )} // m_j : median of j - th group x = SELECT ( M , ceil ( n / 5 ), floor (( ceil ( n / 5 ) +1 ) / 2 )) PARTITION set S around the pivot x into L and R if i <= | L | then return SELECT ( L , | L | , i ) else return SELECT ( R , n \u2013 | L | , i \u2013 | L | ) Choosing the Pivot (1) \u00b6 Divide S into groups of size 5 Choosing the Pivot (2) \u00b6 Divide S into groups of size 5 Find the median of each group Choosing the Pivot (3) \u00b6 Divide S into groups of size 5 Find the median of each group Recursively select the median x of the medians Choosing the Pivot (4) \u00b6 At least half of the medians \\(\\geq x\\) Thus \\(m = \\lceil \\lceil n/5 \\rceil /2 \\rceil\\) groups contribute 3 elements to R except possibly the last group and the group that contains \\(x\\) , \\(|R|\\geq 3(m\u20132)\\geq \\frac{3n}{10}\u20136\\) Choosing the Pivot (5) \u00b6 Similarly \\(|L| \\geq \\frac{3n}{10}\u2013 6\\) Therefore, SELECT is recursively called on at most \\(n-(\\frac{3n}{10}-6)=\\frac{7n}{10}+6\\) elements Selection in Worst Case Linear Time (1) \u00b6 Selection in Worst Case Linear Time (2) \u00b6 Thus recurrence becomes \\(T(n) \\leq T \\big( \\lceil \\frac{n}{5} \\rceil \\big) + T\\big( \\frac{7n}{10}+6 \\big) + \\Theta(n)\\) Guess \\(T(n)=O(n)\\) and prove by induction Inductive step: \\[ \\begin{align*} T(n) & \\leq c \\lceil n/5 \\rceil + c(7n/10+6)+\\Theta(n) \\\\ & \\leq cn/5 + c + 7cn/10 + 6c + \\Theta(n) \\\\ & = 9cn/10 + 7c + \\Theta(n) \\\\ & = cn - [c(n/10-7)-\\Theta(n)] \\leq cn &\\text{( for large c)} \\end{align*} \\] Work at each level of recursion is a constant factor \\((9/10)\\) smaller References \u00b6 Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) Insertion Sort - GeeksforGeeks NIST Dictionary of Algorithms and Data Structures NIST - Dictionary of Algorithms and Data Structures NIST - big-O notation NIST - big-Omega notation \\(-End-Of-Week-3-Course-Module-\\)","title":"Week-3 (Matrix Multiplication/Quick Sort)"},{"location":"week-3/ce100-week-3-matrix/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-3/ce100-week-3-matrix/#week-3-matrix-multiplication-quick-sort","text":"","title":"Week-3 (Matrix Multiplication/ Quick Sort)"},{"location":"week-3/ce100-week-3-matrix/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-3/ce100-week-3-matrix/#matrix-multiplication-quick-sort","text":"","title":"Matrix Multiplication / Quick Sort"},{"location":"week-3/ce100-week-3-matrix/#outline-1","text":"Matrix Multiplication Traditional Recursive Strassen","title":"Outline (1)"},{"location":"week-3/ce100-week-3-matrix/#outline-2","text":"Quicksort Hoare Partitioning Lomuto Partitioning Recursive Sorting","title":"Outline (2)"},{"location":"week-3/ce100-week-3-matrix/#outline-3","text":"Quicksort Analysis Randomized Quicksort Randomized Selection Recursive Medians","title":"Outline (3)"},{"location":"week-3/ce100-week-3-matrix/#matrix-multiplication-1","text":"Input: \\(A=[a_{ij}],B=[b_{ij}]\\) Output: \\(C=[c_{ij}]=A \\cdot B\\) \\(\\Longrightarrow i,j=1,2,3, \\dots, n\\) \\[ \\begin{bmatrix} c_{11} & c_{12} & \\dots & c_{1n} \\\\ c_{21} & c_{22} & \\dots & c_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ c_{n1} & c_{n2} & \\dots & c_{nn} \\\\ \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ a_{n1} & a_{n2} & \\dots & a_{nn} \\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} b_{11} & b_{12} & \\dots & b_{1n} \\\\ b_{21} & b_{22} & \\dots & b_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ b_{n1} & a_{n2} & \\dots & b_{nn} \\\\ \\end{bmatrix} \\]","title":"Matrix Multiplication (1)"},{"location":"week-3/ce100-week-3-matrix/#matrix-multiplication-2","text":"\\(c_{ij}=\\sum \\limits_{1\\leq k \\leq n}^{}a_{ik}.b_{kj}\\)","title":"Matrix Multiplication (2)"},{"location":"week-3/ce100-week-3-matrix/#matrix-multiplication-standard-algorithm","text":"Running Time: \\(\\Theta(n^3)\\) for i = 1 to n do for j = 1 to n do C [ i , j ] = 0 for k = 1 to n do C [ i , j ] = C [ i , j ] + A [ i , k ] + B [ k , j ] endfor endfor endfor","title":"Matrix Multiplication: Standard Algorithm"},{"location":"week-3/ce100-week-3-matrix/#matrix-multiplication-divide-conquer-1","text":"IDEA: Divide the \\(nxn\\) matrix into \\(2x2\\) matrix of \\((n/2)x(n/2)\\) submatrices.","title":"Matrix Multiplication: Divide &amp; Conquer (1)"},{"location":"week-3/ce100-week-3-matrix/#matrix-multiplication-divide-conquer-2","text":"\\[ \\begin{bmatrix} c_{11} & c_{12} \\\\ c_{21} & c_{22} \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} \\cdot \\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{bmatrix} \\] \\[ \\text{8 mults and 4 adds of (n/2)*(n/2) submatrices}= \\begin{cases} c_{11}=a_{11}b_{11}+a_{12}b_{21} \\\\ c_{21}=a_{21}b_{11}+a_{22}b_{21} \\\\ c_{12}=a_{11}b_{12}+a_{12}b_{22} \\\\ c_{22}=a_{21}b_{12}+a_{22}b_{22} \\end{cases} \\]","title":"Matrix Multiplication: Divide &amp; Conquer (2)"},{"location":"week-3/ce100-week-3-matrix/#matrix-multiplication-divide-conquer-3","text":"MATRIX - MULTIPLY ( A , B ) // Assuming that both A and B are nxn matrices if n == 1 then return A * B else // partition A , B , and C as shown before C [ 1 , 1 ] = MATRIX - MULTIPLY ( A [ 1 , 1 ], B [ 1 , 1 ]) + MATRIX - MULTIPLY ( A [ 1 , 2 ], B [ 2 , 1 ]); C [ 1 , 2 ] = MATRIX - MULTIPLY ( A [ 1 , 1 ], B [ 1 , 2 ]) + MATRIX - MULTIPLY ( A [ 1 , 2 ], B [ 2 , 2 ]); C [ 2 , 1 ] = MATRIX - MULTIPLY ( A [ 2 , 1 ], B [ 1 , 1 ]) + MATRIX - MULTIPLY ( A [ 2 , 2 ], B [ 2 , 1 ]); C [ 2 , 2 ] = MATRIX - MULTIPLY ( A [ 2 , 1 ], B [ 1 , 2 ]) + MATRIX - MULTIPLY ( A [ 2 , 2 ], B [ 2 , 2 ]); endif return C","title":"Matrix Multiplication: Divide &amp; Conquer (3)"},{"location":"week-3/ce100-week-3-matrix/#matrix-multiplication-divide-conquer-analysis","text":"\\(T(n) = 8T(n/2) + \\Theta(n^2)\\) \\(8\\) recursive calls \\(\\Longrightarrow 8T(\\cdots)\\) each problem has size \\(n/2\\) \\(\\Longrightarrow \\cdots T(n/2)\\) Submatrix addition \\(\\Longrightarrow \\Theta(n^2)\\)","title":"Matrix Multiplication: Divide &amp; Conquer Analysis"},{"location":"week-3/ce100-week-3-matrix/#matrix-multiplication-solving-the-recurrence","text":"\\(T(n) = 8T(n/2) + \\Theta(n^2)\\) \\(a=8\\) , \\(b=2\\) \\(f(n)=\\Theta(n^2)\\) \\(n^{log_b^a}=n^3\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(n^{log_b^a})\\) Similar with ordinary (iterative) algorithm.","title":"Matrix Multiplication: Solving the Recurrence"},{"location":"week-3/ce100-week-3-matrix/#matrix-multiplication-strassens-idea-1","text":"Compute \\(c_{11},c_{12},c_{21},c_{22}\\) using \\(7\\) recursive multiplications. In normal case we need \\(8\\) as below. \\[ \\begin{bmatrix} c_{11} & c_{12} \\\\ c_{21} & c_{22} \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} \\cdot \\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{bmatrix} \\] \\[ \\text{8 mults and 4 adds of (n/2)*(n/2) submatrices}= \\begin{cases} c_{11}=a_{11}b_{11}+a_{12}b_{21} \\\\ c_{21}=a_{21}b_{11}+a_{22}b_{21} \\\\ c_{12}=a_{11}b_{12}+a_{12}b_{22} \\\\ c_{22}=a_{21}b_{12}+a_{22}b_{22} \\end{cases} \\] section{ font-size: 25px; }","title":"Matrix Multiplication: Strassen\u2019s Idea (1)"},{"location":"week-3/ce100-week-3-matrix/#matrix-multiplication-strassens-idea-2","text":"Reminder: Each submatrix is of size \\((n/2)*(n/2)\\) Each add/sub operation takes \\(\\Theta(n^2)\\) time Compute \\(P1 \\dots P7\\) using \\(7\\) recursive calls to matrix-multiply \\[ \\begin{align*} P_1 & = a_{11} * (b_{12} - b_{22} ) \\\\ P_2 & = (a_{11} + a_{12} ) * b_{22} \\\\ P_3 & = (a_{21} + a_{22} ) * b_{11} \\\\ P_4 & = a_{22} * (b_{21} - b_{11} ) \\\\ P_5 & = (a_{11} + a_{22} ) * (b_{11} + b_{22} ) \\\\ P_6 & = (a_{12} - a_{22} ) * (b_{21} + b_{22} ) \\\\ P_7 & = ( a_{11} - a_{21} ) * (b_{11} + b_{12} ) \\end{align*} \\] section{ font-size: 25px; }","title":"Matrix Multiplication: Strassen\u2019s Idea (2)"},{"location":"week-3/ce100-week-3-matrix/#matrix-multiplication-strassens-idea-3","text":"\\[ \\begin{align*} P_1 &= a_{11} * (b_{12} - b_{22} ) \\\\ P_2 &= (a_{11} + a_{12} ) * b_{22} \\\\ P_3 &= (a_{21} + a_{22} ) * b_{11} \\\\ P_4 &= a_{22} * (b_{21} - b_{11} ) \\\\ P_5 &= (a_{11} + a_{22} ) * (b_{11} + b_{22} ) \\\\ P_6 &= (a_{12} - a_{22} ) * (b_{21} + b_{22} ) \\\\ P_7 &= ( a_{11} - a_{21} ) * (b_{11} + b_{12} ) \\end{align*} \\] How to compute \\(c_{ij}\\) using \\(P1 \\dots P7\\) ? \\[ \\begin{align*} c_{11} & = P_5 + P_4 \u2013 P_2 + P_6 \\\\ c_{12} & = P_1 + P_2 \\\\ c_{21} & = P_3 + P_4 \\\\ c_{22} & = P_5 + P_1 \u2013 P_3 \u2013 P_7 \\end{align*} \\]","title":"Matrix Multiplication: Strassen\u2019s Idea (3)"},{"location":"week-3/ce100-week-3-matrix/#matrix-multiplication-strassens-idea-4","text":"\\(7\\) recursive multiply calls \\(18\\) add/sub operations","title":"Matrix Multiplication: Strassen\u2019s Idea (4)"},{"location":"week-3/ce100-week-3-matrix/#matrix-multiplication-strassens-idea-5","text":"e.g. Show that \\(c_{12} = P_1+P_2\\) : \\[ \\begin{align*} c_{12} & = P_1 + P_2 \\\\ &= a_{11}(b_{12}\u2013b_{22})+(a_{11}+a_{12})b_{22} \\\\ &= a_{11}b_{12}-a_{11}b_{22}+a_{11}b_{22}+a_{12}b_{22} \\\\ &= a_{11}b_{12}+a_{12}b_{22} \\end{align*} \\]","title":"Matrix Multiplication: Strassen\u2019s Idea (5)"},{"location":"week-3/ce100-week-3-matrix/#strassens-algorithm","text":"Divide: Partition \\(A\\) and \\(B\\) into \\((n/2)*(n/2)\\) submatrices. Form terms to be multiplied using \\(+\\) and \\(-\\) . Conquer: Perform \\(7\\) multiplications of \\((n/2)*(n/2)\\) submatrices recursively. Combine: Form \\(C\\) using \\(+\\) and \\(\u2013\\) on \\((n/2)*(n/2)\\) submatrices. Recurrence: \\(T(n) = 7T(n/2) + \\Theta(n^2)\\)","title":"Strassen\u2019s Algorithm"},{"location":"week-3/ce100-week-3-matrix/#strassens-algorithm-solving-the-recurrence-1","text":"\\(T(n) = 7T(n/2) + \\Theta(n^2)\\) \\(a=7\\) , \\(b=2\\) \\(f(n)=\\Theta(n^2)\\) \\(n^{log_b^a}=n^{lg7}\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(n^{log_b^a})\\) \\(T(n)=\\Theta(n^{log_2^7})\\) \\(2^3 = 8, 2^2=4\\) so \\(\\Longrightarrow log_2^7 \\approx 2.81\\) or use https://www.omnicalculator.com/math/log","title":"Strassen\u2019s Algorithm: Solving the Recurrence (1)"},{"location":"week-3/ce100-week-3-matrix/#strassens-algorithm-solving-the-recurrence-2","text":"The number \\(2.81\\) may not seem much smaller than \\(3\\) But, it is significant because the difference is in the exponent. Strassen\u2019s algorithm beats the ordinary algorithm on today\u2019s machines for \\(n \\geq 30\\) or so. Best to date: \\(\\Theta(n^{2.376 \\dots})\\) (of theoretical interest only)","title":"Strassen\u2019s Algorithm: Solving the Recurrence (2)"},{"location":"week-3/ce100-week-3-matrix/#maximum-subarray-problem","text":"Input: An array of values Output: The contiguous subarray that has the largest sum of elements Input array: \\([13][-3][-25][20][-3][-16][-23]\\overbrace{[18][20][-7][12]}^{\\textrm{max. contiguous subarray}}[-22][-4][7]\\)","title":"Maximum Subarray Problem"},{"location":"week-3/ce100-week-3-matrix/#maximum-subarray-problem-divide-conquer-1","text":"Basic idea: Divide the input array into 2 from the middle Pick the best solution among the following: The max subarray of the left half The max subarray of the right half The max subarray crossing the mid-point","title":"Maximum Subarray Problem: Divide &amp; Conquer (1)"},{"location":"week-3/ce100-week-3-matrix/#maximum-subarray-problem-divide-conquer-2","text":"","title":"Maximum Subarray Problem: Divide &amp; Conquer (2)"},{"location":"week-3/ce100-week-3-matrix/#maximum-subarray-problem-divide-conquer-3","text":"Divide: Trivial (divide the array from the middle) Conquer: Recursively compute the max subarrays of the left and right halves Combine: Compute the max-subarray crossing the \\(mid-point\\) (can be done in \\(\\Theta(n)\\) time). Return the max among the following: the max subarray of the \\(\\text{left-subarray}\\) the max subarray of the \\(\\text{rightsubarray}\\) the max subarray crossing the \\(\\text{mid-point}\\) TODO : detailed solution in textbook...","title":"Maximum Subarray Problem: Divide &amp; Conquer (3)"},{"location":"week-3/ce100-week-3-matrix/#conclusion-divide-conquer","text":"Divide and conquer is just one of several powerful techniques for algorithm design. Divide-and-conquer algorithms can be analyzed using recurrences and the master method (so practice this math). Can lead to more efficient algorithms","title":"Conclusion : Divide &amp; Conquer"},{"location":"week-3/ce100-week-3-matrix/#quicksort-1","text":"One of the most-used algorithms in practice Proposed by C.A.R. Hoare in 1962. Divide-and-conquer algorithm In-place algorithm The additional space needed is O(1) The sorted array is returned in the input array Reminder: Insertion-sort is also an in-place algorithm, but Merge-Sort is not in-place. Very practical","title":"Quicksort (1)"},{"location":"week-3/ce100-week-3-matrix/#quicksort-2","text":"Divide: Partition the array into 2 subarrays such that elements in the lower part \\(\\leq\\) elements in the higher part Conquer: Recursively sort 2 subarrays Combine: Trivial (because in-place) Key: Linear-time \\((\\Theta(n))\\) partitioning algorithm section{ font-size: 25px; }","title":"Quicksort (2)"},{"location":"week-3/ce100-week-3-matrix/#divide-partition-the-array-around-a-pivot-element","text":"Choose a pivot element \\(x\\) Rearrange the array such that: Left subarray: All elements \\(\\leq x\\) Right subarray: All elements \\(\\geq x\\)","title":"Divide: Partition the array around a pivot element"},{"location":"week-3/ce100-week-3-matrix/#conquer-recursively-sort-the-subarrays","text":"Note: Everything in the left subarray \u2264 everything in the right subarray Note: Combine is trivial after conquer. Array already sorted. section{ font-size: 25px; }","title":"Conquer: Recursively Sort the Subarrays"},{"location":"week-3/ce100-week-3-matrix/#two-partitioning-algorithms","text":"Hoare\u2019s algorithm: Partitions around the first element of subarray \\((pivot = x = A[p])\\) Lomuto\u2019s algorithm: Partitions around the last element of subarray \\((pivot = x = A[r])\\)","title":"Two partitioning algorithms"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-1","text":"Choose a pivot element: \\(pivot = x = A[p]\\) Grow two regions: from left to right: \\(A[p \\dots i]\\) from right to left: \\(A[j \\dots r]\\) such that: every element in \\(A[p \\dots i] \\leq\\) pivot every element in \\(A[p \\dots i] \\geq\\) pivot","title":"Hoare\u2019s Partitioning Algorithm (1)"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-2","text":"section{ font-size: 25px; }","title":"Hoare\u2019s Partitioning Algorithm (2)"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-3","text":"Elements are exchanged when \\(A[i]\\) is too large to belong to the left region \\(A[j]\\) is too small to belong to the right region assuming that the inequality is strict The two regions \\(A[p \\dots i]\\) and \\(A[j \\dots r]\\) grow until \\(A[i] \\geq pivot \\geq A[j]\\) H - PARTITION ( A , p , r ) pivot = A [ p ] i = p - 1 j = r - 1 while true do repeat j = j - 1 until A [ j ] <= pivot repeat i = i - 1 until A [ i ] <= pivot if i < j then exchange A [ i ] with A [ j ] else return j","title":"Hoare\u2019s Partitioning Algorithm (3)"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-1","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-1)"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-2","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-2)"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-3","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-3)"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-4","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-4)"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-5","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-5)"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-6","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-6)"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-7","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-7)"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-8","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-8)"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-9","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-9)"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-10","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-10)"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-11","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-11)"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-12","text":"section{ font-size: 25px; }","title":"Hoare\u2019s Partitioning Algorithm Example (Step-12)"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-notes","text":"Elements are exchanged when \\(A[i]\\) is too large to belong to the left region \\(A[j]\\) is too small to belong to the right region assuming that the inequality is strict The two regions \\(A[p \\dots i]\\) and \\(A[j \\dots r]\\) grow until \\(A[i] \\geq pivot \\geq A[j]\\) The asymptotic runtime of Hoare\u2019s partitioning algorithm \\(\\Theta(n)\\) H - PARTITION ( A , p , r ) pivot = A [ p ] i = p - 1 j = r - 1 while true do repeat j = j - 1 until A [ j ] <= pivot repeat i = i - 1 until A [ i ] <= pivot if i < j then exchange A [ i ] with A [ j ] else return j section{ font-size: 25px; }","title":"Hoare\u2019s Partitioning Algorithm - Notes"},{"location":"week-3/ce100-week-3-matrix/#quicksort-with-hoares-partitioning-algorithm","text":"QUICKSORT ( A , p , r ) if p < r then q = H - PARTITION ( A , p , r ) QUICKSORT ( A , p , q ) QUICKSORT ( A , q + 1 , r ) endif Initial invocation: QUICKSORT(A,1,n) section{ font-size: 25px; }","title":"Quicksort with Hoare\u2019s Partitioning Algorithm"},{"location":"week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-pivot-selection","text":"if we select pivot to be \\(A[r]\\) instead of \\(A[p]\\) in H-PARTITION Consider the example where \\(A[r]\\) is the largest element in the array: End of H-PARTITION: \\(i = j = r\\) In QUICKSORT: \\(q = r\\) So, recursive call to: QUICKSORT(A, p, q=r) infinite loop","title":"Hoare\u2019s Partitioning Algorithm: Pivot Selection"},{"location":"week-3/ce100-week-3-matrix/#correctness-of-hoares-algorithm-1","text":"We need to prove \\(3\\) claims to show correctness: Indices \\(i\\) and \\(j\\) never reference \\(A\\) outside the interval \\(A[p \\dots r]\\) Split is always non-trivial; i.e., \\(j \\neq r\\) at termination Every element in \\(A[p \\dots j] \\leq\\) every element in \\(A[j+1 \\dots r]\\) at termination","title":"Correctness of Hoare\u2019s Algorithm (1)"},{"location":"week-3/ce100-week-3-matrix/#correctness-of-hoares-algorithm-2","text":"Notations: \\(k\\) : \\(\\#\\) of times the while-loop iterates until termination \\(i_m\\) : the value of index i at the end of iteration \\(m\\) \\(j_m\\) : the value of index j at the end of iteration \\(m\\) \\(x\\) : the value of the pivot element Note : We always have \\(i_1= p\\) and \\(p \\leq j_1 \\leq r\\) because \\(x = A[p]\\)","title":"Correctness of Hoare\u2019s Algorithm (2)"},{"location":"week-3/ce100-week-3-matrix/#correctness-of-hoares-algorithm-3","text":"Lemma 1: Either \\(i_k = j_k\\) or \\(i_k = j_k+1\\) at termination Proof of Lemma 1: The algorithm terminates when \\(i \\geq j\\) (the else condition). So, it is sufficient to prove that \\(i_k \u2013 j_k \\leq 1\\) There are \\(2\\) cases to consider: Case 1: \\(k = 1\\) , i.e. the algorithm terminates in a single iteration Case 2: \\(k > 1\\) , i.e. the alg. does not terminate in a single iter. By contradiction , assume there is a run with \\(i_k \u2013 j_k > 1\\) section{ font-size: 25px; }","title":"Correctness of Hoare\u2019s Algorithm (3)"},{"location":"week-3/ce100-week-3-matrix/#correctness-of-hoares-algorithm-4","text":"Original correctness claims: Indices \\(i\\) and \\(j\\) never reference A outside the interval \\(A[p \\dots r]\\) Split is always non-trivial; i.e., \\(j \\neq r\\) at termination Proof: For \\(k = 1\\) : Trivial because \\(i_1 = j_1 = p\\) ( see Case 1 in proof of Lemma 2 ) For \\(k > 1\\) : \\(i_k > p\\) and \\(j_k < r\\) ( due to the repeat-until loops moving indices ) \\(i_k \\leq r\\) and \\(j_k \\geq p\\) ( due to Lemma 1 and the statement above ) The proof of claims (a) and (b) complete section{ font-size: 25px; }","title":"Correctness of Hoare\u2019s Algorithm (4)"},{"location":"week-3/ce100-week-3-matrix/#correctness-of-hoares-algorithm-5","text":"Lemma 2: At the end of iteration \\(m\\) , where \\(m<k\\) ( i.e. m is not the last iteration ), we must have: \\(A[p \\dots i_m] \\leq x\\) and \\(A[j_m \\dots r] \\geq x\\) Proof of Lemma 2: Base case: \\(m=1\\) and \\(k>1\\) ( i.e. the alg. does not terminate in the first iter. ) Ind. Hyp.: At the end of iteration \\(m-1\\) , where \\(m<k\\) ( i.e. m is not the last iteration ), we must have: \\(A[p \\dots i_m-1] \\leq x\\) and \\(A[j_m-1 \\dots r] \\geq x\\) General case: The lemma holds for \\(m\\) , where \\(m < k\\) Proof of base case complete!","title":"Correctness of Hoare\u2019s Algorithm (5)"},{"location":"week-3/ce100-week-3-matrix/#correctness-of-hoares-algorithm-6","text":"Original correctness claim: \u00a9 Every element in \\(A[ \\dots j] \\leq\\) every element in \\(A[j+ \\dots r]\\) at termination Proof of claim \u00a9 There are \\(3\\) cases to consider: Case 1: \\(k=1\\) , i.e. the algorithm terminates in a single iteration Case 2: \\(k>1\\) and \\(i_k = j_k\\) Case 3: \\(k>1\\) and \\(i_k = j_k + 1\\)","title":"Correctness of Hoare\u2019s Algorithm (6)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-1","text":"Choose a pivot element: \\(pivot = x = A[r]\\) Grow two regions: from left to right: \\(A[p \\dots i]\\) from left to right: \\(A[i+1 \\dots j]\\) such that: every element in \\(A[p \\dots i] \\leq pivot\\) every element in \\(A[i+1 \\dots j] > pivot\\)","title":"Lomuto\u2019s Partitioning Algorithm (1)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-2","text":"","title":"Lomuto\u2019s Partitioning Algorithm (2)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-1","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-1)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-2","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-2)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-3","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-3)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-4","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-4)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-5","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-5)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-6","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-6)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-7","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-7)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-8","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-8)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-9","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-9)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-10","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-10)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-11","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-11)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-12","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-12)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-13","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-13)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-14","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-14)"},{"location":"week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-15","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-15)"},{"location":"week-3/ce100-week-3-matrix/#quicksort-with-lomutos-partitioning-algorithm","text":"QUICKSORT ( A , p , r ) if p < r then q = L - PARTITION ( A , p , r ) QUICKSORT ( A , p , q - 1 ) QUICKSORT ( A , q + 1 , r ) endif Initial invocation: QUICKSORT(A,1,n)","title":"Quicksort with Lomuto\u2019s Partitioning Algorithm"},{"location":"week-3/ce100-week-3-matrix/#comparison-of-hoares-lomutos-algorithms-1","text":"Notation: \\(n=r-p+1\\) \\(pivot=A[p]\\) ( Hoare ) \\(pivot=A[r]\\) ( Lomuto ) \\(\\#\\) of element exchanges: \\(e(n)\\) Hoare: \\(0 \\geq e(n) \\geq \\lfloor \\frac{n}{2} \\rfloor\\) Best : \\(k=1\\) with \\(i_1=j_1=p\\) (i.e., \\(A[p+1 \\dots r]>pivot\\) ) Worst : \\(A[p+1 \\dots p+ \\lfloor \\frac{n}{2} \\rfloor - 1] \\geq pivot \\geq A[p+ \\lceil \\frac{n}{2} \\rceil \\dots r]\\) Lomuto : \\(1 \\leq e(n) \\leq n\\) Best : \\(A[p \\dots r -1]>pivot\\) Worst : \\(A[p \\dots r-1] \\leq pivot\\)","title":"Comparison of Hoare\u2019s &amp; Lomuto\u2019s Algorithms (1)"},{"location":"week-3/ce100-week-3-matrix/#comparison-of-hoares-lomutos-algorithms-2","text":"\\(\\#\\) of element comparisons: \\(c_e(n)\\) Hoare : \\(n+1 \\leq c_e(n) \\leq n+2\\) Best : \\(i_k=j_k\\) Worst : \\(i_k=j_k+1\\) Lomuto : \\(c_e(n)=n-1\\) \\(\\#\\) of index comparisons: \\(c_i(n)\\) Hoare : \\(1 \\leq c_i(n) \\leq \\lfloor \\frac{n}{2} \\rfloor + 1 | (c_i(n)=e(n)+1)\\) Lomuto : \\(c_i(n)=n-1\\)","title":"Comparison of Hoare\u2019s &amp; Lomuto\u2019s Algorithms (2)"},{"location":"week-3/ce100-week-3-matrix/#comparison-of-hoares-lomutos-algorithms-3","text":"\\(\\#\\) of index increment/decrement operations: \\(a(n)\\) Hoare : \\(n+1 \\leq a(n) \\leq n+2 | (a(n)=c_e(n))\\) Lomuto : \\(n \\leq a(n) \\leq 2n-1 | (a(n)=e(n)+(n-1))\\) Hoare\u2019s algorithm is in general faster Hoare behaves better when pivot is repeated in \\(A[p \\dots r]\\) Hoare : Evenly distributes them between left & right regions Lomuto : Puts all of them to the left region","title":"Comparison of Hoare\u2019s &amp; Lomuto\u2019s Algorithms (3)"},{"location":"week-3/ce100-week-3-matrix/#analysis-of-quicksort-1","text":"QUICKSORT ( A , p , r ) if p < r then q = H - PARTITION ( A , p , r ) QUICKSORT ( A , p , q ) QUICKSORT ( A , q + 1 , r ) endif Initial invocation: QUICKSORT(A,1,n) Assume all elements are distinct in the following analysis","title":"Analysis of Quicksort (1)"},{"location":"week-3/ce100-week-3-matrix/#analysis-of-quicksort-2","text":"H-PARTITION always chooses \\(A[p]\\) (the first element) as the pivot. The runtime of QUICKSORT on an already-sorted array is \\(\\Theta(n^2)\\)","title":"Analysis of Quicksort (2)"},{"location":"week-3/ce100-week-3-matrix/#example-an-already-sorted-array","text":"Partitioning always leads to \\(2\\) parts of size \\(1\\) and \\(n-1\\)","title":"Example: An Already Sorted Array"},{"location":"week-3/ce100-week-3-matrix/#worst-case-analysis-of-quicksort","text":"Worst case is when the PARTITION algorithm always returns imbalanced partitions (of size \\(1\\) and \\(n-1\\) ) in every recursive call. This happens when the pivot is selected to be either the min or max element. This happens for H-PARTITION when the input array is already sorted or reverse sorted \\[ \\begin{align*} T(n) &= T(1) + T(n-1) + \u0398(n) \\\\ &= T(n-1) + \u0398(n) \\\\ &= \u0398(n2) \\end{align*} \\]","title":"Worst Case Analysis of Quicksort"},{"location":"week-3/ce100-week-3-matrix/#worst-case-recursion-tree","text":"\\[ T(n) = T(1) + T(n-1) + cn \\]","title":"Worst Case Recursion Tree"},{"location":"week-3/ce100-week-3-matrix/#best-case-analysis-for-intuition-only","text":"If we\u2019re extremely lucky, H-PARTITION splits the array evenly at every recursive call \\[ \\begin{align*} T(n) &= 2T(n/2) + \\Theta(n) \\\\ &= \\Theta(nlgn) \\end{align*} \\] (same as merge sort) Instead of splitting \\(0.5:0.5\\) , if we split \\(0.1:0.9\\) then we need solve following equation. \\[ \\begin{align*} T(n) &= T(n/10) + T(9n/10) + \\Theta(n) \\\\ &= \\Theta(nlgn) \\end{align*} \\]","title":"Best Case Analysis (for intuition only)"},{"location":"week-3/ce100-week-3-matrix/#almost-best-case-analysis","text":"","title":"\u201cAlmost-Best\u201d Case Analysis"},{"location":"week-3/ce100-week-3-matrix/#balanced-partitioning-1","text":"We have seen that if H-PARTITION always splits the array with \\(0.1-to-0.9\\) ratio, the runtime will be \\(\\Theta(nlgn)\\) . Same is true with a split ratio of \\(0.01-to-0.99\\) , etc. Possible to show that if the split has always constant \\((\\Theta(1))\\) proportionality, then the runtime will be \\(\\Theta(nlgn)\\) . In other words, for a constant \\(\\alpha | (0 < \\alpha \u2264 0.5)\\) : \\(\\alpha\u2013to\u2013(1-\\alpha)\\) proportional split yields \\(\\Theta(nlgn)\\) total runtime","title":"Balanced Partitioning (1)"},{"location":"week-3/ce100-week-3-matrix/#balanced-partitioning-2","text":"In the rest of the analysis, assume that all input permutations are equally likely. This is only to gain some intuition We cannot make this assumption for average case analysis We will revisit this assumption later Also, assume that all input elements are distinct.","title":"Balanced Partitioning (2)"},{"location":"week-3/ce100-week-3-matrix/#balanced-partitioning-3","text":"Question: What is the probability that H-PARTITION returns a split that is more balanced than \\(0.1-to-0.9\\) ?","title":"Balanced Partitioning (3)"},{"location":"week-3/ce100-week-3-matrix/#balanced-partitioning-4","text":"Reminder: H-PARTITION will place the pivot in the right partition unless the pivot is the smallest element in the arrays. Question: If the pivot selected is the mth smallest value \\((1 < m \u2264 n)\\) in the input array, what is the size of the left region after partitioning?","title":"Balanced Partitioning (4)"},{"location":"week-3/ce100-week-3-matrix/#balanced-partitioning-5","text":"Question: What is the probability that the pivot selected is the \\(m^{th}\\) smallest value in the array of size \\(n\\) ? \\(1/n\\) ( since all input permutations are equally likely ) Question: What is the probability that the left partition returned by H-PARTITION has size \\(m\\) , where \\(1<m<n\\) ? \\(1/n\\) ( due to the answers to the previous 2 questions )","title":"Balanced Partitioning (5)"},{"location":"week-3/ce100-week-3-matrix/#balanced-partitioning-6","text":"section{ font-size: 25px; } Question: What is the probability that H-PARTITION returns a split that is more balanced than \\(0.1-to-0.9\\) ? \\[ \\begin{align*} Probability &=\\sum \\limits_{q=0.1n+1}^{0.9n-1}\\frac{1}{n} \\\\ &=\\frac{1}{n}(0.9n-1-0.1n-1+1) \\\\ &= 0.8-\\frac{1}{n} \\\\ & \\approx 0.8 \\text{ for large n} \\end{align*} \\]","title":"Balanced Partitioning (6)"},{"location":"week-3/ce100-week-3-matrix/#balanced-partitioning-7","text":"The probability that H-PARTITION yields a split that is more balanced than \\(0.1-to-0.9\\) is \\(80\\%\\) on a random array. Let \\(P_{\\alpha>}\\) be the probability that H-PARTITION yields a split more balanced than \\(\\alpha-to-(1-\\alpha)\\) , where \\(0 < \\alpha \\leq 0.5\\) Repeat the analysis to generalize the previous result section{ font-size: 25px; }","title":"Balanced Partitioning (7)"},{"location":"week-3/ce100-week-3-matrix/#balanced-partitioning-8","text":"Question: What is the probability that H-PARTITION returns a split that is more balanced than \\(\\alpha-to-(1-\\alpha)\\) ? \\[ \\begin{align*} Probability & =\\sum \\limits_{q=\\alpha n+1}^{(1-\\alpha)n-1}\\frac{1}{n} \\\\ & =\\frac{1}{n}((1-\\alpha)n-1- \\alpha n-1+1) \\\\ & = (1-2\\alpha)-\\frac{1}{n} \\\\ & \\approx (1-2\\alpha) \\text{ for large n} \\end{align*} \\]","title":"Balanced Partitioning (8)"},{"location":"week-3/ce100-week-3-matrix/#balanced-partitioning-9","text":"We found \\(P_{\\alpha >}=1-2\\alpha\\) Ex: \\(P_{0.1>}=0.8\\) and \\(P_{0.01>}=0.98\\) Hence, H-PARTITION produces a split more balanced than a \\(0.1-to-0.9\\) split \\(80\\%\\) of the time \\(0.01-to-0.99\\) split \\(98\\%\\) of the time less balanced than a \\(0.1-to-0.9\\) split \\(20\\%\\) of the time \\(0.01-to-0.99\\) split \\(2\\%\\) of the time","title":"Balanced Partitioning (9)"},{"location":"week-3/ce100-week-3-matrix/#intuition-for-the-average-case-1","text":"Assumption: All permutations are equally likely Only for intuition; we\u2019ll revisit this assumption later Unlikely: Splits always the same way at every level Expectation: Some splits will be reasonably balanced Some splits will be fairly unbalanced Average case: A mix of good and bad splits Good and bad splits distributed randomly thru the tree","title":"Intuition for the Average Case (1)"},{"location":"week-3/ce100-week-3-matrix/#intuition-for-the-average-case-2","text":"Assume for intuition: Good and bad splits occur in the alternate levels of the tree Good split: Best case split Bad split: Worst case split","title":"Intuition for the Average Case (2)"},{"location":"week-3/ce100-week-3-matrix/#intuition-for-the-average-case-3","text":"Compare 2-successive levels of avg case vs. 1 level of best case","title":"Intuition for the Average Case (3)"},{"location":"week-3/ce100-week-3-matrix/#intuition-for-the-average-case-4","text":"In terms of the remaining subproblems, two levels of avg case is slightly better than the single level of the best case The avg case has extra divide cost of \\(\\Theta(n)\\) at alternate levels The extra divide cost \\(\\Theta(n)\\) of bad splits absorbed into the \\(\\Theta(n)\\) of good splits. Running time is still \\(\\Theta(nlgn)\\) But, slightly larger hidden constants, because the height of the recursion tree is about twice of that of best case.","title":"Intuition for the Average Case (4)"},{"location":"week-3/ce100-week-3-matrix/#intuition-for-the-average-case-5","text":"Another way of looking at it: Suppose we alternate lucky, unlucky, lucky, unlucky, \\(\\dots\\) We can write the recurrence as: \\(L(n) = 2U(n/2) + \\Theta(n)\\) lucky split (best) \\(U(n) = L(n-1) + \\Theta(n)\\) unlucky split (worst) Solving: $$ \\begin{align*} L(n) & = 2(L(n/2-1) + \\Theta(n/2)) + \\Theta(n) \\ & = 2L(n/2-1) + \\Theta(n) \\ & = \u0398(nlgn) \\end{align*} $$ How can we make sure we are usually lucky for all inputs?","title":"Intuition for the Average Case (5)"},{"location":"week-3/ce100-week-3-matrix/#summary-quicksort-runtime-analysis-1","text":"Worst case: Unbalanced split at every recursive call \\[ \\begin{align*} T(n) & = T(1) + T(n-1) + \\Theta(n) \\\\ T(n) & = \\Theta(n2) \\end{align*} \\] Best case: Balanced split at every recursive call ( extremely lucky ) \\[ \\begin{align*} T(n) & = 2T(n/2) + \\Theta(n) \\\\ T(n) & = \\Theta(nlgn) \\end{align*} \\]","title":"Summary: Quicksort Runtime Analysis (1)"},{"location":"week-3/ce100-week-3-matrix/#summary-quicksort-runtime-analysis-2","text":"Almost-best case: Almost-balanced split at every recursive call \\[ \\begin{align*} T(n) &=T(n/10)+T(9n/10)+ \\Theta(n) \\\\ \\text{or } T(n) &= T(n/100) + T(99n/100) + \u0398(n) \\\\ \\text{or } T(n) &= T(\\alpha n) + T((1-\\alpha n)+ \\Theta(n) \\end{align*} \\] for any constant \\(\\alpha, 0 < \\alpha \\leq 0.5\\)","title":"Summary: Quicksort Runtime Analysis (2)"},{"location":"week-3/ce100-week-3-matrix/#summary-quicksort-runtime-analysis-3","text":"For a random input array, the probability of having a split more balanced than \\(0.1 \u2013 to \u2013 0.9 : 80\\%\\) more balanced than \\(0.01 \u2013 to \u2013 0.99 : 98\\%\\) more balanced than \\(\\alpha \u2013 to \u2013 (1-\\alpha) : 1 \u2013 2 \\alpha\\) for any constant \\(\\alpha, 0 < \\alpha \\leq 0.5\\)","title":"Summary: Quicksort Runtime Analysis (3)"},{"location":"week-3/ce100-week-3-matrix/#summary-quicksort-runtime-analysis-4","text":"Avg case intuition: Different splits expected at different levels some balanced (good), some unbalanced (bad) Avg case intuition: Assume the good and bad splits alternate i.e. good split -> bad split -> good split -> \u2026 \\(T(n) = \\Theta(nlgn)\\) (informal analysis for intuition)","title":"Summary: Quicksort Runtime Analysis (4)"},{"location":"week-3/ce100-week-3-matrix/#randomized-quicksort","text":"In the avg-case analysis, we assumed that all permutations of the input array are equally likely. But, this assumption does not always hold e.g. What if all the input arrays are reverse sorted ? Always worst-case behavior Ideally, the avg-case runtime should be independent of the input permutation . Randomness should be within the algorithm , not based on the distribution of the inputs. i.e. The avg case should hold for all possible inputs","title":"Randomized Quicksort"},{"location":"week-3/ce100-week-3-matrix/#randomized-algorithms-1","text":"Alternative to assuming a uniform distribution: Impose a uniform distribution e.g. Choose a random pivot rather than the first element Typically useful when: there are many ways that an algorithm can proceed but, it\u2019s difficult to determine a way that is always guaranteed to be good . If there are many good alternatives ; simply choose one randomly .","title":"Randomized Algorithms (1)"},{"location":"week-3/ce100-week-3-matrix/#randomized-algorithms-1_1","text":"Ideally: Runtime should be independent of the specific inputs No specific input should cause worst-case behavior Worst-case should be determined only by output of a random number generator. section{ font-size: 25px; }","title":"Randomized Algorithms (1)"},{"location":"week-3/ce100-week-3-matrix/#randomized-quicksort-1","text":"Using Hoare\u2019s partitioning algorithm: R - QUICKSORT ( A , p , r ) if p < r then q = R - PARTITION ( A , p , r ) R - QUICKSORT ( A , p , q ) R - QUICKSORT ( A , q +1 , r ) R - PARTITION ( A , p , r ) s = RANDOM ( p , r ) exchange A [ p ] with A [ s ] return H - PARTITION ( A , p , r ) Alternatively, permuting the whole array would also work but, would be more difficult to analyze section{ font-size: 25px; }","title":"Randomized Quicksort (1)"},{"location":"week-3/ce100-week-3-matrix/#randomized-quicksort-2","text":"Using Lomuto\u2019s partitioning algorithm: R - QUICKSORT ( A , p , r ) if p < r then q = R - PARTITION ( A , p , r ) R - QUICKSORT ( A , p , q -1 ) R - QUICKSORT ( A , q +1 , r ) R - PARTITION ( A , p , r ) s = RANDOM ( p , r ) exchange A [ r ] with A [ s ] return L - PARTITION ( A , p , r ) Alternatively, permuting the whole array would also work but, would be more difficult to analyze","title":"Randomized Quicksort (2)"},{"location":"week-3/ce100-week-3-matrix/#notations-for-formal-analysis","text":"Assume all elements in \\(A[p \\dots r]\\) are distinct Let \\(n = r \u2013 p + 1\\) Let \\(rank(x) = |{A[i]: p \\leq i \\leq r \\text{ and } A[i] \\leq x}|\\) i.e. \\(rank(x)\\) is the number of array elements with value less than or equal to \\(x\\) \\(A=\\{5,9,7,6,8,1,4\\}\\) \\(p=5,r=4\\) \\(rank(5)=3\\) i.e. it is the \\(3^{rd}\\) smallest element in the array","title":"Notations for Formal Analysis"},{"location":"week-3/ce100-week-3-matrix/#formal-analysis-for-average-case","text":"The following analysis will be for Quicksort using Hoare\u2019s partitioning algorithm. Reminder: The pivot is selected randomly and exchanged with \\(A[p]\\) before calling H-PARTITION Let \\(x\\) be the random pivot chosen. What is the probability that \\(rank(x) = i\\) for \\(i = 1, 2, \\dots n\\) ? \\(P(rank(x) = i) = 1/n\\) section{ font-size: 25px; }","title":"Formal Analysis for Average Case"},{"location":"week-3/ce100-week-3-matrix/#various-outcomes-of-h-partition-1","text":"Assume that \\(rank(x)=1\\) i.e. the random pivot chosen is the smallest element What will be the size of the left partition \\((|L|)\\) ? Reminder: Only the elements less than or equal to \\(x\\) will be in the left partition. \\(A=\\{\\overbrace{2}^{p=x=pivot}\\underbrace{,}_{\\Longrightarrow|L|=1 } 9,7,6,8,5,\\overbrace{4}^r\\}\\) \\(p=2,r=4\\) \\(pivot=x=2\\) TODO: convert to image...S6_P9 section{ font-size: 25px; }","title":"Various Outcomes of H-PARTITION (1)"},{"location":"week-3/ce100-week-3-matrix/#various-outcomes-of-h-partition-2","text":"Assume that \\(rank(x)>1\\) i.e. the random pivot chosen is not the smallest element What will be the size of the left partition \\((|L|)\\) ? Reminder: Only the elements less than or equal to \\(x\\) will be in the left partition. Reminder: The pivot will stay in the right region after H-PARTITION if \\(rank(x)>1\\) \\(A=\\{\\overbrace{2}^{p}, 4 \\underbrace{,}_{\\Longrightarrow|L|=rank(x)-1}7,6,8,\\overbrace{5,}^{pivot}\\overbrace{9}^r\\}\\) \\(p=2,r=4\\) \\(pivot=x=5\\) TODO: convert to image...S6_P10","title":"Various Outcomes of H-PARTITION (2)"},{"location":"week-3/ce100-week-3-matrix/#various-outcomes-of-h-partition-summary-1","text":"\\(x: pivot\\) \\(|L|: \\text{size of left region}\\) \\(P(rank(x) = i) = 1/n \\text{ for } 1 \\leq i \\leq n\\) \\(\\text{if } rank(x) = 1 \\text{ then } |L| = 1\\) \\(\\text{if } rank(x) > 1 \\text{ then } |L| = rank(x) - 1\\) \\(P(|L| = 1) = P(rank(x) = 1) + P(rank(x) = 2)\\) \\(P(|L| = 1) = 2/n\\) \\(P(|L| = i) = P(rank(x) = i+1) \\text{ for } 1< i < n\\) \\(P(|L| = i) = 1/n \\text{ for } 1< i < n\\)","title":"Various Outcomes of H-PARTITION - Summary (1)"},{"location":"week-3/ce100-week-3-matrix/#various-outcomes-of-h-partition-summary-2","text":"section{ font-size: 25px; }","title":"Various Outcomes of H-PARTITION - Summary (2)"},{"location":"week-3/ce100-week-3-matrix/#average-case-analysis-recurrence-1","text":"\\(x=pivot\\) \\[ \\begin{align*} T(n) & = \\frac{1}{n}(T(1)+t(n-1) ) & rank:1 \\\\ & + \\frac{1}{n}(T(1)+t(n-1) ) & rank:2 \\\\ & + \\frac{1}{n}(T(2)+t(n-2) ) & rank:3 \\\\ & \\vdots & \\vdots \\\\ & + \\frac{1}{n}(T(i)+t(n-i) ) & rank:i+1 \\\\ & \\vdots & \\vdots \\\\ & + \\frac{1}{n}(T(n-1)+t(1) ) & rank:n \\\\ & + \\Theta(n) \\end{align*} \\] section{ font-size: 25px; }","title":"Average - Case Analysis: Recurrence (1)"},{"location":"week-3/ce100-week-3-matrix/#average-case-analysis-recurrence-2","text":"\\[ \\begin{align*} T(n) &= \\frac{1}{n}\\sum \\limits_{q=1}^{n-1}(T(q)+T(n-q))+\\frac{1}{n}(T(1)+T(n-1))+\\Theta(n)\\\\ & \\text{Note: } \\frac{1}{n}(T(1)+T(n-1))=\\frac{1}{n}(\\Theta(1)+O(n^2))=O(n) \\\\ T(n) &= \\frac{1}{n}\\sum \\limits_{q=1}^{n-1}(T(q)+T(n-q))+\\Theta(n) \\end{align*} \\] for \\(k=1,2,\\dots,n-1\\) each term \\(T(k)\\) appears twice once for \\(q = k\\) and once for \\(q = n\u2212k\\) \\[ T(n) = \\frac{2}{n}\\sum \\limits_{k=1}^{n-1} T(k)+\\Theta(n) \\]","title":"Average - Case Analysis: Recurrence (2)"},{"location":"week-3/ce100-week-3-matrix/#average-case-analysis-solving-recurrence-substitution","text":"Guess: \\(T(n)=O(nlgn)\\) \\(T(k) \u2264 aklgk\\) for \\(k<n\\) , for some constant \\(a > 0\\) \\[ \\begin{align*} T(n) &= \\frac{2}{n} \\sum \\limits_{k=1}^{n-1} T(k)+\\Theta(n) \\\\ & \\leq \\frac{2}{n} \\sum \\limits_{k=1}^{n-1} aklgk+\\Theta(n) \\\\ & \\leq \\frac{2a}{n} \\sum \\limits_{k=1}^{n-1} klgk+\\Theta(n) \\end{align*} \\] Need a tight bound for \\(\\sum klgk\\)","title":"Average - Case Analysis -Solving Recurrence: Substitution"},{"location":"week-3/ce100-week-3-matrix/#tight-bound-for-sum-klgk-1","text":"Bounding the terms \\(\\ \\sum \\limits_{k=1}^{n-1}klgk \\leq \\sum \\limits_{k=1}^{n-1}nlgn = n(n-1)lgn \\leq n^2lgn\\) This bound is not strong enough because \\(T(n) \\leq \\frac{2a}{n}n^2lgn+\\Theta(n)\\) \\(=2anlgn+\\Theta(n)\\) \\(\\Longrightarrow\\) couldn\u2019t prove \\(T(n) \\leq anlgn\\)","title":"Tight bound for \\(\\sum klgk\\) (1)"},{"location":"week-3/ce100-week-3-matrix/#tight-bound-for-sum-klgk-2","text":"Splitting summations: ignore ceilings for simplicity $$ \\sum \\limits_{k=1}^{n-1}klgk \\leq \\sum \\limits_{k=1}^{n/2-1}klgk + \\sum \\limits_{k=n/2}^{n-1}klgk $$ First summation : \\(lgk < lg(n/2)=lgn-1\\) Second summation : \\(lgk < lgn\\) section{ font-size: 25px; }","title":"Tight bound for \\(\\sum klgk\\) (2)"},{"location":"week-3/ce100-week-3-matrix/#splitting-sum-limits_k1n-1klgk-leq-sum-limits_k1n2-1klgk-sum-limits_kn2n-1klgk-3","text":"\\[ \\begin{align*} & \\sum \\limits_{k=1}^{n-1}klgk \\leq (lg(n-1))\\sum \\limits_{k=1}^{n/2-1}k + lgn \\sum \\limits_{k=n/2}^{n-1}k \\\\ &= lgn \\sum \\limits_{k=1}^{n-1}k- \\sum \\limits_{k=1}^{n/2-1}k \\\\ &= \\frac{1}{2}n(n-1)lgn - \\frac{1}{2} \\frac{n}{2}(\\frac{n}{2}-1) \\\\ &= \\frac{1}{2}n^2lgn - \\frac{1}{8}n^2 - \\frac{1}{2}n(lgn-1/2) \\\\ \\end{align*} \\] \\[ \\begin{align*} & \\sum \\limits_{k=1}^{n-1} klgk \\leq \\frac{1}{2}n^2lgn-\\frac{1}{8}n^2 \\ for \\ lgn \\geq 1/2 \\Longrightarrow n \\geq \\sqrt{2} \\end{align*} \\]","title":"Splitting: \\(\\sum \\limits_{k=1}^{n-1}klgk \\leq \\sum \\limits_{k=1}^{n/2-1}klgk + \\sum \\limits_{k=n/2}^{n-1}klgk\\) (3)"},{"location":"week-3/ce100-week-3-matrix/#substituting-sum-limits_k1n-1klgk-leq-frac12n2lgn-frac18n2-4","text":"\\[ \\begin{align*} T(n) & \\leq \\frac{2a}{n}\\sum \\limits_{k=1}^{n-1}klgk+\\Theta(n)\\\\ & \\leq \\frac{2a}{n}(\\frac{1}{2}n^2lgn-\\frac{1}{8}n^2)+\\Theta(n) \\\\ & = anlgn - (\\frac{a}{4}n-\\Theta(n)) \\end{align*} \\] We can choose a large enough so that \\(\\frac{a}{4}n \\geq \\Theta(n)\\) \\[ \\begin{align*} T(n) & \\leq anlgn \\\\ T(n) & = O(nlgn) \\end{align*} \\] Q.E.D.","title":"Substituting: - \\(\\sum \\limits_{k=1}^{n-1}klgk \\leq \\frac{1}{2}n^2lgn-\\frac{1}{8}n^2\\) (4)"},{"location":"week-3/ce100-week-3-matrix/#medians-and-order-statistics","text":"ith order statistic : \\(i^{th}\\) smallest element of a set of \\(n\\) elements minimum: first order statistic maximum: \\(n^{th}\\) order statistic median: \u201chalfway point\u201d of the set \\[ \\begin{align*} i & = \\lfloor \\frac{(n+1)}{2} \\rfloor \\\\ \\text{ or } \\\\ i & = \\lceil \\frac{(n+1)}{2} \\rceil \\end{align*} \\]","title":"Medians and Order Statistics"},{"location":"week-3/ce100-week-3-matrix/#selection-problem","text":"Selection problem: Select the \\(i^{th}\\) smallest of \\(n\\) elements Na\u00efve algorithm: Sort the input array \\(A\\) ; then return \\(A[i]\\) \\(T(n) = \\theta(nlgn)\\) using e.g. merge sort (but not quicksort) Can we do any better?","title":"Selection Problem"},{"location":"week-3/ce100-week-3-matrix/#selection-in-expected-linear-time","text":"Randomized algorithm using divide and conquer Similar to randomized quicksort Like quicksort: Partitions input array recursively Unlike quicksort: Makes a single recursive call Reminder: Quicksort makes two recursive calls Expected runtime: \\(\\Theta(n)\\) Reminder: Expected runtime of quicksort: \\(\\Theta(nlgn)\\)","title":"Selection in Expected Linear Time"},{"location":"week-3/ce100-week-3-matrix/#selection-in-expected-linear-time-example-1","text":"Select the \\(2^{nd}\\) smallest element: \\[ \\begin{align*} A & = \\{6,10,13,5,8,3,2,11\\} \\\\ i & = 2 \\\\ \\end{align*} \\] Partition the input array: \\[ \\begin{align*} A & = \\{\\underbrace{2,3,5,}_{\\text{left subarray} }\\underbrace{13,8,10,6,11}_{\\text{right subarray}}\\} \\end{align*} \\] make a recursive call to select the \\(2^{nd}\\) smallest element in left subarray","title":"Selection in Expected Linear Time: Example 1"},{"location":"week-3/ce100-week-3-matrix/#selection-in-expected-linear-time-example-2","text":"Select the \\(7^{th}\\) smallest element: \\[ \\begin{align*} A & = \\{6,10,13,5,8,3,2,11\\} \\\\ i & = 7 \\\\ \\end{align*} \\] Partition the input array: \\[ \\begin{align*} A & = \\{\\underbrace{2,3,5,}_{\\text{left subarray} }\\underbrace{13,8,10,6,11}_{\\text{right subarray}}\\} \\end{align*} \\] make a recursive call to select the \\(4^{th}\\) smallest element in right subarray","title":"Selection in Expected Linear Time: Example 2"},{"location":"week-3/ce100-week-3-matrix/#selection-in-expected-linear-time-1","text":"R - SELECT ( A , p , r , i ) if p == r then return A [ p ]; q = R - PARTITION ( A , p , r ) k = q \u2013 p +1 ; if i <= k then return R - SELECT ( A , p , q , i ); else return R - SELECT ( A , q +1 , r , i - k ); \\[ \\begin{align*} A & = \\{ \\underbrace{ | }_{p} \\dots \\leq x \\text{(k smallest elements)} \\dots \\underbrace{ | }_{q} \\dots \\geq x \\dots \\underbrace{ | }_{r} \\} \\\\ x & = pivot \\end{align*} \\]","title":"Selection in Expected Linear Time (1)"},{"location":"week-3/ce100-week-3-matrix/#selection-in-expected-linear-time-2","text":"\\[ \\begin{align*} A & = \\{ \\overbrace{ | }^{p} \\underbrace{ \\dots \\leq x \\dots }_{L} \\overbrace{ | }^{q} \\underbrace{ \\dots \\geq x \\dots }_{R} \\overbrace{ | }^{r} \\} \\\\ x & = pivot \\end{align*} \\] All elements in \\(L \\leq\\) all elements in \\(R\\) \\(L\\) contains: \\(|L| = q\u2013p+1\\) \\(=\\) k smallest elements of \\(A[p...r]\\) if \\(i \\leq |L| = k\\) then search \\(L\\) recursively for its \\(i^{th}\\) smallest element else search \\(R\\) recursively for its \\((i-k)^{th}\\) smallest element","title":"Selection in Expected Linear Time (2)"},{"location":"week-3/ce100-week-3-matrix/#runtime-analysis-1","text":"Worst case: Imbalanced partitioning at every level and the recursive call always to the larger partition \\[ \\begin{align*} & = \\{1,\\underbrace{2,3,4,5,6,7,8}_{\\text{recursive call}} \\} & i & = 8 \\\\ & = \\{2,\\underbrace{3,4,5,6,7,8}_{\\text{recursive call}} \\} & i & = 7 \\end{align*} \\]","title":"Runtime Analysis (1)"},{"location":"week-3/ce100-week-3-matrix/#runtime-analysis-2","text":"Worst case: Worse than the na\u00efve method (based on sorting) \\[ \\begin{align*} T(n) &= T(n-1) + \\Theta(n) \\\\ T(n) &= \\Theta(n^2) \\end{align*} \\] Best case: Balanced partitioning at every recursive level \\[ \\begin{align*} T(n) &= T(n/2) + \\Theta(n) \\\\ T(n) &= \\Theta(n) \\end{align*} \\] Avg case: Expected runtime \u2013 need analysis T.B.D. section{ font-size: 25px; }","title":"Runtime Analysis (2)"},{"location":"week-3/ce100-week-3-matrix/#reminder-various-outcomes-of-h-partition","text":"\\(x: pivot\\) \\(|L|: \\text{size of left region}\\) \\(P(rank(x) = i) = 1/n \\text{ for } 1 \\leq i \\leq n\\) \\(\\text{if } rank(x) = 1 \\text{ then } |L| = 1\\) \\(\\text{if } rank(x) > 1 \\text{ then } |L| = rank(x) - 1\\) \\(P(|L| = 1) = P(rank(x) = 1) + P(rank(x) = 2)\\) \\(P(|L| = 1) = 2/n\\) \\(P(|L| = i) = P(rank(x) = i+1) \\text{ for } 1< i < n\\) \\(P(|L| = i) = 1/n \\text{ for } 1< i < n\\)","title":"Reminder: Various Outcomes of H-PARTITION"},{"location":"week-3/ce100-week-3-matrix/#average-case-analysis-of-randomized-select","text":"To compute the upper bound for the avg case , assume that the \\(i^{th}\\) element always falls into the larger partition . \\[ \\begin{align*} A & = \\{ \\overbrace{ | }^{p} \\underbrace{ \\dots \\leq x \\dots }_{Left Partition} \\overbrace{ | }^{q} \\underbrace{ \\dots \\geq x \\dots }_{Right Partition} \\overbrace{ | }^{r} \\} \\\\ x & = pivot \\end{align*} \\] We will analyze the case where the recursive call is always made to the larger partition This will give us an upper bound for the avg case","title":"Average Case Analysis of Randomized Select"},{"location":"week-3/ce100-week-3-matrix/#various-outcomes-of-h-partition","text":"","title":"Various Outcomes of H-PARTITION"},{"location":"week-3/ce100-week-3-matrix/#average-case-analysis-of-randomized-select-1","text":"\\[ \\text{Recall:} P(|L|=i) = \\begin{cases} 2/n & \\text{for } i=1 \\\\ 1/n & \\text{for } i=2,3,\\dots,n-1 \\end{cases} \\] Upper bound: Assume \\(i^{th}\\) element always falls into the larger part. \\[ \\begin{align*} T(n) &\\leq \\frac{1}{n}T(max(1,n-1))+\\frac{1}{n}\\sum \\limits_{q=1}^{n-1}T(max(q,n-q))+O(n) \\\\ Note: & \\frac{1}{n}T(max(1,n-1)) = \\frac{1}{n}T(n-1)=\\frac{1}{n}O(n^2) = O(n) \\\\ \\therefore \\text{(3 dot mean therefore) } & T(n) \\leq \\frac{1}{n}\\sum \\limits_{q=1}^{n-1}T(max(q,n-q))+O(n) \\end{align*} \\]","title":"Average-Case Analysis of Randomized Select (1)"},{"location":"week-3/ce100-week-3-matrix/#average-case-analysis-of-randomized-select-2","text":"\\[ \\begin{align*} \\therefore T(n) \\leq \\frac{1}{n}\\sum \\limits_{q=1}^{n-1}T(max(q,n-q))+O(n) \\end{align*} \\] \\[ max(q, n\u2013q) = \\begin{cases} q & \\text{ if } q \\geq \\lceil n/2 \\rceil \\\\ n-q & \\text{ if } q < \\lceil n/2 \\rceil \\\\ \\end{cases} \\] \\(n\\) is odd: \\(T(k)\\) appears twice for \\(k=\\lceil n/2 \\rceil+1,\\lceil n/2 \\rceil+2,\\dots,n\u20131\\) \\(n\\) is even: \\(T(\\lceil n/2 \\rceil)\\) appears once \\(T(k)\\) appears twice for \\(k = \\lceil n/2 \\rceil +1, \\lceil n/2 \\rceil+2,\\dots,n\u20131\\)","title":"Average-Case Analysis of Randomized Select (2)"},{"location":"week-3/ce100-week-3-matrix/#average-case-analysis-of-randomized-select-3","text":"Hence, in both cases: \\[ \\begin{align*} \\sum \\limits_{q=1}^{n-1} T(max(q,n-q))+O(n) & \\leq 2\\sum \\limits_{q=\\lceil n/2 \\rceil}^{n-1} T(q)+O(n) \\\\ \\therefore T(n) & \\leq \\frac{2}{n} \\sum \\limits_{q=\\lceil n/2 \\rceil}^{n-1}T(q)+O(n) \\end{align*} \\] section{ font-size: 25px; }","title":"Average-Case Analysis of Randomized Select (3)"},{"location":"week-3/ce100-week-3-matrix/#average-case-analysis-of-randomized-select-4","text":"\\[ \\begin{align*} T(n) & \\leq \\frac{2}{n} \\sum \\limits_{q=\\lceil n/2 \\rceil}^{n-1}T(q)+O(n) \\end{align*} \\] By substitution guess \\(T(n) = O(n)\\) Inductive hypothesis: \\(T(k) \\leq ck, \\forall k<n\\) \\[ \\begin{align*} T(n) & \\leq \\frac{2}{n} \\sum \\limits_{q=\\lceil n/2 \\rceil}^{n-1}ck+O(n) \\\\ & = \\frac{2c}{n} \\Bigg(\\sum \\limits_{k=1}^{n-1}k-\\sum \\limits_{k=1}^{\\lceil n/2 \\rceil-1}k \\Bigg)+ O(n) \\\\ & = \\frac{2c}{n} \\Bigg(\\frac{1}{2}n(n-1)-\\frac{1}{2} \\lceil \\frac{n}{2} \\rceil \\bigg( \\frac{n}{2}-1 \\bigg) \\Bigg)+ O(n) \\end{align*} \\]","title":"Average-Case Analysis of Randomized Select (4)"},{"location":"week-3/ce100-week-3-matrix/#average-case-analysis-of-randomized-select-5","text":"\\[ \\begin{align*} T(n)& \\leq \\frac{2c}{n} \\Bigg(\\frac{1}{2}n(n-1)-\\frac{1}{2} \\lceil \\frac{n}{2} \\rceil \\bigg( \\frac{n}{2}-1 \\bigg) \\Bigg)+ O(n) \\\\ & \\leq c(n-1)-\\frac{c}{4}n+\\frac{c}{2}+O(n) \\\\ & = cn - \\frac{c}{4}n - \\frac{c}{2} + O(n) \\\\ & = cn - \\Bigg( \\bigg( \\frac{c}{4}n+\\frac{c}{2}\\bigg) + O(n) \\Bigg) \\\\ & \\leq cn \\end{align*} \\] since we can choose c large enough so that \\((cn/4+c/2 )\\) dominates \\(O(n)\\)","title":"Average-Case Analysis of Randomized Select (5)"},{"location":"week-3/ce100-week-3-matrix/#summary-of-randomized-order-statistic-selection","text":"Works fast: linear expected time Excellent algorithm in practise But, the worst case is very bad: \\(\\Theta(n^2)\\) Blum, Floyd, Pratt, Rivest & Tarjan[1973] algorithms are runs in linear time in the worst case . Generate a good pivot recursively section{ font-size: 25px; }","title":"Summary of Randomized Order-Statistic Selection"},{"location":"week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time","text":"// return i - th element in set S with n elements SELECT ( S , n , i ) if n <= 5 then SORT S and return the i - th element DIVIDE S into ceil ( n / 5 ) groups // first ceil ( n / 5 ) groups are of size 5 , last group is of size n mod 5 FIND median set M = { m , \u2026 , m_ceil ( n / 5 )} // m_j : median of j - th group x = SELECT ( M , ceil ( n / 5 ), floor (( ceil ( n / 5 ) +1 ) / 2 )) PARTITION set S around the pivot x into L and R if i <= | L | then return SELECT ( L , | L | , i ) else return SELECT ( R , n \u2013 | L | , i \u2013 | L | )","title":"Selection in Worst Case Linear Time"},{"location":"week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-1","text":"Input: Array \\(S\\) and index \\(i\\) Output: The \\(i^{th}\\) smallest value \\[ \\begin{array}{ccc} 25 & 9 & 16 & 8 & 11 & 27 & 39 & 42 & 15 & 6 32 & 14 & 36 & 20 & 33 & 22 & 31 & 4 & 17 & 3 & 30 & 41 \\\\ 2 & 13 & 19 & 7 & 21 & 10 & 34 & 1 & 37 & 23 & 40 & 5 & 29 & 18 & 24 & 12 & 38 & 28 & 26 & 35 & 43 \\end{array} \\]","title":"Selection in Worst Case Linear Time - Example (1)"},{"location":"week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-2","text":"Step 1: Divide the input array into groups of size \\(5\\) \\[ \\overbrace{ \\begin{array}{ccc} 25 & 9 & 16 & 8 & 11 \\\\ 27 & 39 & 42 & 15 & 6 \\\\ 32 & 14 & 36 & 20 & 33 \\\\ 22 & 31 & 4 & 17 & 3 \\\\ 30 & 41 & 2 & 13 & 19 \\\\ 7 & 21 & 10 & 34 & 1 \\\\ 37 & 23 & 40 & 5 & 29 \\\\ 18 & 24 & 12 & 38 & 28 \\\\ 26 & 35 & 43 \\end{array} }^{\\text{group size}=5} \\] section{ font-size: 25px; }","title":"Selection in Worst Case Linear Time - Example (2)"},{"location":"week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-3","text":"Step 2: Compute the median of each group ( \\(\\Theta(n)\\) ) \\[ \\begin{array}{ccc} 25 & 16 & \\overbrace{11}^{Medians} & 8 & 9 \\\\ 39 & 42 & 27 & 6 & 15 \\\\ 36 & 33 & 32 & 20 & 14 \\\\ 22 & 31 & 17 & 3 & 4 \\\\ 41 & 30 & 19 & 13 & 2 \\\\ 21 & 34 & 10 & 1 & 7 \\\\ 37 & 40 & 29 & 23 & 5 \\\\ 38 & 28 & 24 & 12 & 18 \\\\ & 26 & 35 & 43 \\end{array} \\] Let \\(M\\) be the set of the medians computed: \\(M = \\{11, 27, 32, 17, 19, 10, 29, 24, 35\\}\\)","title":"Selection in Worst Case Linear Time - Example (3)"},{"location":"week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-4","text":"Step 3: Compute the median of the median group \\(M\\) \\(x \\leftarrow SELECT(M,|M|,\\lfloor (|M|+1)/2 \\rfloor)\\) where \\(|M|=\\lceil n/5 \\rceil\\) Let \\(M\\) be the set of the medians computed: \\(M = \\{11, 27, 32, 17, 19, 10, 29, \\overbrace{24}^{Median}, 35\\}\\) \\(Median = 24\\) The runtime of the recursive call: \\(T(|M|)=T(\\lceil n/5 \\rceil)\\)","title":"Selection in Worst Case Linear Time - Example (4)"},{"location":"week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-5","text":"Step 4: Partition the input array \\(S\\) around the median-of-medians \\(x\\) \\[ \\begin{array}{ccc} 25 & 9 & 16 & 8 & 11 & 27 & 39 & 42 & 15 & 6 32 & 14 & 36 & 20 & 33 & 22 & 31 & 4 & 17 & 3 & 30 & 41 \\\\ 2 & 13 & 19 & 7 & 21 & 10 & 34 & 1 & 37 & 23 & 40 & 5 & 29 & 18 & 24 & 12 & 38 & 28 & 26 & 35 & 43 \\end{array} \\] Partition \\(S\\) around \\(x = 24\\) Claim: Partitioning around x is guaranteed to be well-balanced. section{ font-size: 25px; }","title":"Selection in Worst Case Linear Time - Example (5)"},{"location":"week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-6","text":"\\(M\\) : Median, \\(M^*\\) : Median of Medians \\[ \\begin{array}{ccc} 41 & 30 & \\overbrace{19}^{M} & 13 & 2 \\\\ 21 & 34 & 10 & 1 & 7 \\\\ 22 & 31 & 17 & 3 & 4 \\\\ 25 & 16 & 11 & 8 & 9 \\\\ 38 & 28 & \\overbrace{24}^{M^*} & 12 & 18 \\\\ 36 & 33 & 32 & 20 & 14 \\\\ 37 & 40 & 29 & 23 & 5 \\\\ 39 & 42 & 27 & 6 & 15 \\\\ & 26 & 35 & 43 \\end{array} \\] About half of the medians greater than \\(x=24\\) (about \\(n/10\\) )","title":"Selection in Worst Case Linear Time - Example (6)"},{"location":"week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-7","text":"","title":"Selection in Worst Case Linear Time - Example (7)"},{"location":"week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-8","text":"","title":"Selection in Worst Case Linear Time - Example (8)"},{"location":"week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-9","text":"\\[ S = \\begin{array}{ccc} \\{ 25 & 9 & 16 & 8 & 11 & 27 & 39 & 42 & 15 & 6 32 & 14 & 36 & 20 & 33 & 22 & 31 & 4 & 17 & 3 & 30 & 41 \\\\ 2 & 13 & 19 & 7 & 21 & 10 & 34 & 1 & 37 & 23 & 40 & 5 & 29 & 18 & 24 & 12 & 38 & 28 & 26 & 35 & 43 \\} \\end{array} \\] Partitioning \\(S\\) around \\(x = 24\\) will lead to partitions of sizes \\(\\sim 3n/10\\) and \\(\\sim 7n/10\\) in the worst case . Step 5: Make a recursive call to one of the partitions if i <= | L | then return SELECT ( L , | L | , i ) else return SELECT ( R , n -| L | , i -| L | ) section{ font-size: 25px; }","title":"Selection in Worst Case Linear Time - Example (9)"},{"location":"week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time_1","text":"// return i - th element in set S with n elements SELECT ( S , n , i ) if n <= 5 then SORT S and return the i - th element DIVIDE S into ceil ( n / 5 ) groups // first ceil ( n / 5 ) groups are of size 5 , last group is of size n mod 5 FIND median set M = { m , \u2026 , m_ceil ( n / 5 )} // m_j : median of j - th group x = SELECT ( M , ceil ( n / 5 ), floor (( ceil ( n / 5 ) +1 ) / 2 )) PARTITION set S around the pivot x into L and R if i <= | L | then return SELECT ( L , | L | , i ) else return SELECT ( R , n \u2013 | L | , i \u2013 | L | )","title":"Selection in Worst Case Linear Time"},{"location":"week-3/ce100-week-3-matrix/#choosing-the-pivot-1","text":"Divide S into groups of size 5","title":"Choosing the Pivot (1)"},{"location":"week-3/ce100-week-3-matrix/#choosing-the-pivot-2","text":"Divide S into groups of size 5 Find the median of each group","title":"Choosing the Pivot (2)"},{"location":"week-3/ce100-week-3-matrix/#choosing-the-pivot-3","text":"Divide S into groups of size 5 Find the median of each group Recursively select the median x of the medians","title":"Choosing the Pivot (3)"},{"location":"week-3/ce100-week-3-matrix/#choosing-the-pivot-4","text":"At least half of the medians \\(\\geq x\\) Thus \\(m = \\lceil \\lceil n/5 \\rceil /2 \\rceil\\) groups contribute 3 elements to R except possibly the last group and the group that contains \\(x\\) , \\(|R|\\geq 3(m\u20132)\\geq \\frac{3n}{10}\u20136\\)","title":"Choosing the Pivot (4)"},{"location":"week-3/ce100-week-3-matrix/#choosing-the-pivot-5","text":"Similarly \\(|L| \\geq \\frac{3n}{10}\u2013 6\\) Therefore, SELECT is recursively called on at most \\(n-(\\frac{3n}{10}-6)=\\frac{7n}{10}+6\\) elements","title":"Choosing the Pivot (5)"},{"location":"week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-1","text":"","title":"Selection in Worst Case Linear Time (1)"},{"location":"week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-2","text":"Thus recurrence becomes \\(T(n) \\leq T \\big( \\lceil \\frac{n}{5} \\rceil \\big) + T\\big( \\frac{7n}{10}+6 \\big) + \\Theta(n)\\) Guess \\(T(n)=O(n)\\) and prove by induction Inductive step: \\[ \\begin{align*} T(n) & \\leq c \\lceil n/5 \\rceil + c(7n/10+6)+\\Theta(n) \\\\ & \\leq cn/5 + c + 7cn/10 + 6c + \\Theta(n) \\\\ & = 9cn/10 + 7c + \\Theta(n) \\\\ & = cn - [c(n/10-7)-\\Theta(n)] \\leq cn &\\text{( for large c)} \\end{align*} \\] Work at each level of recursion is a constant factor \\((9/10)\\) smaller","title":"Selection in Worst Case Linear Time (2)"},{"location":"week-3/ce100-week-3-matrix/#references","text":"Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) Insertion Sort - GeeksforGeeks NIST Dictionary of Algorithms and Data Structures NIST - Dictionary of Algorithms and Data Structures NIST - big-O notation NIST - big-Omega notation \\(-End-Of-Week-3-Course-Module-\\)","title":"References"},{"location":"week-4/ce100-week-4-heap/","text":"CE100 Algorithms and Programming II \u00b6 Week-4 (Heap/Heap Sort) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Heap/Heap Sort \u00b6 Outline (1) \u00b6 Heaps Max / Min Heap Heap Data Structure Heapify Iterative Recursive Outline (2) \u00b6 Extract-Max Build Heap Outline (3) \u00b6 Heap Sort Priority Queues Linked Lists Radix Sort Counting Sort Heapsort \u00b6 Worst-case runtime: \\(O(nlgn)\\) Sorts in-place Uses a special data structure (heap) to manage information during execution of the algorithm Another design paradigm Heap Data Structure (1) \u00b6 Nearly complete binary tree Completely filled on all levels except possibly the lowest level Heap Data Structure (2) \u00b6 Height of node i: Length of the longest simple downward path from i to a leaf Height of the tree: height of the root Heap Data Structures (3) \u00b6 Depth of node i: Length of the simple downward path from the root to node i Heap Property: Min-Heap \u00b6 The smallest element in any subtree is the root element in a min-heap Min heap: For every node i other than root , \\(A[parent(i)] \\leq A[i]\\) Parent node is always smaller than the child nodes Heap Property: Max-Heap \u00b6 The largest element in any subtree is the root element in a max-heap We will focus on max-heaps Max heap: For every node i other than root , \\(A[parent(i)] \u2265 A[i]\\) Parent node is always larger than the child nodes Heap Data Structures (4) \u00b6 Heap Data Structures (5) \u00b6 Computing left child, right child, and parent indices very fast left(i) = 2i \\(\\Longrightarrow\\) binary left shift right(i) = 2i+1 \\(\\Longrightarrow\\) binary left shift, then set the lowest bit to 1 parent(i) = floor(i/2) \\(\\Longrightarrow\\) right shift in binary \\(A[1]\\) is always the root element Array \\(A\\) has two attributes: length(A): The number of elements in \\(A\\) n = heap-size(A): The number elements in \\(heap\\) \\(n \\leq length(A)\\) Heap Operations : EXTRACT-MAX (1) \u00b6 EXTRACT - MAX ( A , n ) max = A [ 1 ] A [ 1 ] = A [ n ] n = n - 1 HEAPIFY ( A , 1 , n ) return max Heap Operations : EXTRACT-MAX (2) \u00b6 Return the max element,and reorganize the heap to maintain heap property Heap Operations: HEAPIFY (1) \u00b6 Heap Operations: HEAPIFY (2) \u00b6 Maintaining heap property: Subtrees rooted at \\(left[i]\\) and \\(right[i]\\) are already heaps. But, \\(A[i]\\) may violate the heap property (i.e., may be smaller than its children) Idea: Float down the value at \\(A[i]\\) in the heap so that subtree rooted at \\(i\\) becomes a heap. Heap Operations: HEAPIFY (2) \u00b6 HEAPIFY ( A , i , n ) largest = i if 2i <= n and A [ 2i ] > A [ i ] then largest = 2i ; endif if 2i+1 <= n and A [ 2i+1 ] > A [ largest ] then largest = 2i+1 ; endif if largest != i then exchange A [ i ] with A [ largest ]; HEAPIFY ( A , largest , n ); endif Heap Operations: HEAPIFY (3) \u00b6 Heap Operations: HEAPIFY (4) \u00b6 Heap Operations: HEAPIFY (5) \u00b6 Heap Operations: HEAPIFY (6) \u00b6 Heap Operations: HEAPIFY (7) \u00b6 Heap Operations: HEAPIFY (8) \u00b6 Intuitive Analysis of HEAPIFY \u00b6 Consider \\(HEAPIFY(A, i, n)\\) let \\(h(i)\\) be the height of node \\(i\\) at most \\(h(i)\\) recursion levels Constant work at each level: \\(\\Theta(1)\\) Therefore \\(T(i)=O(h(i))\\) Heap is almost-complete binary tree \\(h(n)=O(lgn)\\) Thus \\(T(n)=O(lgn)\\) Formal Analysis of HEAPIFY \u00b6 What is the recurrence? Depends on the size of the subtree on which recursive call is made In the next, we try to compute an upper bound for this subtree . Reminder: Binary trees \u00b6 For a complete binary tree: \\(\\#\\) of nodes at depth \\(d\\) : \\(2^d\\) \\(\\#\\) of nodes with depths less than \\(d\\) : \\(2^d-1\\) Formal Analysis of HEAPIFY (1) \u00b6 Worst case occurs when last row of the subtree \\(S_i\\) rooted at node \\(i\\) is half full \\(T(n) \\leq T(|S_{L(i)}|) + \\Theta(1)\\) \\(S_{L(i)}\\) and \\(S_{R(i)}\\) are complete binary trees of heights \\(h(i)-1\\) and \\(h(i)-2\\) , respectively Formal Analysis of HEAPIFY (2) \u00b6 Let \\(m\\) be the number of leaf nodes in \\(S_{L(i)}\\) \\(|S_{L(i)}|=\\overbrace{m}^{ext.}+\\overbrace{(m\u20131)}^{int.}=2m\u20131\\) \\(|S_{R(i)}|=\\overbrace{\\frac{m}{2}}^{ext.}+\\overbrace{(\\frac{m}{2}\u20131)}^{int.}=m\u20131\\) \\(|S_{L(i)}|+|S_{R(i)}|+1=n\\) Formal Analysis of HEAPIFY (2) \u00b6 \\[ \\begin{align*} (2m\u20131)+(m\u20131)+1 &=n \\\\ m &= (n+1)/3 \\\\ |S_{L(i)}| &= 2m \u2013 1 \\\\ &=2(n+1)/3 \u2013 1 \\\\ &=(2n/3+2/3) \u20131 \\\\ &=\\frac{2n}{3}-\\frac{1}{3} \\leq \\frac{2n}{3} \\\\ T(n) & \\leq T(2n/3) + \\Theta(1) \\\\ T(n) &= O(lgn) \\end{align*} \\] By CASE-2 of Master Theorem \\(\\Longrightarrow\\) \\(T(n)=\\Theta(n^{log_b^a}lgn)\\) Formal Analysis of HEAPIFY (2) \u00b6 Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(1)\\) i.e., \\(f(n)\\) and \\(n^{log_b^a}\\) grow at similar rates Solution: \\(T(n)=\\Theta(n^{log_b^a}lgn)\\) \\(T(n) \\leq T(2n/3) + \\Theta(1)\\) (drop constants.) \\(T(n) \\leq \\Theta(n^{log_3^1}lgn)\\) \\(T(n) \\leq \\Theta(n^0lgn)\\) \\(T(n) = O(lgn)\\) HEAPIFY: Efficiency Issues \u00b6 Recursion vs Iteration: In the absence of tail recursion, iterative version is in general more efficient because of the pop/push operations to/from stack at each level of recursion . Heap Operations: HEAPIFY (1) \u00b6 Recursive HEAPIFY ( A , i , n ) largest = i if 2i <= n and A [ 2i ] > A [ i ] then largest = 2i if 2i+1 <= n and A [ 2i+1 ] > A [ largest ] then largest = 2i+1 if largest != i then exchange A [ i ] with A [ largest ] HEAPIFY ( A , largest , n ) Heap Operations: HEAPIFY (2) \u00b6 Iterative HEAPIFY ( A , i , n ) j = i while ( true ) do largest = j if 2 j <= n and A [ 2 j ] > A [ j ] then largest = 2 j if 2 j +1 <= n and A [ 2 j +1 ] > A [ largest ] then largest = 2 j +1 if largest != j then exchange A [ j ] with A [ largest ] j = largest else return Heap Operations: HEAPIFY (3) \u00b6 Heap Operations: Building Heap \u00b6 Given an arbitrary array, how to build a heap from scratch? Basic idea: Call \\(HEAPIFY\\) on each node bottom up Start from the leaves (which trivially satisfy the heap property) Process nodes in bottom up order. When \\(HEAPIFY\\) is called on node \\(i\\) , the subtrees connected to the \\(left\\) and \\(right\\) subtrees already satisfy the heap property. Storage of the leaves (Lemma) \u00b6 Lemma: The last \\(\\lceil \\frac{n}{2} \\rceil\\) nodes of a heap are all leaves. Storage of the leaves (Proof of Lemma) (1) \u00b6 Lemma: last \\(\\lceil n/2 \\rceil\\) nodes of a heap are all leaves Proof : \\(m=2^{d-1}\\) : \\(\\#\\) nodes at level \\(d-1\\) \\(f\\) : \\(\\#\\) nodes at level \\(d\\) (last level) \\(\\#\\) of nodes with depth \\(d-1\\) : \\(m\\) \\(\\#\\) of nodes with depth \\(<d-1\\) : \\(m-1\\) \\(\\#\\) of nodes with depth \\(d\\) : \\(f\\) Total \\(\\#\\) of nodes : \\(n=f+2m-1\\) Storage of the leaves (Proof of Lemma) (2) \u00b6 Total \\(\\#\\) of nodes : \\(f=n-2m+1\\) \\[ \\begin{align*} \\text{\\# of leaves: }&=f+m-\\lceil f/2 \\rceil \\\\ &= m+\\lfloor f/2 \\rfloor \\\\ &= m+\\lfloor (n-2m+1)/2 \\rfloor \\\\ &= \\lfloor (n+1)/2 \\rfloor \\\\ &= \\lceil n/2 \\rceil \\end{align*} \\] Proof is Completed Heap Operations: Building Heap \u00b6 BUILD - HEAP ( A , n ) for i = ceil ( n / 2 ) downto 1 do HEAPIFY ( A , i , n ) Reminder: The last \\(\\lceil n/2 \\rceil\\) nodes of a heap are all leaves , which trivially satisfy the heap property Build-Heap Example (Step-1) \u00b6 Build-Heap Example (Step-2) \u00b6 Build-Heap Example (Step-3) \u00b6 Build-Heap Example (Step-4) \u00b6 Build-Heap Example (Step-5) \u00b6 Build-Heap Example (Step-6) \u00b6 Build-Heap Example (Step-7) \u00b6 Build-Heap Example (Step-8) \u00b6 Build-Heap Example (Step-9) \u00b6 Build-Heap: Runtime Analysis \u00b6 Simple analysis: \\(O(n)\\) calls to \\(HEAPIFY\\) , each of which takes \\(O(lgn)\\) time \\(O(nlgn)\\) \\(\\Longrightarrow\\) loose bound In general, a good approach: Start by proving an easy bound Then, try to tighten it Is there a tighter bound? section{ font-size: 25px; } Build-Heap: Tighter Running Time Analysis \u00b6 If the heap is complete binary tree then \\(h_{\\ell} = d \u2013 \\ell\\) Otherwise, nodes at a given level do not all have the same height, But we have \\(d \u2013 \\ell \u2013 1 \\leq h_{\\ell} \\leq d \u2013 \\ell\\) section{ font-size: 25px; } Build-Heap: Tighter Running Time Analysis \u00b6 Assume that all nodes at level \\(\\ell= d \u2013 1\\) are processed \\[ \\begin{align*} T(n) &=\\sum \\limits_{\\ell=0}^{d-1}n_{\\ell}O(h_{\\ell})=O(\\sum \\limits_{\\ell=0}^{d-1}n_{\\ell}h_{\\ell}) \\begin{cases} n_{\\ell}=2^{\\ell} = \\# \\text{ of nodes at level }\\ell \\\\ h_{\\ell}=\\text{height of nodes at level } \\ell \\end{cases} \\\\ \\therefore T(n) &= O \\bigg( \\sum \\limits_{\\ell=0}^{d-1}2^{\\ell}(d-\\ell) \\bigg) \\\\ \\text{Let } & h=d-\\ell \\Longrightarrow \\ell = d-h \\text{ change of variables} \\\\ T(n) &= O\\bigg(\\sum \\limits_{h=1}^{d}h2^{d-h} \\bigg)=O\\bigg(\\sum \\limits_{h=1}^{d}h \\frac{2^d}{2^h} \\bigg) = O\\bigg(2^d\\sum \\limits_{h=1}^{d}h (1/2)^h\\bigg) \\\\ \\text{ but } & 2^d = \\Theta(n) \\Longrightarrow O\\bigg(n\\sum \\limits_{h=1}^{d}h (1/2)^h \\bigg) \\end{align*} \\] section{ font-size: 25px; } Build-Heap: Tighter Running Time Analysis \u00b6 \\[ \\sum \\limits_{h=1}^{d}h(1/2)^h \\leq \\sum \\limits_{h=0}^{d}h(1/2)^h \\leq \\sum \\limits_{h=0}^{\\infty}h(1/2)^h \\] recall infinite decreasing geometric series \\[ \\sum \\limits_{k=0}^{\\infty} x^k = \\frac{1}{1-x} \\text{ where } |x|<1 \\] differentiate both sides \\[ \\sum \\limits_{k=0}^{\\infty}kx^{k-1} = \\frac{1}{(1-x)^2} \\] Build-Heap: Tighter Running Time Analysis \u00b6 \\[ \\sum \\limits_{k=0}^{\\infty}kx^{k-1} = \\frac{1}{(1-x)^2} \\] then, multiply both sides by \\(x\\) \\[ \\sum \\limits_{k=0}^{\\infty}kx^k = \\frac{x}{(1-x)^2} \\] in our case: \\(x = 1/2\\) and \\(k = h\\) \\[ \\therefore \\sum \\limits_{h=0}^{\\infty}h(1/2)^h = \\frac{1/2}{(1-(1/2))^2}=2=O(1) \\\\ \\therefore T(n)=O(n\\sum \\limits_{h=1}^{d}h(1/2)^h)=O(n) \\] Heapsort Algorithm Steps \u00b6 (1) Build a heap on array \\(A[1 \\dots n]\\) by calling \\(BUILD-HEAP(A, n)\\) (2) The largest element is stored at the root \\(A[1]\\) Put it into its correct final position \\(A[n]\\) by \\(A[1] \\longleftrightarrow A[n]\\) (3) Discard node \\(n\\) from the heap (4) Subtrees \\((S2 \\& S3)\\) rooted at children of root remain as heaps, but the new root element may violate the heap property. Make \\(A[1 \\dots n-1]\\) a heap by calling \\(HEAPIFY(A,1,n-1)\\) (5) \\(n \\leftarrow n-1\\) (6) Repeat steps (2-4) until \\(n=2\\) Heapsort Algorithm Example (Step-1) \u00b6 Heapsort Algorithm Example (Step-2) \u00b6 Heapsort Algorithm Example (Step-3) \u00b6 Heapsort Algorithm Example (Step-4) \u00b6 Heapsort Algorithm Example (Step-5) \u00b6 Heapsort Algorithm Example (Step-6) \u00b6 Heapsort Algorithm Example (Step-7) \u00b6 Heapsort Algorithm Example (Step-8) \u00b6 Heapsort Algorithm Example (Step-9) \u00b6 Heapsort Algorithm Example (Step-10) \u00b6 Heapsort Algorithm Example (Step-11) \u00b6 Heapsort Algorithm Example (Step-12) \u00b6 Heapsort Algorithm Example (Step-13) \u00b6 Heapsort Algorithm Example (Step-14) \u00b6 Heapsort Algorithm Example (Step-15) \u00b6 Heapsort Algorithm Example (Step-16) \u00b6 Heapsort Algorithm Example (Step-17) \u00b6 Heapsort Algorithm Example (Step-18) \u00b6 Heapsort Algorithm Example (Step-19) \u00b6 Heapsort Algorithm: Runtime Analysis \u00b6 \\[ \\begin{align*} T(n) &= \\Theta(n)+\\sum \\limits_{i=2}^{n}O(lgi) \\\\ &= \\Theta(n)+O\\bigg( \\sum \\limits_{i=2}^{n}O(lgn) \\bigg) \\\\ &= O(nlgn) \\end{align*} \\] Heapsort - Notes \u00b6 Heapsort is a very good algorithm but, a good implementation of quicksort always beats heapsort in practice However, heap data structure has many popular applications, and it can be efficiently used for implementing priority queues Data structures for Dynamic Sets \u00b6 Consider sets of records having key and satellite data Operations on Dynamic Sets \u00b6 Queries: Simply return info; \\(MAX(S) / MIN(S):\\) (Query) return \\(x \\in S\\) with the largest/smallest \\(key\\) \\(SEARCH(S, k):\\) (Query) return \\(x \\in S\\) with \\(key[x]= k\\) \\(SUCCESSOR(S, x) / PREDECESSOR(S, x):\\) (Query) return \\(y \\in S\\) which is the next larger/smaller element after \\(x\\) Modifying operations: Change the set \\(INSERT(S, x):\\) (Modifying) \\(S \\leftarrow S \\cup \\{x\\}\\) \\(DELETE(S, x):\\) (Modifying) \\(S \\leftarrow S - \\{x\\}\\) \\(\\text{EXTRACT-MAX}(S) / \\text{EXTRACT-MIN}(S):\\) (Modifying) return and delete \\(x \\in S\\) with the largest/smallest \\(key\\) Different data structures support/optimize different operations Priority Queues (PQ) \u00b6 Supports \\(INSERT\\) \\(MAX / MIN\\) \\(\\text{EXTRACT-MAX} / \\text{EXTRACT-MIN}\\) Priority Queues (PQ) \u00b6 One application: Schedule jobs on a shared resource PQ keeps track of jobs and their relative priorities When a job is finished or interrupted, highest priority job is selected from those pending using \\(\\text{EXTRACT-MAX}\\) A new job can be added at any time using \\(INSERT\\) Priority Queues (PQ) \u00b6 Another application: Event-driven simulation Events to be simulated are the items in the PQ Each event is associated with a time of occurrence which serves as a \\(key\\) Simulation of an event can cause other events to be simulated in the future Use \\(\\text{EXTRACT-MIN}\\) at each step to choose the next event to simulate As new events are produced insert them into the PQ using \\(INSERT\\) Implementation of Priority Queue \u00b6 Sorted linked list: Simplest implementation \\(INSERT\\) \\(O(n)\\) time Scan the list to find place and splice in the new item \\(\\text{EXTRACT-MAX}\\) \\(O(1)\\) time Take the first element Fast extraction but slow insertion. Implementation of Priority Queue \u00b6 Unsorted linked list: Simplest implementation \\(INSERT\\) \\(O(1)\\) time Put the new item at front \\(\\text{EXTRACT-MAX}\\) \\(O(n)\\) time Scan the whole list Fast insertion but slow extraction. Sorted linked list is better on the average Sorted list: on the average, scans \\(n/2\\) element per insertion Unsorted list: always scans \\(n\\) element at each extraction Heap Implementation of PQ \u00b6 \\(INSERT\\) and \\(\\text{EXTRACT-MAX}\\) are both \\(O(lgn)\\) good compromise between fast insertion but slow extraction and vice versa \\(\\text{EXTRACT-MAX}\\) : already discussed \\(\\text{HEAP-EXTRACT-MAX}\\) \\(INSERT\\) : Insertion is like that of Insertion-Sort. HEAP - INSERT ( A , key , n ) n = n +1 i = n while i > 1 and A [ floor ( i / 2 )] < key do A [ i ] = A [ floor ( i / 2 )] i = floor ( i / 2 ) A [ i ] = key Heap Implementation of PQ \u00b6 Traverses \\(O(lgn)\\) nodes, as \\(HEAPIFY\\) does but makes fewer comparisons and assignments \\(HEAPIFY\\) : compares parent with both children \\(HEAP-INSERT\\) : with only one HEAP-INSERT Example (Step-1) \u00b6 HEAP-INSERT Example (Step-2) \u00b6 HEAP-INSERT Example (Step-3) \u00b6 HEAP-INSERT Example (Step-4) \u00b6 HEAP-INSERT Example (Step-5) \u00b6 Heap Increase Key \u00b6 Key value of \\(i^{th}\\) element of heap is increased from \\(A[i]\\) to \\(key\\) HEAP - INCREASE - KEY ( A , i , key ) if key < A [ i ] then return error while i > 1 and A [ floor ( i / 2 )] < key do A [ i ] = A [ floor ( i / 2 )] i = floor ( i / 2 ) A [ i ] = key HEAP-INCREASE-KEY Example (Step-1) \u00b6 HEAP-INCREASE-KEY Example (Step-2) \u00b6 HEAP-INCREASE-KEY Example (Step-3) \u00b6 HEAP-INCREASE-KEY Example (Step-4) \u00b6 HEAP-INCREASE-KEY Example (Step-5) \u00b6 Heap Implementation of Priority Queue (PQ) \u00b6 section{ font-size: 25px; } Summary: Max Heap \u00b6 Heapify(A, i) Works when both child subtrees of node i are heaps \" Floats down \" node i to satisfy the heap property Runtime: \\(O(lgn)\\) Max(A, n) Returns the max element of the heap (no modification) Runtime: \\(O(1)\\) Extract-Max(A, n) Returns and removes the max element of the heap Fills the gap in \\(A[1]\\) with \\(A[n]\\) , then calls Heapify(A,1) Runtime: \\(O(lgn)\\) section{ font-size: 25px; } Summary: Max Heap \u00b6 Build-Heap(A, n) Given an arbitrary array, builds a heap from scratch Runtime: \\(O(n)\\) Min(A, n) How to return the min element in a max-heap? Worst case runtime: \\(O(n)\\) because ~half of the heap elements are leaf nodes Instead, use a min-heap for efficient min operations Search(A, x) For an arbitrary \\(x\\) value, the worst-case runtime: \\(O(n)\\) Use a sorted array instead for efficient search operations Summary: Max Heap \u00b6 Increase-Key(A, i, x) Increase the key of node \\(i\\) (from \\(A[i]\\) to \\(x\\) ) \u201c Float up \u201d \\(x\\) until heap property is satisfied Runtime: \\(O(lgn)\\) Decrease-Key(A, i, x) Decrease the key of node \\(i\\) (from \\(A[i]\\) to \\(x\\) ) Call Heapify(A, i) Runtime: \\(O(lgn)\\) Phone Operator Problem \u00b6 A phone operator answering \\(n\\) phones Each phone \\(i\\) has \\(x_i\\) people waiting in line for their calls to be answered. Phone operator needs to answer the phone with the largest number of people waiting in line. New calls come continuously, and some people hang up after waiting. Phone Operator Solution \u00b6 Step 1: Define the following array: \\(A[i]\\) : the ith element in heap \\(A[i].id\\) : the index of the corresponding phone \\(A[i].key\\) : \\(\\#\\) of people waiting in line for phone with index \\(A[i].id\\) Phone Operator Solution \u00b6 Step 2: \\(\\text{Build-Max-Heap}(A, n)\\) Execution: When the operator wants to answer a phone: \\(id = A[1].id\\) \\(\\text{Decrease-Key}(A, 1, A[1].key-1)\\) answer phone with index \\(id\\) When a new call comes in to phone i: \\(\\text{Increase-Key}(A, i, A[i].key+1)\\) When a call drops from phone i: \\(\\text{Decrease-Key}(A, i, A[i].key-1)\\) Linked Lists \u00b6 Like arrays, Linked List is a linear data structure. Unlike arrays, linked list elements are not stored at a contiguous location; the elements are linked using pointers. Linked Lists - C Definition \u00b6 C // A linked list node struct Node { int data ; struct Node * next ; }; Linked Lists - Cpp Definition \u00b6 Cpp class Node { public : int data ; Node * next ; }; Linked Lists - Java Definition \u00b6 Java class LinkedList { Node head ; // head of the list /* Linked list Node*/ class Node { int data ; Node next ; // Constructor to create a new node // Next is by default initialized // as null Node ( int d ) { data = d ; } } } Linked Lists - Csharp Definition \u00b6 Csharp class LinkedList { // The first node(head) of the linked list // Will be an object of type Node (null by default) Node head ; class Node { int data ; Node next ; // Constructor to create a new node Node ( int d ) { data = d ; } } } Priority Queue using Linked List Methods \u00b6 Implement Priority Queue using Linked Lists. push(): This function is used to insert a new data into the queue. pop(): This function removes the element with the highest priority from the queue. peek()/top(): This function is used to get the highest priority element in the queue without removing it from the queue. Priority Queue using Linked List Algorithm \u00b6 PUSH ( HEAD , DATA , PRIORITY ) Create NEW.Data = DATA & NEW.Priority = PRIORITY If HEAD.priority < NEW.Priority NEW -> NEXT = HEAD HEAD = NEW Else Set TEMP to head of the list Endif WHILE TEMP -> NEXT != NULL and TEMP -> NEXT -> PRIORITY > PRIORITY THEN TEMP = TEMP -> NEXT ENDWHILE NEW -> NEXT = TEMP -> NEXT TEMP -> NEXT = NEW Priority Queue using Linked List Algorithm \u00b6 POP ( HEAD ) // Set the head of the list to the next node in the list. HEAD = HEAD -> NEXT. Free the node at the head of the list PEEK ( HEAD ) : Return HEAD -> DATA Priority Queue using Linked List Notes \u00b6 LinkedList is already sorted. Time Complexities and Comparison with Binary Heap peek() push() pop() Linked List \\(O(1)\\) \\(O(n)\\) \\(O(1)\\) Binary Heap \\(O(1)\\) \\(O(lgn)\\) \\(O(lgn)\\) Sorting in Linear Time \u00b6 How Fast Can We Sort? \u00b6 The algorithms we have seen so far: Based on comparison of elements We only care about the relative ordering between the elements (not the actual values) The smallest worst-case runtime we have seen so far: \\(O(nlgn)\\) Is \\(O(nlgn)\\) the best we can do? Comparison sorts: Only use comparisons to determine the relative order of elements. Decision Trees for Comparison Sorts \u00b6 Represent a sorting algorithm abstractly in terms of a decision tree A binary tree that represents the comparisons between elements in the sorting algorithm Control, data movement, and other aspects are ignored One decision tree corresponds to one sorting algorithm and one value of \\(n\\) ( input size ) Reminder: Insertion Sort Step-By-Step Description (1) \u00b6 Reminder: Insertion Sort Step-By-Step Description (2) \u00b6 Reminder: Insertion Sort Step-By-Step Description (3) \u00b6 Different Outcomes for Insertion Sort and n=3 \u00b6 Input : \\(<a_1,a_2,a_3>\\) Decision Tree for Insertion Sort and n=3 \u00b6 Decision Tree Model for Comparison Sorts \u00b6 Internal node \\((i:j)\\) : Comparison between elements \\(a_i\\) and \\(a_j\\) Leaf node: An output of the sorting algorithm Path from root to a leaf: The execution of the sorting algorithm for a given input All possible executions are captured by the decision tree All possible outcomes (permutations) are in the leaf nodes Decision Tree for Insertion Sort and n=3 \u00b6 Input: \\(<9, 4, 6>\\) Decision Tree Model \u00b6 A decision tree can model the execution of any comparison sort: One tree for each input size \\(n\\) View the algorithm as splitting whenever it compares two elements The tree contains the comparisons along all possible instruction traces The running time of the algorithm \\(=\\) the length of the path taken Worst case running time \\(=\\) height of the tree Counting Sort \u00b6 Lower Bound for Comparison Sorts \u00b6 Let \\(n\\) be the number of elements in the input array. What is the \\(min\\) number of leaves in the decision tree? \\(n!\\) (because there are n! permutations of the input array, and all possible outputs must be captured in the leaves) What is the max number of leaves in a binary tree of height \\(h\\) ? \\(\\Longrightarrow\\) \\(2^h\\) So, we must have: $$ 2^h \\geq n! $$ Lower Bound for Decision Tree Sorting \u00b6 Theorem: Any comparison sort algorithm requires \\(\\Omega(nlgn)\\) comparisons in the worst case. Proof: We\u2019ll prove that any decision tree corresponding to a comparison sort algorithm must have height \\(\\Omega(nlgn)\\) \\[ \\begin{align*} 2^h & \\geq n! \\\\ h & \\geq lg(n!) \\\\ & \\geq lg((n/e)^n) (Stirling Approximation) \\\\ & = nlgn - nlge \\\\ & = \\Omega(nlgn) \\end{align*} \\] Lower Bound for Decision Tree Sorting \u00b6 Corollary: Heapsort and merge sort are asymptotically optimal comparison sorts. Proof: The \\(O(nlgn)\\) upper bounds on the runtimes for heapsort and merge sort match the \\(\\Omega(nlgn)\\) worst-case lower bound from the previous theorem. Sorting in Linear Time \u00b6 Counting sort: No comparisons between elements Input: \\(A[1 \\dots n]\\) , where \\(A[j] \\in \\{1, 2,\\dots, k\\}\\) Output: \\(B[1 \\dots n]\\) , sorted Auxiliary storage: \\(C[1 \\dots k]\\) Counting Sort-1 \u00b6 Counting Sort-2 \u00b6 Step 1: Initialize all counts to 0 Counting Sort-3 \u00b6 Step 2: Count the number of occurrences of each value in the input array Counting Sort-4 \u00b6 Step 3: Compute the number of elements less than or equal to each value Counting Sort-5 \u00b6 Step 4: Populate the output array There are \\(C[3] = 3\\) elements that are \\(\\leq 3\\) Counting Sort-6 \u00b6 Step 4: Populate the output array There are \\(C[4]=5\\) elements that are \\(\\leq 4\\) Counting Sort-7 \u00b6 Step 4: Populate the output array There are \\(C[3]=2\\) elements that are \\(\\leq 3\\) Counting Sort-8 \u00b6 Step 4: Populate the output array There are \\(C[1]=1\\) elements that are \\(\\leq 1\\) Counting Sort-9 \u00b6 Step 4: Populate the output array There are \\(C[4]=4\\) elements that are \\(\\leq 4\\) Counting Sort: Runtime Analysis \u00b6 Total Runtime: \\(\\Theta(n+k)\\) \\(n\\) : size of the input array \\(k\\) : the range of input values Counting Sort: Runtime \u00b6 Runtime is \\(\\Theta(n+k)\\) If \\(k=O(n)\\) , then counting sort takes \\(\\Theta(n)\\) Question: We proved a lower bound of \\(\\Theta(nlgn)\\) before! Where is the fallacy? Answer: \\(\\Theta(nlgn)\\) lower bound is for comparison-based sorting Counting sort is not a comparison sort In fact, not a single comparison between elements occurs! Stable Sorting \u00b6 Counting sort is a stable sort: It preserves the input order among equal elements. i.e. The numbers with the same value appear in the output array in the same order as they do in the input array. Note : Which other sorting algorithms have this property? Radix Sort \u00b6 Origin: Herman Hollerith\u2019s card-sorting machine for the 1890 US Census. Basic idea: Digit-by-digit sorting Two variations: Sort from MSD to LSD (bad idea) Sort from LSD to MSD (good idea) ( LSD/MSD: Least/most significant digit ) Herman Hollerith (1860-1929) \u00b6 The 1880 U.S. Census took almost 10 years to process. While a lecturer at MIT, Hollerith prototyped punched-card technology . His machines, including a card sorter , allowed the 1890 census total to be reported in 6 weeks . He founded the Tabulating Machine Company in 1911, which merged with other companies in 1924 to form International Business Machines(IBM) . Hollerith Punched Card \u00b6 Punched card: A piece of stiff paper that contains digital information represented by the presence or absence of holes. 12 rows and 24 columns coded for age, state of residency, gender, etc. Modern IBM card \u00b6 One character per column So, that\u2019s why text windows have 80 columns! for more samples visit https://en.wikipedia.org/wiki/Punched_card Hollerith Tabulating Machine and Sorter \u00b6 Mechanically sorts the cards based on the hole locations. Sorting performed for one column at a time Human operator needed to load/retrieve/move cards at each stage Hollerith\u2019s MSD-First Radix Sort \u00b6 Sort starting from the most significant digit (MSD) Then, sort each of the resulting bins recursively At the end, combine the decks in order Hollerith\u2019s MSD-First Radix Sort \u00b6 To sort a subset of cards recursively: All the other cards need to be removed from the machine, because the machine can handle only one sorting problem at a time. The human operator needs to keep track of the intermediate card piles Hollerith\u2019s MSD-First Radix Sort \u00b6 MSD-first sorting may require: very large number of sorting passes very large number of intermediate card piles to maintain S(d): \\(\\#\\) of passes needed to sort d-digit numbers (worst-case) Recurrence: \\(S(d)=10S(d-1)+1\\) with \\(S(1)=1\\) Reminder: Recursive call made to each subset with the same most significant digit(MSD) Hollerith\u2019s MSD-First Radix Sort \u00b6 Recurrence: \\(S(d)=10S(d-1)+1\\) \\[ \\begin{align*} S(d) &= 10 S(d-1) + 1 \\\\ & = 10 \\bigg(10 S(d-2) + 1 \\bigg) + 1 \\\\ & = 10 \\Big(10 \\bigg(10 S(d-3) + 1\\bigg) + 1 \\Big) + 1 \\\\ & = 10i S(d-i) + 10i-1 + 10i-2 + \\dots + 101 + 100 \\\\ &=\\sum \\limits_{i=0}^{d-1}10^i \\end{align*} \\] Iteration terminates when \\(i = d-1\\) with \\(S(d-(d-1)) = S(1) = 1\\) Hollerith\u2019s MSD-First Radix Sort \u00b6 Recurrence: \\(S(d)=10S(d-1)+1\\) \\[ \\begin{align*} S(d) &=\\sum \\limits_{i=0}^{d-1}10^i \\\\ & = \\frac{10^d-1}{10-1} \\\\ & = \\frac{1}{9}(10^d-1)\\\\ & \\Downarrow \\\\ S(d)&=\\frac{1}{9}(10^d-1) \\end{align*} \\] Hollerith\u2019s MSD-First Radix Sort \u00b6 \\(P(d)\\) : \\(\\#\\) of intermediate card piles maintained (worst-case) Reminder: Each routing pass generates 9 intermediate piles except the sorting passes on least significant digits (LSDs) There are \\(10^{d-1}\\) sorting calls to LSDs \\[ \\begin{align*} P(d) &= 9(S(d)\u201310^{d-1}) \\\\ &= 9\\frac{(10^{d\u20131})}{9\u2013 10^{d-1}} \\\\ &= (10^{d\u20131}\u20139 * 10^{d-1}) \\\\ &= 10^{d-1} - 1 \\end{align*} \\] Hollerith\u2019s MSD-First Radix Sort \u00b6 \\[ \\begin{align*} P(d) &= 10^{d-1} - 1 \\end{align*} \\] Alternative solution: Solve the recurrence \\[ \\begin{align*} P(d) &= 10P(d-1)+9 \\\\ P(1) &= 0 \\\\ \\end{align*} \\] Hollerith\u2019s MSD-First Radix Sort \u00b6 Example: To sort \\(3\\) digit numbers, in the worst case: \\(S(d) = (1/9) (103-1) = 111\\) sorting passes needed \\(P(d) = 10d-1-1 = 99\\) intermediate card piles generated MSD-first approach has more recursive calls and intermediate storage requirement Expensive for a tabulating machine to sort punched cards Overhead of recursive calls in a modern computer section{ font-size: 25px; } LSD-First Radix Sort \u00b6 Least significant digit ( LSD )-first radix sort seems to be a folk invention originated by machine operators. It is the counter-intuitive, but the better algorithm. Basic Algorithm: Sort numbers on their LSD first ( Stable Sorting Needed ) Combine the cards into a single deck in order Continue this sorting process for the other digits from the LSD to MSD Requires only \\(d\\) sorting passes No intermediate card pile generated LSD-first Radix Sort Example \u00b6 Correctness of Radix Sort (LSD-first) \u00b6 Proof by induction: Base case: \\(d=1\\) is correct ( trivial ) Inductive hyp: Assume the first \\(d-1\\) digits are sorted correctly Prove that all \\(d\\) digits are sorted correctly after sorting digit \\(d\\) Two numbers that differ in digit \\(d\\) are correctly sorted ( e.g. 355 and 657 ) Two numbers equal in digit d are put in the same order as the input ( correct order ) Radix Sort Runtime \u00b6 Use counting-sort to sort each digit Reminder: Counting sort complexity: \\(\\Theta(n+k)\\) \\(n\\) : size of input array \\(k\\) : the range of the values Radix sort runtime: \\(\\Theta(d(n+k))\\) \\(d\\) : \\(\\#\\) of digits How to choose the \\(d\\) and \\(k\\) ? Radix Sort: Runtime \u2013 Example 1 \u00b6 We have flexibility in choosing \\(d\\) and \\(k\\) Assume we are trying to sort 32-bit words We can define each digit to be 4 bits Then, the range for each digit \\(k=2^4=16\\) So, counting sort will take \\(\\Theta(n+16)\\) The number of digits \\(d =32/4=8\\) Radix sort runtime: \\(\\Theta(8(n+16)) = \\Theta(n)\\) \\(\\overbrace{[4bits|4bits|4bits|4bits|4bits|4bits|4bits|4bits]}^{\\text{32-bits}}\\) Radix Sort: Runtime \u2013 Example 2 \u00b6 We have flexibility in choosing \\(d\\) and \\(k\\) Assume we are trying to sort 32-bit words Or, we can define each digit to be 8 bits Then, the range for each digit \\(k = 2^8 = 256\\) So, counting sort will take \\(\\Theta(n+256)\\) The number of digits \\(d = 32/8 = 4\\) Radix sort runtime: \\(\\Theta(4(n+256)) = \\Theta(n)\\) \\(\\overbrace{[8bits|8bits|8bits|8bits]}^{\\text{32-bits}}\\) section{ font-size: 25px; } Radix Sort: Runtime \u00b6 Assume we are trying to sort \\(b\\) -bit words Define each digit to be \\(r\\) bits Then, the range for each digit \\(k = 2^r\\) So, counting sort will take \\(\\Theta(n+2^r)\\) The number of digits \\(d = b/r\\) Radix sort runtime: \\[ \\begin{align*} T(n,b)&=\\Theta \\bigg( \\frac{b}{r}(n+2^r) \\bigg) \\end{align*} \\] \\(\\overbrace{[rbits|rbits|rbits|rbits]}^{b/r \\text{ bits}}\\) Radix Sort: Runtime Analysis \u00b6 \\[ \\begin{align*} T(n,b)&=\\Theta \\bigg( \\frac{b}{r}(n+2^r) \\bigg) \\end{align*} \\] Minimize \\(T(n,b)\\) by differentiating and setting to \\(0\\) Or, intuitively: We want to balance the terms \\((b/r)\\) and \\((n + 2^r)\\) Choose \\(r \\approx lgn\\) If we choose \\(r << lgn \\Longrightarrow (n + 2^r)\\) term doesn\u2019t improve If we choose \\(r >> lgn \\Longrightarrow (n + 2^r)\\) increases exponentially Radix Sort: Runtime Analysis \u00b6 \\[ \\begin{align*} T(n,b)&=\\Theta \\bigg( \\frac{b}{r}(n+2^r) \\bigg) \\end{align*} \\] \\[ \\begin{align*} \\text{Choose } r=lgn \\Longrightarrow T(n,b)=\\Theta(bn/lgn) \\end{align*} \\] For numbers in the range from \\(0\\) to \\(n^d \u2013 1\\) , we have: The number of bits \\(b = lg(nd ) = d lgn\\) Radix sort runs in \\(\\Theta(dn)\\) Radix Sort: Conclusions \u00b6 \\[ \\begin{align*} \\text{Choose } r=lgn \\Longrightarrow T(n,b)=\\Theta(bn/lgn) \\end{align*} \\] Example: Compare radix sort with merge sort/heapsort \\(1\\) million ( \\(2^{20}\\) ), \\(32\\) -bit numbers \\((n = 2^{20}, b = 32)\\) Radix sort: \\(\\lfloor 32/20 \\rfloor = 2\\) passes Merge sort/heap sort: \\(lgn = 20\\) passes Downsides: Radix sort has little locality of reference (more cache misses) The version that uses counting sort is not in-place On modern processors, a well-tuned quicksort implementation typically runs faster. References \u00b6 Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) Insertion Sort - GeeksforGeeks Priority Queue Using Linked List - GeeksforGeeks Priority Queue Using Linked List - JavatPoint NIST Dictionary of Algorithms and Data Structures NIST - Dictionary of Algorithms and Data Structures \\(-End-Of-Week-4-Course-Module-\\)","title":"Week-4 (Heap/Heap Sort)"},{"location":"week-4/ce100-week-4-heap/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-4/ce100-week-4-heap/#week-4-heapheap-sort","text":"","title":"Week-4 (Heap/Heap Sort)"},{"location":"week-4/ce100-week-4-heap/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-4/ce100-week-4-heap/#heapheap-sort","text":"","title":"Heap/Heap Sort"},{"location":"week-4/ce100-week-4-heap/#outline-1","text":"Heaps Max / Min Heap Heap Data Structure Heapify Iterative Recursive","title":"Outline (1)"},{"location":"week-4/ce100-week-4-heap/#outline-2","text":"Extract-Max Build Heap","title":"Outline (2)"},{"location":"week-4/ce100-week-4-heap/#outline-3","text":"Heap Sort Priority Queues Linked Lists Radix Sort Counting Sort","title":"Outline (3)"},{"location":"week-4/ce100-week-4-heap/#heapsort","text":"Worst-case runtime: \\(O(nlgn)\\) Sorts in-place Uses a special data structure (heap) to manage information during execution of the algorithm Another design paradigm","title":"Heapsort"},{"location":"week-4/ce100-week-4-heap/#heap-data-structure-1","text":"Nearly complete binary tree Completely filled on all levels except possibly the lowest level","title":"Heap Data Structure (1)"},{"location":"week-4/ce100-week-4-heap/#heap-data-structure-2","text":"Height of node i: Length of the longest simple downward path from i to a leaf Height of the tree: height of the root","title":"Heap Data Structure (2)"},{"location":"week-4/ce100-week-4-heap/#heap-data-structures-3","text":"Depth of node i: Length of the simple downward path from the root to node i","title":"Heap Data Structures (3)"},{"location":"week-4/ce100-week-4-heap/#heap-property-min-heap","text":"The smallest element in any subtree is the root element in a min-heap Min heap: For every node i other than root , \\(A[parent(i)] \\leq A[i]\\) Parent node is always smaller than the child nodes","title":"Heap Property: Min-Heap"},{"location":"week-4/ce100-week-4-heap/#heap-property-max-heap","text":"The largest element in any subtree is the root element in a max-heap We will focus on max-heaps Max heap: For every node i other than root , \\(A[parent(i)] \u2265 A[i]\\) Parent node is always larger than the child nodes","title":"Heap Property: Max-Heap"},{"location":"week-4/ce100-week-4-heap/#heap-data-structures-4","text":"","title":"Heap Data Structures (4)"},{"location":"week-4/ce100-week-4-heap/#heap-data-structures-5","text":"Computing left child, right child, and parent indices very fast left(i) = 2i \\(\\Longrightarrow\\) binary left shift right(i) = 2i+1 \\(\\Longrightarrow\\) binary left shift, then set the lowest bit to 1 parent(i) = floor(i/2) \\(\\Longrightarrow\\) right shift in binary \\(A[1]\\) is always the root element Array \\(A\\) has two attributes: length(A): The number of elements in \\(A\\) n = heap-size(A): The number elements in \\(heap\\) \\(n \\leq length(A)\\)","title":"Heap Data Structures (5)"},{"location":"week-4/ce100-week-4-heap/#heap-operations-extract-max-1","text":"EXTRACT - MAX ( A , n ) max = A [ 1 ] A [ 1 ] = A [ n ] n = n - 1 HEAPIFY ( A , 1 , n ) return max","title":"Heap Operations : EXTRACT-MAX (1)"},{"location":"week-4/ce100-week-4-heap/#heap-operations-extract-max-2","text":"Return the max element,and reorganize the heap to maintain heap property","title":"Heap Operations : EXTRACT-MAX (2)"},{"location":"week-4/ce100-week-4-heap/#heap-operations-heapify-1","text":"","title":"Heap Operations: HEAPIFY (1)"},{"location":"week-4/ce100-week-4-heap/#heap-operations-heapify-2","text":"Maintaining heap property: Subtrees rooted at \\(left[i]\\) and \\(right[i]\\) are already heaps. But, \\(A[i]\\) may violate the heap property (i.e., may be smaller than its children) Idea: Float down the value at \\(A[i]\\) in the heap so that subtree rooted at \\(i\\) becomes a heap.","title":"Heap Operations: HEAPIFY (2)"},{"location":"week-4/ce100-week-4-heap/#heap-operations-heapify-2_1","text":"HEAPIFY ( A , i , n ) largest = i if 2i <= n and A [ 2i ] > A [ i ] then largest = 2i ; endif if 2i+1 <= n and A [ 2i+1 ] > A [ largest ] then largest = 2i+1 ; endif if largest != i then exchange A [ i ] with A [ largest ]; HEAPIFY ( A , largest , n ); endif","title":"Heap Operations: HEAPIFY (2)"},{"location":"week-4/ce100-week-4-heap/#heap-operations-heapify-3","text":"","title":"Heap Operations: HEAPIFY (3)"},{"location":"week-4/ce100-week-4-heap/#heap-operations-heapify-4","text":"","title":"Heap Operations: HEAPIFY (4)"},{"location":"week-4/ce100-week-4-heap/#heap-operations-heapify-5","text":"","title":"Heap Operations: HEAPIFY (5)"},{"location":"week-4/ce100-week-4-heap/#heap-operations-heapify-6","text":"","title":"Heap Operations: HEAPIFY (6)"},{"location":"week-4/ce100-week-4-heap/#heap-operations-heapify-7","text":"","title":"Heap Operations: HEAPIFY (7)"},{"location":"week-4/ce100-week-4-heap/#heap-operations-heapify-8","text":"","title":"Heap Operations: HEAPIFY (8)"},{"location":"week-4/ce100-week-4-heap/#intuitive-analysis-of-heapify","text":"Consider \\(HEAPIFY(A, i, n)\\) let \\(h(i)\\) be the height of node \\(i\\) at most \\(h(i)\\) recursion levels Constant work at each level: \\(\\Theta(1)\\) Therefore \\(T(i)=O(h(i))\\) Heap is almost-complete binary tree \\(h(n)=O(lgn)\\) Thus \\(T(n)=O(lgn)\\)","title":"Intuitive Analysis of HEAPIFY"},{"location":"week-4/ce100-week-4-heap/#formal-analysis-of-heapify","text":"What is the recurrence? Depends on the size of the subtree on which recursive call is made In the next, we try to compute an upper bound for this subtree .","title":"Formal Analysis of HEAPIFY"},{"location":"week-4/ce100-week-4-heap/#reminder-binary-trees","text":"For a complete binary tree: \\(\\#\\) of nodes at depth \\(d\\) : \\(2^d\\) \\(\\#\\) of nodes with depths less than \\(d\\) : \\(2^d-1\\)","title":"Reminder: Binary trees"},{"location":"week-4/ce100-week-4-heap/#formal-analysis-of-heapify-1","text":"Worst case occurs when last row of the subtree \\(S_i\\) rooted at node \\(i\\) is half full \\(T(n) \\leq T(|S_{L(i)}|) + \\Theta(1)\\) \\(S_{L(i)}\\) and \\(S_{R(i)}\\) are complete binary trees of heights \\(h(i)-1\\) and \\(h(i)-2\\) , respectively","title":"Formal Analysis of HEAPIFY (1)"},{"location":"week-4/ce100-week-4-heap/#formal-analysis-of-heapify-2","text":"Let \\(m\\) be the number of leaf nodes in \\(S_{L(i)}\\) \\(|S_{L(i)}|=\\overbrace{m}^{ext.}+\\overbrace{(m\u20131)}^{int.}=2m\u20131\\) \\(|S_{R(i)}|=\\overbrace{\\frac{m}{2}}^{ext.}+\\overbrace{(\\frac{m}{2}\u20131)}^{int.}=m\u20131\\) \\(|S_{L(i)}|+|S_{R(i)}|+1=n\\)","title":"Formal Analysis of HEAPIFY (2)"},{"location":"week-4/ce100-week-4-heap/#formal-analysis-of-heapify-2_1","text":"\\[ \\begin{align*} (2m\u20131)+(m\u20131)+1 &=n \\\\ m &= (n+1)/3 \\\\ |S_{L(i)}| &= 2m \u2013 1 \\\\ &=2(n+1)/3 \u2013 1 \\\\ &=(2n/3+2/3) \u20131 \\\\ &=\\frac{2n}{3}-\\frac{1}{3} \\leq \\frac{2n}{3} \\\\ T(n) & \\leq T(2n/3) + \\Theta(1) \\\\ T(n) &= O(lgn) \\end{align*} \\] By CASE-2 of Master Theorem \\(\\Longrightarrow\\) \\(T(n)=\\Theta(n^{log_b^a}lgn)\\)","title":"Formal Analysis of HEAPIFY (2)"},{"location":"week-4/ce100-week-4-heap/#formal-analysis-of-heapify-2_2","text":"Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(1)\\) i.e., \\(f(n)\\) and \\(n^{log_b^a}\\) grow at similar rates Solution: \\(T(n)=\\Theta(n^{log_b^a}lgn)\\) \\(T(n) \\leq T(2n/3) + \\Theta(1)\\) (drop constants.) \\(T(n) \\leq \\Theta(n^{log_3^1}lgn)\\) \\(T(n) \\leq \\Theta(n^0lgn)\\) \\(T(n) = O(lgn)\\)","title":"Formal Analysis of HEAPIFY (2)"},{"location":"week-4/ce100-week-4-heap/#heapify-efficiency-issues","text":"Recursion vs Iteration: In the absence of tail recursion, iterative version is in general more efficient because of the pop/push operations to/from stack at each level of recursion .","title":"HEAPIFY: Efficiency Issues"},{"location":"week-4/ce100-week-4-heap/#heap-operations-heapify-1_1","text":"Recursive HEAPIFY ( A , i , n ) largest = i if 2i <= n and A [ 2i ] > A [ i ] then largest = 2i if 2i+1 <= n and A [ 2i+1 ] > A [ largest ] then largest = 2i+1 if largest != i then exchange A [ i ] with A [ largest ] HEAPIFY ( A , largest , n )","title":"Heap Operations: HEAPIFY (1)"},{"location":"week-4/ce100-week-4-heap/#heap-operations-heapify-2_2","text":"Iterative HEAPIFY ( A , i , n ) j = i while ( true ) do largest = j if 2 j <= n and A [ 2 j ] > A [ j ] then largest = 2 j if 2 j +1 <= n and A [ 2 j +1 ] > A [ largest ] then largest = 2 j +1 if largest != j then exchange A [ j ] with A [ largest ] j = largest else return","title":"Heap Operations: HEAPIFY (2)"},{"location":"week-4/ce100-week-4-heap/#heap-operations-heapify-3_1","text":"","title":"Heap Operations: HEAPIFY (3)"},{"location":"week-4/ce100-week-4-heap/#heap-operations-building-heap","text":"Given an arbitrary array, how to build a heap from scratch? Basic idea: Call \\(HEAPIFY\\) on each node bottom up Start from the leaves (which trivially satisfy the heap property) Process nodes in bottom up order. When \\(HEAPIFY\\) is called on node \\(i\\) , the subtrees connected to the \\(left\\) and \\(right\\) subtrees already satisfy the heap property.","title":"Heap Operations: Building Heap"},{"location":"week-4/ce100-week-4-heap/#storage-of-the-leaves-lemma","text":"Lemma: The last \\(\\lceil \\frac{n}{2} \\rceil\\) nodes of a heap are all leaves.","title":"Storage of the leaves (Lemma)"},{"location":"week-4/ce100-week-4-heap/#storage-of-the-leaves-proof-of-lemma-1","text":"Lemma: last \\(\\lceil n/2 \\rceil\\) nodes of a heap are all leaves Proof : \\(m=2^{d-1}\\) : \\(\\#\\) nodes at level \\(d-1\\) \\(f\\) : \\(\\#\\) nodes at level \\(d\\) (last level) \\(\\#\\) of nodes with depth \\(d-1\\) : \\(m\\) \\(\\#\\) of nodes with depth \\(<d-1\\) : \\(m-1\\) \\(\\#\\) of nodes with depth \\(d\\) : \\(f\\) Total \\(\\#\\) of nodes : \\(n=f+2m-1\\)","title":"Storage of the leaves (Proof of Lemma) (1)"},{"location":"week-4/ce100-week-4-heap/#storage-of-the-leaves-proof-of-lemma-2","text":"Total \\(\\#\\) of nodes : \\(f=n-2m+1\\) \\[ \\begin{align*} \\text{\\# of leaves: }&=f+m-\\lceil f/2 \\rceil \\\\ &= m+\\lfloor f/2 \\rfloor \\\\ &= m+\\lfloor (n-2m+1)/2 \\rfloor \\\\ &= \\lfloor (n+1)/2 \\rfloor \\\\ &= \\lceil n/2 \\rceil \\end{align*} \\] Proof is Completed","title":"Storage of the leaves (Proof of Lemma) (2)"},{"location":"week-4/ce100-week-4-heap/#heap-operations-building-heap_1","text":"BUILD - HEAP ( A , n ) for i = ceil ( n / 2 ) downto 1 do HEAPIFY ( A , i , n ) Reminder: The last \\(\\lceil n/2 \\rceil\\) nodes of a heap are all leaves , which trivially satisfy the heap property","title":"Heap Operations: Building Heap"},{"location":"week-4/ce100-week-4-heap/#build-heap-example-step-1","text":"","title":"Build-Heap Example (Step-1)"},{"location":"week-4/ce100-week-4-heap/#build-heap-example-step-2","text":"","title":"Build-Heap Example (Step-2)"},{"location":"week-4/ce100-week-4-heap/#build-heap-example-step-3","text":"","title":"Build-Heap Example (Step-3)"},{"location":"week-4/ce100-week-4-heap/#build-heap-example-step-4","text":"","title":"Build-Heap Example (Step-4)"},{"location":"week-4/ce100-week-4-heap/#build-heap-example-step-5","text":"","title":"Build-Heap Example (Step-5)"},{"location":"week-4/ce100-week-4-heap/#build-heap-example-step-6","text":"","title":"Build-Heap Example (Step-6)"},{"location":"week-4/ce100-week-4-heap/#build-heap-example-step-7","text":"","title":"Build-Heap Example (Step-7)"},{"location":"week-4/ce100-week-4-heap/#build-heap-example-step-8","text":"","title":"Build-Heap Example (Step-8)"},{"location":"week-4/ce100-week-4-heap/#build-heap-example-step-9","text":"","title":"Build-Heap Example (Step-9)"},{"location":"week-4/ce100-week-4-heap/#build-heap-runtime-analysis","text":"Simple analysis: \\(O(n)\\) calls to \\(HEAPIFY\\) , each of which takes \\(O(lgn)\\) time \\(O(nlgn)\\) \\(\\Longrightarrow\\) loose bound In general, a good approach: Start by proving an easy bound Then, try to tighten it Is there a tighter bound? section{ font-size: 25px; }","title":"Build-Heap: Runtime Analysis"},{"location":"week-4/ce100-week-4-heap/#build-heap-tighter-running-time-analysis","text":"If the heap is complete binary tree then \\(h_{\\ell} = d \u2013 \\ell\\) Otherwise, nodes at a given level do not all have the same height, But we have \\(d \u2013 \\ell \u2013 1 \\leq h_{\\ell} \\leq d \u2013 \\ell\\) section{ font-size: 25px; }","title":"Build-Heap: Tighter Running Time Analysis"},{"location":"week-4/ce100-week-4-heap/#build-heap-tighter-running-time-analysis_1","text":"Assume that all nodes at level \\(\\ell= d \u2013 1\\) are processed \\[ \\begin{align*} T(n) &=\\sum \\limits_{\\ell=0}^{d-1}n_{\\ell}O(h_{\\ell})=O(\\sum \\limits_{\\ell=0}^{d-1}n_{\\ell}h_{\\ell}) \\begin{cases} n_{\\ell}=2^{\\ell} = \\# \\text{ of nodes at level }\\ell \\\\ h_{\\ell}=\\text{height of nodes at level } \\ell \\end{cases} \\\\ \\therefore T(n) &= O \\bigg( \\sum \\limits_{\\ell=0}^{d-1}2^{\\ell}(d-\\ell) \\bigg) \\\\ \\text{Let } & h=d-\\ell \\Longrightarrow \\ell = d-h \\text{ change of variables} \\\\ T(n) &= O\\bigg(\\sum \\limits_{h=1}^{d}h2^{d-h} \\bigg)=O\\bigg(\\sum \\limits_{h=1}^{d}h \\frac{2^d}{2^h} \\bigg) = O\\bigg(2^d\\sum \\limits_{h=1}^{d}h (1/2)^h\\bigg) \\\\ \\text{ but } & 2^d = \\Theta(n) \\Longrightarrow O\\bigg(n\\sum \\limits_{h=1}^{d}h (1/2)^h \\bigg) \\end{align*} \\] section{ font-size: 25px; }","title":"Build-Heap: Tighter Running Time Analysis"},{"location":"week-4/ce100-week-4-heap/#build-heap-tighter-running-time-analysis_2","text":"\\[ \\sum \\limits_{h=1}^{d}h(1/2)^h \\leq \\sum \\limits_{h=0}^{d}h(1/2)^h \\leq \\sum \\limits_{h=0}^{\\infty}h(1/2)^h \\] recall infinite decreasing geometric series \\[ \\sum \\limits_{k=0}^{\\infty} x^k = \\frac{1}{1-x} \\text{ where } |x|<1 \\] differentiate both sides \\[ \\sum \\limits_{k=0}^{\\infty}kx^{k-1} = \\frac{1}{(1-x)^2} \\]","title":"Build-Heap: Tighter Running Time Analysis"},{"location":"week-4/ce100-week-4-heap/#build-heap-tighter-running-time-analysis_3","text":"\\[ \\sum \\limits_{k=0}^{\\infty}kx^{k-1} = \\frac{1}{(1-x)^2} \\] then, multiply both sides by \\(x\\) \\[ \\sum \\limits_{k=0}^{\\infty}kx^k = \\frac{x}{(1-x)^2} \\] in our case: \\(x = 1/2\\) and \\(k = h\\) \\[ \\therefore \\sum \\limits_{h=0}^{\\infty}h(1/2)^h = \\frac{1/2}{(1-(1/2))^2}=2=O(1) \\\\ \\therefore T(n)=O(n\\sum \\limits_{h=1}^{d}h(1/2)^h)=O(n) \\]","title":"Build-Heap: Tighter Running Time Analysis"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-steps","text":"(1) Build a heap on array \\(A[1 \\dots n]\\) by calling \\(BUILD-HEAP(A, n)\\) (2) The largest element is stored at the root \\(A[1]\\) Put it into its correct final position \\(A[n]\\) by \\(A[1] \\longleftrightarrow A[n]\\) (3) Discard node \\(n\\) from the heap (4) Subtrees \\((S2 \\& S3)\\) rooted at children of root remain as heaps, but the new root element may violate the heap property. Make \\(A[1 \\dots n-1]\\) a heap by calling \\(HEAPIFY(A,1,n-1)\\) (5) \\(n \\leftarrow n-1\\) (6) Repeat steps (2-4) until \\(n=2\\)","title":"Heapsort Algorithm Steps"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-1","text":"","title":"Heapsort Algorithm Example (Step-1)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-2","text":"","title":"Heapsort Algorithm Example (Step-2)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-3","text":"","title":"Heapsort Algorithm Example (Step-3)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-4","text":"","title":"Heapsort Algorithm Example (Step-4)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-5","text":"","title":"Heapsort Algorithm Example (Step-5)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-6","text":"","title":"Heapsort Algorithm Example (Step-6)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-7","text":"","title":"Heapsort Algorithm Example (Step-7)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-8","text":"","title":"Heapsort Algorithm Example (Step-8)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-9","text":"","title":"Heapsort Algorithm Example (Step-9)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-10","text":"","title":"Heapsort Algorithm Example (Step-10)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-11","text":"","title":"Heapsort Algorithm Example (Step-11)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-12","text":"","title":"Heapsort Algorithm Example (Step-12)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-13","text":"","title":"Heapsort Algorithm Example (Step-13)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-14","text":"","title":"Heapsort Algorithm Example (Step-14)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-15","text":"","title":"Heapsort Algorithm Example (Step-15)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-16","text":"","title":"Heapsort Algorithm Example (Step-16)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-17","text":"","title":"Heapsort Algorithm Example (Step-17)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-18","text":"","title":"Heapsort Algorithm Example (Step-18)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-19","text":"","title":"Heapsort Algorithm Example (Step-19)"},{"location":"week-4/ce100-week-4-heap/#heapsort-algorithm-runtime-analysis","text":"\\[ \\begin{align*} T(n) &= \\Theta(n)+\\sum \\limits_{i=2}^{n}O(lgi) \\\\ &= \\Theta(n)+O\\bigg( \\sum \\limits_{i=2}^{n}O(lgn) \\bigg) \\\\ &= O(nlgn) \\end{align*} \\]","title":"Heapsort Algorithm: Runtime Analysis"},{"location":"week-4/ce100-week-4-heap/#heapsort-notes","text":"Heapsort is a very good algorithm but, a good implementation of quicksort always beats heapsort in practice However, heap data structure has many popular applications, and it can be efficiently used for implementing priority queues","title":"Heapsort - Notes"},{"location":"week-4/ce100-week-4-heap/#data-structures-for-dynamic-sets","text":"Consider sets of records having key and satellite data","title":"Data structures for Dynamic Sets"},{"location":"week-4/ce100-week-4-heap/#operations-on-dynamic-sets","text":"Queries: Simply return info; \\(MAX(S) / MIN(S):\\) (Query) return \\(x \\in S\\) with the largest/smallest \\(key\\) \\(SEARCH(S, k):\\) (Query) return \\(x \\in S\\) with \\(key[x]= k\\) \\(SUCCESSOR(S, x) / PREDECESSOR(S, x):\\) (Query) return \\(y \\in S\\) which is the next larger/smaller element after \\(x\\) Modifying operations: Change the set \\(INSERT(S, x):\\) (Modifying) \\(S \\leftarrow S \\cup \\{x\\}\\) \\(DELETE(S, x):\\) (Modifying) \\(S \\leftarrow S - \\{x\\}\\) \\(\\text{EXTRACT-MAX}(S) / \\text{EXTRACT-MIN}(S):\\) (Modifying) return and delete \\(x \\in S\\) with the largest/smallest \\(key\\) Different data structures support/optimize different operations","title":"Operations on Dynamic Sets"},{"location":"week-4/ce100-week-4-heap/#priority-queues-pq","text":"Supports \\(INSERT\\) \\(MAX / MIN\\) \\(\\text{EXTRACT-MAX} / \\text{EXTRACT-MIN}\\)","title":"Priority Queues (PQ)"},{"location":"week-4/ce100-week-4-heap/#priority-queues-pq_1","text":"One application: Schedule jobs on a shared resource PQ keeps track of jobs and their relative priorities When a job is finished or interrupted, highest priority job is selected from those pending using \\(\\text{EXTRACT-MAX}\\) A new job can be added at any time using \\(INSERT\\)","title":"Priority Queues (PQ)"},{"location":"week-4/ce100-week-4-heap/#priority-queues-pq_2","text":"Another application: Event-driven simulation Events to be simulated are the items in the PQ Each event is associated with a time of occurrence which serves as a \\(key\\) Simulation of an event can cause other events to be simulated in the future Use \\(\\text{EXTRACT-MIN}\\) at each step to choose the next event to simulate As new events are produced insert them into the PQ using \\(INSERT\\)","title":"Priority Queues (PQ)"},{"location":"week-4/ce100-week-4-heap/#implementation-of-priority-queue","text":"Sorted linked list: Simplest implementation \\(INSERT\\) \\(O(n)\\) time Scan the list to find place and splice in the new item \\(\\text{EXTRACT-MAX}\\) \\(O(1)\\) time Take the first element Fast extraction but slow insertion.","title":"Implementation of Priority Queue"},{"location":"week-4/ce100-week-4-heap/#implementation-of-priority-queue_1","text":"Unsorted linked list: Simplest implementation \\(INSERT\\) \\(O(1)\\) time Put the new item at front \\(\\text{EXTRACT-MAX}\\) \\(O(n)\\) time Scan the whole list Fast insertion but slow extraction. Sorted linked list is better on the average Sorted list: on the average, scans \\(n/2\\) element per insertion Unsorted list: always scans \\(n\\) element at each extraction","title":"Implementation of Priority Queue"},{"location":"week-4/ce100-week-4-heap/#heap-implementation-of-pq","text":"\\(INSERT\\) and \\(\\text{EXTRACT-MAX}\\) are both \\(O(lgn)\\) good compromise between fast insertion but slow extraction and vice versa \\(\\text{EXTRACT-MAX}\\) : already discussed \\(\\text{HEAP-EXTRACT-MAX}\\) \\(INSERT\\) : Insertion is like that of Insertion-Sort. HEAP - INSERT ( A , key , n ) n = n +1 i = n while i > 1 and A [ floor ( i / 2 )] < key do A [ i ] = A [ floor ( i / 2 )] i = floor ( i / 2 ) A [ i ] = key","title":"Heap Implementation of PQ"},{"location":"week-4/ce100-week-4-heap/#heap-implementation-of-pq_1","text":"Traverses \\(O(lgn)\\) nodes, as \\(HEAPIFY\\) does but makes fewer comparisons and assignments \\(HEAPIFY\\) : compares parent with both children \\(HEAP-INSERT\\) : with only one","title":"Heap Implementation of PQ"},{"location":"week-4/ce100-week-4-heap/#heap-insert-example-step-1","text":"","title":"HEAP-INSERT Example (Step-1)"},{"location":"week-4/ce100-week-4-heap/#heap-insert-example-step-2","text":"","title":"HEAP-INSERT Example (Step-2)"},{"location":"week-4/ce100-week-4-heap/#heap-insert-example-step-3","text":"","title":"HEAP-INSERT Example (Step-3)"},{"location":"week-4/ce100-week-4-heap/#heap-insert-example-step-4","text":"","title":"HEAP-INSERT Example (Step-4)"},{"location":"week-4/ce100-week-4-heap/#heap-insert-example-step-5","text":"","title":"HEAP-INSERT Example (Step-5)"},{"location":"week-4/ce100-week-4-heap/#heap-increase-key","text":"Key value of \\(i^{th}\\) element of heap is increased from \\(A[i]\\) to \\(key\\) HEAP - INCREASE - KEY ( A , i , key ) if key < A [ i ] then return error while i > 1 and A [ floor ( i / 2 )] < key do A [ i ] = A [ floor ( i / 2 )] i = floor ( i / 2 ) A [ i ] = key","title":"Heap Increase Key"},{"location":"week-4/ce100-week-4-heap/#heap-increase-key-example-step-1","text":"","title":"HEAP-INCREASE-KEY Example (Step-1)"},{"location":"week-4/ce100-week-4-heap/#heap-increase-key-example-step-2","text":"","title":"HEAP-INCREASE-KEY Example (Step-2)"},{"location":"week-4/ce100-week-4-heap/#heap-increase-key-example-step-3","text":"","title":"HEAP-INCREASE-KEY Example (Step-3)"},{"location":"week-4/ce100-week-4-heap/#heap-increase-key-example-step-4","text":"","title":"HEAP-INCREASE-KEY Example (Step-4)"},{"location":"week-4/ce100-week-4-heap/#heap-increase-key-example-step-5","text":"","title":"HEAP-INCREASE-KEY Example (Step-5)"},{"location":"week-4/ce100-week-4-heap/#heap-implementation-of-priority-queue-pq","text":"section{ font-size: 25px; }","title":"Heap Implementation of Priority Queue (PQ)"},{"location":"week-4/ce100-week-4-heap/#summary-max-heap","text":"Heapify(A, i) Works when both child subtrees of node i are heaps \" Floats down \" node i to satisfy the heap property Runtime: \\(O(lgn)\\) Max(A, n) Returns the max element of the heap (no modification) Runtime: \\(O(1)\\) Extract-Max(A, n) Returns and removes the max element of the heap Fills the gap in \\(A[1]\\) with \\(A[n]\\) , then calls Heapify(A,1) Runtime: \\(O(lgn)\\) section{ font-size: 25px; }","title":"Summary: Max Heap"},{"location":"week-4/ce100-week-4-heap/#summary-max-heap_1","text":"Build-Heap(A, n) Given an arbitrary array, builds a heap from scratch Runtime: \\(O(n)\\) Min(A, n) How to return the min element in a max-heap? Worst case runtime: \\(O(n)\\) because ~half of the heap elements are leaf nodes Instead, use a min-heap for efficient min operations Search(A, x) For an arbitrary \\(x\\) value, the worst-case runtime: \\(O(n)\\) Use a sorted array instead for efficient search operations","title":"Summary: Max Heap"},{"location":"week-4/ce100-week-4-heap/#summary-max-heap_2","text":"Increase-Key(A, i, x) Increase the key of node \\(i\\) (from \\(A[i]\\) to \\(x\\) ) \u201c Float up \u201d \\(x\\) until heap property is satisfied Runtime: \\(O(lgn)\\) Decrease-Key(A, i, x) Decrease the key of node \\(i\\) (from \\(A[i]\\) to \\(x\\) ) Call Heapify(A, i) Runtime: \\(O(lgn)\\)","title":"Summary: Max Heap"},{"location":"week-4/ce100-week-4-heap/#phone-operator-problem","text":"A phone operator answering \\(n\\) phones Each phone \\(i\\) has \\(x_i\\) people waiting in line for their calls to be answered. Phone operator needs to answer the phone with the largest number of people waiting in line. New calls come continuously, and some people hang up after waiting.","title":"Phone Operator Problem"},{"location":"week-4/ce100-week-4-heap/#phone-operator-solution","text":"Step 1: Define the following array: \\(A[i]\\) : the ith element in heap \\(A[i].id\\) : the index of the corresponding phone \\(A[i].key\\) : \\(\\#\\) of people waiting in line for phone with index \\(A[i].id\\)","title":"Phone Operator Solution"},{"location":"week-4/ce100-week-4-heap/#phone-operator-solution_1","text":"Step 2: \\(\\text{Build-Max-Heap}(A, n)\\) Execution: When the operator wants to answer a phone: \\(id = A[1].id\\) \\(\\text{Decrease-Key}(A, 1, A[1].key-1)\\) answer phone with index \\(id\\) When a new call comes in to phone i: \\(\\text{Increase-Key}(A, i, A[i].key+1)\\) When a call drops from phone i: \\(\\text{Decrease-Key}(A, i, A[i].key-1)\\)","title":"Phone Operator Solution"},{"location":"week-4/ce100-week-4-heap/#linked-lists","text":"Like arrays, Linked List is a linear data structure. Unlike arrays, linked list elements are not stored at a contiguous location; the elements are linked using pointers.","title":"Linked Lists"},{"location":"week-4/ce100-week-4-heap/#linked-lists-c-definition","text":"C // A linked list node struct Node { int data ; struct Node * next ; };","title":"Linked Lists - C Definition"},{"location":"week-4/ce100-week-4-heap/#linked-lists-cpp-definition","text":"Cpp class Node { public : int data ; Node * next ; };","title":"Linked Lists - Cpp Definition"},{"location":"week-4/ce100-week-4-heap/#linked-lists-java-definition","text":"Java class LinkedList { Node head ; // head of the list /* Linked list Node*/ class Node { int data ; Node next ; // Constructor to create a new node // Next is by default initialized // as null Node ( int d ) { data = d ; } } }","title":"Linked Lists - Java Definition"},{"location":"week-4/ce100-week-4-heap/#linked-lists-csharp-definition","text":"Csharp class LinkedList { // The first node(head) of the linked list // Will be an object of type Node (null by default) Node head ; class Node { int data ; Node next ; // Constructor to create a new node Node ( int d ) { data = d ; } } }","title":"Linked Lists - Csharp Definition"},{"location":"week-4/ce100-week-4-heap/#priority-queue-using-linked-list-methods","text":"Implement Priority Queue using Linked Lists. push(): This function is used to insert a new data into the queue. pop(): This function removes the element with the highest priority from the queue. peek()/top(): This function is used to get the highest priority element in the queue without removing it from the queue.","title":"Priority Queue using Linked List Methods"},{"location":"week-4/ce100-week-4-heap/#priority-queue-using-linked-list-algorithm","text":"PUSH ( HEAD , DATA , PRIORITY ) Create NEW.Data = DATA & NEW.Priority = PRIORITY If HEAD.priority < NEW.Priority NEW -> NEXT = HEAD HEAD = NEW Else Set TEMP to head of the list Endif WHILE TEMP -> NEXT != NULL and TEMP -> NEXT -> PRIORITY > PRIORITY THEN TEMP = TEMP -> NEXT ENDWHILE NEW -> NEXT = TEMP -> NEXT TEMP -> NEXT = NEW","title":"Priority Queue using Linked List Algorithm"},{"location":"week-4/ce100-week-4-heap/#priority-queue-using-linked-list-algorithm_1","text":"POP ( HEAD ) // Set the head of the list to the next node in the list. HEAD = HEAD -> NEXT. Free the node at the head of the list PEEK ( HEAD ) : Return HEAD -> DATA","title":"Priority Queue using Linked List Algorithm"},{"location":"week-4/ce100-week-4-heap/#priority-queue-using-linked-list-notes","text":"LinkedList is already sorted. Time Complexities and Comparison with Binary Heap peek() push() pop() Linked List \\(O(1)\\) \\(O(n)\\) \\(O(1)\\) Binary Heap \\(O(1)\\) \\(O(lgn)\\) \\(O(lgn)\\)","title":"Priority Queue using Linked List Notes"},{"location":"week-4/ce100-week-4-heap/#sorting-in-linear-time","text":"","title":"Sorting in Linear Time"},{"location":"week-4/ce100-week-4-heap/#how-fast-can-we-sort","text":"The algorithms we have seen so far: Based on comparison of elements We only care about the relative ordering between the elements (not the actual values) The smallest worst-case runtime we have seen so far: \\(O(nlgn)\\) Is \\(O(nlgn)\\) the best we can do? Comparison sorts: Only use comparisons to determine the relative order of elements.","title":"How Fast Can We Sort?"},{"location":"week-4/ce100-week-4-heap/#decision-trees-for-comparison-sorts","text":"Represent a sorting algorithm abstractly in terms of a decision tree A binary tree that represents the comparisons between elements in the sorting algorithm Control, data movement, and other aspects are ignored One decision tree corresponds to one sorting algorithm and one value of \\(n\\) ( input size )","title":"Decision Trees for Comparison Sorts"},{"location":"week-4/ce100-week-4-heap/#reminder-insertion-sort-step-by-step-description-1","text":"","title":"Reminder: Insertion Sort Step-By-Step Description (1)"},{"location":"week-4/ce100-week-4-heap/#reminder-insertion-sort-step-by-step-description-2","text":"","title":"Reminder: Insertion Sort Step-By-Step Description (2)"},{"location":"week-4/ce100-week-4-heap/#reminder-insertion-sort-step-by-step-description-3","text":"","title":"Reminder: Insertion Sort Step-By-Step Description (3)"},{"location":"week-4/ce100-week-4-heap/#different-outcomes-for-insertion-sort-and-n3","text":"Input : \\(<a_1,a_2,a_3>\\)","title":"Different Outcomes for Insertion Sort and n=3"},{"location":"week-4/ce100-week-4-heap/#decision-tree-for-insertion-sort-and-n3","text":"","title":"Decision Tree for Insertion Sort and n=3"},{"location":"week-4/ce100-week-4-heap/#decision-tree-model-for-comparison-sorts","text":"Internal node \\((i:j)\\) : Comparison between elements \\(a_i\\) and \\(a_j\\) Leaf node: An output of the sorting algorithm Path from root to a leaf: The execution of the sorting algorithm for a given input All possible executions are captured by the decision tree All possible outcomes (permutations) are in the leaf nodes","title":"Decision Tree Model for Comparison Sorts"},{"location":"week-4/ce100-week-4-heap/#decision-tree-for-insertion-sort-and-n3_1","text":"Input: \\(<9, 4, 6>\\)","title":"Decision Tree for Insertion Sort and n=3"},{"location":"week-4/ce100-week-4-heap/#decision-tree-model","text":"A decision tree can model the execution of any comparison sort: One tree for each input size \\(n\\) View the algorithm as splitting whenever it compares two elements The tree contains the comparisons along all possible instruction traces The running time of the algorithm \\(=\\) the length of the path taken Worst case running time \\(=\\) height of the tree","title":"Decision Tree Model"},{"location":"week-4/ce100-week-4-heap/#counting-sort","text":"","title":"Counting Sort"},{"location":"week-4/ce100-week-4-heap/#lower-bound-for-comparison-sorts","text":"Let \\(n\\) be the number of elements in the input array. What is the \\(min\\) number of leaves in the decision tree? \\(n!\\) (because there are n! permutations of the input array, and all possible outputs must be captured in the leaves) What is the max number of leaves in a binary tree of height \\(h\\) ? \\(\\Longrightarrow\\) \\(2^h\\) So, we must have: $$ 2^h \\geq n! $$","title":"Lower Bound for Comparison Sorts"},{"location":"week-4/ce100-week-4-heap/#lower-bound-for-decision-tree-sorting","text":"Theorem: Any comparison sort algorithm requires \\(\\Omega(nlgn)\\) comparisons in the worst case. Proof: We\u2019ll prove that any decision tree corresponding to a comparison sort algorithm must have height \\(\\Omega(nlgn)\\) \\[ \\begin{align*} 2^h & \\geq n! \\\\ h & \\geq lg(n!) \\\\ & \\geq lg((n/e)^n) (Stirling Approximation) \\\\ & = nlgn - nlge \\\\ & = \\Omega(nlgn) \\end{align*} \\]","title":"Lower Bound for Decision Tree Sorting"},{"location":"week-4/ce100-week-4-heap/#lower-bound-for-decision-tree-sorting_1","text":"Corollary: Heapsort and merge sort are asymptotically optimal comparison sorts. Proof: The \\(O(nlgn)\\) upper bounds on the runtimes for heapsort and merge sort match the \\(\\Omega(nlgn)\\) worst-case lower bound from the previous theorem.","title":"Lower Bound for Decision Tree Sorting"},{"location":"week-4/ce100-week-4-heap/#sorting-in-linear-time_1","text":"Counting sort: No comparisons between elements Input: \\(A[1 \\dots n]\\) , where \\(A[j] \\in \\{1, 2,\\dots, k\\}\\) Output: \\(B[1 \\dots n]\\) , sorted Auxiliary storage: \\(C[1 \\dots k]\\)","title":"Sorting in Linear Time"},{"location":"week-4/ce100-week-4-heap/#counting-sort-1","text":"","title":"Counting Sort-1"},{"location":"week-4/ce100-week-4-heap/#counting-sort-2","text":"Step 1: Initialize all counts to 0","title":"Counting Sort-2"},{"location":"week-4/ce100-week-4-heap/#counting-sort-3","text":"Step 2: Count the number of occurrences of each value in the input array","title":"Counting Sort-3"},{"location":"week-4/ce100-week-4-heap/#counting-sort-4","text":"Step 3: Compute the number of elements less than or equal to each value","title":"Counting Sort-4"},{"location":"week-4/ce100-week-4-heap/#counting-sort-5","text":"Step 4: Populate the output array There are \\(C[3] = 3\\) elements that are \\(\\leq 3\\)","title":"Counting Sort-5"},{"location":"week-4/ce100-week-4-heap/#counting-sort-6","text":"Step 4: Populate the output array There are \\(C[4]=5\\) elements that are \\(\\leq 4\\)","title":"Counting Sort-6"},{"location":"week-4/ce100-week-4-heap/#counting-sort-7","text":"Step 4: Populate the output array There are \\(C[3]=2\\) elements that are \\(\\leq 3\\)","title":"Counting Sort-7"},{"location":"week-4/ce100-week-4-heap/#counting-sort-8","text":"Step 4: Populate the output array There are \\(C[1]=1\\) elements that are \\(\\leq 1\\)","title":"Counting Sort-8"},{"location":"week-4/ce100-week-4-heap/#counting-sort-9","text":"Step 4: Populate the output array There are \\(C[4]=4\\) elements that are \\(\\leq 4\\)","title":"Counting Sort-9"},{"location":"week-4/ce100-week-4-heap/#counting-sort-runtime-analysis","text":"Total Runtime: \\(\\Theta(n+k)\\) \\(n\\) : size of the input array \\(k\\) : the range of input values","title":"Counting Sort: Runtime Analysis"},{"location":"week-4/ce100-week-4-heap/#counting-sort-runtime","text":"Runtime is \\(\\Theta(n+k)\\) If \\(k=O(n)\\) , then counting sort takes \\(\\Theta(n)\\) Question: We proved a lower bound of \\(\\Theta(nlgn)\\) before! Where is the fallacy? Answer: \\(\\Theta(nlgn)\\) lower bound is for comparison-based sorting Counting sort is not a comparison sort In fact, not a single comparison between elements occurs!","title":"Counting Sort: Runtime"},{"location":"week-4/ce100-week-4-heap/#stable-sorting","text":"Counting sort is a stable sort: It preserves the input order among equal elements. i.e. The numbers with the same value appear in the output array in the same order as they do in the input array. Note : Which other sorting algorithms have this property?","title":"Stable Sorting"},{"location":"week-4/ce100-week-4-heap/#radix-sort","text":"Origin: Herman Hollerith\u2019s card-sorting machine for the 1890 US Census. Basic idea: Digit-by-digit sorting Two variations: Sort from MSD to LSD (bad idea) Sort from LSD to MSD (good idea) ( LSD/MSD: Least/most significant digit )","title":"Radix Sort"},{"location":"week-4/ce100-week-4-heap/#herman-hollerith-1860-1929","text":"The 1880 U.S. Census took almost 10 years to process. While a lecturer at MIT, Hollerith prototyped punched-card technology . His machines, including a card sorter , allowed the 1890 census total to be reported in 6 weeks . He founded the Tabulating Machine Company in 1911, which merged with other companies in 1924 to form International Business Machines(IBM) .","title":"Herman Hollerith (1860-1929)"},{"location":"week-4/ce100-week-4-heap/#hollerith-punched-card","text":"Punched card: A piece of stiff paper that contains digital information represented by the presence or absence of holes. 12 rows and 24 columns coded for age, state of residency, gender, etc.","title":"Hollerith Punched Card"},{"location":"week-4/ce100-week-4-heap/#modern-ibm-card","text":"One character per column So, that\u2019s why text windows have 80 columns! for more samples visit https://en.wikipedia.org/wiki/Punched_card","title":"Modern IBM card"},{"location":"week-4/ce100-week-4-heap/#hollerith-tabulating-machine-and-sorter","text":"Mechanically sorts the cards based on the hole locations. Sorting performed for one column at a time Human operator needed to load/retrieve/move cards at each stage","title":"Hollerith Tabulating Machine and Sorter"},{"location":"week-4/ce100-week-4-heap/#holleriths-msd-first-radix-sort","text":"Sort starting from the most significant digit (MSD) Then, sort each of the resulting bins recursively At the end, combine the decks in order","title":"Hollerith\u2019s MSD-First Radix Sort"},{"location":"week-4/ce100-week-4-heap/#holleriths-msd-first-radix-sort_1","text":"To sort a subset of cards recursively: All the other cards need to be removed from the machine, because the machine can handle only one sorting problem at a time. The human operator needs to keep track of the intermediate card piles","title":"Hollerith\u2019s MSD-First Radix Sort"},{"location":"week-4/ce100-week-4-heap/#holleriths-msd-first-radix-sort_2","text":"MSD-first sorting may require: very large number of sorting passes very large number of intermediate card piles to maintain S(d): \\(\\#\\) of passes needed to sort d-digit numbers (worst-case) Recurrence: \\(S(d)=10S(d-1)+1\\) with \\(S(1)=1\\) Reminder: Recursive call made to each subset with the same most significant digit(MSD)","title":"Hollerith\u2019s MSD-First Radix Sort"},{"location":"week-4/ce100-week-4-heap/#holleriths-msd-first-radix-sort_3","text":"Recurrence: \\(S(d)=10S(d-1)+1\\) \\[ \\begin{align*} S(d) &= 10 S(d-1) + 1 \\\\ & = 10 \\bigg(10 S(d-2) + 1 \\bigg) + 1 \\\\ & = 10 \\Big(10 \\bigg(10 S(d-3) + 1\\bigg) + 1 \\Big) + 1 \\\\ & = 10i S(d-i) + 10i-1 + 10i-2 + \\dots + 101 + 100 \\\\ &=\\sum \\limits_{i=0}^{d-1}10^i \\end{align*} \\] Iteration terminates when \\(i = d-1\\) with \\(S(d-(d-1)) = S(1) = 1\\)","title":"Hollerith\u2019s MSD-First Radix Sort"},{"location":"week-4/ce100-week-4-heap/#holleriths-msd-first-radix-sort_4","text":"Recurrence: \\(S(d)=10S(d-1)+1\\) \\[ \\begin{align*} S(d) &=\\sum \\limits_{i=0}^{d-1}10^i \\\\ & = \\frac{10^d-1}{10-1} \\\\ & = \\frac{1}{9}(10^d-1)\\\\ & \\Downarrow \\\\ S(d)&=\\frac{1}{9}(10^d-1) \\end{align*} \\]","title":"Hollerith\u2019s MSD-First Radix Sort"},{"location":"week-4/ce100-week-4-heap/#holleriths-msd-first-radix-sort_5","text":"\\(P(d)\\) : \\(\\#\\) of intermediate card piles maintained (worst-case) Reminder: Each routing pass generates 9 intermediate piles except the sorting passes on least significant digits (LSDs) There are \\(10^{d-1}\\) sorting calls to LSDs \\[ \\begin{align*} P(d) &= 9(S(d)\u201310^{d-1}) \\\\ &= 9\\frac{(10^{d\u20131})}{9\u2013 10^{d-1}} \\\\ &= (10^{d\u20131}\u20139 * 10^{d-1}) \\\\ &= 10^{d-1} - 1 \\end{align*} \\]","title":"Hollerith\u2019s MSD-First Radix Sort"},{"location":"week-4/ce100-week-4-heap/#holleriths-msd-first-radix-sort_6","text":"\\[ \\begin{align*} P(d) &= 10^{d-1} - 1 \\end{align*} \\] Alternative solution: Solve the recurrence \\[ \\begin{align*} P(d) &= 10P(d-1)+9 \\\\ P(1) &= 0 \\\\ \\end{align*} \\]","title":"Hollerith\u2019s MSD-First Radix Sort"},{"location":"week-4/ce100-week-4-heap/#holleriths-msd-first-radix-sort_7","text":"Example: To sort \\(3\\) digit numbers, in the worst case: \\(S(d) = (1/9) (103-1) = 111\\) sorting passes needed \\(P(d) = 10d-1-1 = 99\\) intermediate card piles generated MSD-first approach has more recursive calls and intermediate storage requirement Expensive for a tabulating machine to sort punched cards Overhead of recursive calls in a modern computer section{ font-size: 25px; }","title":"Hollerith\u2019s MSD-First Radix Sort"},{"location":"week-4/ce100-week-4-heap/#lsd-first-radix-sort","text":"Least significant digit ( LSD )-first radix sort seems to be a folk invention originated by machine operators. It is the counter-intuitive, but the better algorithm. Basic Algorithm: Sort numbers on their LSD first ( Stable Sorting Needed ) Combine the cards into a single deck in order Continue this sorting process for the other digits from the LSD to MSD Requires only \\(d\\) sorting passes No intermediate card pile generated","title":"LSD-First Radix Sort"},{"location":"week-4/ce100-week-4-heap/#lsd-first-radix-sort-example","text":"","title":"LSD-first Radix Sort Example"},{"location":"week-4/ce100-week-4-heap/#correctness-of-radix-sort-lsd-first","text":"Proof by induction: Base case: \\(d=1\\) is correct ( trivial ) Inductive hyp: Assume the first \\(d-1\\) digits are sorted correctly Prove that all \\(d\\) digits are sorted correctly after sorting digit \\(d\\) Two numbers that differ in digit \\(d\\) are correctly sorted ( e.g. 355 and 657 ) Two numbers equal in digit d are put in the same order as the input ( correct order )","title":"Correctness of Radix Sort (LSD-first)"},{"location":"week-4/ce100-week-4-heap/#radix-sort-runtime","text":"Use counting-sort to sort each digit Reminder: Counting sort complexity: \\(\\Theta(n+k)\\) \\(n\\) : size of input array \\(k\\) : the range of the values Radix sort runtime: \\(\\Theta(d(n+k))\\) \\(d\\) : \\(\\#\\) of digits How to choose the \\(d\\) and \\(k\\) ?","title":"Radix Sort Runtime"},{"location":"week-4/ce100-week-4-heap/#radix-sort-runtime-example-1","text":"We have flexibility in choosing \\(d\\) and \\(k\\) Assume we are trying to sort 32-bit words We can define each digit to be 4 bits Then, the range for each digit \\(k=2^4=16\\) So, counting sort will take \\(\\Theta(n+16)\\) The number of digits \\(d =32/4=8\\) Radix sort runtime: \\(\\Theta(8(n+16)) = \\Theta(n)\\) \\(\\overbrace{[4bits|4bits|4bits|4bits|4bits|4bits|4bits|4bits]}^{\\text{32-bits}}\\)","title":"Radix Sort: Runtime \u2013 Example 1"},{"location":"week-4/ce100-week-4-heap/#radix-sort-runtime-example-2","text":"We have flexibility in choosing \\(d\\) and \\(k\\) Assume we are trying to sort 32-bit words Or, we can define each digit to be 8 bits Then, the range for each digit \\(k = 2^8 = 256\\) So, counting sort will take \\(\\Theta(n+256)\\) The number of digits \\(d = 32/8 = 4\\) Radix sort runtime: \\(\\Theta(4(n+256)) = \\Theta(n)\\) \\(\\overbrace{[8bits|8bits|8bits|8bits]}^{\\text{32-bits}}\\) section{ font-size: 25px; }","title":"Radix Sort: Runtime \u2013 Example 2"},{"location":"week-4/ce100-week-4-heap/#radix-sort-runtime_1","text":"Assume we are trying to sort \\(b\\) -bit words Define each digit to be \\(r\\) bits Then, the range for each digit \\(k = 2^r\\) So, counting sort will take \\(\\Theta(n+2^r)\\) The number of digits \\(d = b/r\\) Radix sort runtime: \\[ \\begin{align*} T(n,b)&=\\Theta \\bigg( \\frac{b}{r}(n+2^r) \\bigg) \\end{align*} \\] \\(\\overbrace{[rbits|rbits|rbits|rbits]}^{b/r \\text{ bits}}\\)","title":"Radix Sort: Runtime"},{"location":"week-4/ce100-week-4-heap/#radix-sort-runtime-analysis","text":"\\[ \\begin{align*} T(n,b)&=\\Theta \\bigg( \\frac{b}{r}(n+2^r) \\bigg) \\end{align*} \\] Minimize \\(T(n,b)\\) by differentiating and setting to \\(0\\) Or, intuitively: We want to balance the terms \\((b/r)\\) and \\((n + 2^r)\\) Choose \\(r \\approx lgn\\) If we choose \\(r << lgn \\Longrightarrow (n + 2^r)\\) term doesn\u2019t improve If we choose \\(r >> lgn \\Longrightarrow (n + 2^r)\\) increases exponentially","title":"Radix Sort: Runtime Analysis"},{"location":"week-4/ce100-week-4-heap/#radix-sort-runtime-analysis_1","text":"\\[ \\begin{align*} T(n,b)&=\\Theta \\bigg( \\frac{b}{r}(n+2^r) \\bigg) \\end{align*} \\] \\[ \\begin{align*} \\text{Choose } r=lgn \\Longrightarrow T(n,b)=\\Theta(bn/lgn) \\end{align*} \\] For numbers in the range from \\(0\\) to \\(n^d \u2013 1\\) , we have: The number of bits \\(b = lg(nd ) = d lgn\\) Radix sort runs in \\(\\Theta(dn)\\)","title":"Radix Sort: Runtime Analysis"},{"location":"week-4/ce100-week-4-heap/#radix-sort-conclusions","text":"\\[ \\begin{align*} \\text{Choose } r=lgn \\Longrightarrow T(n,b)=\\Theta(bn/lgn) \\end{align*} \\] Example: Compare radix sort with merge sort/heapsort \\(1\\) million ( \\(2^{20}\\) ), \\(32\\) -bit numbers \\((n = 2^{20}, b = 32)\\) Radix sort: \\(\\lfloor 32/20 \\rfloor = 2\\) passes Merge sort/heap sort: \\(lgn = 20\\) passes Downsides: Radix sort has little locality of reference (more cache misses) The version that uses counting sort is not in-place On modern processors, a well-tuned quicksort implementation typically runs faster.","title":"Radix Sort: Conclusions"},{"location":"week-4/ce100-week-4-heap/#references","text":"Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) Insertion Sort - GeeksforGeeks Priority Queue Using Linked List - GeeksforGeeks Priority Queue Using Linked List - JavatPoint NIST Dictionary of Algorithms and Data Structures NIST - Dictionary of Algorithms and Data Structures \\(-End-Of-Week-4-Course-Module-\\)","title":"References"},{"location":"week-5/ce100-week-5-dp/","text":"CE100 Algorithms and Programming II \u00b6 Week-5 (Dynamic Programming) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Quicksort Sort \u00b6 Outline \u00b6 Convex Hull (Divide & Conquer) Dynamic Programming Introduction Divide-and-Conquer (DAC) vs Dynamic Programming (DP) Fibonacci Numbers Recursive Solution Bottom-Up Solution Optimization Problems Development of a DP Algorithms Matrix-Chain Multiplication Matrix Multiplication and Row Columns Definitions Cost of Multiplication Operations (pxqxr) Counting the Number of Parenthesizations The Structure of Optimal Parenthesization Characterize the structure of an optimal solution A Recursive Solution Direct Recursion Inefficiency. Computing the optimal Cost of Matrix-Chain Multiplication Bottom-up Computation Algorithm for Computing the Optimal Costs MATRIX-CHAIN-ORDER Construction and Optimal Solution MATRIX-CHAIN-MULTIPLY Summary Dynamic Programming - Introduction \u00b6 An algorithm design paradigm like divide-and-conquer Programming: A tabular method (not writing computer code) Older sense of planning or scheduling, typically by filling in a table Divide-and-Conquer (DAC): subproblems are independent Dynamic Programming (DP): subproblems are not independent Overlapping subproblems: subproblems share sub-subproblems In solving problems with overlapping subproblems A DAC algorithm does redundant work Repeatedly solves common subproblems A DP algorithm solves each problem just once Saves its result in a table Problem 1: Fibonacci Numbers Recursive Solution \u00b6 Reminder: \\[ \\begin{align*} & F(0)=0 \\text{ and } F(1)=1 \\\\ & F(n)=F(n-1)+F(n-2) \\\\[10 pt] &\\text{REC-FIBO}(n) \\{ \\\\ & \\quad \\text{if} \\ n < 2 \\\\ & \\qquad \\text{return} \\ n \\\\ & \\quad \\text{else} \\\\ & \\qquad \\text{return} \\ \\text{REC-FIBO}(n-1) + \\text{REC-FIBO}(n-2) \\ \\} \\end{align*} \\] Overlapping subproblems in different recursive calls. Repeated work! Problem 1: Fibonacci Numbers Recursive Solution \u00b6 Recurrence: exponential runtime \\[ T(n) = T(n-1) + T(n-2) + 1 \\] Recursive algorithm inefficient because it recomputes the same \\(F(i)\\) repeatedly in different branches of the recursion tree. Problem 1: Fibonacci Numbers Bottom-up Computation \u00b6 Reminder: \\[ \\begin{align*} & F(0)=0 \\text{ and } F(1)=1 \\\\ & F(n)=F(n-1)+F(n-2) \\end{align*} \\] Runtime \\(\\Theta(n)\\) ITER - FIBO ( n ) F [ 0 ] = 0 F [ 1 ] = 1 for i = 2 to n do F [ i ] = F [ i -1 ] + F [ i -2 ] return F [ n ] Optimization Problems \u00b6 DP typically applied to optimization problems In an optimization problem There are many possible solutions (feasible solutions) Each solution has a value Want to find an optimal solution to the problem A solution with the optimal value (min or max value) Wrong to say the optimal solution to the problem There may be several solutions with the same optimal value Development of a DP Algorithm \u00b6 Step-1 . Characterize the structure of an optimal solution Step-2 . Recursively define the value of an optimal solution Step-3 . Compute the value of an optimal solution in a bottom-up fashion Step-4 . Construct an optimal solution from the information computed in Step 3 section{ font-size: 25px; } Problem 2: Matric Chain Multiplication \u00b6 Input: a sequence (chain) \\(\\langle A_1,A_2, \\dots , A_n\\rangle\\) of \\(n\\) matrices Aim: compute the product \\(A_1 \\cdot A_2 \\cdot \\dots A_n\\) A product of matrices is fully parenthesized if It is either a single matrix Or, the product of two fully parenthesized matrix products surrounded by a pair of parentheses. \\(\\bigg(A_i(A_{i+1}A_{i+2} \\dots A_j) \\bigg)\\) \\(\\bigg((A_iA_{i+1}A_{i+2} \\dots A_{j-1})A_j \\bigg)\\) \\(\\bigg((A_iA_{i+1}A_{i+2} \\dots A_k)(A_{k+1}A_{k+2} \\dots A_j)\\bigg)\\) for \\(i \\leq k < j\\) All parenthesizations yield the same product; matrix product is associative section{ font-size: 25px; } Matrix-chain Multiplication: An Example Parenthesization \u00b6 Input: \\(\\langle A_1,A_2,A_3,A_4\\rangle\\) ( \\(5\\) distinct ways of full parenthesization) \\[ \\begin{align*} & \\bigg(A_1\\Big(A_2(A_3A_4)\\Big)\\bigg) \\\\ & \\bigg(A_1\\Big((A_2A_3)A_4\\Big)\\bigg) \\\\ & \\bigg((A_1A_2)(A_3A_4)\\bigg) \\\\ & \\bigg(\\Big(A_1(A_2A_3)A_4\\Big)\\bigg) \\\\ & \\bigg(\\Big((A_1A_2)A_3\\Big)A_4\\bigg) \\end{align*} \\] The way we parenthesize a chain of matrices can have a dramatic effect on the cost of computing the product Matrix-chain Multiplication: Reminder \u00b6 MATRIX - MULTIPLY ( A , B ) if cols [ A ] != rows [ B ] then error ( \u201c incompatible dimensions \u201d ) for i = 1 to rows [ A ] do for j = 1 to cols [ B ] do C [ i , j ] = 0 for k = 1 to cols [ A ] do C [ i , j ] = C [ i , j ] + A [ i , k ] \u00b7 B [ k , j ] return C Matrix Chain Multiplication: Example \u00b6 \\(A1:10\\text{x}100\\) , \\(A2:100\\text{x}5\\) , \\(A3:5\\text{x}50\\) Which paranthesization is better? \\((A1A2)A3\\) or \\(A1(A2A3)\\) ? Matrix Chain Multiplication: Example \u00b6 \\(A1:10 \\times 100\\) , \\(A2:100 \\times 5\\) , \\(A3:5 \\times 50\\) Which paranthesization is better? \\((A1A2)A3\\) or \\(A1(A2A3)\\) ? Matrix Chain Multiplication: Example \u00b6 \\(A1:10 \\times 100\\) , \\(A2:100 \\times 5\\) , \\(A3:5 \\times 50\\) Which paranthesization is better? \\((A1A2)A3\\) or \\(A1(A2A3)\\) ? In summary: \\((A1A2)A3\\) = \\(\\#\\) of multiply-add ops: \\(7500\\) \\(A1(A2A3)\\) = \\(\\#\\) of multiple-add ops: \\(75000\\) First parenthesization yields 10x faster computation Matrix-chain Multiplication Problem \u00b6 Input: A chain \\(\\langle A_1,A_2, \\dots ,A_n\\rangle\\) of \\(n\\) matrices, where \\(A_i\\) is a \\(p_{i-1} \\times p_i\\) matrix Objective: Fully parenthesize the product \\(A_1 \\cdot A_2 \\dots A_n\\) such that the number of scalar mult-adds is minimized. section{ font-size: 25px; } Counting the Number of Parenthesizations \u00b6 Brute force approach: exhaustively check all parenthesizations \\(P(n)\\) : \\(\\#\\) of parenthesizations of a sequence of n matrices We can split sequence between \\(k^{th}\\) and \\((k+1)^{st}\\) matrices for any \\(k=1, 2, \\dots , n-1\\) , then parenthesize the two resulting sequences independently, i.e., \\[ (A_1 A_2 A_3 \\dots A_k \\overbrace{)(}^{break-point}A_{k+1} A_{k+2} \\dots A_n) \\] We obtain the recurrence \\[ P(1)=1 \\text{ and } P(n)=\\sum \\limits_{k=1}^{n-1}P(k)P(n-k) \\] Number of Parenthesizations: \u00b6 \\(P(1)=1\\) and \\(P(n)=\\sum \\limits_{k=1}^{n-1}P(k)P(n-k)\\) The recurrence generates the sequence of Catalan Numbers Solution is \\(P(n)=C(n-1)\\) where \\[ C(n)=\\frac{1}{n+1} {2n \\choose n} = \\Omega(4^n / n^{3/2}) \\] The number of solutions is exponential in \\(n\\) Therefore, brute force approach is a poor strategy The Structure of Optimal Parenthesization \u00b6 Notation: \\(A_{i..j}\\) : The matrix that results from evaluation of the product: \\(A_i A_{i+1} A_{i+2} \\dots A_j\\) Observation: Consider the last multiplication operation in any parenthesization: \\((A_1 A_2 \\dots A_k) \\cdot (A_{k+1} A_{k+2} \\dots A_n)\\) There is a \\(k\\) value \\((1 \\leq k < n)\\) such that: First, the product \\(A_1 \\dots k\\) is computed Then, the product \\(A_{k+1 \\dots n}\\) is computed Finally, the matrices \\(A_{1 \\dots k}\\) and \\(A_{k+1 \\dots n}\\) are multiplied Step 1: Characterize the Structure of an Optimal Solution \u00b6 An optimal parenthesization of product \\(A_1 A_2 \\dots A_n\\) will be: \\((A_1 A_2 \\dots A_k) \\cdot (A_{k+1} A_{k+2} \\dots A_n)\\) for some \\(k\\) value The cost of this optimal parenthesization will be: \\(=\\) Cost of computing \\(A_{1 \\dots k}\\) \\(+\\) Cost of computing \\(A_{k+1 \\dots n}\\) \\(+\\) Cost of multiplying \\(A_{1 \\dots k} \\cdot A_{k+1 \\dots n}\\) Step 1: Characterize the Structure of an Optimal Solution \u00b6 Key observation: Given optimal parenthesization \\((A_1 A_2 A_3 \\dots A_k) \\cdot (A_{k+1} A_{k+2} \\dots A_n)\\) Parenthesization of the subchain \\(A_1 A_2 A_3 \\dots A_k\\) Parenthesization of the subchain \\(A_{k+1} A_{k+2} \\dots A_n\\) should both be optimal Thus, optimal solution to an instance of the problem contains optimal solutions to subproblem instances i.e. , optimal substructure within an optimal solution exists. Step 2: A Recursive Solution \u00b6 Step 2: Define the value of an optimal solution recursively in terms of optimal solutions to the subproblems Assume we are trying to determine the min cost of computing \\(A_{i \\dots j}\\) \\(m_{i,j}\\) : min \\(\\#\\) of scalar multiply-add opns needed to compute \\(A_{i \\dots j}\\) Note: The optimal cost of the original problem: \\(m_{1,n}\\) How to compute \\(m_{i,j}\\) recursively? Step 2: A Recursive Solution \u00b6 Base case: \\(m_{i,i}=0\\) (single matrix, no multiplication) Let the size of matrix \\(A_i\\) be \\((p_{i-1} \\times p_i)\\) Consider an optimal parenthesization of chain \\(A_i \\dots A_j : (A_i \\dots A_k) \\cdot (A_{k+1} \\dots A_j)\\) The optimal cost: \\(m_{i,j} = m_{i,k} + m_{k+1,j} + p_{i-1} \\times p_k \\times p_j\\) where: \\(m_{i,k}\\) : Optimal cost of computing \\(A_{i \\dots k}\\) \\(m_{k+1,j}\\) : Optimal cost of computing \\(A_{k+1 \\dots j}\\) \\(p_{i-1} \\times p_k \\times p_j\\) : Cost of multiplying \\(A_{i \\dots k}\\) and \\(A_{k+1 \\dots j}\\) Step 2: A Recursive Solution \u00b6 In an optimal parenthesization: \\(k\\) must be chosen to minimize \\(m_{ij}\\) The recursive formulation for \\(m_{ij}\\) : \\[ \\begin{align*} m_{ij} = \\begin{cases} 0 & if & i=j \\\\ \\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} & if & i<j \\end{cases} \\end{align*} \\] Step 2: A Recursive Solution \u00b6 The \\(m_{ij}\\) values give the costs of optimal solutions to subproblems In order to keep track of how to construct an optimal solution Define \\(s_{ij}\\) to be the value of \\(k\\) which yields the optimal split of the subchain \\(A_{i \\dots j}\\) That is, \\(s_{ij}=k\\) such that \\(m_{ij} = m_{ik} + m_{k+1,j} +p_{i-1} p_k p_j\\) holds section{ font-size: 25px; } Direct Recursion: Inefficient! \u00b6 Recursive Matrix-Chain ( RMC ) Order RMC ( p , i , j ) if ( i == j ) then return 0 m [ i , j ] = INF for k = i to j -1 do q = RMC ( p , i , k ) + RMC ( p , k +1 , j ) + p_ { i -1 } p_k p_j if q < m [ i , j ] then m [ i , j ] = q endfor return m [ i , j ] Direct Recursion: Inefficient! \u00b6 Recursion tree for \\(RMC(p,1,4)\\) Nodes are labeled with \\(i\\) and \\(j\\) values Computing the Optimal Cost ( Matrix-Chain Multiplication ) \u00b6 An important observation: - We have relatively few subproblems - one problem for each choice of \\(i\\) and \\(j\\) satisfying \\(1 \\leq i \\leq j \\leq n\\) - total \\(n + (n-1) + \\dots + 2 + 1 = \\frac{1}{2}n(n+1) = \\Theta(n2)\\) subproblems - We can write a recursive algorithm based on recurrence. - However, a recursive algorithm may encounter each subproblem many times in different branches of the recursion tree - This property, overlapping subproblems , is the second important feature for applicability of dynamic programming Computing the Optimal Cost ( Matrix-Chain Multiplication ) \u00b6 Compute the value of an optimal solution in a bottom-up fashion matrix \\(A_i\\) has dimensions \\(p_{i-1} \\times p_i\\) for \\(i = 1, 2, \\dots , n\\) the input is a sequence \\(\\langle p_0, p_1, \\dots, p_n \\rangle\\) where \\(length[p] = n + 1\\) Procedure uses the following auxiliary tables: \\(m[1 \\dots n, 1 \\dots n]\\) : for storing the \\(m[i,j]\\) costs \\(s[1 \\dots n, 1 \\dots n]\\) : records which index of \\(k\\) achieved the optimal cost in computing \\(m[i,j]\\) Bottom-Up Computation \u00b6 How to choose the order in which we process \\(m_{ij}\\) values? Before computing \\(m_{ij}\\) , we have to make sure that the values for \\(m_{ik}\\) and \\(m_{k+1,j}\\) have been computed for all \\(k\\) . \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] Bottom-Up Computation \u00b6 \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] \\(m_{ij}\\) must be processed after \\(m_{ik}\\) and \\(m_{j,k+1}\\) Reminder: \\(m_{ij}\\) computed only for \\(j > i\\) Bottom-Up Computation \u00b6 \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] \\(m_{ij}\\) must be processed after \\(m_{ik}\\) and \\(m_{j,k+1}\\) How to set up the iterations over \\(i\\) and \\(j\\) to compute \\(m_{ij}\\) ? Bottom-Up Computation \u00b6 \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] If the entries \\(m_{ij}\\) are computed in the shown order, then \\(m_{ik}\\) and \\(m_{k+1,j}\\) values are guaranteed to be computed before \\(m_{ij}\\) . Bottom-Up Computation \u00b6 \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] Bottom-Up Computation \u00b6 \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] section{ font-size: 25px; } Algorithm for Computing the Optimal Costs \u00b6 Note : l \\(=\\ell\\) and p_{i-1} p_k p_j \\(=p_{i-1} p_k p_j\\) MATRIX - CHAIN - ORDER ( p ) n = length [ p ] -1 for i = 1 to n do m [ i , i ] = 0 endfor for l = 2 to n do for i = 1 to n n - l +1 do j = i + l -1 m [ i , j ] = INF for k = i to j -1 do q = m [ i , k ] + m [ k +1 , j ] + p_ { i -1 } p_k p_j if q < m [ i , j ] then m [ i , j ] = q s [ i , j ] = k endfor endfor endfor return m and s Algorithm for Computing the Optimal Costs \u00b6 The algorithm first computes \\(m[i, i] \\leftarrow 0\\) for \\(i=1,2, \\dots ,n\\) min costs for all chains of length 1 Then , for \\(\\ell = 2,3, \\dots,n\\) computes \\(m[i, i+\\ell-1]\\) for \\(i=1,\\dots,n-\\ell+1\\) min costs for all chains of length \\(\\ell\\) For each value of \\(\\ell = 2, 3, \\dots ,n\\) , \\(m[i, i+\\ell-1]\\) depends only on table entries \\(m[i,k] \\& m[k+1, i+\\ell-1]\\) for \\(i\\leq k < i+\\ell-1\\) , which are already computed Algorithm for Computing the Optimal Costs \u00b6 \\[ \\begin{align} \\begin{aligned} \\text{compute } m[i,i+1] \\\\ \\underbrace{ \\{ m[1,2],m[2,3], \\dots ,m[n-1,n]\\} }_{(n-1) \\text{ values}} \\end{aligned} & \\begin{cases} & \\ell=2 \\\\ & \\text{for } i=1 \\text{ to } n-1 \\text{ do } \\\\ & \\quad m[i,i+1]=\\infty \\\\ & \\quad \\quad \\text{for } k=i \\text{ to } i \\text{ do } \\\\ & \\quad \\quad \\quad \\vdots \\end{cases} \\\\ \\begin{aligned} \\text{compute } m[i,i+2] \\\\ \\underbrace{ \\{ m[1,3],m[2,4], \\dots ,m[n-2,n]\\} }_{(n-2) \\text{ values}} \\end{aligned} & \\begin{cases} & \\ell=3 \\\\ & \\text{for } i=1 \\text{ to } n-2 \\text{ do } \\\\ & \\quad m[i,i+2]=\\infty \\\\ & \\quad \\quad \\text{for } k=i \\text{ to } i+1 \\text{ do } \\\\ & \\quad \\quad \\quad \\vdots \\end{cases} \\\\ \\begin{aligned} \\text{compute } m[i,i+3] \\\\ \\underbrace{ \\{ m[1,4],m[2,5], \\dots ,m[n-3,n]\\} }_{(n-3) \\text{ values}} \\end{aligned} & \\begin{cases} & \\ell=4 \\\\ & \\text{for } i=1 \\text{ to } n-3 \\text{ do } \\\\ & \\quad m[i,i+3]=\\infty \\\\ & \\quad \\quad \\text{for } k=i \\text{ to } i+2 \\text{ do } \\\\ & \\quad \\quad \\quad \\vdots \\end{cases} \\end{align} \\] Table access pattern in computing \\(m[i, j]\\) s for \\(\\ell=j-i+1\\) \u00b6 \\[ \\begin{align*} & \\text{for} \\ k \\leftarrow i \\ \\text{to} \\ j-1 \\ \\text{do} \\\\ & \\quad q \\leftarrow m[i,k]+m[k+1,j]+p_{i-1}p_kp_j \\end{align*} \\] Table access pattern in computing \\(m[i, j]\\) s for \\(\\ell=j-i+1\\) \u00b6 \\[ \\begin{align*} & \\bigg( (A_i) \\overset{mult.}{ \\vdots } (A_{i+1}A_{i+2} \\dots A_j) \\bigg) \\end{align*} \\] Table access pattern in computing \\(m[i, j]\\) s for \\(\\ell=j-i+1\\) \u00b6 \\[ \\begin{align*} \\bigg( (A_iA_{i+1}) \\overset{mult.}{ \\vdots } (A_{i+2} \\dots A_j) \\bigg) \\end{align*} \\] Table access pattern in computing \\(m[i, j]\\) s for \\(\\ell=j-i+1\\) \u00b6 \\[ \\begin{align*} \\bigg( (A_iA_{i+1}A_{i+2}) \\overset{mult.}{ \\vdots } (A_{i+3} \\dots A_j) \\bigg) \\end{align*} \\] Table access pattern in computing \\(m[i, j]\\) s for \\(\\ell=j-i+1\\) \u00b6 \\[ \\begin{align*} \\bigg( (A_iA_{i+1} \\dots A_{j-1}) \\overset{mult.}{ \\vdots } (A_j) \\bigg) \\end{align*} \\] Table access pattern Example \u00b6 Compute \\(m_{25}\\) Choose the \\(k\\) value that leads to min cost \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\\\[10pt] \\begin{align*} \\begin{aligned} A_1 &: (30 \\times 35) \\\\ A_2 &: (35 \\times 15) \\\\ A_3 &: (15 \\times 5) \\\\ A_4 &: (5 \\times 10) \\\\ A_5 &: (10 \\times 20) \\\\ A_6 &: (20 \\times 25) \\end{aligned} \\begin{aligned} & ((A_2) \\overbrace{\\vdots}^{ (k=2) } (A_3 A_4 A_5)) \\\\[10 pt] \\quad cost &= m_{22} + m_{35} + p_1p_2p_5 \\\\ &= 0 + 2500 + 35 \\times 15 \\times 20 \\\\ &= 13000 \\end{aligned} \\end{align*} \\] Table access pattern Example \u00b6 Compute \\(m_{25}\\) Choose the \\(k\\) value that leads to min cost \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\\\[10pt] \\begin{align*} \\begin{aligned} A_1 &: (30 \\times 35) \\\\ A_2 &: (35 \\times 15) \\\\ A_3 &: (15 \\times 5) \\\\ A_4 &: (5 \\times 10) \\\\ A_5 &: (10 \\times 20) \\\\ A_6 &: (20 \\times 25) \\end{aligned} \\begin{aligned} & ((A_2 A_3) \\overbrace{\\vdots}^{ (k=3) } (A_4 A_5)) \\\\[10 pt] \\quad cost &= m_{23} + m_{45} + p_1p_3p_5 \\\\ &= 2625 + 1000 + 35 \\times 5 \\times 20 \\\\ &= 7125 \\end{aligned} \\end{align*} \\] Table access pattern Example \u00b6 Compute \\(m_{25}\\) Choose the \\(k\\) value that leads to min cost \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\\\[10pt] \\begin{align*} \\begin{aligned} A_1 &: (30 \\times 35) \\\\ A_2 &: (35 \\times 15) \\\\ A_3 &: (15 \\times 5) \\\\ A_4 &: (5 \\times 10) \\\\ A_5 &: (10 \\times 20) \\\\ A_6 &: (20 \\times 25) \\end{aligned} \\begin{aligned} & ((A_2 A_3 A_4)\\overbrace{\\vdots}^{ (k=4) }(A_5)) \\\\[10 pt] \\quad cost &= m_{24} + m_{55} + p_1p_4p_5 \\\\ &= 4375 + 0 + 35 \\times 10 \\times 20 \\\\ &= 11375 \\end{aligned} \\end{align*} \\] Table access pattern Example \u00b6 Compute \\(m_{25}\\) Choose the \\(k\\) value that leads to min cost \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\\\[10pt] \\begin{align*} \\begin{aligned} A_1 &: (30 \\times 35) \\\\ A_2 &: (35 \\times 15) \\\\ A_3 &: (15 \\times 5) \\\\ A_4 &: (5 \\times 10) \\\\ A_5 &: (10 \\times 20) \\\\ A_6 &: (20 \\times 25) \\end{aligned} \\quad \\begin{aligned} & ((A_2)\\overbrace{\\vdots}^{ (k=2) } (A_3 A_4 A_5)) \\rightarrow m_{22} + m_{35} + p_1p_2p_5 = 13000 \\\\ & ((A_2 A_3) \\overbrace{\\vdots}^{ (k=3) } (A_4 A_5)) \\rightarrow m_{23} + m_{45} + p_1p_3p_5 = \\overbrace{ \\boldsymbol{7125}}^{selected} \\Leftarrow \\text{min} \\\\ & ((A_2 A_3 A_4)\\overbrace{\\vdots}^{ (k=4) }(A_5)) \\rightarrow m_{24} + m_{55} + p_1p_4p_5 = 11375 \\\\[20 pt] & m_{25} = 7125 \\\\ & s_{25} = 3 \\end{aligned} \\end{align*} \\] Constructing an Optimal Solution \u00b6 MATRIX-CHAIN-ORDER determines the optimal \\(\\#\\) of scalar mults/adds needed to compute a matrix-chain product it does not directly show how to multiply the matrices That is, it determines the cost of the optimal solution(s) it does not show how to obtain an optimal solution Each entry \\(s[i, j]\\) records the value of \\(k\\) such that optimal parenthesization of \\(A_i \\dots A_j\\) splits the product between \\(A_k\\) & \\(A_{k+1}\\) We know that the final matrix multiplication in computing \\(A_{1 \\dots n}\\) optimally is \\(A_{1 \\dots s[1,n]} \\times A_{s[1,n]+1,n}\\) Example: Constructing an Optimal Solution \u00b6 Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) What is the optimal top-level split for: \\(A_1 A_2 A_3 A_4 A_5 A_6\\) \\(s_{16}=3\\) Example: Constructing an Optimal Solution \u00b6 Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) \\((A_1 A_2 A_3) \\overbrace{\\vdots}^{ (k=4) } (A_4 A_5 A_6)\\) What is the optimal split for \\(A_1 \\dots A_3\\) ? ( \\(s_{13}=1\\) ) What is the optimal split for \\(A_4 \\dots A_6\\) ? ( \\(s_{46}=5\\) ) Example: Constructing an Optimal Solution \u00b6 Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) \\(\\Big((A_1) \\overbrace{\\vdots}^{ (k=1) } (A_2 A_3) \\Big) \\Big( (A_4 A_5) \\overbrace{\\vdots}^{ (k=5) } (A_6) \\Big)\\) What is the optimal split for \\(A_1 \\dots A_3\\) ? ( \\(s_{13}=1\\) ) What is the optimal split for \\(A_4 \\dots A_6\\) ? ( \\(s_{46}=5\\) ) Example: Constructing an Optimal Solution \u00b6 Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) \\(\\Big((A_1) (A_2 A_3) \\Big) \\Big( (A_4 A_5) (A_6) \\Big)\\) What is the optimal split for \\(A_2 A_3\\) ? ( \\(s_{23}=2\\) ) What is the optimal split for \\(A_4 A_5\\) ? ( \\(s_{45}=4\\) ) Example: Constructing an Optimal Solution \u00b6 Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) \\(\\bigg(\\Big(A_1\\Big)\\Big((A_2)\\overbrace{\\vdots}^{ (k=2) }(A_3)\\Big) \\bigg) \\bigg( \\Big((A_4)\\overbrace{\\vdots}^{ (k=4) }(A_5)\\Big) \\Big(A_6\\Big) \\bigg)\\) What is the optimal split for \\(A_2 A_3\\) ? ( \\(s_{23}=2\\) ) What is the optimal split for \\(A_4 A_5\\) ? ( \\(s_{45}=4\\) ) Constructing an Optimal Solution \u00b6 section{ font-size: 20px; } Earlier optimal matrix multiplications can be computed recursively Given: the chain of matrices \\(A = \\langle A_1, A_2, \\dots A_n \\rangle\\) the s table computed by \\(\\text{MATRIX-CHAIN-ORDER}\\) The following recursive procedure computes the matrix-chain product \\(A_{i \\dots j}\\) \\[ \\begin{align*} & \\text{MATRIX-CHAIN-MULTIPLY}(A, s, i, j) \\\\ & \\quad \\text{if} \\ j > i \\ \\text{then} \\\\ & \\qquad X \\longleftarrow \\text{MATRIX-CHAIN-MULTIPLY}(A, s, i, s[i, j]) \\\\ & \\qquad Y \\longleftarrow \\text{MATRIX-CHAIN-MULTIPLY}(A, s, s[i, j]+1, j) \\\\ & \\qquad \\text{return} \\ \\text{MATRIX-MULTIPLY}(X, Y) \\\\ & \\quad \\text{else} \\\\ & \\qquad return A_i \\end{align*} \\] Invocation: \\(\\text{MATRIX-CHAIN-MULTIPLY}(A, s, 1, n)\\) Example: Recursive Construction of an Optimal Solution \u00b6 Example: Recursive Construction of an Optimal Solution \u00b6 Example: Recursive Construction of an Optimal Solution \u00b6 Table reference pattern for \\(m[i, j]\\) \\((1 \\leq i \\leq j \\leq n)\\) \u00b6 \\(m[i, j]\\) is referenced for the computation of \\(m[i, r] \\ \\text{for} \\ j < r \\leq n \\ (n - j )\\) times \\(m[r, j] \\ \\text{for} \\ 1 \\leq r < i \\ (i - 1 )\\) times Table reference pattern for \\(m[i, j]\\) \\((1 \\leq i \\leq j \\leq n)\\) \u00b6 \\(R(i, j)\\) = \\(\\#\\) of times that \\(m[i, j]\\) is referenced in computing other entries \\[ \\begin{align*} R(i, j) &= (n - j) + (i-1) \\\\ &=(n-1) - (j-i) \\end{align*} \\] The total \\(\\#\\) of references for the entire table is: \\(\\sum \\limits_{i=1}^{n}\\sum \\limits_{j=i}^{n}R(i,j)= \\frac{n^3-n}{3}\\) Summary \u00b6 Identification of the optimal substructure property Recursive formulation to compute the cost of the optimal solution Bottom-up computation of the table entries Constructing the optimal solution by backtracing the table entries References \u00b6 Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) \\(-End-Of-Week-5-Course-Module-\\)","title":"Week-5 (Dynamic Programming)"},{"location":"week-5/ce100-week-5-dp/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-5/ce100-week-5-dp/#week-5-dynamic-programming","text":"","title":"Week-5 (Dynamic Programming)"},{"location":"week-5/ce100-week-5-dp/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-5/ce100-week-5-dp/#quicksort-sort","text":"","title":"Quicksort Sort"},{"location":"week-5/ce100-week-5-dp/#outline","text":"Convex Hull (Divide & Conquer) Dynamic Programming Introduction Divide-and-Conquer (DAC) vs Dynamic Programming (DP) Fibonacci Numbers Recursive Solution Bottom-Up Solution Optimization Problems Development of a DP Algorithms Matrix-Chain Multiplication Matrix Multiplication and Row Columns Definitions Cost of Multiplication Operations (pxqxr) Counting the Number of Parenthesizations The Structure of Optimal Parenthesization Characterize the structure of an optimal solution A Recursive Solution Direct Recursion Inefficiency. Computing the optimal Cost of Matrix-Chain Multiplication Bottom-up Computation Algorithm for Computing the Optimal Costs MATRIX-CHAIN-ORDER Construction and Optimal Solution MATRIX-CHAIN-MULTIPLY Summary","title":"Outline"},{"location":"week-5/ce100-week-5-dp/#dynamic-programming-introduction","text":"An algorithm design paradigm like divide-and-conquer Programming: A tabular method (not writing computer code) Older sense of planning or scheduling, typically by filling in a table Divide-and-Conquer (DAC): subproblems are independent Dynamic Programming (DP): subproblems are not independent Overlapping subproblems: subproblems share sub-subproblems In solving problems with overlapping subproblems A DAC algorithm does redundant work Repeatedly solves common subproblems A DP algorithm solves each problem just once Saves its result in a table","title":"Dynamic Programming - Introduction"},{"location":"week-5/ce100-week-5-dp/#problem-1-fibonacci-numbers-recursive-solution","text":"Reminder: \\[ \\begin{align*} & F(0)=0 \\text{ and } F(1)=1 \\\\ & F(n)=F(n-1)+F(n-2) \\\\[10 pt] &\\text{REC-FIBO}(n) \\{ \\\\ & \\quad \\text{if} \\ n < 2 \\\\ & \\qquad \\text{return} \\ n \\\\ & \\quad \\text{else} \\\\ & \\qquad \\text{return} \\ \\text{REC-FIBO}(n-1) + \\text{REC-FIBO}(n-2) \\ \\} \\end{align*} \\] Overlapping subproblems in different recursive calls. Repeated work!","title":"Problem 1: Fibonacci Numbers Recursive Solution"},{"location":"week-5/ce100-week-5-dp/#problem-1-fibonacci-numbers-recursive-solution_1","text":"Recurrence: exponential runtime \\[ T(n) = T(n-1) + T(n-2) + 1 \\] Recursive algorithm inefficient because it recomputes the same \\(F(i)\\) repeatedly in different branches of the recursion tree.","title":"Problem 1: Fibonacci Numbers Recursive Solution"},{"location":"week-5/ce100-week-5-dp/#problem-1-fibonacci-numbers-bottom-up-computation","text":"Reminder: \\[ \\begin{align*} & F(0)=0 \\text{ and } F(1)=1 \\\\ & F(n)=F(n-1)+F(n-2) \\end{align*} \\] Runtime \\(\\Theta(n)\\) ITER - FIBO ( n ) F [ 0 ] = 0 F [ 1 ] = 1 for i = 2 to n do F [ i ] = F [ i -1 ] + F [ i -2 ] return F [ n ]","title":"Problem 1: Fibonacci Numbers Bottom-up Computation"},{"location":"week-5/ce100-week-5-dp/#optimization-problems","text":"DP typically applied to optimization problems In an optimization problem There are many possible solutions (feasible solutions) Each solution has a value Want to find an optimal solution to the problem A solution with the optimal value (min or max value) Wrong to say the optimal solution to the problem There may be several solutions with the same optimal value","title":"Optimization Problems"},{"location":"week-5/ce100-week-5-dp/#development-of-a-dp-algorithm","text":"Step-1 . Characterize the structure of an optimal solution Step-2 . Recursively define the value of an optimal solution Step-3 . Compute the value of an optimal solution in a bottom-up fashion Step-4 . Construct an optimal solution from the information computed in Step 3 section{ font-size: 25px; }","title":"Development of a DP Algorithm"},{"location":"week-5/ce100-week-5-dp/#problem-2-matric-chain-multiplication","text":"Input: a sequence (chain) \\(\\langle A_1,A_2, \\dots , A_n\\rangle\\) of \\(n\\) matrices Aim: compute the product \\(A_1 \\cdot A_2 \\cdot \\dots A_n\\) A product of matrices is fully parenthesized if It is either a single matrix Or, the product of two fully parenthesized matrix products surrounded by a pair of parentheses. \\(\\bigg(A_i(A_{i+1}A_{i+2} \\dots A_j) \\bigg)\\) \\(\\bigg((A_iA_{i+1}A_{i+2} \\dots A_{j-1})A_j \\bigg)\\) \\(\\bigg((A_iA_{i+1}A_{i+2} \\dots A_k)(A_{k+1}A_{k+2} \\dots A_j)\\bigg)\\) for \\(i \\leq k < j\\) All parenthesizations yield the same product; matrix product is associative section{ font-size: 25px; }","title":"Problem 2: Matric Chain Multiplication"},{"location":"week-5/ce100-week-5-dp/#matrix-chain-multiplication-an-example-parenthesization","text":"Input: \\(\\langle A_1,A_2,A_3,A_4\\rangle\\) ( \\(5\\) distinct ways of full parenthesization) \\[ \\begin{align*} & \\bigg(A_1\\Big(A_2(A_3A_4)\\Big)\\bigg) \\\\ & \\bigg(A_1\\Big((A_2A_3)A_4\\Big)\\bigg) \\\\ & \\bigg((A_1A_2)(A_3A_4)\\bigg) \\\\ & \\bigg(\\Big(A_1(A_2A_3)A_4\\Big)\\bigg) \\\\ & \\bigg(\\Big((A_1A_2)A_3\\Big)A_4\\bigg) \\end{align*} \\] The way we parenthesize a chain of matrices can have a dramatic effect on the cost of computing the product","title":"Matrix-chain Multiplication: An Example Parenthesization"},{"location":"week-5/ce100-week-5-dp/#matrix-chain-multiplication-reminder","text":"MATRIX - MULTIPLY ( A , B ) if cols [ A ] != rows [ B ] then error ( \u201c incompatible dimensions \u201d ) for i = 1 to rows [ A ] do for j = 1 to cols [ B ] do C [ i , j ] = 0 for k = 1 to cols [ A ] do C [ i , j ] = C [ i , j ] + A [ i , k ] \u00b7 B [ k , j ] return C","title":"Matrix-chain Multiplication: Reminder"},{"location":"week-5/ce100-week-5-dp/#matrix-chain-multiplication-example","text":"\\(A1:10\\text{x}100\\) , \\(A2:100\\text{x}5\\) , \\(A3:5\\text{x}50\\) Which paranthesization is better? \\((A1A2)A3\\) or \\(A1(A2A3)\\) ?","title":"Matrix Chain Multiplication: Example"},{"location":"week-5/ce100-week-5-dp/#matrix-chain-multiplication-example_1","text":"\\(A1:10 \\times 100\\) , \\(A2:100 \\times 5\\) , \\(A3:5 \\times 50\\) Which paranthesization is better? \\((A1A2)A3\\) or \\(A1(A2A3)\\) ?","title":"Matrix Chain Multiplication: Example"},{"location":"week-5/ce100-week-5-dp/#matrix-chain-multiplication-example_2","text":"\\(A1:10 \\times 100\\) , \\(A2:100 \\times 5\\) , \\(A3:5 \\times 50\\) Which paranthesization is better? \\((A1A2)A3\\) or \\(A1(A2A3)\\) ? In summary: \\((A1A2)A3\\) = \\(\\#\\) of multiply-add ops: \\(7500\\) \\(A1(A2A3)\\) = \\(\\#\\) of multiple-add ops: \\(75000\\) First parenthesization yields 10x faster computation","title":"Matrix Chain Multiplication: Example"},{"location":"week-5/ce100-week-5-dp/#matrix-chain-multiplication-problem","text":"Input: A chain \\(\\langle A_1,A_2, \\dots ,A_n\\rangle\\) of \\(n\\) matrices, where \\(A_i\\) is a \\(p_{i-1} \\times p_i\\) matrix Objective: Fully parenthesize the product \\(A_1 \\cdot A_2 \\dots A_n\\) such that the number of scalar mult-adds is minimized. section{ font-size: 25px; }","title":"Matrix-chain Multiplication Problem"},{"location":"week-5/ce100-week-5-dp/#counting-the-number-of-parenthesizations","text":"Brute force approach: exhaustively check all parenthesizations \\(P(n)\\) : \\(\\#\\) of parenthesizations of a sequence of n matrices We can split sequence between \\(k^{th}\\) and \\((k+1)^{st}\\) matrices for any \\(k=1, 2, \\dots , n-1\\) , then parenthesize the two resulting sequences independently, i.e., \\[ (A_1 A_2 A_3 \\dots A_k \\overbrace{)(}^{break-point}A_{k+1} A_{k+2} \\dots A_n) \\] We obtain the recurrence \\[ P(1)=1 \\text{ and } P(n)=\\sum \\limits_{k=1}^{n-1}P(k)P(n-k) \\]","title":"Counting the Number of Parenthesizations"},{"location":"week-5/ce100-week-5-dp/#number-of-parenthesizations","text":"\\(P(1)=1\\) and \\(P(n)=\\sum \\limits_{k=1}^{n-1}P(k)P(n-k)\\) The recurrence generates the sequence of Catalan Numbers Solution is \\(P(n)=C(n-1)\\) where \\[ C(n)=\\frac{1}{n+1} {2n \\choose n} = \\Omega(4^n / n^{3/2}) \\] The number of solutions is exponential in \\(n\\) Therefore, brute force approach is a poor strategy","title":"Number of Parenthesizations:"},{"location":"week-5/ce100-week-5-dp/#the-structure-of-optimal-parenthesization","text":"Notation: \\(A_{i..j}\\) : The matrix that results from evaluation of the product: \\(A_i A_{i+1} A_{i+2} \\dots A_j\\) Observation: Consider the last multiplication operation in any parenthesization: \\((A_1 A_2 \\dots A_k) \\cdot (A_{k+1} A_{k+2} \\dots A_n)\\) There is a \\(k\\) value \\((1 \\leq k < n)\\) such that: First, the product \\(A_1 \\dots k\\) is computed Then, the product \\(A_{k+1 \\dots n}\\) is computed Finally, the matrices \\(A_{1 \\dots k}\\) and \\(A_{k+1 \\dots n}\\) are multiplied","title":"The Structure of Optimal Parenthesization"},{"location":"week-5/ce100-week-5-dp/#step-1-characterize-the-structure-of-an-optimal-solution","text":"An optimal parenthesization of product \\(A_1 A_2 \\dots A_n\\) will be: \\((A_1 A_2 \\dots A_k) \\cdot (A_{k+1} A_{k+2} \\dots A_n)\\) for some \\(k\\) value The cost of this optimal parenthesization will be: \\(=\\) Cost of computing \\(A_{1 \\dots k}\\) \\(+\\) Cost of computing \\(A_{k+1 \\dots n}\\) \\(+\\) Cost of multiplying \\(A_{1 \\dots k} \\cdot A_{k+1 \\dots n}\\)","title":"Step 1: Characterize the Structure of an Optimal Solution"},{"location":"week-5/ce100-week-5-dp/#step-1-characterize-the-structure-of-an-optimal-solution_1","text":"Key observation: Given optimal parenthesization \\((A_1 A_2 A_3 \\dots A_k) \\cdot (A_{k+1} A_{k+2} \\dots A_n)\\) Parenthesization of the subchain \\(A_1 A_2 A_3 \\dots A_k\\) Parenthesization of the subchain \\(A_{k+1} A_{k+2} \\dots A_n\\) should both be optimal Thus, optimal solution to an instance of the problem contains optimal solutions to subproblem instances i.e. , optimal substructure within an optimal solution exists.","title":"Step 1: Characterize the Structure of an Optimal Solution"},{"location":"week-5/ce100-week-5-dp/#step-2-a-recursive-solution","text":"Step 2: Define the value of an optimal solution recursively in terms of optimal solutions to the subproblems Assume we are trying to determine the min cost of computing \\(A_{i \\dots j}\\) \\(m_{i,j}\\) : min \\(\\#\\) of scalar multiply-add opns needed to compute \\(A_{i \\dots j}\\) Note: The optimal cost of the original problem: \\(m_{1,n}\\) How to compute \\(m_{i,j}\\) recursively?","title":"Step 2: A Recursive Solution"},{"location":"week-5/ce100-week-5-dp/#step-2-a-recursive-solution_1","text":"Base case: \\(m_{i,i}=0\\) (single matrix, no multiplication) Let the size of matrix \\(A_i\\) be \\((p_{i-1} \\times p_i)\\) Consider an optimal parenthesization of chain \\(A_i \\dots A_j : (A_i \\dots A_k) \\cdot (A_{k+1} \\dots A_j)\\) The optimal cost: \\(m_{i,j} = m_{i,k} + m_{k+1,j} + p_{i-1} \\times p_k \\times p_j\\) where: \\(m_{i,k}\\) : Optimal cost of computing \\(A_{i \\dots k}\\) \\(m_{k+1,j}\\) : Optimal cost of computing \\(A_{k+1 \\dots j}\\) \\(p_{i-1} \\times p_k \\times p_j\\) : Cost of multiplying \\(A_{i \\dots k}\\) and \\(A_{k+1 \\dots j}\\)","title":"Step 2: A Recursive Solution"},{"location":"week-5/ce100-week-5-dp/#step-2-a-recursive-solution_2","text":"In an optimal parenthesization: \\(k\\) must be chosen to minimize \\(m_{ij}\\) The recursive formulation for \\(m_{ij}\\) : \\[ \\begin{align*} m_{ij} = \\begin{cases} 0 & if & i=j \\\\ \\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} & if & i<j \\end{cases} \\end{align*} \\]","title":"Step 2: A Recursive Solution"},{"location":"week-5/ce100-week-5-dp/#step-2-a-recursive-solution_3","text":"The \\(m_{ij}\\) values give the costs of optimal solutions to subproblems In order to keep track of how to construct an optimal solution Define \\(s_{ij}\\) to be the value of \\(k\\) which yields the optimal split of the subchain \\(A_{i \\dots j}\\) That is, \\(s_{ij}=k\\) such that \\(m_{ij} = m_{ik} + m_{k+1,j} +p_{i-1} p_k p_j\\) holds section{ font-size: 25px; }","title":"Step 2: A Recursive Solution"},{"location":"week-5/ce100-week-5-dp/#direct-recursion-inefficient","text":"Recursive Matrix-Chain ( RMC ) Order RMC ( p , i , j ) if ( i == j ) then return 0 m [ i , j ] = INF for k = i to j -1 do q = RMC ( p , i , k ) + RMC ( p , k +1 , j ) + p_ { i -1 } p_k p_j if q < m [ i , j ] then m [ i , j ] = q endfor return m [ i , j ]","title":"Direct Recursion: Inefficient!"},{"location":"week-5/ce100-week-5-dp/#direct-recursion-inefficient_1","text":"Recursion tree for \\(RMC(p,1,4)\\) Nodes are labeled with \\(i\\) and \\(j\\) values","title":"Direct Recursion: Inefficient!"},{"location":"week-5/ce100-week-5-dp/#computing-the-optimal-cost-matrix-chain-multiplication","text":"An important observation: - We have relatively few subproblems - one problem for each choice of \\(i\\) and \\(j\\) satisfying \\(1 \\leq i \\leq j \\leq n\\) - total \\(n + (n-1) + \\dots + 2 + 1 = \\frac{1}{2}n(n+1) = \\Theta(n2)\\) subproblems - We can write a recursive algorithm based on recurrence. - However, a recursive algorithm may encounter each subproblem many times in different branches of the recursion tree - This property, overlapping subproblems , is the second important feature for applicability of dynamic programming","title":"Computing the Optimal Cost (Matrix-Chain Multiplication)"},{"location":"week-5/ce100-week-5-dp/#computing-the-optimal-cost-matrix-chain-multiplication_1","text":"Compute the value of an optimal solution in a bottom-up fashion matrix \\(A_i\\) has dimensions \\(p_{i-1} \\times p_i\\) for \\(i = 1, 2, \\dots , n\\) the input is a sequence \\(\\langle p_0, p_1, \\dots, p_n \\rangle\\) where \\(length[p] = n + 1\\) Procedure uses the following auxiliary tables: \\(m[1 \\dots n, 1 \\dots n]\\) : for storing the \\(m[i,j]\\) costs \\(s[1 \\dots n, 1 \\dots n]\\) : records which index of \\(k\\) achieved the optimal cost in computing \\(m[i,j]\\)","title":"Computing the Optimal Cost (Matrix-Chain Multiplication)"},{"location":"week-5/ce100-week-5-dp/#bottom-up-computation","text":"How to choose the order in which we process \\(m_{ij}\\) values? Before computing \\(m_{ij}\\) , we have to make sure that the values for \\(m_{ik}\\) and \\(m_{k+1,j}\\) have been computed for all \\(k\\) . \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\]","title":"Bottom-Up Computation"},{"location":"week-5/ce100-week-5-dp/#bottom-up-computation_1","text":"\\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] \\(m_{ij}\\) must be processed after \\(m_{ik}\\) and \\(m_{j,k+1}\\) Reminder: \\(m_{ij}\\) computed only for \\(j > i\\)","title":"Bottom-Up Computation"},{"location":"week-5/ce100-week-5-dp/#bottom-up-computation_2","text":"\\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] \\(m_{ij}\\) must be processed after \\(m_{ik}\\) and \\(m_{j,k+1}\\) How to set up the iterations over \\(i\\) and \\(j\\) to compute \\(m_{ij}\\) ?","title":"Bottom-Up Computation"},{"location":"week-5/ce100-week-5-dp/#bottom-up-computation_3","text":"\\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] If the entries \\(m_{ij}\\) are computed in the shown order, then \\(m_{ik}\\) and \\(m_{k+1,j}\\) values are guaranteed to be computed before \\(m_{ij}\\) .","title":"Bottom-Up Computation"},{"location":"week-5/ce100-week-5-dp/#bottom-up-computation_4","text":"\\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\]","title":"Bottom-Up Computation"},{"location":"week-5/ce100-week-5-dp/#bottom-up-computation_5","text":"\\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] section{ font-size: 25px; }","title":"Bottom-Up Computation"},{"location":"week-5/ce100-week-5-dp/#algorithm-for-computing-the-optimal-costs","text":"Note : l \\(=\\ell\\) and p_{i-1} p_k p_j \\(=p_{i-1} p_k p_j\\) MATRIX - CHAIN - ORDER ( p ) n = length [ p ] -1 for i = 1 to n do m [ i , i ] = 0 endfor for l = 2 to n do for i = 1 to n n - l +1 do j = i + l -1 m [ i , j ] = INF for k = i to j -1 do q = m [ i , k ] + m [ k +1 , j ] + p_ { i -1 } p_k p_j if q < m [ i , j ] then m [ i , j ] = q s [ i , j ] = k endfor endfor endfor return m and s","title":"Algorithm for Computing the Optimal Costs"},{"location":"week-5/ce100-week-5-dp/#algorithm-for-computing-the-optimal-costs_1","text":"The algorithm first computes \\(m[i, i] \\leftarrow 0\\) for \\(i=1,2, \\dots ,n\\) min costs for all chains of length 1 Then , for \\(\\ell = 2,3, \\dots,n\\) computes \\(m[i, i+\\ell-1]\\) for \\(i=1,\\dots,n-\\ell+1\\) min costs for all chains of length \\(\\ell\\) For each value of \\(\\ell = 2, 3, \\dots ,n\\) , \\(m[i, i+\\ell-1]\\) depends only on table entries \\(m[i,k] \\& m[k+1, i+\\ell-1]\\) for \\(i\\leq k < i+\\ell-1\\) , which are already computed","title":"Algorithm for Computing the Optimal Costs"},{"location":"week-5/ce100-week-5-dp/#algorithm-for-computing-the-optimal-costs_2","text":"\\[ \\begin{align} \\begin{aligned} \\text{compute } m[i,i+1] \\\\ \\underbrace{ \\{ m[1,2],m[2,3], \\dots ,m[n-1,n]\\} }_{(n-1) \\text{ values}} \\end{aligned} & \\begin{cases} & \\ell=2 \\\\ & \\text{for } i=1 \\text{ to } n-1 \\text{ do } \\\\ & \\quad m[i,i+1]=\\infty \\\\ & \\quad \\quad \\text{for } k=i \\text{ to } i \\text{ do } \\\\ & \\quad \\quad \\quad \\vdots \\end{cases} \\\\ \\begin{aligned} \\text{compute } m[i,i+2] \\\\ \\underbrace{ \\{ m[1,3],m[2,4], \\dots ,m[n-2,n]\\} }_{(n-2) \\text{ values}} \\end{aligned} & \\begin{cases} & \\ell=3 \\\\ & \\text{for } i=1 \\text{ to } n-2 \\text{ do } \\\\ & \\quad m[i,i+2]=\\infty \\\\ & \\quad \\quad \\text{for } k=i \\text{ to } i+1 \\text{ do } \\\\ & \\quad \\quad \\quad \\vdots \\end{cases} \\\\ \\begin{aligned} \\text{compute } m[i,i+3] \\\\ \\underbrace{ \\{ m[1,4],m[2,5], \\dots ,m[n-3,n]\\} }_{(n-3) \\text{ values}} \\end{aligned} & \\begin{cases} & \\ell=4 \\\\ & \\text{for } i=1 \\text{ to } n-3 \\text{ do } \\\\ & \\quad m[i,i+3]=\\infty \\\\ & \\quad \\quad \\text{for } k=i \\text{ to } i+2 \\text{ do } \\\\ & \\quad \\quad \\quad \\vdots \\end{cases} \\end{align} \\]","title":"Algorithm for Computing the Optimal Costs"},{"location":"week-5/ce100-week-5-dp/#table-access-pattern-in-computing-mi-js-for-ellj-i1","text":"\\[ \\begin{align*} & \\text{for} \\ k \\leftarrow i \\ \\text{to} \\ j-1 \\ \\text{do} \\\\ & \\quad q \\leftarrow m[i,k]+m[k+1,j]+p_{i-1}p_kp_j \\end{align*} \\]","title":"Table access pattern in computing \\(m[i, j]\\)s for \\(\\ell=j-i+1\\)"},{"location":"week-5/ce100-week-5-dp/#table-access-pattern-in-computing-mi-js-for-ellj-i1_1","text":"\\[ \\begin{align*} & \\bigg( (A_i) \\overset{mult.}{ \\vdots } (A_{i+1}A_{i+2} \\dots A_j) \\bigg) \\end{align*} \\]","title":"Table access pattern in computing \\(m[i, j]\\)s for \\(\\ell=j-i+1\\)"},{"location":"week-5/ce100-week-5-dp/#table-access-pattern-in-computing-mi-js-for-ellj-i1_2","text":"\\[ \\begin{align*} \\bigg( (A_iA_{i+1}) \\overset{mult.}{ \\vdots } (A_{i+2} \\dots A_j) \\bigg) \\end{align*} \\]","title":"Table access pattern in computing \\(m[i, j]\\)s for \\(\\ell=j-i+1\\)"},{"location":"week-5/ce100-week-5-dp/#table-access-pattern-in-computing-mi-js-for-ellj-i1_3","text":"\\[ \\begin{align*} \\bigg( (A_iA_{i+1}A_{i+2}) \\overset{mult.}{ \\vdots } (A_{i+3} \\dots A_j) \\bigg) \\end{align*} \\]","title":"Table access pattern in computing \\(m[i, j]\\)s for \\(\\ell=j-i+1\\)"},{"location":"week-5/ce100-week-5-dp/#table-access-pattern-in-computing-mi-js-for-ellj-i1_4","text":"\\[ \\begin{align*} \\bigg( (A_iA_{i+1} \\dots A_{j-1}) \\overset{mult.}{ \\vdots } (A_j) \\bigg) \\end{align*} \\]","title":"Table access pattern in computing \\(m[i, j]\\)s for \\(\\ell=j-i+1\\)"},{"location":"week-5/ce100-week-5-dp/#table-access-pattern-example","text":"Compute \\(m_{25}\\) Choose the \\(k\\) value that leads to min cost \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\\\[10pt] \\begin{align*} \\begin{aligned} A_1 &: (30 \\times 35) \\\\ A_2 &: (35 \\times 15) \\\\ A_3 &: (15 \\times 5) \\\\ A_4 &: (5 \\times 10) \\\\ A_5 &: (10 \\times 20) \\\\ A_6 &: (20 \\times 25) \\end{aligned} \\begin{aligned} & ((A_2) \\overbrace{\\vdots}^{ (k=2) } (A_3 A_4 A_5)) \\\\[10 pt] \\quad cost &= m_{22} + m_{35} + p_1p_2p_5 \\\\ &= 0 + 2500 + 35 \\times 15 \\times 20 \\\\ &= 13000 \\end{aligned} \\end{align*} \\]","title":"Table access pattern Example"},{"location":"week-5/ce100-week-5-dp/#table-access-pattern-example_1","text":"Compute \\(m_{25}\\) Choose the \\(k\\) value that leads to min cost \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\\\[10pt] \\begin{align*} \\begin{aligned} A_1 &: (30 \\times 35) \\\\ A_2 &: (35 \\times 15) \\\\ A_3 &: (15 \\times 5) \\\\ A_4 &: (5 \\times 10) \\\\ A_5 &: (10 \\times 20) \\\\ A_6 &: (20 \\times 25) \\end{aligned} \\begin{aligned} & ((A_2 A_3) \\overbrace{\\vdots}^{ (k=3) } (A_4 A_5)) \\\\[10 pt] \\quad cost &= m_{23} + m_{45} + p_1p_3p_5 \\\\ &= 2625 + 1000 + 35 \\times 5 \\times 20 \\\\ &= 7125 \\end{aligned} \\end{align*} \\]","title":"Table access pattern Example"},{"location":"week-5/ce100-week-5-dp/#table-access-pattern-example_2","text":"Compute \\(m_{25}\\) Choose the \\(k\\) value that leads to min cost \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\\\[10pt] \\begin{align*} \\begin{aligned} A_1 &: (30 \\times 35) \\\\ A_2 &: (35 \\times 15) \\\\ A_3 &: (15 \\times 5) \\\\ A_4 &: (5 \\times 10) \\\\ A_5 &: (10 \\times 20) \\\\ A_6 &: (20 \\times 25) \\end{aligned} \\begin{aligned} & ((A_2 A_3 A_4)\\overbrace{\\vdots}^{ (k=4) }(A_5)) \\\\[10 pt] \\quad cost &= m_{24} + m_{55} + p_1p_4p_5 \\\\ &= 4375 + 0 + 35 \\times 10 \\times 20 \\\\ &= 11375 \\end{aligned} \\end{align*} \\]","title":"Table access pattern Example"},{"location":"week-5/ce100-week-5-dp/#table-access-pattern-example_3","text":"Compute \\(m_{25}\\) Choose the \\(k\\) value that leads to min cost \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\\\[10pt] \\begin{align*} \\begin{aligned} A_1 &: (30 \\times 35) \\\\ A_2 &: (35 \\times 15) \\\\ A_3 &: (15 \\times 5) \\\\ A_4 &: (5 \\times 10) \\\\ A_5 &: (10 \\times 20) \\\\ A_6 &: (20 \\times 25) \\end{aligned} \\quad \\begin{aligned} & ((A_2)\\overbrace{\\vdots}^{ (k=2) } (A_3 A_4 A_5)) \\rightarrow m_{22} + m_{35} + p_1p_2p_5 = 13000 \\\\ & ((A_2 A_3) \\overbrace{\\vdots}^{ (k=3) } (A_4 A_5)) \\rightarrow m_{23} + m_{45} + p_1p_3p_5 = \\overbrace{ \\boldsymbol{7125}}^{selected} \\Leftarrow \\text{min} \\\\ & ((A_2 A_3 A_4)\\overbrace{\\vdots}^{ (k=4) }(A_5)) \\rightarrow m_{24} + m_{55} + p_1p_4p_5 = 11375 \\\\[20 pt] & m_{25} = 7125 \\\\ & s_{25} = 3 \\end{aligned} \\end{align*} \\]","title":"Table access pattern Example"},{"location":"week-5/ce100-week-5-dp/#constructing-an-optimal-solution","text":"MATRIX-CHAIN-ORDER determines the optimal \\(\\#\\) of scalar mults/adds needed to compute a matrix-chain product it does not directly show how to multiply the matrices That is, it determines the cost of the optimal solution(s) it does not show how to obtain an optimal solution Each entry \\(s[i, j]\\) records the value of \\(k\\) such that optimal parenthesization of \\(A_i \\dots A_j\\) splits the product between \\(A_k\\) & \\(A_{k+1}\\) We know that the final matrix multiplication in computing \\(A_{1 \\dots n}\\) optimally is \\(A_{1 \\dots s[1,n]} \\times A_{s[1,n]+1,n}\\)","title":"Constructing an Optimal Solution"},{"location":"week-5/ce100-week-5-dp/#example-constructing-an-optimal-solution","text":"Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) What is the optimal top-level split for: \\(A_1 A_2 A_3 A_4 A_5 A_6\\) \\(s_{16}=3\\)","title":"Example: Constructing an Optimal Solution"},{"location":"week-5/ce100-week-5-dp/#example-constructing-an-optimal-solution_1","text":"Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) \\((A_1 A_2 A_3) \\overbrace{\\vdots}^{ (k=4) } (A_4 A_5 A_6)\\) What is the optimal split for \\(A_1 \\dots A_3\\) ? ( \\(s_{13}=1\\) ) What is the optimal split for \\(A_4 \\dots A_6\\) ? ( \\(s_{46}=5\\) )","title":"Example: Constructing an Optimal Solution"},{"location":"week-5/ce100-week-5-dp/#example-constructing-an-optimal-solution_2","text":"Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) \\(\\Big((A_1) \\overbrace{\\vdots}^{ (k=1) } (A_2 A_3) \\Big) \\Big( (A_4 A_5) \\overbrace{\\vdots}^{ (k=5) } (A_6) \\Big)\\) What is the optimal split for \\(A_1 \\dots A_3\\) ? ( \\(s_{13}=1\\) ) What is the optimal split for \\(A_4 \\dots A_6\\) ? ( \\(s_{46}=5\\) )","title":"Example: Constructing an Optimal Solution"},{"location":"week-5/ce100-week-5-dp/#example-constructing-an-optimal-solution_3","text":"Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) \\(\\Big((A_1) (A_2 A_3) \\Big) \\Big( (A_4 A_5) (A_6) \\Big)\\) What is the optimal split for \\(A_2 A_3\\) ? ( \\(s_{23}=2\\) ) What is the optimal split for \\(A_4 A_5\\) ? ( \\(s_{45}=4\\) )","title":"Example: Constructing an Optimal Solution"},{"location":"week-5/ce100-week-5-dp/#example-constructing-an-optimal-solution_4","text":"Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) \\(\\bigg(\\Big(A_1\\Big)\\Big((A_2)\\overbrace{\\vdots}^{ (k=2) }(A_3)\\Big) \\bigg) \\bigg( \\Big((A_4)\\overbrace{\\vdots}^{ (k=4) }(A_5)\\Big) \\Big(A_6\\Big) \\bigg)\\) What is the optimal split for \\(A_2 A_3\\) ? ( \\(s_{23}=2\\) ) What is the optimal split for \\(A_4 A_5\\) ? ( \\(s_{45}=4\\) )","title":"Example: Constructing an Optimal Solution"},{"location":"week-5/ce100-week-5-dp/#constructing-an-optimal-solution_1","text":"section{ font-size: 20px; } Earlier optimal matrix multiplications can be computed recursively Given: the chain of matrices \\(A = \\langle A_1, A_2, \\dots A_n \\rangle\\) the s table computed by \\(\\text{MATRIX-CHAIN-ORDER}\\) The following recursive procedure computes the matrix-chain product \\(A_{i \\dots j}\\) \\[ \\begin{align*} & \\text{MATRIX-CHAIN-MULTIPLY}(A, s, i, j) \\\\ & \\quad \\text{if} \\ j > i \\ \\text{then} \\\\ & \\qquad X \\longleftarrow \\text{MATRIX-CHAIN-MULTIPLY}(A, s, i, s[i, j]) \\\\ & \\qquad Y \\longleftarrow \\text{MATRIX-CHAIN-MULTIPLY}(A, s, s[i, j]+1, j) \\\\ & \\qquad \\text{return} \\ \\text{MATRIX-MULTIPLY}(X, Y) \\\\ & \\quad \\text{else} \\\\ & \\qquad return A_i \\end{align*} \\] Invocation: \\(\\text{MATRIX-CHAIN-MULTIPLY}(A, s, 1, n)\\)","title":"Constructing an Optimal Solution"},{"location":"week-5/ce100-week-5-dp/#example-recursive-construction-of-an-optimal-solution","text":"","title":"Example: Recursive Construction of an Optimal Solution"},{"location":"week-5/ce100-week-5-dp/#example-recursive-construction-of-an-optimal-solution_1","text":"","title":"Example: Recursive Construction of an Optimal Solution"},{"location":"week-5/ce100-week-5-dp/#example-recursive-construction-of-an-optimal-solution_2","text":"","title":"Example: Recursive Construction of an Optimal Solution"},{"location":"week-5/ce100-week-5-dp/#table-reference-pattern-for-mi-j-1-leq-i-leq-j-leq-n","text":"\\(m[i, j]\\) is referenced for the computation of \\(m[i, r] \\ \\text{for} \\ j < r \\leq n \\ (n - j )\\) times \\(m[r, j] \\ \\text{for} \\ 1 \\leq r < i \\ (i - 1 )\\) times","title":"Table reference pattern for \\(m[i, j]\\) \\((1 \\leq i \\leq j \\leq n)\\)"},{"location":"week-5/ce100-week-5-dp/#table-reference-pattern-for-mi-j-1-leq-i-leq-j-leq-n_1","text":"\\(R(i, j)\\) = \\(\\#\\) of times that \\(m[i, j]\\) is referenced in computing other entries \\[ \\begin{align*} R(i, j) &= (n - j) + (i-1) \\\\ &=(n-1) - (j-i) \\end{align*} \\] The total \\(\\#\\) of references for the entire table is: \\(\\sum \\limits_{i=1}^{n}\\sum \\limits_{j=i}^{n}R(i,j)= \\frac{n^3-n}{3}\\)","title":"Table reference pattern for \\(m[i, j]\\) \\((1 \\leq i \\leq j \\leq n)\\)"},{"location":"week-5/ce100-week-5-dp/#summary","text":"Identification of the optimal substructure property Recursive formulation to compute the cost of the optimal solution Bottom-up computation of the table entries Constructing the optimal solution by backtracing the table entries","title":"Summary"},{"location":"week-5/ce100-week-5-dp/#references","text":"Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) \\(-End-Of-Week-5-Course-Module-\\)","title":"References"},{"location":"week-6/ce100-week-6-lcs/","text":"CE100 Algorithms and Programming II \u00b6 Week-6 (Matrix Chain Order / LCS) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Matrix Chain Order / Longest Common Subsequence \u00b6 Outline \u00b6 Elements of Dynamic Programming Optimal Substructure Overlapping Subproblems Recursive Matrix Chain Order Memoization Top-Down Approach RMC MemoizedMatrixChain LookupC Dynamic Programming vs Memoization Summary Dynamic Programming Problem-2 : Longest Common Subsequence Definitions LCS Problem Notations Optimal Substructure of LCS Proof Case-1 Proof Case-2 Proof Case-3 A recursive solution to subproblems (inefficient) Computing the length of and LCS LCS Data Structure for DP Bottom-Up Computation Constructing and LCS PRINT-LCS Back-pointer space optimization for LCS length Most Common Dynamic Programming Interview Questions Elements of Dynamic Programming \u00b6 When should we look for a DP solution to an optimization problem? Two key ingredients for the problem Optimal substructure Overlapping subproblems DP Hallmark #1 \u00b6 Optimal Substructure A problem exhibits optimal substructure if an optimal solution to a problem contains within it optimal solutions to subproblems Example: matrix-chain-multiplication Optimal parenthesization of \\(A_1 A_2 \\dots A_n\\) that splits the product between \\(A_k\\) and \\(A_{k+1}\\) , contains within it optimal soln\u2019s to the problems of parenthesizing \\(A_1A_2 \\dots A_k\\) and \\(A_{k+1} A_{k+2} \\dots A_n\\) Optimal Substructure \u00b6 Finding a suitable space of subproblems Iterate on subproblem instances Example: matrix-chain-multiplication Iterate and look at the structure of optimal soln\u2019s to subproblems, sub-subproblems, and so forth Discover that all subproblems consists of subchains of \\(\\langle A_1, A_2, \\dots , A_n \\rangle\\) Thus, the set of chains of the form \\(\\langle A_i,A_{i+1}, \\dots , A_j \\rangle\\) for \\(1 \\leq i \\leq j \\leq n\\) Makes a natural and reasonable space of subproblems DP Hallmark #2 \u00b6 Overlapping Subproblems Total number of distinct subproblems should be polynomial in the input size When a recursive algorithm revisits the same problem over and over again , We say that the optimization problem has overlapping subproblems Overlapping Subproblems \u00b6 DP algorithms typically take advantage of overlapping subproblems by solving each problem once then storing the solutions in a table where it can be looked up when needed using constant time per lookup section{ font-size: 25px; } Overlapping Subproblems \u00b6 Recursive matrix-chain order \\[ \\begin{align*} & \\text{RMC}(p, i, j) \\{ \\\\[5 pt] & \\quad \\text{if} \\ i = j \\ \\text{then} \\\\ & \\qquad \\text{return} \\ 0 \\\\[5 pt] & \\quad m[i, j] \\leftarrow \\infty \\\\[5 pt] & \\quad \\text{for} \\ k \\leftarrow i \\text{to} \\ j - 1 \\ \\text{do} \\\\ & \\qquad q \\leftarrow \\text{RMC}(p, i, k) + \\text{RMC}(p, k+1, j) + p_{i-1} p_k p_j \\\\[5 pt] & \\quad if \\ q < m[i, j] \\ \\text{then} \\\\ & \\qquad m[i, j] \\leftarrow q \\\\[5 pt] & \\quad \\text{return} \\ m[i, j] \\ \\} \\end{align*} \\] Direct Recursion: Inefficient! \u00b6 Recursion tree for \\(RMC(p,1,4)\\) Nodes are labeled with \\(i\\) and \\(j\\) values section{ font-size: 25px; } Running Time of RMC \u00b6 \\(T(1) \\geq 1\\) \\(T(n) \\geq 1 + \\sum \\limits_{k=1}^{n-1} (T(k)+T(n-k)+1)\\ \\text{for} \\ n>1\\) For \\(i =1,2, \\dots ,n\\) each term \\(T(i)\\) appears twice Once as \\(T(k)\\) , and once as \\(T(n-k)\\) Collect \\(n-1,\\) \\(1\\) \u2019s in the summation together with the front \\(1\\) \\[ \\begin{align*} T(n) \\geq 2 \\sum \\limits_{i=1}^{n-1}T(i)+n \\end{align*} \\] Prove that \\(T(n)= \\Omega(2n)\\) using the substitution method section{ font-size: 25px; } Running Time of RMC: Prove that \\(T(n)= \\Omega(2n)\\) \u00b6 Try to show that \\(T(n) \\geq 2^{n-1}\\) ( by substitution ) Base case: \\(T(1) \\geq 1 = 2^0 = 2^{1-1}\\) for \\(n=1\\) Ind. Hyp.: \\[ \\begin{align*} T(i) & \\geq 2^{i-1} \\ \\text{for all} \\ i=1, 2, \\dots, n-1 \\ \\text{and} \\ n \\geq 2 \\\\ T(n) & \\geq 2 \\sum \\limits_{i=1}^{n-1}2^{i-1} + n \\\\[15 pt] & = 2 \\sum \\limits_{i=1}^{n-1} 2^{i-1} + n \\\\ & = 2(2^{n-1}-1) + n \\\\ & = 2^{n-1} + (2^{n-1} - 2 + n) \\\\ & \\Rightarrow T(n) \\geq 2^{n-1} \\ \\text{ Q.E.D.} \\end{align*} \\] section{ font-size: 25px; } Running Time of RMC: \\(T(n) \\geq 2^{n-1}\\) \u00b6 Whenever a recursion tree for the natural recursive solution to a problem contains the same subproblem repeatedly the total number of different subproblems is small it is a good idea to see if \\(DP (Dynamic \\ Programming)\\) can be applied Memoization \u00b6 Offers the efficiency of the usual \\(DP\\) approach while maintaining top-down strategy Idea is to memoize the natural, but inefficient, recursive algorithm Memoized Recursive Algorithm \u00b6 Maintains an entry in a table for the soln to each subproblem Each table entry contains a special value to indicate that the entry has yet to be filled in When the subproblem is first encountered its solution is computed and then stored in the table Each subsequent time that the subproblem encountered the value stored in the table is simply looked up and returned Memoized Recursive Matrix-chain Order \u00b6 Shaded subtrees are looked-up rather than recomputing \\[ \\begin{align*} \\begin{aligned} & \\text{MemoizedMatrixChain(p)} \\\\ & \\quad n \\leftarrow length[p] - 1 \\\\ & \\quad \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad \\text{for} \\ j \\leftarrow 1 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad \\quad m[i, j] \\leftarrow \\infty \\\\ & \\quad \\text{return} \\ \\text{LookupC}(p, 1, n) \\Longrightarrow \\end{aligned} \\begin{aligned} & \\Longrightarrow \\text{LookupC}(p, i, j) \\\\ & \\quad \\text{if} \\ m[i, j] = \\infty \\ \\text{then} \\\\ & \\qquad \\text{if} \\ i = j \\ \\text{then} \\\\ & \\qquad \\quad m[i, j] \\leftarrow 0 \\\\ & \\qquad \\text{else} \\\\ & \\qquad \\quad \\text{for} \\ k \\leftarrow i \\ \\text{to} \\ j-1 \\ \\text{do} \\\\ & \\qquad \\quad \\quad q \\leftarrow \\text{LookupC}(p, i, k) + \\text{LookupC}(p, k+1, j) + p_{i-1} p_k p_j \\\\ & \\qquad \\quad \\quad \\text{if} \\ q < m[i, j] \\ \\text{then} \\\\ & \\qquad \\quad \\quad \\quad m[i, j] \\leftarrow q \\\\ & \\quad \\text{return} \\ m[i, j] \\end{aligned} \\end{align*} \\] Memoized Recursive Algorithm \u00b6 The approach assumes that The set of all possible subproblem parameters are known The relation between the table positions and subproblems is established Another approach is to memoize by using hashing with subproblem parameters as key section{ font-size: 25px; } Dynamic Programming vs Memoization Summary (1) \u00b6 Matrix-chain multiplication can be solved in \\(O(n^3)\\) time by either a top-down memoized recursive algorithm or a bottom-up dynamic programming algorithm Both methods exploit the overlapping subproblems property There are only \\(\\Theta(n^2)\\) different subproblems in total Both methods compute the soln to each problem once Without memoization the natural recursive algorithm runs in exponential time since subproblems are solved repeatedly section{ font-size: 25px; } Dynamic Programming vs Memoization Summary (2) \u00b6 In general practice If all subproblems must be solved at once a bottom-up DP algorithm always outperforms a top-down memoized algorithm by a constant factor because, bottom-up DP algorithm Has no overhead for recursion Less overhead for maintaining the table DP: Regular pattern of table accesses can be exploited to reduce the time and/or space requirements even further Memoized: If some problems need not be solved at all, it has the advantage of avoiding solutions to those subproblems Problem 3: Longest Common Subsequence \u00b6 Definitions A subsequence of a given sequence is just the given sequence with some elements (possibly none) left out Example: \\(X = \\langle A, B, C, B, D, A, B \\rangle\\) \\(Z = \\langle B, C, D, B \\rangle\\) \\(Z\\) is a subsequence of \\(X\\) Problem 3: Longest Common Subsequence \u00b6 Definitions Formal definition: Given a sequence \\(X = \\langle x_1, x_2, \\dots , x_m \\rangle\\) , sequence \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) is a subsequence of \\(X\\) if \\(\\exists\\) a strictly increasing sequence \\(\\langle i_1, i_2,\\dots, i_k \\rangle\\) of indices of \\(X\\) such that \\(x_{i_j} = z_j\\) for all \\(j = 1, 2, \u2026, k\\) , where \\(1 \\leq k \\leq m\\) Example: \\(Z = \\langle B,C,D,B \\rangle\\) is a subsequence of \\(X = \\langle A,B,C,B,D,A,B \\rangle\\) with the index sequence \\(\\langle i_1, i_2, i_3, i_4 \\rangle = \\langle 2, 3, 5, 7 \\rangle\\) section{ font-size: 25px; } Problem 3: Longest Common Subsequence \u00b6 Definitions If \\(Z\\) is a subsequence of both \\(X\\) and \\(Y\\) , we denote \\(Z\\) as a common subsequence of \\(X\\) and \\(Y\\) . Example: \\[ \\begin{align*} X &= \\langle A,B^*,C^*,B,D,A^*,B \\rangle \\\\ Y &= \\langle B^*,D,C^*,A^*,B,A \\rangle \\end{align*} \\] \\(Z_1 = \\langle B^*, C^*, A^* \\rangle\\) is a common subsequence ( of length 3 ) of \\(X\\) and \\(Y\\) . Two longest common subsequence (LCSs) of \\(X\\) and \\(Y\\) ? \\(Z2 = \\langle B, C, B, A \\rangle\\) of length \\(4\\) \\(Z3 = \\langle B, D, A, B \\rangle\\) of length \\(4\\) The optimal solution value = 4 Longest Common Subsequence (LCS) Problem \u00b6 LCS problem: Given two sequences \\(X = \\langle x_1, x_2, \\dots, x_m \\rangle\\) and \\(Y = \\langle y_1, y_2, \\dots , y_n \\rangle\\) , find the LCS of \\(X \\& Y\\) Brute force approach: Enumerate all subsequences of \\(X\\) Check if each subsequence is also a subsequence of \\(Y\\) Keep track of the LCS What is the complexity? There are \\(2^m\\) subsequences of \\(X\\) Exponential runtime Notation \u00b6 Notation: Let \\(X_i\\) denote the \\(i^{th}\\) prefix of \\(X\\) i.e. \\(X_i = \\langle x_1, x_2, \\dots, x_i \\rangle\\) Example: \\[ \\begin{align*} X &= \\langle A, B, C, B, D, A, B \\rangle \\\\[10 pt] X_4 &= \\langle A, B, C, B \\rangle \\\\ X_0 &= \\langle \\rangle \\end{align*} \\] Optimal Substructure of an LCS \u00b6 Let \\(X = <x1, x2, \u2026, xm>\\) and \\(Y = \\langle y_1, y_2, \\dots, y_n \\rangle\\) are given Let \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\) Question 1: If \\(x_m = y_n\\) , how to define the optimal substructure? We must have \\(z_k = x_m = y_n\\) and \\(Z_{k-1} = \\text{LCS}(X_{m-1}, Y_{n-1})\\) Optimal Substructure of an LCS \u00b6 Let \\(X = <x1, x2, \u2026, xm>\\) and \\(Y = \\langle y_1, y_2, \\dots, y_n \\rangle\\) are given Let \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\) Question 2: If \\(x_m \\neq y_n \\ \\text{and} \\ z_k \\neq x_m\\) , how to define the optimal substructure? We must have \\(Z = \\text{LCS}(X_{m-1}, Y)\\) Optimal Substructure of an LCS \u00b6 Let \\(X = <x1, x2, \u2026, xm>\\) and \\(Y = \\langle y_1, y_2, \\dots, y_n \\rangle\\) are given Let \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\) Question 3: If \\(x_m \\neq y_n \\ \\text{and} \\ z_k \\neq y_n\\) , how to define the optimal substructure? We must have \\(Z = \\text{LCS}(X, Y_{n-1})\\) Theorem: Optimal Substructure of an LCS \u00b6 Let \\(X = \\langle x_1, x_2, \\dots, x_m \\rangle\\) and Y = are given Let \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\) Theorem: Optimal substructure of an LCS: If \\(x_m = y_n\\) then \\(z_k = x_m = y_n\\) and \\(Z_{k-1}\\) is an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) If \\(x_m \\neq y_n\\) and \\(z_k \\neq x_m\\) then \\(Z\\) is an LCS of \\(X_{m-1}\\) and \\(Y\\) If \\(x_m \\neq y_n\\) and \\(z_k \\neq y_n\\) then \\(Z\\) is an LCS of \\(X\\) and \\(Y_{n-1}\\) Optimal Substructure Theorem (case 1) \u00b6 If \\(x_m = y_n\\) then \\(z_k = x_m = y_n\\) and \\(Z_{k-1}\\) is an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) Optimal Substructure Theorem (case 2) \u00b6 If \\(x_m \\neq y_n\\) and \\(z_k \\neq x_m\\) then \\(Z\\) is an LCS of \\(X_{m-1}\\) and \\(Y\\) Optimal Substructure Theorem (case 3) \u00b6 If \\(x_m \\neq y_n\\) and \\(z_k \\neq y_n\\) then \\(Z\\) is an LCS of \\(X\\) and \\(Y_{n-1}\\) Proof of Optimal Substructure Theorem (case 1) \u00b6 If \\(x_m = y_n\\) then \\(z_k = x_m = y_n\\) and \\(Z_{k-1}\\) is an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) Proof: If \\(z_k \\neq x_m = y_n\\) then we can append \\(x_m = y_n\\) to \\(Z\\) to obtain a common subsequence of length \\(k+1 \\Longrightarrow\\) contradiction Thus, we must have \\(z_k = x_m = y_n\\) Hence, the prefix \\(Z_{k-1}\\) is a length-( \\(k-1\\) ) CS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) We have to show that \\(Z_{k-1}\\) is in fact an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) Proof by contradiction: Assume that \\(\\exists\\) a CS \\(W\\) of \\(X_{m-1}\\) and \\(Y_{n-1}\\) with \\(|W| = k\\) Then appending \\(x_m = y_n\\) to \\(W\\) produces a CS of length \\(k+1\\) Proof of Optimal Substructure Theorem (case 2) \u00b6 If \\(x_m \\neq y_n\\) and \\(z_k \\neq x_m\\) then \\(Z\\) is an LCS of \\(X_{m-1}\\) and \\(Y\\) Proof : If \\(z_k \\neq x_m\\) then \\(Z\\) is a CS of \\(X_{m-1}\\) and \\(Y_n\\) We have to show that \\(Z\\) is in fact an LCS of \\(X_{m-1}\\) and \\(Y_n\\) (Proof by contradiction) Assume that \\(\\exists\\) a CS \\(W\\) of \\(X_{m-1}\\) and \\(Y_n\\) with \\(|W| > k\\) Then \\(W\\) would also be a CS of \\(X\\) and \\(Y\\) Contradiction to the assumption that \\(Z\\) is an LCS of \\(X\\) and \\(Y\\) with \\(|Z| = k\\) Case 3: Dual of the proof for (case 2) A Recursive Solution to Subproblems \u00b6 Theorem implies that there are one or two subproblems to examine if \\(x_m = y_n\\) then we must solve the subproblem of finding an LCS of \\(X_{m-1} \\& Y_{n-1}\\) appending \\(x_m = y_n\\) to this LCS yields an LCS of \\(X \\& Y\\) else we must solve two subproblems finding an LCS of \\(X_{m-1} \\& Y\\) finding an LCS of \\(X \\& Y_{n-1}\\) longer of these two LCS s is an LCS of \\(X \\& Y\\) endif section{ font-size: 25px; } Recursive Algorithm (Inefficient) \u00b6 \\[ \\begin{align*} & \\text{LCS}(X, Y) \\ \\{ \\\\ & \\quad m \\leftarrow length[X] \\\\ & \\quad n \\leftarrow length[Y] \\\\ & \\quad \\text{if} \\ x_m = y_n \\ \\text{then} \\\\ & \\qquad Z \\leftarrow \\text{LCS}(X_{m-1}, Y_{n-1}) \\triangleright \\text{solve one subproblem} \\\\ & \\qquad \\text{return} \\ \\langle Z, x_m = y_n \\rangle \\triangleright \\text{append} \\ x_m = y_n \\ \\text{to} \\ Z \\\\ & \\quad else \\\\ & \\qquad Z^{'} \\leftarrow \\text{LCS}(X_{m-1}, Y) \\triangleright \\text{solve two subproblems} \\\\ & \\qquad Z^{''} \\leftarrow \\text{LCS}(X, Y_{n-1}) \\\\ & \\qquad \\text{return longer of} \\ Z^{'} \\ \\text{and} \\ Z^{''} \\\\ & \\} \\end{align*} \\] A Recursive Solution \u00b6 \\(c[i, j]:\\) length of an LCS of \\(X_i\\) and \\(Y_j\\) \\[ \\begin{align*} c[i,j] = \\begin{cases} & 0 & \\text{if}& \\ i=0 \\ \\text{or} \\ j=0 \\\\ & c[i-1,j-1]+1 & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i=y_j \\\\ & \\text{max}\\{c[i,j-1],c[i-1,j]\\} & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i \\neq y_j \\\\ \\end{cases} \\end{align*} \\] Computing the Length of an LCS \u00b6 We can easily write an exponential-time recursive algorithm based on the given recurrence. \\(\\Longrightarrow\\) Inefficient! How many distinct subproblems to solve? \\(\\Theta(mn)\\) Overlapping subproblems property: Many subproblems share the same sub-subproblems. e.g. Finding an LCS to \\(X_{m-1} \\& Y\\) and an LCS to \\(X \\& Y_{n-1}\\) has the sub-subproblem of finding an LCS to \\(X_{m-1} \\& Y_{n-1}\\) Therefore, we can use dynamic programming . Data Structures \u00b6 Let: \\(c[i, j]:\\) length of an LCS of \\(X_i\\) and \\(Y_j\\) \\(b[i, j]:\\) direction towards the table entry corresponding to the optimal subproblem solution chosen when computing \\(c[i, j]\\) . Used to simplify the construction of an optimal solution at the end. Maintain the following tables: \\(c[0 \\dots m, 0 \\dots n]\\) \\(b[1 \\dots m, 1 \\dots n]\\) Bottom-up Computation \u00b6 Reminder: \\[ \\begin{align*} c[i,j] = \\begin{cases} & 0 & \\text{if}& \\ i=0 \\ \\text{or} \\ j=0 \\\\ & c[i-1,j-1]+1 & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i=y_j \\\\ & \\text{max}\\{c[i,j-1],c[i-1,j]\\} & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i \\neq y_j \\\\ \\end{cases} \\end{align*} \\] How to choose the order in which we process \\(c[i, j]\\) values? The values for \\(c[i-1, j-1]\\) , \\(c[i, j-1]\\) , and \\(c[i-1,j]\\) must be computed before computing \\(c[i, j]\\) . Bottom-up Computation \u00b6 section{ font-size: 25px; } \\[ \\begin{align*} c[i,j] = \\begin{cases} & 0 & \\text{if}& \\ i=0 \\ \\text{or} \\ j=0 \\\\ & c[i-1,j-1]+1 & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i=y_j \\\\ & \\text{max}\\{c[i,j-1],c[i-1,j]\\} & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i \\neq y_j \\\\ \\end{cases} \\end{align*} \\] Need to process: \\(c[i, j]\\) after computing: \\(c[i-1, j-1]\\) , \\(c[i, j-1]\\) , \\(c[i-1,j]\\) Bottom-up Computation \u00b6 section{ font-size: 25px; } \\[ \\begin{align*} c[i,j] = \\begin{cases} & 0 & \\text{if}& \\ i=0 \\ \\text{or} \\ j=0 \\\\ & c[i-1,j-1]+1 & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i=y_j \\\\ & \\text{max}\\{c[i,j-1],c[i-1,j]\\} & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i \\neq y_j \\\\ \\end{cases} \\end{align*} \\] \\[ \\Downarrow \\] \\[ \\begin{align*} & \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ m \\\\ & \\quad \\text{for} \\ j \\leftarrow 1 \\ \\text{to} \\ n \\\\ & \\qquad \\dots \\\\ & \\qquad \\dots \\\\ & \\qquad c[i, j] = \\cdots \\end{align*} \\] section{ font-size: 25px; } Computing the Length of an LCS \u00b6 \\[ \\begin{align*} \\frac{\\text{Total Runtime} = \\Theta(mn)}{\\text{Total Space} = \\Theta(mn)} \\begin{cases} & LCS-LENGTH(X,Y) \\\\ & \\quad m \\leftarrow length[X]; n \\leftarrow length[Y] \\\\ & \\quad \\text{for} \\ i \\leftarrow 0 \\ \\text{to} \\ m \\ \\text{do} \\ c[i, 0] \\leftarrow 0 \\\\ & \\quad \\text{for} \\ j \\leftarrow 0 \\ \\text{to} \\ n \\ \\text{do} \\ c[0, j] \\leftarrow 0 \\\\ & \\quad \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ m \\ \\text{do} \\\\ & \\qquad \\text{for} \\ j \\leftarrow 1 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad \\quad \\text{if} \\ x_i = y_j \\ \\text{then} \\\\ & \\qquad \\quad \\quad c[i, j] \\leftarrow c[i-1, j-1]+1 \\\\ & \\qquad \\quad \\quad b[i, j] \\leftarrow \" \\nwarrow \" \\\\ & \\qquad \\quad \\text{else if} \\ c[i - 1, j] \\geq c[i, j-1] \\\\ & \\qquad \\quad \\quad c[i, j] \\leftarrow c[i-1, j] \\\\ & \\qquad \\quad \\quad b[i, j] \\leftarrow \"\\uparrow \" \\\\ & \\qquad \\quad \\text{else} \\\\ & \\qquad \\quad \\quad c[i, j] \\leftarrow c[i, j-1] \\\\ & \\qquad \\quad \\quad b[i, j] \\leftarrow \" \\leftarrow \" \\\\ \\end{cases} \\end{align*} \\] section{ font-size: 25px; } Computing the Length of an LCS-1 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-2 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-3 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-4 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-5 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-6 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-7 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-8 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-9 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-10 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-11 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-12 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-13 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ Running-time = \\(O(mn)\\) since each table entry takes \\(O(1)\\) time to compute section{ font-size: 25px; } Computing the Length of an LCS-14 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ Running-time = \\(O(mn)\\) since each table entry takes \\(O(1)\\) time to compute LCS of \\(X \\& Y = \\langle B, C, B, A \\rangle\\) Constructing an LCS \u00b6 The \\(b\\) table returned by LCS-LENGTH can be used to quickly construct an LCS of \\(X \\& Y\\) Begin at \\(b[m, n]\\) and trace through the table following arrows Whenever you encounter a \" \\(\\nwarrow\\) \" in entry \\(b[i, j]\\) it implies that \\(x_i = y_j\\) is an element of LCS The elements of LCS are encountered in reverse order Constructing an LCS \u00b6 section{ font-size: 21px; } The recursive procedure \\(\\text{PRINT-LCS}\\) prints out \\(\\text{LCS}\\) in proper order This procedure takes \\(O(m+n)\\) time since at least one of \\(i\\) and \\(j\\) is decremented in each stage of the recursion \\[ \\begin{align*} & \\text{PRINT-LCS}(b, X, i, j) \\\\ & \\quad \\text{if} \\ i = 0 \\ \\text{or} j = 0 \\ \\text{then} \\\\ & \\quad \\text{return} \\\\ & \\quad \\text{if} \\ b[i, j] = \" \\nwarrow \" \\ \\text{then} \\\\ & \\qquad \\text{PRINT-LCS}(b, X, i-1, j-1) \\\\ & \\qquad \\text{print} \\ x_i \\\\ & \\quad \\text{else if} \\ b[i, j] = \" \\uparrow \" \\ \\text{then} \\\\ & \\qquad \\text{PRINT-LCS}(b, X, i-1, j) \\\\ & \\quad \\text{else} \\\\ & \\qquad \\text{PRINT-LCS}(b, X, i, j-1) \\end{align*} \\] The initial invocation: \\(\\text{PRINT-LCS}(b, X, length[X], length[Y])\\) section{ font-size: 25px; } Do we really need the b table (back-pointers)? \u00b6 Question: From which neighbor did we expand to the highlighted cell? Answer: Upper-left neighbor,because \\(X[i] = Y[j]\\) . section{ font-size: 25px; } Do we really need the b table (back-pointers)? \u00b6 Question: From which neighbor did we expand to the highlighted cell? Answer: Left neighbor, because \\(X[i] \\neq Y[j]\\) and \\(LCS[i, j-1] > LCS[i-1, j]\\) . section{ font-size: 25px; } Do we really need the b table (back-pointers)? \u00b6 Question: From which neighbor did we expand to the highlighted cell? Answer: Upper neighbor,because \\(X[i] \\neq Y[j]\\) and \\(LCS[i, j-1] = LCS[i-1, j]\\) . (See pseudo-code to see how ties are handled.) section{ font-size: 25px; } Improving the Space Requirements \u00b6 We can eliminate the b table altogether each \\(c[i, j]\\) entry depends only on \\(3\\) other \\(c\\) table entries: \\(c[i-1, j-1]\\) , \\(c[i-1, j]\\) and \\(c[i, j-1]\\) Given the value of \\(c[i, j]\\) : We can determine in \\(O(1)\\) time which of these \\(3\\) values was used to compute \\(c[i, j]\\) without inspecting table \\(b\\) We save \\(\\Theta(mn)\\) space by this method However, space requirement is still \\(\\Theta(mn)\\) since we need \\(\\Theta(mn)\\) space for the \\(c\\) table anyway section{ font-size: 25px; } What if we store the last 2 rows only? \u00b6 To compute \\(c[i, j]\\) , we only need \\(c[i-1, j-1]\\) , \\(c[i-1, j]\\) ,and \\(c[i-1, j-1]\\) So, we can store only the last two rows. section{ font-size: 25px; } What if we store the last 2 rows only? \u00b6 To compute \\(c[i, j]\\) , we only need \\(c[i-1, j-1]\\) , \\(c[i-1, j]\\) , and \\(c[i-1, j-1]\\) So, we can store only the last two rows. section{ font-size: 25px; } What if we store the last 2 rows only? \u00b6 To compute \\(c[i, j]\\) , we only need \\(c[i-1, j-1]\\) , \\(c[i-1, j]\\) , and \\(c[i-1, j-1]\\) So, we can store only the last two rows. This reduces space complexity from \\(\\Theta(mn)\\) to \\(\\Theta(n)\\) . Is there a problem with this approach? section{ font-size: 25px; } What if we store the last 2 rows only? \u00b6 Is there a problem with this approach? We cannot construct the optimal solution because we cannot backtrace anymore. This approach works if we only need the length of an LCS, not the actual LCS. Problem 4 Optimal Binary Search Tree \u00b6 Reminder: Binary Search Tree (BST) \u00b6 Binary Search Tree Example \u00b6 Example: English-to-French translation Organize (English, French) word pairs in a BST Keyword: English word Satellite Data: French word We can search for an English word (node key) efficiently, and return the corresponding French word (satellite data). ASCII Table \u00b6 section{ font-size: 25px; } Binary Search Tree Example \u00b6 Suppose we know the frequency of each keyword in texts: $$ \\underset{5\\%}{\\underline{begin}}, \\underset{40\\%}{\\underline{do}}, \\underset{8\\%}{\\underline{else}}, \\underset{4\\%}{\\underline{end}}, \\underset{10\\%}{\\underline{if}}, \\underset{10\\%}{\\underline{then}}, \\underset{23\\%}{\\underline{while}}, $$ section{ font-size: 25px; } Cost of a Binary Search Tree \u00b6 Example: If we search for keyword \"while\" , we need to access \\(3\\) nodes. So, \\(23%\\) of the queries will have cost of \\(3\\) . \\[ \\begin{align*} \\text{Total Cost} &= \\sum \\limits_{i}^{}(\\text{depth}(i)+1)\\text{freq}(i) \\\\ &= 1 \\times 0.04 + 2 \\times 0.4 + \\\\ & 2 \\times 0.1 + 3 \\times 0.05 + \\\\ & 3 \\times 0.08 + 3 \\times 0.1 + \\\\ & 3 \\times 0.23 \\\\ &= 2.42 \\end{align*} \\] section{ font-size: 25px; } Cost of a Binary Search Tree \u00b6 Example: If we search for keyword \"while\" , we need to access \\(3\\) nodes. So, \\(23%\\) of the queries will have cost of \\(3\\) . \\[ \\begin{align*} \\text{Total Cost} &= \\sum \\limits_{i}^{}(\\text{depth}(i)+1)\\text{freq}(i) \\\\ &= 1 \\times 0.4 + 2 \\times 0.05 + 2 \\times 0.23 + \\\\ & 3 \\times 0.1 + 4 \\times 0.08 + \\\\ & 4 \\times 0.1 + 5 \\times 0.04 \\\\ &= 2.18 \\end{align*} \\] This is in fact an optimal BST. section{ font-size: 25px; } Optimal Binary Search Tree Problem \u00b6 Given: A collection of \\(n\\) keys \\(K_1 < K_2 < \\dots K_n\\) to be stored in a BST . The corresponding \\(p_i\\) values for \\(1 \\leq i \\leq n\\) \\(p_i\\) : probability of searching for key \\(K_i\\) Find: An optimal BST with minimum total cost: \\[ \\begin{align*} \\text{Total Cost} &= \\sum \\limits_{i}^{}(\\text{depth}(i)+1)\\text{freq}(i) \\end{align*} \\] Note: The BST will be static. Only search operations will be performed. No insert, no delete, etc. section{ font-size: 25px; } Cost of a Binary Search Tree \u00b6 Lemma 1 : Let \\(Tij\\) be a BST containing keys \\(K_i < K_{i+1} < \\dots < K_j\\) . Let \\(T_L\\) and \\(T_R\\) be the left and right subtrees of \\(T\\) . Then we have: \\[ \\begin{align*} \\text{cost}(T_{ij})=\\text{cost}(T_{L})+\\text{cost}(T_{R})+\\sum \\limits_{h=i}^{j}p_h \\end{align*} \\] Intuition: When we add the root node, the depth of each node in \\(T_L\\) and \\(T_R\\) increases by \\(1\\) . So, the cost of node \\(h\\) increases by \\(p_h\\) . In addition, the cost of root node \\(r\\) is \\(p_r\\) . That\u2019s why, we have the last term at the end of the formula above. section{ font-size: 25px; } Optimal Substructure Property \u00b6 Lemma 2: Optimal substructure property Consider an optimal BST \\(T_{ij}\\) for keys \\(K_i < K_{i+1} < \\dots < K_j\\) Let \\(K_m\\) be the key at the root of \\(T_{ij}\\) Then: \\(T_{i,m-1}\\) is an optimal BST for subproblem containing keys: \\(K_i < \\dots < K_{m-1}\\) \\(T_{m+1,j}\\) is an optimal BST for subproblem containing keys: \\(K_{m+1} < \\dots < K_j\\) \\[ \\begin{align*} \\text{cost}(T_{ij})=\\text{cost}(T_{i,m-1})+\\text{cost}(T_{m+1,j})+\\sum \\limits_{h=i}^{j}p_h \\end{align*} \\] section{ font-size: 25px; } Recursive Formulation \u00b6 Note: We don\u2019t know which root vertex leads to the minimum total cost. So, we need to try each vertex \\(m\\) , and choose the one with minimum total cost. \\(c[i, j]\\) : cost of an optimal BST \\(T_{ij}\\) for the subproblem \\(K_i < \\dots < K_j\\) \\[ \\begin{align*} & c[i,j] = \\begin{cases} & 0 & \\text{if} \\ i>j \\\\ & \\underset{i \\leq r \\leq j}{\\text{min}}\\{ c[i,r-1]+c[r+1,j]+P_{ij} \\} & \\text{otherwise} \\\\ \\end{cases} \\\\ & \\text{where} \\ P_{ij}= \\sum \\limits_{h=i}^{j}p_h \\end{align*} \\] section{ font-size: 25px; } Bottom-up computation \u00b6 \\[ \\begin{align*} & c[i,j] = \\begin{cases} & 0 & \\text{if} \\ i>j \\\\ & \\underset{i \\leq r \\leq j}{\\text{min}}\\{ c[i,r-1]+c[r+1,j]+P_{ij} \\} & \\text{otherwise} \\\\ \\end{cases} \\end{align*} \\] How to choose the order in which we process \\(c[i, j]\\) values? Before computing \\(c[i, j]\\) , we have to make sure that the values for \\(c[i, r-1]\\) and \\(c[r+1,j]\\) have been computed for all \\(r\\) . section{ font-size: 25px; } Bottom-up computation \u00b6 \\[ \\begin{align*} & c[i,j] = \\begin{cases} & 0 & \\text{if} \\ i>j \\\\ & \\underset{i \\leq r \\leq j}{\\text{min}}\\{ c[i,r-1]+c[r+1,j]+P_{ij} \\} & \\text{otherwise} \\\\ \\end{cases} \\end{align*} \\] \\(c[i,j]\\) must be processed after \\(c[i,r-1]\\) and \\(c[r+1,j]\\) section{ font-size: 25px; } Bottom-up computation \u00b6 \\[ \\begin{align*} & c[i,j] = \\begin{cases} & 0 & \\text{if} \\ i>j \\\\ & \\underset{i \\leq r \\leq j}{\\text{min}}\\{ c[i,r-1]+c[r+1,j]+P_{ij} \\} & \\text{otherwise} \\\\ \\end{cases} \\end{align*} \\] If the entries \\(c[i,j]\\) are computed in the shown order, then \\(c[i,r-1]\\) and \\(c[r+1,j]\\) values are guaranteed to be computed before \\(c[i,j]\\) . section{ font-size: 25px; } Computing the Optimal BST Cost \u00b6 \\[ \\begin{align*} & \\text{OPTIMAL-BST-COST} (p, n) \\\\ & \\quad \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad c[i, i-1] \\leftarrow 0 \\\\ & \\qquad c[i, i] \\leftarrow p[i] \\\\ & \\qquad R[i, j] \\leftarrow i \\\\ & \\quad PS[1] \\leftarrow p[1] \\Longleftarrow PS[i] \\rightarrow \\text{ prefix-sum } (i): \\text{Sum of all} \\ p[j] \\ \\text{values for} \\ j \\leq i \\\\ & \\quad \\text{for} \\ i \\leftarrow 2 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad PS[i] \\leftarrow p[i] + PS[i-1] \\Longleftarrow \\text{compute the prefix sum} \\\\ & \\quad \\text{for} \\ d \\leftarrow 1 \\ \\text{to} \\ n\u22121 \\ \\text{do} \\Longleftarrow \\text{BSTs with} \\ d+1 \\ \\text{consecutive keys} \\\\ & \\qquad \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ n \u2013 d \\ \\text{do} \\\\ & \\qquad \\quad j \\leftarrow i + d \\\\ & \\qquad \\quad c[i, j] \\leftarrow \\infty \\\\ & \\qquad \\quad \\text{for} \\ r \\leftarrow i \\ \\text{to} \\ j \\ \\text{do} \\\\ & \\qquad \\qquad q \\leftarrow min\\{c[i,r-1] + c[r+1, j]\\} + PS[j] \u2013 PS[i-1]\\} \\\\ & \\qquad \\qquad \\text{if} \\ q < c[i, j] \\ \\text{then} \\\\ & \\qquad \\qquad \\quad c[i, j] \\leftarrow q \\\\ & \\qquad \\qquad \\quad R[i, j] \\leftarrow r \\\\ & \\quad \\text{return} \\ c[1, n], R \\end{align*} \\] section{ font-size: 25px; } Note on Prefix Sum \u00b6 We need \\(P_{ij}\\) values for each \\(i, j (1 \u2264 i \u2264 n \\ \\text{and} \\ 1 \u2264 j \u2264 n)\\) , where: \\[ \\begin{align*} P_{ij} = \\sum \\limits_{h=i}^{j}p_h \\end{align*} \\] If we compute the summation directly for every \\((i, j)\\) pair, the runtime would be \\(\\Theta(n^3)\\) . Instead, we spend \\(O(n)\\) time in preprocessing to compute the prefix sum array PS . Then we can compute each \\(P_{ij}\\) in \\(O(1)\\) time using PS . section{ font-size: 25px; } Note on Prefix Sum \u00b6 In preprocessing, compute for each \\(i\\) : \\(PS[i]\\) : the sum of \\(p[j]\\) values for \\(1 \\leq j \\leq i\\) Then, we can compute \\(P_{ij}\\) in \\(O(1)\\) time as follows: \\(P_{ij} = PS[i] \u2013 PS[j-1]\\) Example: \\[ \\begin{align*} p &: \\overset{1}{0.05} \\ \\overset{2}{0.02} \\ \\overset{3}{0.06} \\ \\overset{4}{0.07} \\ \\overset{5}{0.20} \\ \\overset{6}{0.05} \\ \\overset{7}{0.08} \\ \\overset{8}{0.02} \\\\ PS &: \\overset{1}{0.05} \\ \\overset{2}{0.07} \\ \\overset{3}{0.13} \\ \\overset{4}{0.20} \\ \\overset{5}{0.40} \\ \\overset{6}{0.45} \\ \\overset{7}{0.53} \\ \\overset{8}{0.55} \\\\[10 pt] P_{27} &= PS[7] \u2013 PS[1] = 0.53 \u2013 0.05 = 0.48 \\\\ P_{36} &= PS[6] \u2013 PS[2] = 0.45 \u2013 0.07 = 0.38 \\end{align*} \\] REVIEW \u00b6 Overlapping Subproblems Property in Dynamic Programming \u00b6 Dynamic Programming is an algorithmic paradigm that solves a given complex problem by breaking it into subproblems and stores the results of subproblems to avoid computing the same results again. Overlapping Subproblems Property in Dynamic Programming \u00b6 Following are the two main properties of a problem that suggests that the given problem can be solved using Dynamic programming. Overlapping Subproblems Optimal Substructure Overlapping Subproblems \u00b6 Like Divide and Conquer, Dynamic Programming combines solutions to sub-problems. Dynamic Programming is mainly used when solutions of the same subproblems are needed again and again. In dynamic programming, computed solutions to subproblems are stored in a table so that these don\u2019t have to be recomputed. So Dynamic Programming is not useful when there are no common (overlapping) subproblems because there is no point storing the solutions if they are not needed again. Overlapping Subproblems \u00b6 For example, Binary Search doesn\u2019t have common subproblems. If we take an example of following recursive program for Fibonacci Numbers, there are many subproblems that are solved again and again. Simple Recursion \u00b6 \\(f(n) = f(n-1) + f(n-2)\\) C sample code: #include <stdio.h> // a simple recursive program to compute fibonacci numbers int fib ( int n ) { if ( n <= 1 ) return n ; else return fib ( n -1 ) + fib ( n -2 ); } int main () { int n = 5 ; printf ( \"Fibonacci number is %d \" , fib ( n )); return 0 ; } Simple Recursion \u00b6 Output Fibonacci number is 5 Simple Recursion \u00b6 \\(f(n) = f(n-1) + f(n-2)\\) /* a simple recursive program for Fibonacci numbers */ public class Fibonacci { public static void main ( String [] args ) { int n = Integer . parseInt ( args [ 0 ] ); System . out . println ( fib ( n )); } public static int fib ( int n ) { if ( n <= 1 ) return n ; return fib ( n - 1 ) + fib ( n - 2 ); } } Simple Recursion \u00b6 \\(f(n) = f(n-1) + f(n-2)\\) public class Fibonacci { public static void Main ( string [] args ) { int n = int . Parse ( args [ 0 ]); Console . WriteLine ( fib ( n )); } public static int fib ( int n ) { if ( n <= 1 ) return n ; return fib ( n - 1 ) + fib ( n - 2 ); } } Recursion tree for execution of fib(5) \u00b6 fib(5) / \\ fib(4) fib(3) / \\ / \\ fib(3) fib(2) fib(2) fib(1) / \\ / \\ / \\ fib(2) fib(1) fib(1) fib(0) fib(1) fib(0) / \\ fib(1) fib(0) We can see that the function fib(3) is being called 2 times. If we would have stored the value of fib(3) , then instead of computing it again, we could have reused the old stored value. Recursion tree for execution of fib(5) \u00b6 There are following two different ways to store the values so that these values can be reused: Memoization (Top Down) Tabulation (Bottom Up) Memoization (Top Down) \u00b6 The memoized program for a problem is similar to the recursive version with a small modification that looks into a lookup table before computing solutions. We initialize a lookup array with all initial values as NIL . Whenever we need the solution to a subproblem, we first look into the lookup table. If the precomputed value is there then we return that value, otherwise, we calculate the value and put the result in the lookup table so that it can be reused later. Memoization (Top Down) \u00b6 Following is the memoized version for the nth Fibonacci Number. C++ Version: /* C++ program for Memoized version for nth Fibonacci number */ #include <bits/stdc++.h> using namespace std ; #define NIL -1 #define MAX 100 int lookup [ MAX ]; Memoization (Top Down) \u00b6 C++ Version: /* Function to initialize NIL values in lookup table */ void _initialize () { int i ; for ( i = 0 ; i < MAX ; i ++ ) lookup [ i ] = NIL ; } Memoization (Top Down) \u00b6 C++ Version: /* function for nth Fibonacci number */ int fib ( int n ) { if ( lookup [ n ] == NIL ) { if ( n <= 1 ) lookup [ n ] = n ; else lookup [ n ] = fib ( n - 1 ) + fib ( n - 2 ); } return lookup [ n ]; } Memoization (Top Down) \u00b6 C++ Version: // Driver code int main () { int n = 40 ; _initialize (); cout << \"Fibonacci number is \" << fib ( n ); return 0 ; } Memoization (Top Down) \u00b6 Java Version: /* Java program for Memoized version */ public class Fibonacci { final int MAX = 100 ; final int NIL = - 1 ; int lookup [] = new int [ MAX ] ; /* Function to initialize NIL values in lookup table */ void _initialize () { for ( int i = 0 ; i < MAX ; i ++ ) lookup [ i ] = NIL ; } Memoization (Top Down) \u00b6 Java Version: /* function for nth Fibonacci number */ int fib ( int n ) { if ( lookup [ n ] == NIL ) { if ( n <= 1 ) lookup [ n ] = n ; else lookup [ n ] = fib ( n - 1 ) + fib ( n - 2 ); } return lookup [ n ] ; } Memoization (Top Down) \u00b6 Java Version: public static void main ( String [] args ) { Fibonacci f = new Fibonacci (); int n = 40 ; f . _initialize (); System . out . println ( \"Fibonacci number is\" + \" \" + f . fib ( n )); } } Memoization (Top Down) \u00b6 C# Version: // C# program for Memoized versionof nth Fibonacci number using System ; class FiboCalcMemoized { static int MAX = 100 ; static int NIL = - 1 ; static int [] lookup = new int [ MAX ]; /* Function to initialize NIL values in lookup table */ static void initialize () { for ( int i = 0 ; i < MAX ; i ++) lookup [ i ] = NIL ; } Memoization (Top Down) \u00b6 C# Version: /* function for nth Fibonacci number */ static int fib ( int n ) { if ( lookup [ n ] == NIL ) { if ( n <= 1 ) lookup [ n ] = n ; else lookup [ n ] = fib ( n - 1 ) + fib ( n - 2 ); } return lookup [ n ]; } Memoization (Top Down) \u00b6 C# Version: // Driver code public static void Main () { int n = 40 ; initialize (); Console . Write ( \"Fibonacci number is\" + \" \" + fib ( n )); } } Tabulation (Bottom Up) \u00b6 The tabulated program for a given problem builds a table in bottom-up fashion and returns the last entry from the table. For example, for the same Fibonacci number, we first calculate fib(0) then fib(1) then fib(2) then fib(3) , and so on. So literally, we are building the solutions of subproblems bottom-up. Tabulation (Bottom Up) \u00b6 C++ Version: /* C program for Tabulated version */ #include <stdio.h> int fib ( int n ) { int f [ n + 1 ]; int i ; f [ 0 ] = 0 ; f [ 1 ] = 1 ; for ( i = 2 ; i <= n ; i ++ ) f [ i ] = f [ i - 1 ] + f [ i - 2 ]; return f [ n ]; } Tabulation (Bottom Up) \u00b6 C++ Version: ... int main () { int n = 9 ; printf ( \"Fibonacci number is %d \" , fib ( n )); return 0 ; } Output: Fibonacci number is 34 Tabulation (Bottom Up) \u00b6 Java Version: /* Java program for Tabulated version */ public class Fibonacci { public static void main ( String [] args ) { int n = 9 ; System . out . println ( \"Fibonacci number is \" + fib ( n )); } Tabulation (Bottom Up) \u00b6 Java Version: /* Function to calculate nth Fibonacci number */ static int fib ( int n ) { int f [] = new int [ n + 1 ] ; f [ 0 ] = 0 ; f [ 1 ] = 1 ; for ( int i = 2 ; i <= n ; i ++ ) f [ i ] = f [ i - 1 ] + f [ i - 2 ] ; return f [ n ] ; } } Tabulation (Bottom Up) \u00b6 C# Version: // C# program for Tabulated version using System ; class Fibonacci { static int fib ( int n ) { int [] f = new int [ n + 1 ]; f [ 0 ] = 0 ; f [ 1 ] = 1 ; for ( int i = 2 ; i <= n ; i ++) f [ i ] = f [ i - 1 ] + f [ i - 2 ]; return f [ n ]; } public static void Main () { int n = 9 ; Console . Write ( \"Fibonacci number is\" + \" \" + fib ( n )); } } Both Tabulated and Memoized store the solutions of subproblems. In Memoized version, the table is filled on demand while in the Tabulated version, starting from the first entry, all entries are filled one by one. Unlike the Tabulated version, all entries of the lookup table are not necessarily filled in Memoized version. To see the optimization achieved by Memoized and Tabulated solutions over the basic Recursive solution, see the time taken by following runs for calculating the 40 th Fibonacci number: Recursive Solution: https://ide.geeksforgeeks.org/vHt6ly Memoized Solution: https://ide.geeksforgeeks.org/Z94jYR Tabulated Solution: https://ide.geeksforgeeks.org/12C5bP Optimal Substructure Property in Dynamic Programming \u00b6 A given problems has Optimal Substructure Property if optimal solution of the given problem can be obtained by using optimal solutions of its subproblems. For example, the Shortest Path problem has following optimal substructure property: If a node x lies in the shortest path from a source node u to destination node v then the shortest path from u to v is combination of shortest path from u to x and shortest path from x to v. The standard All Pair Shortest Path algorithm like Floyd\u2013Warshall and Single Source Shortest path algorithm for negative weight edges like Bellman\u2013Ford are typical examples of Dynamic Programming. Optimal Substructure Property in Dynamic Programming \u00b6 On the other hand, the Longest Path problem doesn\u2019t have the Optimal Substructure property. Here by Longest Path we mean longest simple path (path without cycle) between two nodes Optimal Substructure Property in Dynamic Programming \u00b6 There are two longest paths from q to t: q\u2192r\u2192t and q\u2192s\u2192t. Unlike shortest paths, these longest paths do not have the optimal substructure property. For example, the longest path q\u2192r\u2192t is not a combination of longest path from q to r and longest path from r to t, because the longest path from q to r is q\u2192s\u2192t\u2192r and the longest path from r to t is r\u2192q\u2192s\u2192t. Most Common Dynamic Programming Interview Questions \u00b6 Problem-1: Longest Increasing Subsequence \u00b6 Problem-1: Longest Increasing Subsequence Problem-1: Longest Increasing Subsequence \u00b6 Problem-2: Edit Distance \u00b6 Problem-2: Edit Distance Problem-2: Edit Distance (Recursive) \u00b6 Problem-2: Edit Distance (DP) \u00b6 https://www.coursera.org/learn/dna-sequencing Problem-2: Edit Distance (DP) \u00b6 Problem-2: Edit Distance (Other) \u00b6 Problem-3: Partition a set into two subsets such that the difference of subset sums is minimum \u00b6 Problem-3: Partition a set into two subsets such that the difference of subset sums is minimum Problem-4: Count number of ways to cover a distance \u00b6 Problem-4: Count number of ways to cover a distance Problem-5: Find the longest path in a matrix with given constraints \u00b6 Problem-5: Find the longest path in a matrix with given constraints Problem-6: Subset Sum Problem \u00b6 Problem-6: Subset Sum Problem Problem-7: Optimal Strategy for a Game \u00b6 Problem-7: Optimal Strategy for a Game Problem-8: 0-1 Knapsack Problem \u00b6 Problem-8: 0-1 Knapsack Problem Problem-9: Boolean Parenthesization Problem \u00b6 Problem-9: Boolean Parenthesization Problem Problem-10: Shortest Common Supersequence \u00b6 Problem-10: Shortest Common Supersequence Problem-11: Partition Problem \u00b6 Problem-11: Partition Problem Problem-12: Cutting a Rod \u00b6 Problem-12: Cutting a Rod Problem-13: Coin Change \u00b6 Problem-13: Coin Change Problem-14: Word Break Problem \u00b6 Problem-14: Word Break Problem Problem-15: Maximum Product Cutting \u00b6 Problem-15: Maximum Product Cutting Problem-16: Dice Throw \u00b6 Problem-16: Dice Throw Problem-16: Dice Throw \u00b6 Problem-17: Box Stacking Problem \u00b6 Problem-17: Box Stacking Problem Problem-18: Egg Dropping Puzzle \u00b6 Problem-18: Egg Dropping Puzzle References \u00b6 Introduction to Algorithms, Third Edition | The MIT Press CLRS Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) \\(-End-Of-Week-6-Course-Module-\\)","title":"Week-6 (Matrix Chain Order / LCS)"},{"location":"week-6/ce100-week-6-lcs/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-6/ce100-week-6-lcs/#week-6-matrix-chain-order-lcs","text":"","title":"Week-6 (Matrix Chain Order / LCS)"},{"location":"week-6/ce100-week-6-lcs/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-6/ce100-week-6-lcs/#matrix-chain-order-longest-common-subsequence","text":"","title":"Matrix Chain Order / Longest Common Subsequence"},{"location":"week-6/ce100-week-6-lcs/#outline","text":"Elements of Dynamic Programming Optimal Substructure Overlapping Subproblems Recursive Matrix Chain Order Memoization Top-Down Approach RMC MemoizedMatrixChain LookupC Dynamic Programming vs Memoization Summary Dynamic Programming Problem-2 : Longest Common Subsequence Definitions LCS Problem Notations Optimal Substructure of LCS Proof Case-1 Proof Case-2 Proof Case-3 A recursive solution to subproblems (inefficient) Computing the length of and LCS LCS Data Structure for DP Bottom-Up Computation Constructing and LCS PRINT-LCS Back-pointer space optimization for LCS length Most Common Dynamic Programming Interview Questions","title":"Outline"},{"location":"week-6/ce100-week-6-lcs/#elements-of-dynamic-programming","text":"When should we look for a DP solution to an optimization problem? Two key ingredients for the problem Optimal substructure Overlapping subproblems","title":"Elements of Dynamic Programming"},{"location":"week-6/ce100-week-6-lcs/#dp-hallmark-1","text":"Optimal Substructure A problem exhibits optimal substructure if an optimal solution to a problem contains within it optimal solutions to subproblems Example: matrix-chain-multiplication Optimal parenthesization of \\(A_1 A_2 \\dots A_n\\) that splits the product between \\(A_k\\) and \\(A_{k+1}\\) , contains within it optimal soln\u2019s to the problems of parenthesizing \\(A_1A_2 \\dots A_k\\) and \\(A_{k+1} A_{k+2} \\dots A_n\\)","title":"DP Hallmark #1"},{"location":"week-6/ce100-week-6-lcs/#optimal-substructure","text":"Finding a suitable space of subproblems Iterate on subproblem instances Example: matrix-chain-multiplication Iterate and look at the structure of optimal soln\u2019s to subproblems, sub-subproblems, and so forth Discover that all subproblems consists of subchains of \\(\\langle A_1, A_2, \\dots , A_n \\rangle\\) Thus, the set of chains of the form \\(\\langle A_i,A_{i+1}, \\dots , A_j \\rangle\\) for \\(1 \\leq i \\leq j \\leq n\\) Makes a natural and reasonable space of subproblems","title":"Optimal Substructure"},{"location":"week-6/ce100-week-6-lcs/#dp-hallmark-2","text":"Overlapping Subproblems Total number of distinct subproblems should be polynomial in the input size When a recursive algorithm revisits the same problem over and over again , We say that the optimization problem has overlapping subproblems","title":"DP Hallmark #2"},{"location":"week-6/ce100-week-6-lcs/#overlapping-subproblems","text":"DP algorithms typically take advantage of overlapping subproblems by solving each problem once then storing the solutions in a table where it can be looked up when needed using constant time per lookup section{ font-size: 25px; }","title":"Overlapping Subproblems"},{"location":"week-6/ce100-week-6-lcs/#overlapping-subproblems_1","text":"Recursive matrix-chain order \\[ \\begin{align*} & \\text{RMC}(p, i, j) \\{ \\\\[5 pt] & \\quad \\text{if} \\ i = j \\ \\text{then} \\\\ & \\qquad \\text{return} \\ 0 \\\\[5 pt] & \\quad m[i, j] \\leftarrow \\infty \\\\[5 pt] & \\quad \\text{for} \\ k \\leftarrow i \\text{to} \\ j - 1 \\ \\text{do} \\\\ & \\qquad q \\leftarrow \\text{RMC}(p, i, k) + \\text{RMC}(p, k+1, j) + p_{i-1} p_k p_j \\\\[5 pt] & \\quad if \\ q < m[i, j] \\ \\text{then} \\\\ & \\qquad m[i, j] \\leftarrow q \\\\[5 pt] & \\quad \\text{return} \\ m[i, j] \\ \\} \\end{align*} \\]","title":"Overlapping Subproblems"},{"location":"week-6/ce100-week-6-lcs/#direct-recursion-inefficient","text":"Recursion tree for \\(RMC(p,1,4)\\) Nodes are labeled with \\(i\\) and \\(j\\) values section{ font-size: 25px; }","title":"Direct Recursion: Inefficient!"},{"location":"week-6/ce100-week-6-lcs/#running-time-of-rmc","text":"\\(T(1) \\geq 1\\) \\(T(n) \\geq 1 + \\sum \\limits_{k=1}^{n-1} (T(k)+T(n-k)+1)\\ \\text{for} \\ n>1\\) For \\(i =1,2, \\dots ,n\\) each term \\(T(i)\\) appears twice Once as \\(T(k)\\) , and once as \\(T(n-k)\\) Collect \\(n-1,\\) \\(1\\) \u2019s in the summation together with the front \\(1\\) \\[ \\begin{align*} T(n) \\geq 2 \\sum \\limits_{i=1}^{n-1}T(i)+n \\end{align*} \\] Prove that \\(T(n)= \\Omega(2n)\\) using the substitution method section{ font-size: 25px; }","title":"Running Time of RMC"},{"location":"week-6/ce100-week-6-lcs/#running-time-of-rmc-prove-that-tn-omega2n","text":"Try to show that \\(T(n) \\geq 2^{n-1}\\) ( by substitution ) Base case: \\(T(1) \\geq 1 = 2^0 = 2^{1-1}\\) for \\(n=1\\) Ind. Hyp.: \\[ \\begin{align*} T(i) & \\geq 2^{i-1} \\ \\text{for all} \\ i=1, 2, \\dots, n-1 \\ \\text{and} \\ n \\geq 2 \\\\ T(n) & \\geq 2 \\sum \\limits_{i=1}^{n-1}2^{i-1} + n \\\\[15 pt] & = 2 \\sum \\limits_{i=1}^{n-1} 2^{i-1} + n \\\\ & = 2(2^{n-1}-1) + n \\\\ & = 2^{n-1} + (2^{n-1} - 2 + n) \\\\ & \\Rightarrow T(n) \\geq 2^{n-1} \\ \\text{ Q.E.D.} \\end{align*} \\] section{ font-size: 25px; }","title":"Running Time of RMC: Prove that \\(T(n)= \\Omega(2n)\\)"},{"location":"week-6/ce100-week-6-lcs/#running-time-of-rmc-tn-geq-2n-1","text":"Whenever a recursion tree for the natural recursive solution to a problem contains the same subproblem repeatedly the total number of different subproblems is small it is a good idea to see if \\(DP (Dynamic \\ Programming)\\) can be applied","title":"Running Time of RMC: \\(T(n) \\geq 2^{n-1}\\)"},{"location":"week-6/ce100-week-6-lcs/#memoization","text":"Offers the efficiency of the usual \\(DP\\) approach while maintaining top-down strategy Idea is to memoize the natural, but inefficient, recursive algorithm","title":"Memoization"},{"location":"week-6/ce100-week-6-lcs/#memoized-recursive-algorithm","text":"Maintains an entry in a table for the soln to each subproblem Each table entry contains a special value to indicate that the entry has yet to be filled in When the subproblem is first encountered its solution is computed and then stored in the table Each subsequent time that the subproblem encountered the value stored in the table is simply looked up and returned","title":"Memoized Recursive Algorithm"},{"location":"week-6/ce100-week-6-lcs/#memoized-recursive-matrix-chain-order","text":"Shaded subtrees are looked-up rather than recomputing \\[ \\begin{align*} \\begin{aligned} & \\text{MemoizedMatrixChain(p)} \\\\ & \\quad n \\leftarrow length[p] - 1 \\\\ & \\quad \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad \\text{for} \\ j \\leftarrow 1 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad \\quad m[i, j] \\leftarrow \\infty \\\\ & \\quad \\text{return} \\ \\text{LookupC}(p, 1, n) \\Longrightarrow \\end{aligned} \\begin{aligned} & \\Longrightarrow \\text{LookupC}(p, i, j) \\\\ & \\quad \\text{if} \\ m[i, j] = \\infty \\ \\text{then} \\\\ & \\qquad \\text{if} \\ i = j \\ \\text{then} \\\\ & \\qquad \\quad m[i, j] \\leftarrow 0 \\\\ & \\qquad \\text{else} \\\\ & \\qquad \\quad \\text{for} \\ k \\leftarrow i \\ \\text{to} \\ j-1 \\ \\text{do} \\\\ & \\qquad \\quad \\quad q \\leftarrow \\text{LookupC}(p, i, k) + \\text{LookupC}(p, k+1, j) + p_{i-1} p_k p_j \\\\ & \\qquad \\quad \\quad \\text{if} \\ q < m[i, j] \\ \\text{then} \\\\ & \\qquad \\quad \\quad \\quad m[i, j] \\leftarrow q \\\\ & \\quad \\text{return} \\ m[i, j] \\end{aligned} \\end{align*} \\]","title":"Memoized Recursive Matrix-chain Order"},{"location":"week-6/ce100-week-6-lcs/#memoized-recursive-algorithm_1","text":"The approach assumes that The set of all possible subproblem parameters are known The relation between the table positions and subproblems is established Another approach is to memoize by using hashing with subproblem parameters as key section{ font-size: 25px; }","title":"Memoized Recursive Algorithm"},{"location":"week-6/ce100-week-6-lcs/#dynamic-programming-vs-memoization-summary-1","text":"Matrix-chain multiplication can be solved in \\(O(n^3)\\) time by either a top-down memoized recursive algorithm or a bottom-up dynamic programming algorithm Both methods exploit the overlapping subproblems property There are only \\(\\Theta(n^2)\\) different subproblems in total Both methods compute the soln to each problem once Without memoization the natural recursive algorithm runs in exponential time since subproblems are solved repeatedly section{ font-size: 25px; }","title":"Dynamic Programming vs Memoization Summary (1)"},{"location":"week-6/ce100-week-6-lcs/#dynamic-programming-vs-memoization-summary-2","text":"In general practice If all subproblems must be solved at once a bottom-up DP algorithm always outperforms a top-down memoized algorithm by a constant factor because, bottom-up DP algorithm Has no overhead for recursion Less overhead for maintaining the table DP: Regular pattern of table accesses can be exploited to reduce the time and/or space requirements even further Memoized: If some problems need not be solved at all, it has the advantage of avoiding solutions to those subproblems","title":"Dynamic Programming vs Memoization Summary (2)"},{"location":"week-6/ce100-week-6-lcs/#problem-3-longest-common-subsequence","text":"Definitions A subsequence of a given sequence is just the given sequence with some elements (possibly none) left out Example: \\(X = \\langle A, B, C, B, D, A, B \\rangle\\) \\(Z = \\langle B, C, D, B \\rangle\\) \\(Z\\) is a subsequence of \\(X\\)","title":"Problem 3: Longest Common Subsequence"},{"location":"week-6/ce100-week-6-lcs/#problem-3-longest-common-subsequence_1","text":"Definitions Formal definition: Given a sequence \\(X = \\langle x_1, x_2, \\dots , x_m \\rangle\\) , sequence \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) is a subsequence of \\(X\\) if \\(\\exists\\) a strictly increasing sequence \\(\\langle i_1, i_2,\\dots, i_k \\rangle\\) of indices of \\(X\\) such that \\(x_{i_j} = z_j\\) for all \\(j = 1, 2, \u2026, k\\) , where \\(1 \\leq k \\leq m\\) Example: \\(Z = \\langle B,C,D,B \\rangle\\) is a subsequence of \\(X = \\langle A,B,C,B,D,A,B \\rangle\\) with the index sequence \\(\\langle i_1, i_2, i_3, i_4 \\rangle = \\langle 2, 3, 5, 7 \\rangle\\) section{ font-size: 25px; }","title":"Problem 3: Longest Common Subsequence"},{"location":"week-6/ce100-week-6-lcs/#problem-3-longest-common-subsequence_2","text":"Definitions If \\(Z\\) is a subsequence of both \\(X\\) and \\(Y\\) , we denote \\(Z\\) as a common subsequence of \\(X\\) and \\(Y\\) . Example: \\[ \\begin{align*} X &= \\langle A,B^*,C^*,B,D,A^*,B \\rangle \\\\ Y &= \\langle B^*,D,C^*,A^*,B,A \\rangle \\end{align*} \\] \\(Z_1 = \\langle B^*, C^*, A^* \\rangle\\) is a common subsequence ( of length 3 ) of \\(X\\) and \\(Y\\) . Two longest common subsequence (LCSs) of \\(X\\) and \\(Y\\) ? \\(Z2 = \\langle B, C, B, A \\rangle\\) of length \\(4\\) \\(Z3 = \\langle B, D, A, B \\rangle\\) of length \\(4\\) The optimal solution value = 4","title":"Problem 3: Longest Common Subsequence"},{"location":"week-6/ce100-week-6-lcs/#longest-common-subsequence-lcs-problem","text":"LCS problem: Given two sequences \\(X = \\langle x_1, x_2, \\dots, x_m \\rangle\\) and \\(Y = \\langle y_1, y_2, \\dots , y_n \\rangle\\) , find the LCS of \\(X \\& Y\\) Brute force approach: Enumerate all subsequences of \\(X\\) Check if each subsequence is also a subsequence of \\(Y\\) Keep track of the LCS What is the complexity? There are \\(2^m\\) subsequences of \\(X\\) Exponential runtime","title":"Longest Common Subsequence (LCS) Problem"},{"location":"week-6/ce100-week-6-lcs/#notation","text":"Notation: Let \\(X_i\\) denote the \\(i^{th}\\) prefix of \\(X\\) i.e. \\(X_i = \\langle x_1, x_2, \\dots, x_i \\rangle\\) Example: \\[ \\begin{align*} X &= \\langle A, B, C, B, D, A, B \\rangle \\\\[10 pt] X_4 &= \\langle A, B, C, B \\rangle \\\\ X_0 &= \\langle \\rangle \\end{align*} \\]","title":"Notation"},{"location":"week-6/ce100-week-6-lcs/#optimal-substructure-of-an-lcs","text":"Let \\(X = <x1, x2, \u2026, xm>\\) and \\(Y = \\langle y_1, y_2, \\dots, y_n \\rangle\\) are given Let \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\) Question 1: If \\(x_m = y_n\\) , how to define the optimal substructure? We must have \\(z_k = x_m = y_n\\) and \\(Z_{k-1} = \\text{LCS}(X_{m-1}, Y_{n-1})\\)","title":"Optimal Substructure of an LCS"},{"location":"week-6/ce100-week-6-lcs/#optimal-substructure-of-an-lcs_1","text":"Let \\(X = <x1, x2, \u2026, xm>\\) and \\(Y = \\langle y_1, y_2, \\dots, y_n \\rangle\\) are given Let \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\) Question 2: If \\(x_m \\neq y_n \\ \\text{and} \\ z_k \\neq x_m\\) , how to define the optimal substructure? We must have \\(Z = \\text{LCS}(X_{m-1}, Y)\\)","title":"Optimal Substructure of an LCS"},{"location":"week-6/ce100-week-6-lcs/#optimal-substructure-of-an-lcs_2","text":"Let \\(X = <x1, x2, \u2026, xm>\\) and \\(Y = \\langle y_1, y_2, \\dots, y_n \\rangle\\) are given Let \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\) Question 3: If \\(x_m \\neq y_n \\ \\text{and} \\ z_k \\neq y_n\\) , how to define the optimal substructure? We must have \\(Z = \\text{LCS}(X, Y_{n-1})\\)","title":"Optimal Substructure of an LCS"},{"location":"week-6/ce100-week-6-lcs/#theorem-optimal-substructure-of-an-lcs","text":"Let \\(X = \\langle x_1, x_2, \\dots, x_m \\rangle\\) and Y = are given Let \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\) Theorem: Optimal substructure of an LCS: If \\(x_m = y_n\\) then \\(z_k = x_m = y_n\\) and \\(Z_{k-1}\\) is an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) If \\(x_m \\neq y_n\\) and \\(z_k \\neq x_m\\) then \\(Z\\) is an LCS of \\(X_{m-1}\\) and \\(Y\\) If \\(x_m \\neq y_n\\) and \\(z_k \\neq y_n\\) then \\(Z\\) is an LCS of \\(X\\) and \\(Y_{n-1}\\)","title":"Theorem: Optimal Substructure of an LCS"},{"location":"week-6/ce100-week-6-lcs/#optimal-substructure-theorem-case-1","text":"If \\(x_m = y_n\\) then \\(z_k = x_m = y_n\\) and \\(Z_{k-1}\\) is an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\)","title":"Optimal Substructure Theorem (case 1)"},{"location":"week-6/ce100-week-6-lcs/#optimal-substructure-theorem-case-2","text":"If \\(x_m \\neq y_n\\) and \\(z_k \\neq x_m\\) then \\(Z\\) is an LCS of \\(X_{m-1}\\) and \\(Y\\)","title":"Optimal Substructure Theorem (case 2)"},{"location":"week-6/ce100-week-6-lcs/#optimal-substructure-theorem-case-3","text":"If \\(x_m \\neq y_n\\) and \\(z_k \\neq y_n\\) then \\(Z\\) is an LCS of \\(X\\) and \\(Y_{n-1}\\)","title":"Optimal Substructure Theorem (case 3)"},{"location":"week-6/ce100-week-6-lcs/#proof-of-optimal-substructure-theorem-case-1","text":"If \\(x_m = y_n\\) then \\(z_k = x_m = y_n\\) and \\(Z_{k-1}\\) is an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) Proof: If \\(z_k \\neq x_m = y_n\\) then we can append \\(x_m = y_n\\) to \\(Z\\) to obtain a common subsequence of length \\(k+1 \\Longrightarrow\\) contradiction Thus, we must have \\(z_k = x_m = y_n\\) Hence, the prefix \\(Z_{k-1}\\) is a length-( \\(k-1\\) ) CS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) We have to show that \\(Z_{k-1}\\) is in fact an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) Proof by contradiction: Assume that \\(\\exists\\) a CS \\(W\\) of \\(X_{m-1}\\) and \\(Y_{n-1}\\) with \\(|W| = k\\) Then appending \\(x_m = y_n\\) to \\(W\\) produces a CS of length \\(k+1\\)","title":"Proof of Optimal Substructure Theorem (case 1)"},{"location":"week-6/ce100-week-6-lcs/#proof-of-optimal-substructure-theorem-case-2","text":"If \\(x_m \\neq y_n\\) and \\(z_k \\neq x_m\\) then \\(Z\\) is an LCS of \\(X_{m-1}\\) and \\(Y\\) Proof : If \\(z_k \\neq x_m\\) then \\(Z\\) is a CS of \\(X_{m-1}\\) and \\(Y_n\\) We have to show that \\(Z\\) is in fact an LCS of \\(X_{m-1}\\) and \\(Y_n\\) (Proof by contradiction) Assume that \\(\\exists\\) a CS \\(W\\) of \\(X_{m-1}\\) and \\(Y_n\\) with \\(|W| > k\\) Then \\(W\\) would also be a CS of \\(X\\) and \\(Y\\) Contradiction to the assumption that \\(Z\\) is an LCS of \\(X\\) and \\(Y\\) with \\(|Z| = k\\) Case 3: Dual of the proof for (case 2)","title":"Proof of Optimal Substructure Theorem (case 2)"},{"location":"week-6/ce100-week-6-lcs/#a-recursive-solution-to-subproblems","text":"Theorem implies that there are one or two subproblems to examine if \\(x_m = y_n\\) then we must solve the subproblem of finding an LCS of \\(X_{m-1} \\& Y_{n-1}\\) appending \\(x_m = y_n\\) to this LCS yields an LCS of \\(X \\& Y\\) else we must solve two subproblems finding an LCS of \\(X_{m-1} \\& Y\\) finding an LCS of \\(X \\& Y_{n-1}\\) longer of these two LCS s is an LCS of \\(X \\& Y\\) endif section{ font-size: 25px; }","title":"A Recursive Solution to Subproblems"},{"location":"week-6/ce100-week-6-lcs/#recursive-algorithm-inefficient","text":"\\[ \\begin{align*} & \\text{LCS}(X, Y) \\ \\{ \\\\ & \\quad m \\leftarrow length[X] \\\\ & \\quad n \\leftarrow length[Y] \\\\ & \\quad \\text{if} \\ x_m = y_n \\ \\text{then} \\\\ & \\qquad Z \\leftarrow \\text{LCS}(X_{m-1}, Y_{n-1}) \\triangleright \\text{solve one subproblem} \\\\ & \\qquad \\text{return} \\ \\langle Z, x_m = y_n \\rangle \\triangleright \\text{append} \\ x_m = y_n \\ \\text{to} \\ Z \\\\ & \\quad else \\\\ & \\qquad Z^{'} \\leftarrow \\text{LCS}(X_{m-1}, Y) \\triangleright \\text{solve two subproblems} \\\\ & \\qquad Z^{''} \\leftarrow \\text{LCS}(X, Y_{n-1}) \\\\ & \\qquad \\text{return longer of} \\ Z^{'} \\ \\text{and} \\ Z^{''} \\\\ & \\} \\end{align*} \\]","title":"Recursive Algorithm (Inefficient)"},{"location":"week-6/ce100-week-6-lcs/#a-recursive-solution","text":"\\(c[i, j]:\\) length of an LCS of \\(X_i\\) and \\(Y_j\\) \\[ \\begin{align*} c[i,j] = \\begin{cases} & 0 & \\text{if}& \\ i=0 \\ \\text{or} \\ j=0 \\\\ & c[i-1,j-1]+1 & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i=y_j \\\\ & \\text{max}\\{c[i,j-1],c[i-1,j]\\} & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i \\neq y_j \\\\ \\end{cases} \\end{align*} \\]","title":"A Recursive Solution"},{"location":"week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs","text":"We can easily write an exponential-time recursive algorithm based on the given recurrence. \\(\\Longrightarrow\\) Inefficient! How many distinct subproblems to solve? \\(\\Theta(mn)\\) Overlapping subproblems property: Many subproblems share the same sub-subproblems. e.g. Finding an LCS to \\(X_{m-1} \\& Y\\) and an LCS to \\(X \\& Y_{n-1}\\) has the sub-subproblem of finding an LCS to \\(X_{m-1} \\& Y_{n-1}\\) Therefore, we can use dynamic programming .","title":"Computing the Length of an LCS"},{"location":"week-6/ce100-week-6-lcs/#data-structures","text":"Let: \\(c[i, j]:\\) length of an LCS of \\(X_i\\) and \\(Y_j\\) \\(b[i, j]:\\) direction towards the table entry corresponding to the optimal subproblem solution chosen when computing \\(c[i, j]\\) . Used to simplify the construction of an optimal solution at the end. Maintain the following tables: \\(c[0 \\dots m, 0 \\dots n]\\) \\(b[1 \\dots m, 1 \\dots n]\\)","title":"Data Structures"},{"location":"week-6/ce100-week-6-lcs/#bottom-up-computation","text":"Reminder: \\[ \\begin{align*} c[i,j] = \\begin{cases} & 0 & \\text{if}& \\ i=0 \\ \\text{or} \\ j=0 \\\\ & c[i-1,j-1]+1 & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i=y_j \\\\ & \\text{max}\\{c[i,j-1],c[i-1,j]\\} & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i \\neq y_j \\\\ \\end{cases} \\end{align*} \\] How to choose the order in which we process \\(c[i, j]\\) values? The values for \\(c[i-1, j-1]\\) , \\(c[i, j-1]\\) , and \\(c[i-1,j]\\) must be computed before computing \\(c[i, j]\\) .","title":"Bottom-up Computation"},{"location":"week-6/ce100-week-6-lcs/#bottom-up-computation_1","text":"section{ font-size: 25px; } \\[ \\begin{align*} c[i,j] = \\begin{cases} & 0 & \\text{if}& \\ i=0 \\ \\text{or} \\ j=0 \\\\ & c[i-1,j-1]+1 & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i=y_j \\\\ & \\text{max}\\{c[i,j-1],c[i-1,j]\\} & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i \\neq y_j \\\\ \\end{cases} \\end{align*} \\] Need to process: \\(c[i, j]\\) after computing: \\(c[i-1, j-1]\\) , \\(c[i, j-1]\\) , \\(c[i-1,j]\\)","title":"Bottom-up Computation"},{"location":"week-6/ce100-week-6-lcs/#bottom-up-computation_2","text":"section{ font-size: 25px; } \\[ \\begin{align*} c[i,j] = \\begin{cases} & 0 & \\text{if}& \\ i=0 \\ \\text{or} \\ j=0 \\\\ & c[i-1,j-1]+1 & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i=y_j \\\\ & \\text{max}\\{c[i,j-1],c[i-1,j]\\} & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i \\neq y_j \\\\ \\end{cases} \\end{align*} \\] \\[ \\Downarrow \\] \\[ \\begin{align*} & \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ m \\\\ & \\quad \\text{for} \\ j \\leftarrow 1 \\ \\text{to} \\ n \\\\ & \\qquad \\dots \\\\ & \\qquad \\dots \\\\ & \\qquad c[i, j] = \\cdots \\end{align*} \\] section{ font-size: 25px; }","title":"Bottom-up Computation"},{"location":"week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs_1","text":"\\[ \\begin{align*} \\frac{\\text{Total Runtime} = \\Theta(mn)}{\\text{Total Space} = \\Theta(mn)} \\begin{cases} & LCS-LENGTH(X,Y) \\\\ & \\quad m \\leftarrow length[X]; n \\leftarrow length[Y] \\\\ & \\quad \\text{for} \\ i \\leftarrow 0 \\ \\text{to} \\ m \\ \\text{do} \\ c[i, 0] \\leftarrow 0 \\\\ & \\quad \\text{for} \\ j \\leftarrow 0 \\ \\text{to} \\ n \\ \\text{do} \\ c[0, j] \\leftarrow 0 \\\\ & \\quad \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ m \\ \\text{do} \\\\ & \\qquad \\text{for} \\ j \\leftarrow 1 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad \\quad \\text{if} \\ x_i = y_j \\ \\text{then} \\\\ & \\qquad \\quad \\quad c[i, j] \\leftarrow c[i-1, j-1]+1 \\\\ & \\qquad \\quad \\quad b[i, j] \\leftarrow \" \\nwarrow \" \\\\ & \\qquad \\quad \\text{else if} \\ c[i - 1, j] \\geq c[i, j-1] \\\\ & \\qquad \\quad \\quad c[i, j] \\leftarrow c[i-1, j] \\\\ & \\qquad \\quad \\quad b[i, j] \\leftarrow \"\\uparrow \" \\\\ & \\qquad \\quad \\text{else} \\\\ & \\qquad \\quad \\quad c[i, j] \\leftarrow c[i, j-1] \\\\ & \\qquad \\quad \\quad b[i, j] \\leftarrow \" \\leftarrow \" \\\\ \\end{cases} \\end{align*} \\] section{ font-size: 25px; }","title":"Computing the Length of an LCS"},{"location":"week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-1","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-1"},{"location":"week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-2","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-2"},{"location":"week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-3","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-3"},{"location":"week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-4","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-4"},{"location":"week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-5","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-5"},{"location":"week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-6","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-6"},{"location":"week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-7","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-7"},{"location":"week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-8","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-8"},{"location":"week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-9","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-9"},{"location":"week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-10","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-10"},{"location":"week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-11","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-11"},{"location":"week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-12","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-12"},{"location":"week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-13","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ Running-time = \\(O(mn)\\) since each table entry takes \\(O(1)\\) time to compute section{ font-size: 25px; }","title":"Computing the Length of an LCS-13"},{"location":"week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-14","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ Running-time = \\(O(mn)\\) since each table entry takes \\(O(1)\\) time to compute LCS of \\(X \\& Y = \\langle B, C, B, A \\rangle\\)","title":"Computing the Length of an LCS-14"},{"location":"week-6/ce100-week-6-lcs/#constructing-an-lcs","text":"The \\(b\\) table returned by LCS-LENGTH can be used to quickly construct an LCS of \\(X \\& Y\\) Begin at \\(b[m, n]\\) and trace through the table following arrows Whenever you encounter a \" \\(\\nwarrow\\) \" in entry \\(b[i, j]\\) it implies that \\(x_i = y_j\\) is an element of LCS The elements of LCS are encountered in reverse order","title":"Constructing an LCS"},{"location":"week-6/ce100-week-6-lcs/#constructing-an-lcs_1","text":"section{ font-size: 21px; } The recursive procedure \\(\\text{PRINT-LCS}\\) prints out \\(\\text{LCS}\\) in proper order This procedure takes \\(O(m+n)\\) time since at least one of \\(i\\) and \\(j\\) is decremented in each stage of the recursion \\[ \\begin{align*} & \\text{PRINT-LCS}(b, X, i, j) \\\\ & \\quad \\text{if} \\ i = 0 \\ \\text{or} j = 0 \\ \\text{then} \\\\ & \\quad \\text{return} \\\\ & \\quad \\text{if} \\ b[i, j] = \" \\nwarrow \" \\ \\text{then} \\\\ & \\qquad \\text{PRINT-LCS}(b, X, i-1, j-1) \\\\ & \\qquad \\text{print} \\ x_i \\\\ & \\quad \\text{else if} \\ b[i, j] = \" \\uparrow \" \\ \\text{then} \\\\ & \\qquad \\text{PRINT-LCS}(b, X, i-1, j) \\\\ & \\quad \\text{else} \\\\ & \\qquad \\text{PRINT-LCS}(b, X, i, j-1) \\end{align*} \\] The initial invocation: \\(\\text{PRINT-LCS}(b, X, length[X], length[Y])\\) section{ font-size: 25px; }","title":"Constructing an LCS"},{"location":"week-6/ce100-week-6-lcs/#do-we-really-need-the-b-table-back-pointers","text":"Question: From which neighbor did we expand to the highlighted cell? Answer: Upper-left neighbor,because \\(X[i] = Y[j]\\) . section{ font-size: 25px; }","title":"Do we really need the b table (back-pointers)?"},{"location":"week-6/ce100-week-6-lcs/#do-we-really-need-the-b-table-back-pointers_1","text":"Question: From which neighbor did we expand to the highlighted cell? Answer: Left neighbor, because \\(X[i] \\neq Y[j]\\) and \\(LCS[i, j-1] > LCS[i-1, j]\\) . section{ font-size: 25px; }","title":"Do we really need the b table (back-pointers)?"},{"location":"week-6/ce100-week-6-lcs/#do-we-really-need-the-b-table-back-pointers_2","text":"Question: From which neighbor did we expand to the highlighted cell? Answer: Upper neighbor,because \\(X[i] \\neq Y[j]\\) and \\(LCS[i, j-1] = LCS[i-1, j]\\) . (See pseudo-code to see how ties are handled.) section{ font-size: 25px; }","title":"Do we really need the b table (back-pointers)?"},{"location":"week-6/ce100-week-6-lcs/#improving-the-space-requirements","text":"We can eliminate the b table altogether each \\(c[i, j]\\) entry depends only on \\(3\\) other \\(c\\) table entries: \\(c[i-1, j-1]\\) , \\(c[i-1, j]\\) and \\(c[i, j-1]\\) Given the value of \\(c[i, j]\\) : We can determine in \\(O(1)\\) time which of these \\(3\\) values was used to compute \\(c[i, j]\\) without inspecting table \\(b\\) We save \\(\\Theta(mn)\\) space by this method However, space requirement is still \\(\\Theta(mn)\\) since we need \\(\\Theta(mn)\\) space for the \\(c\\) table anyway section{ font-size: 25px; }","title":"Improving the Space Requirements"},{"location":"week-6/ce100-week-6-lcs/#what-if-we-store-the-last-2-rows-only","text":"To compute \\(c[i, j]\\) , we only need \\(c[i-1, j-1]\\) , \\(c[i-1, j]\\) ,and \\(c[i-1, j-1]\\) So, we can store only the last two rows. section{ font-size: 25px; }","title":"What if we store the last 2 rows only?"},{"location":"week-6/ce100-week-6-lcs/#what-if-we-store-the-last-2-rows-only_1","text":"To compute \\(c[i, j]\\) , we only need \\(c[i-1, j-1]\\) , \\(c[i-1, j]\\) , and \\(c[i-1, j-1]\\) So, we can store only the last two rows. section{ font-size: 25px; }","title":"What if we store the last 2 rows only?"},{"location":"week-6/ce100-week-6-lcs/#what-if-we-store-the-last-2-rows-only_2","text":"To compute \\(c[i, j]\\) , we only need \\(c[i-1, j-1]\\) , \\(c[i-1, j]\\) , and \\(c[i-1, j-1]\\) So, we can store only the last two rows. This reduces space complexity from \\(\\Theta(mn)\\) to \\(\\Theta(n)\\) . Is there a problem with this approach? section{ font-size: 25px; }","title":"What if we store the last 2 rows only?"},{"location":"week-6/ce100-week-6-lcs/#what-if-we-store-the-last-2-rows-only_3","text":"Is there a problem with this approach? We cannot construct the optimal solution because we cannot backtrace anymore. This approach works if we only need the length of an LCS, not the actual LCS.","title":"What if we store the last 2 rows only?"},{"location":"week-6/ce100-week-6-lcs/#problem-4-optimal-binary-search-tree","text":"","title":"Problem 4 Optimal Binary Search Tree"},{"location":"week-6/ce100-week-6-lcs/#reminder-binary-search-tree-bst","text":"","title":"Reminder: Binary Search Tree (BST)"},{"location":"week-6/ce100-week-6-lcs/#binary-search-tree-example","text":"Example: English-to-French translation Organize (English, French) word pairs in a BST Keyword: English word Satellite Data: French word We can search for an English word (node key) efficiently, and return the corresponding French word (satellite data).","title":"Binary Search Tree Example"},{"location":"week-6/ce100-week-6-lcs/#ascii-table","text":"section{ font-size: 25px; }","title":"ASCII Table"},{"location":"week-6/ce100-week-6-lcs/#binary-search-tree-example_1","text":"Suppose we know the frequency of each keyword in texts: $$ \\underset{5\\%}{\\underline{begin}}, \\underset{40\\%}{\\underline{do}}, \\underset{8\\%}{\\underline{else}}, \\underset{4\\%}{\\underline{end}}, \\underset{10\\%}{\\underline{if}}, \\underset{10\\%}{\\underline{then}}, \\underset{23\\%}{\\underline{while}}, $$ section{ font-size: 25px; }","title":"Binary Search Tree Example"},{"location":"week-6/ce100-week-6-lcs/#cost-of-a-binary-search-tree","text":"Example: If we search for keyword \"while\" , we need to access \\(3\\) nodes. So, \\(23%\\) of the queries will have cost of \\(3\\) . \\[ \\begin{align*} \\text{Total Cost} &= \\sum \\limits_{i}^{}(\\text{depth}(i)+1)\\text{freq}(i) \\\\ &= 1 \\times 0.04 + 2 \\times 0.4 + \\\\ & 2 \\times 0.1 + 3 \\times 0.05 + \\\\ & 3 \\times 0.08 + 3 \\times 0.1 + \\\\ & 3 \\times 0.23 \\\\ &= 2.42 \\end{align*} \\] section{ font-size: 25px; }","title":"Cost of a Binary Search Tree"},{"location":"week-6/ce100-week-6-lcs/#cost-of-a-binary-search-tree_1","text":"Example: If we search for keyword \"while\" , we need to access \\(3\\) nodes. So, \\(23%\\) of the queries will have cost of \\(3\\) . \\[ \\begin{align*} \\text{Total Cost} &= \\sum \\limits_{i}^{}(\\text{depth}(i)+1)\\text{freq}(i) \\\\ &= 1 \\times 0.4 + 2 \\times 0.05 + 2 \\times 0.23 + \\\\ & 3 \\times 0.1 + 4 \\times 0.08 + \\\\ & 4 \\times 0.1 + 5 \\times 0.04 \\\\ &= 2.18 \\end{align*} \\] This is in fact an optimal BST. section{ font-size: 25px; }","title":"Cost of a Binary Search Tree"},{"location":"week-6/ce100-week-6-lcs/#optimal-binary-search-tree-problem","text":"Given: A collection of \\(n\\) keys \\(K_1 < K_2 < \\dots K_n\\) to be stored in a BST . The corresponding \\(p_i\\) values for \\(1 \\leq i \\leq n\\) \\(p_i\\) : probability of searching for key \\(K_i\\) Find: An optimal BST with minimum total cost: \\[ \\begin{align*} \\text{Total Cost} &= \\sum \\limits_{i}^{}(\\text{depth}(i)+1)\\text{freq}(i) \\end{align*} \\] Note: The BST will be static. Only search operations will be performed. No insert, no delete, etc. section{ font-size: 25px; }","title":"Optimal Binary Search Tree Problem"},{"location":"week-6/ce100-week-6-lcs/#cost-of-a-binary-search-tree_2","text":"Lemma 1 : Let \\(Tij\\) be a BST containing keys \\(K_i < K_{i+1} < \\dots < K_j\\) . Let \\(T_L\\) and \\(T_R\\) be the left and right subtrees of \\(T\\) . Then we have: \\[ \\begin{align*} \\text{cost}(T_{ij})=\\text{cost}(T_{L})+\\text{cost}(T_{R})+\\sum \\limits_{h=i}^{j}p_h \\end{align*} \\] Intuition: When we add the root node, the depth of each node in \\(T_L\\) and \\(T_R\\) increases by \\(1\\) . So, the cost of node \\(h\\) increases by \\(p_h\\) . In addition, the cost of root node \\(r\\) is \\(p_r\\) . That\u2019s why, we have the last term at the end of the formula above. section{ font-size: 25px; }","title":"Cost of a Binary Search Tree"},{"location":"week-6/ce100-week-6-lcs/#optimal-substructure-property","text":"Lemma 2: Optimal substructure property Consider an optimal BST \\(T_{ij}\\) for keys \\(K_i < K_{i+1} < \\dots < K_j\\) Let \\(K_m\\) be the key at the root of \\(T_{ij}\\) Then: \\(T_{i,m-1}\\) is an optimal BST for subproblem containing keys: \\(K_i < \\dots < K_{m-1}\\) \\(T_{m+1,j}\\) is an optimal BST for subproblem containing keys: \\(K_{m+1} < \\dots < K_j\\) \\[ \\begin{align*} \\text{cost}(T_{ij})=\\text{cost}(T_{i,m-1})+\\text{cost}(T_{m+1,j})+\\sum \\limits_{h=i}^{j}p_h \\end{align*} \\] section{ font-size: 25px; }","title":"Optimal Substructure Property"},{"location":"week-6/ce100-week-6-lcs/#recursive-formulation","text":"Note: We don\u2019t know which root vertex leads to the minimum total cost. So, we need to try each vertex \\(m\\) , and choose the one with minimum total cost. \\(c[i, j]\\) : cost of an optimal BST \\(T_{ij}\\) for the subproblem \\(K_i < \\dots < K_j\\) \\[ \\begin{align*} & c[i,j] = \\begin{cases} & 0 & \\text{if} \\ i>j \\\\ & \\underset{i \\leq r \\leq j}{\\text{min}}\\{ c[i,r-1]+c[r+1,j]+P_{ij} \\} & \\text{otherwise} \\\\ \\end{cases} \\\\ & \\text{where} \\ P_{ij}= \\sum \\limits_{h=i}^{j}p_h \\end{align*} \\] section{ font-size: 25px; }","title":"Recursive Formulation"},{"location":"week-6/ce100-week-6-lcs/#bottom-up-computation_3","text":"\\[ \\begin{align*} & c[i,j] = \\begin{cases} & 0 & \\text{if} \\ i>j \\\\ & \\underset{i \\leq r \\leq j}{\\text{min}}\\{ c[i,r-1]+c[r+1,j]+P_{ij} \\} & \\text{otherwise} \\\\ \\end{cases} \\end{align*} \\] How to choose the order in which we process \\(c[i, j]\\) values? Before computing \\(c[i, j]\\) , we have to make sure that the values for \\(c[i, r-1]\\) and \\(c[r+1,j]\\) have been computed for all \\(r\\) . section{ font-size: 25px; }","title":"Bottom-up computation"},{"location":"week-6/ce100-week-6-lcs/#bottom-up-computation_4","text":"\\[ \\begin{align*} & c[i,j] = \\begin{cases} & 0 & \\text{if} \\ i>j \\\\ & \\underset{i \\leq r \\leq j}{\\text{min}}\\{ c[i,r-1]+c[r+1,j]+P_{ij} \\} & \\text{otherwise} \\\\ \\end{cases} \\end{align*} \\] \\(c[i,j]\\) must be processed after \\(c[i,r-1]\\) and \\(c[r+1,j]\\) section{ font-size: 25px; }","title":"Bottom-up computation"},{"location":"week-6/ce100-week-6-lcs/#bottom-up-computation_5","text":"\\[ \\begin{align*} & c[i,j] = \\begin{cases} & 0 & \\text{if} \\ i>j \\\\ & \\underset{i \\leq r \\leq j}{\\text{min}}\\{ c[i,r-1]+c[r+1,j]+P_{ij} \\} & \\text{otherwise} \\\\ \\end{cases} \\end{align*} \\] If the entries \\(c[i,j]\\) are computed in the shown order, then \\(c[i,r-1]\\) and \\(c[r+1,j]\\) values are guaranteed to be computed before \\(c[i,j]\\) . section{ font-size: 25px; }","title":"Bottom-up computation"},{"location":"week-6/ce100-week-6-lcs/#computing-the-optimal-bst-cost","text":"\\[ \\begin{align*} & \\text{OPTIMAL-BST-COST} (p, n) \\\\ & \\quad \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad c[i, i-1] \\leftarrow 0 \\\\ & \\qquad c[i, i] \\leftarrow p[i] \\\\ & \\qquad R[i, j] \\leftarrow i \\\\ & \\quad PS[1] \\leftarrow p[1] \\Longleftarrow PS[i] \\rightarrow \\text{ prefix-sum } (i): \\text{Sum of all} \\ p[j] \\ \\text{values for} \\ j \\leq i \\\\ & \\quad \\text{for} \\ i \\leftarrow 2 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad PS[i] \\leftarrow p[i] + PS[i-1] \\Longleftarrow \\text{compute the prefix sum} \\\\ & \\quad \\text{for} \\ d \\leftarrow 1 \\ \\text{to} \\ n\u22121 \\ \\text{do} \\Longleftarrow \\text{BSTs with} \\ d+1 \\ \\text{consecutive keys} \\\\ & \\qquad \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ n \u2013 d \\ \\text{do} \\\\ & \\qquad \\quad j \\leftarrow i + d \\\\ & \\qquad \\quad c[i, j] \\leftarrow \\infty \\\\ & \\qquad \\quad \\text{for} \\ r \\leftarrow i \\ \\text{to} \\ j \\ \\text{do} \\\\ & \\qquad \\qquad q \\leftarrow min\\{c[i,r-1] + c[r+1, j]\\} + PS[j] \u2013 PS[i-1]\\} \\\\ & \\qquad \\qquad \\text{if} \\ q < c[i, j] \\ \\text{then} \\\\ & \\qquad \\qquad \\quad c[i, j] \\leftarrow q \\\\ & \\qquad \\qquad \\quad R[i, j] \\leftarrow r \\\\ & \\quad \\text{return} \\ c[1, n], R \\end{align*} \\] section{ font-size: 25px; }","title":"Computing the Optimal BST Cost"},{"location":"week-6/ce100-week-6-lcs/#note-on-prefix-sum","text":"We need \\(P_{ij}\\) values for each \\(i, j (1 \u2264 i \u2264 n \\ \\text{and} \\ 1 \u2264 j \u2264 n)\\) , where: \\[ \\begin{align*} P_{ij} = \\sum \\limits_{h=i}^{j}p_h \\end{align*} \\] If we compute the summation directly for every \\((i, j)\\) pair, the runtime would be \\(\\Theta(n^3)\\) . Instead, we spend \\(O(n)\\) time in preprocessing to compute the prefix sum array PS . Then we can compute each \\(P_{ij}\\) in \\(O(1)\\) time using PS . section{ font-size: 25px; }","title":"Note on Prefix Sum"},{"location":"week-6/ce100-week-6-lcs/#note-on-prefix-sum_1","text":"In preprocessing, compute for each \\(i\\) : \\(PS[i]\\) : the sum of \\(p[j]\\) values for \\(1 \\leq j \\leq i\\) Then, we can compute \\(P_{ij}\\) in \\(O(1)\\) time as follows: \\(P_{ij} = PS[i] \u2013 PS[j-1]\\) Example: \\[ \\begin{align*} p &: \\overset{1}{0.05} \\ \\overset{2}{0.02} \\ \\overset{3}{0.06} \\ \\overset{4}{0.07} \\ \\overset{5}{0.20} \\ \\overset{6}{0.05} \\ \\overset{7}{0.08} \\ \\overset{8}{0.02} \\\\ PS &: \\overset{1}{0.05} \\ \\overset{2}{0.07} \\ \\overset{3}{0.13} \\ \\overset{4}{0.20} \\ \\overset{5}{0.40} \\ \\overset{6}{0.45} \\ \\overset{7}{0.53} \\ \\overset{8}{0.55} \\\\[10 pt] P_{27} &= PS[7] \u2013 PS[1] = 0.53 \u2013 0.05 = 0.48 \\\\ P_{36} &= PS[6] \u2013 PS[2] = 0.45 \u2013 0.07 = 0.38 \\end{align*} \\]","title":"Note on Prefix Sum"},{"location":"week-6/ce100-week-6-lcs/#review","text":"","title":"REVIEW"},{"location":"week-6/ce100-week-6-lcs/#overlapping-subproblems-property-in-dynamic-programming","text":"Dynamic Programming is an algorithmic paradigm that solves a given complex problem by breaking it into subproblems and stores the results of subproblems to avoid computing the same results again.","title":"Overlapping Subproblems Property in Dynamic Programming"},{"location":"week-6/ce100-week-6-lcs/#overlapping-subproblems-property-in-dynamic-programming_1","text":"Following are the two main properties of a problem that suggests that the given problem can be solved using Dynamic programming. Overlapping Subproblems Optimal Substructure","title":"Overlapping Subproblems Property in Dynamic Programming"},{"location":"week-6/ce100-week-6-lcs/#overlapping-subproblems_2","text":"Like Divide and Conquer, Dynamic Programming combines solutions to sub-problems. Dynamic Programming is mainly used when solutions of the same subproblems are needed again and again. In dynamic programming, computed solutions to subproblems are stored in a table so that these don\u2019t have to be recomputed. So Dynamic Programming is not useful when there are no common (overlapping) subproblems because there is no point storing the solutions if they are not needed again.","title":"Overlapping Subproblems"},{"location":"week-6/ce100-week-6-lcs/#overlapping-subproblems_3","text":"For example, Binary Search doesn\u2019t have common subproblems. If we take an example of following recursive program for Fibonacci Numbers, there are many subproblems that are solved again and again.","title":"Overlapping Subproblems"},{"location":"week-6/ce100-week-6-lcs/#simple-recursion","text":"\\(f(n) = f(n-1) + f(n-2)\\) C sample code: #include <stdio.h> // a simple recursive program to compute fibonacci numbers int fib ( int n ) { if ( n <= 1 ) return n ; else return fib ( n -1 ) + fib ( n -2 ); } int main () { int n = 5 ; printf ( \"Fibonacci number is %d \" , fib ( n )); return 0 ; }","title":"Simple Recursion"},{"location":"week-6/ce100-week-6-lcs/#simple-recursion_1","text":"Output Fibonacci number is 5","title":"Simple Recursion"},{"location":"week-6/ce100-week-6-lcs/#simple-recursion_2","text":"\\(f(n) = f(n-1) + f(n-2)\\) /* a simple recursive program for Fibonacci numbers */ public class Fibonacci { public static void main ( String [] args ) { int n = Integer . parseInt ( args [ 0 ] ); System . out . println ( fib ( n )); } public static int fib ( int n ) { if ( n <= 1 ) return n ; return fib ( n - 1 ) + fib ( n - 2 ); } }","title":"Simple Recursion"},{"location":"week-6/ce100-week-6-lcs/#simple-recursion_3","text":"\\(f(n) = f(n-1) + f(n-2)\\) public class Fibonacci { public static void Main ( string [] args ) { int n = int . Parse ( args [ 0 ]); Console . WriteLine ( fib ( n )); } public static int fib ( int n ) { if ( n <= 1 ) return n ; return fib ( n - 1 ) + fib ( n - 2 ); } }","title":"Simple Recursion"},{"location":"week-6/ce100-week-6-lcs/#recursion-tree-for-execution-of-fib5","text":"fib(5) / \\ fib(4) fib(3) / \\ / \\ fib(3) fib(2) fib(2) fib(1) / \\ / \\ / \\ fib(2) fib(1) fib(1) fib(0) fib(1) fib(0) / \\ fib(1) fib(0) We can see that the function fib(3) is being called 2 times. If we would have stored the value of fib(3) , then instead of computing it again, we could have reused the old stored value.","title":"Recursion tree for execution of fib(5)"},{"location":"week-6/ce100-week-6-lcs/#recursion-tree-for-execution-of-fib5_1","text":"There are following two different ways to store the values so that these values can be reused: Memoization (Top Down) Tabulation (Bottom Up)","title":"Recursion tree for execution of fib(5)"},{"location":"week-6/ce100-week-6-lcs/#memoization-top-down","text":"The memoized program for a problem is similar to the recursive version with a small modification that looks into a lookup table before computing solutions. We initialize a lookup array with all initial values as NIL . Whenever we need the solution to a subproblem, we first look into the lookup table. If the precomputed value is there then we return that value, otherwise, we calculate the value and put the result in the lookup table so that it can be reused later.","title":"Memoization (Top Down)"},{"location":"week-6/ce100-week-6-lcs/#memoization-top-down_1","text":"Following is the memoized version for the nth Fibonacci Number. C++ Version: /* C++ program for Memoized version for nth Fibonacci number */ #include <bits/stdc++.h> using namespace std ; #define NIL -1 #define MAX 100 int lookup [ MAX ];","title":"Memoization (Top Down)"},{"location":"week-6/ce100-week-6-lcs/#memoization-top-down_2","text":"C++ Version: /* Function to initialize NIL values in lookup table */ void _initialize () { int i ; for ( i = 0 ; i < MAX ; i ++ ) lookup [ i ] = NIL ; }","title":"Memoization (Top Down)"},{"location":"week-6/ce100-week-6-lcs/#memoization-top-down_3","text":"C++ Version: /* function for nth Fibonacci number */ int fib ( int n ) { if ( lookup [ n ] == NIL ) { if ( n <= 1 ) lookup [ n ] = n ; else lookup [ n ] = fib ( n - 1 ) + fib ( n - 2 ); } return lookup [ n ]; }","title":"Memoization (Top Down)"},{"location":"week-6/ce100-week-6-lcs/#memoization-top-down_4","text":"C++ Version: // Driver code int main () { int n = 40 ; _initialize (); cout << \"Fibonacci number is \" << fib ( n ); return 0 ; }","title":"Memoization (Top Down)"},{"location":"week-6/ce100-week-6-lcs/#memoization-top-down_5","text":"Java Version: /* Java program for Memoized version */ public class Fibonacci { final int MAX = 100 ; final int NIL = - 1 ; int lookup [] = new int [ MAX ] ; /* Function to initialize NIL values in lookup table */ void _initialize () { for ( int i = 0 ; i < MAX ; i ++ ) lookup [ i ] = NIL ; }","title":"Memoization (Top Down)"},{"location":"week-6/ce100-week-6-lcs/#memoization-top-down_6","text":"Java Version: /* function for nth Fibonacci number */ int fib ( int n ) { if ( lookup [ n ] == NIL ) { if ( n <= 1 ) lookup [ n ] = n ; else lookup [ n ] = fib ( n - 1 ) + fib ( n - 2 ); } return lookup [ n ] ; }","title":"Memoization (Top Down)"},{"location":"week-6/ce100-week-6-lcs/#memoization-top-down_7","text":"Java Version: public static void main ( String [] args ) { Fibonacci f = new Fibonacci (); int n = 40 ; f . _initialize (); System . out . println ( \"Fibonacci number is\" + \" \" + f . fib ( n )); } }","title":"Memoization (Top Down)"},{"location":"week-6/ce100-week-6-lcs/#memoization-top-down_8","text":"C# Version: // C# program for Memoized versionof nth Fibonacci number using System ; class FiboCalcMemoized { static int MAX = 100 ; static int NIL = - 1 ; static int [] lookup = new int [ MAX ]; /* Function to initialize NIL values in lookup table */ static void initialize () { for ( int i = 0 ; i < MAX ; i ++) lookup [ i ] = NIL ; }","title":"Memoization (Top Down)"},{"location":"week-6/ce100-week-6-lcs/#memoization-top-down_9","text":"C# Version: /* function for nth Fibonacci number */ static int fib ( int n ) { if ( lookup [ n ] == NIL ) { if ( n <= 1 ) lookup [ n ] = n ; else lookup [ n ] = fib ( n - 1 ) + fib ( n - 2 ); } return lookup [ n ]; }","title":"Memoization (Top Down)"},{"location":"week-6/ce100-week-6-lcs/#memoization-top-down_10","text":"C# Version: // Driver code public static void Main () { int n = 40 ; initialize (); Console . Write ( \"Fibonacci number is\" + \" \" + fib ( n )); } }","title":"Memoization (Top Down)"},{"location":"week-6/ce100-week-6-lcs/#tabulation-bottom-up","text":"The tabulated program for a given problem builds a table in bottom-up fashion and returns the last entry from the table. For example, for the same Fibonacci number, we first calculate fib(0) then fib(1) then fib(2) then fib(3) , and so on. So literally, we are building the solutions of subproblems bottom-up.","title":"Tabulation (Bottom Up)"},{"location":"week-6/ce100-week-6-lcs/#tabulation-bottom-up_1","text":"C++ Version: /* C program for Tabulated version */ #include <stdio.h> int fib ( int n ) { int f [ n + 1 ]; int i ; f [ 0 ] = 0 ; f [ 1 ] = 1 ; for ( i = 2 ; i <= n ; i ++ ) f [ i ] = f [ i - 1 ] + f [ i - 2 ]; return f [ n ]; }","title":"Tabulation (Bottom Up)"},{"location":"week-6/ce100-week-6-lcs/#tabulation-bottom-up_2","text":"C++ Version: ... int main () { int n = 9 ; printf ( \"Fibonacci number is %d \" , fib ( n )); return 0 ; } Output: Fibonacci number is 34","title":"Tabulation (Bottom Up)"},{"location":"week-6/ce100-week-6-lcs/#tabulation-bottom-up_3","text":"Java Version: /* Java program for Tabulated version */ public class Fibonacci { public static void main ( String [] args ) { int n = 9 ; System . out . println ( \"Fibonacci number is \" + fib ( n )); }","title":"Tabulation (Bottom Up)"},{"location":"week-6/ce100-week-6-lcs/#tabulation-bottom-up_4","text":"Java Version: /* Function to calculate nth Fibonacci number */ static int fib ( int n ) { int f [] = new int [ n + 1 ] ; f [ 0 ] = 0 ; f [ 1 ] = 1 ; for ( int i = 2 ; i <= n ; i ++ ) f [ i ] = f [ i - 1 ] + f [ i - 2 ] ; return f [ n ] ; } }","title":"Tabulation (Bottom Up)"},{"location":"week-6/ce100-week-6-lcs/#tabulation-bottom-up_5","text":"C# Version: // C# program for Tabulated version using System ; class Fibonacci { static int fib ( int n ) { int [] f = new int [ n + 1 ]; f [ 0 ] = 0 ; f [ 1 ] = 1 ; for ( int i = 2 ; i <= n ; i ++) f [ i ] = f [ i - 1 ] + f [ i - 2 ]; return f [ n ]; } public static void Main () { int n = 9 ; Console . Write ( \"Fibonacci number is\" + \" \" + fib ( n )); } } Both Tabulated and Memoized store the solutions of subproblems. In Memoized version, the table is filled on demand while in the Tabulated version, starting from the first entry, all entries are filled one by one. Unlike the Tabulated version, all entries of the lookup table are not necessarily filled in Memoized version. To see the optimization achieved by Memoized and Tabulated solutions over the basic Recursive solution, see the time taken by following runs for calculating the 40 th Fibonacci number: Recursive Solution: https://ide.geeksforgeeks.org/vHt6ly Memoized Solution: https://ide.geeksforgeeks.org/Z94jYR Tabulated Solution: https://ide.geeksforgeeks.org/12C5bP","title":"Tabulation (Bottom Up)"},{"location":"week-6/ce100-week-6-lcs/#optimal-substructure-property-in-dynamic-programming","text":"A given problems has Optimal Substructure Property if optimal solution of the given problem can be obtained by using optimal solutions of its subproblems. For example, the Shortest Path problem has following optimal substructure property: If a node x lies in the shortest path from a source node u to destination node v then the shortest path from u to v is combination of shortest path from u to x and shortest path from x to v. The standard All Pair Shortest Path algorithm like Floyd\u2013Warshall and Single Source Shortest path algorithm for negative weight edges like Bellman\u2013Ford are typical examples of Dynamic Programming.","title":"Optimal Substructure Property in Dynamic Programming"},{"location":"week-6/ce100-week-6-lcs/#optimal-substructure-property-in-dynamic-programming_1","text":"On the other hand, the Longest Path problem doesn\u2019t have the Optimal Substructure property. Here by Longest Path we mean longest simple path (path without cycle) between two nodes","title":"Optimal Substructure Property in Dynamic Programming"},{"location":"week-6/ce100-week-6-lcs/#optimal-substructure-property-in-dynamic-programming_2","text":"There are two longest paths from q to t: q\u2192r\u2192t and q\u2192s\u2192t. Unlike shortest paths, these longest paths do not have the optimal substructure property. For example, the longest path q\u2192r\u2192t is not a combination of longest path from q to r and longest path from r to t, because the longest path from q to r is q\u2192s\u2192t\u2192r and the longest path from r to t is r\u2192q\u2192s\u2192t.","title":"Optimal Substructure Property in Dynamic Programming"},{"location":"week-6/ce100-week-6-lcs/#most-common-dynamic-programming-interview-questions","text":"","title":"Most Common Dynamic Programming Interview Questions"},{"location":"week-6/ce100-week-6-lcs/#problem-1-longest-increasing-subsequence","text":"Problem-1: Longest Increasing Subsequence","title":"Problem-1: Longest Increasing Subsequence"},{"location":"week-6/ce100-week-6-lcs/#problem-1-longest-increasing-subsequence_1","text":"","title":"Problem-1: Longest Increasing Subsequence"},{"location":"week-6/ce100-week-6-lcs/#problem-2-edit-distance","text":"Problem-2: Edit Distance","title":"Problem-2: Edit Distance"},{"location":"week-6/ce100-week-6-lcs/#problem-2-edit-distance-recursive","text":"","title":"Problem-2: Edit Distance (Recursive)"},{"location":"week-6/ce100-week-6-lcs/#problem-2-edit-distance-dp","text":"https://www.coursera.org/learn/dna-sequencing","title":"Problem-2: Edit Distance (DP)"},{"location":"week-6/ce100-week-6-lcs/#problem-2-edit-distance-dp_1","text":"","title":"Problem-2: Edit Distance (DP)"},{"location":"week-6/ce100-week-6-lcs/#problem-2-edit-distance-other","text":"","title":"Problem-2: Edit Distance (Other)"},{"location":"week-6/ce100-week-6-lcs/#problem-3-partition-a-set-into-two-subsets-such-that-the-difference-of-subset-sums-is-minimum","text":"Problem-3: Partition a set into two subsets such that the difference of subset sums is minimum","title":"Problem-3: Partition a set into two subsets such that the difference of subset sums is minimum"},{"location":"week-6/ce100-week-6-lcs/#problem-4-count-number-of-ways-to-cover-a-distance","text":"Problem-4: Count number of ways to cover a distance","title":"Problem-4: Count number of ways to cover a distance"},{"location":"week-6/ce100-week-6-lcs/#problem-5-find-the-longest-path-in-a-matrix-with-given-constraints","text":"Problem-5: Find the longest path in a matrix with given constraints","title":"Problem-5: Find the longest path in a matrix with given constraints"},{"location":"week-6/ce100-week-6-lcs/#problem-6-subset-sum-problem","text":"Problem-6: Subset Sum Problem","title":"Problem-6: Subset Sum Problem"},{"location":"week-6/ce100-week-6-lcs/#problem-7-optimal-strategy-for-a-game","text":"Problem-7: Optimal Strategy for a Game","title":"Problem-7: Optimal Strategy for a Game"},{"location":"week-6/ce100-week-6-lcs/#problem-8-0-1-knapsack-problem","text":"Problem-8: 0-1 Knapsack Problem","title":"Problem-8: 0-1 Knapsack Problem"},{"location":"week-6/ce100-week-6-lcs/#problem-9-boolean-parenthesization-problem","text":"Problem-9: Boolean Parenthesization Problem","title":"Problem-9: Boolean Parenthesization Problem"},{"location":"week-6/ce100-week-6-lcs/#problem-10-shortest-common-supersequence","text":"Problem-10: Shortest Common Supersequence","title":"Problem-10: Shortest Common Supersequence"},{"location":"week-6/ce100-week-6-lcs/#problem-11-partition-problem","text":"Problem-11: Partition Problem","title":"Problem-11: Partition Problem"},{"location":"week-6/ce100-week-6-lcs/#problem-12-cutting-a-rod","text":"Problem-12: Cutting a Rod","title":"Problem-12: Cutting a Rod"},{"location":"week-6/ce100-week-6-lcs/#problem-13-coin-change","text":"Problem-13: Coin Change","title":"Problem-13: Coin Change"},{"location":"week-6/ce100-week-6-lcs/#problem-14-word-break-problem","text":"Problem-14: Word Break Problem","title":"Problem-14: Word Break Problem"},{"location":"week-6/ce100-week-6-lcs/#problem-15-maximum-product-cutting","text":"Problem-15: Maximum Product Cutting","title":"Problem-15: Maximum Product Cutting"},{"location":"week-6/ce100-week-6-lcs/#problem-16-dice-throw","text":"Problem-16: Dice Throw","title":"Problem-16: Dice Throw"},{"location":"week-6/ce100-week-6-lcs/#problem-16-dice-throw_1","text":"","title":"Problem-16: Dice Throw"},{"location":"week-6/ce100-week-6-lcs/#problem-17-box-stacking-problem","text":"Problem-17: Box Stacking Problem","title":"Problem-17: Box Stacking Problem"},{"location":"week-6/ce100-week-6-lcs/#problem-18-egg-dropping-puzzle","text":"Problem-18: Egg Dropping Puzzle","title":"Problem-18: Egg Dropping Puzzle"},{"location":"week-6/ce100-week-6-lcs/#references","text":"Introduction to Algorithms, Third Edition | The MIT Press CLRS Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) \\(-End-Of-Week-6-Course-Module-\\)","title":"References"},{"location":"week-7/ce100-week-7-knapsack/","text":"CE100 Algorithms and Programming II \u00b6 Week-7 (Greedy Algorithms, Knapsack) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Greedy Algorithms, Knapsack \u00b6 Outline \u00b6 Greedy Algorithms and Dynamic Programming Differences Greedy Algorithms Activity Selection Problem Knapsack Problems The 0-1 knapsack problem The fractional knapsack problem References \u00b6 TODO","title":"Week-7 (Greedy Algorithms, Knapsack)"},{"location":"week-7/ce100-week-7-knapsack/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-7/ce100-week-7-knapsack/#week-7-greedy-algorithms-knapsack","text":"","title":"Week-7 (Greedy Algorithms, Knapsack)"},{"location":"week-7/ce100-week-7-knapsack/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-7/ce100-week-7-knapsack/#greedy-algorithms-knapsack","text":"","title":"Greedy Algorithms, Knapsack"},{"location":"week-7/ce100-week-7-knapsack/#outline","text":"Greedy Algorithms and Dynamic Programming Differences Greedy Algorithms Activity Selection Problem Knapsack Problems The 0-1 knapsack problem The fractional knapsack problem","title":"Outline"},{"location":"week-7/ce100-week-7-knapsack/#references","text":"TODO","title":"References"},{"location":"week-8/ce100-week-8-midterm/","text":"CE100 Algorithms and Programming II \u00b6 Week-8 (Midterm) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Midterm \u00b6 Outline \u00b6 References \u00b6 TODO","title":"Week-8 (Midterm)"},{"location":"week-8/ce100-week-8-midterm/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-8/ce100-week-8-midterm/#week-8-midterm","text":"","title":"Week-8 (Midterm)"},{"location":"week-8/ce100-week-8-midterm/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-8/ce100-week-8-midterm/#midterm","text":"","title":"Midterm"},{"location":"week-8/ce100-week-8-midterm/#outline","text":"","title":"Outline"},{"location":"week-8/ce100-week-8-midterm/#references","text":"TODO","title":"References"},{"location":"week-9/ce100-week-9-huffman/","text":"CE100 Algorithms and Programming II \u00b6 Week-9 (Huffman Coding) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Huffman Coding \u00b6 Outline \u00b6 Heap Data Structure Heap Sort Huffman Coding References \u00b6 TODO","title":"Week-9 (Huffman Coding)"},{"location":"week-9/ce100-week-9-huffman/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"week-9/ce100-week-9-huffman/#week-9-huffman-coding","text":"","title":"Week-9 (Huffman Coding)"},{"location":"week-9/ce100-week-9-huffman/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"week-9/ce100-week-9-huffman/#huffman-coding","text":"","title":"Huffman Coding"},{"location":"week-9/ce100-week-9-huffman/#outline","text":"Heap Data Structure Heap Sort Huffman Coding","title":"Outline"},{"location":"week-9/ce100-week-9-huffman/#references","text":"TODO","title":"References"},{"location":"tr/","text":"","title":"Home"},{"location":"tr/license/","text":"License \u00b6 MIT License Copyright \u00a9 2019-2022 U\u011fur CORUH Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"tr/license/#license","text":"MIT License Copyright \u00a9 2019-2022 U\u011fur CORUH Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"tr/changelog/","text":"Changelog \u00b6 Material for Algorithm Lovers \u00b6 1.0.0 _ October 20, 2020 \u00b6 Initial release","title":"Changelog"},{"location":"tr/changelog/#changelog","text":"","title":"Changelog"},{"location":"tr/changelog/#material-for-algorithm-lovers","text":"","title":"Material for Algorithm Lovers"},{"location":"tr/changelog/#1.0.0","text":"Initial release","title":"1.0.0 _ October 20, 2020"},{"location":"tr/resume/","text":"\u00d6zge\u00e7mi\u015f \u00b6 \u0130ndir \u00d6zge\u00e7mi\u015f-\u0130ngilizce \u00d6zge\u00e7mi\u015f-T\u00fcrk\u00e7e \u0130ngilizce T\u00fcrk\u00e7e","title":"Resume"},{"location":"tr/resume/#ozgecmis","text":"\u0130ndir \u00d6zge\u00e7mi\u015f-\u0130ngilizce \u00d6zge\u00e7mi\u015f-T\u00fcrk\u00e7e \u0130ngilizce T\u00fcrk\u00e7e","title":"\u00d6zge\u00e7mi\u015f"},{"location":"tr/syllabus/syllabus/","text":"Recep Tayyip Erdogan University \u00b6 Faculty of Engineering and Architecture \u00b6 Computer Engineering \u00b6 CE100 Algorithms and Programming-II \u00b6 Syllabus \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Download WORD , PDF Instructor Asst. Prof. Dr. U\u011fur CORUH Contact Information ugur.coruh@erdogan.edu.tr Office No F-301 Google Classroom Code bafwwt6 Lecture Hours and Days TBD Lecture Classroom \u0130BBF 402 Level-4 Office Hours Meetings will be scheduled over Google Meet with your university account and email and performed via demand emails. Please send emails with the subject starting with [CE100] tag for the fast response and write formal, clear, and short emails Lecture and Communication Language English Theory/Laboratory Course Hour Per Week 3/2 Hours Credit 4 Prerequisite CE103- Algorithms and Programming I Corequisite TBD Requirement TBD *TBD: To Be Defined. A.Course Description \u00b6 This course continues the CE103 Algorithms and Programming I course. This course taught programming skills in Algorithms and Programming I course met. This course taught programming skills in Algorithms and Programming I with common problems and their solution algorithms. This lecture is about analyzing and understanding how algorithms work for common issues. The class will be based on expertise sharing and guiding students to find learning methods and practice for algorithm and programming topics. By making programming applications and projects in the courses, the learning process will be strengthened by practicing rather than theory. B.Course Learning Outcomes \u00b6 After completing this course satisfactorily, a student will be able to: Interpret a computational problem specification and algorithmic solution and implement a C/C++, Java or C# application to solve that problem. Argue the correctness of algorithms using inductive proofs and invariants. Understand algorithm design steps Argue algorithm cost calculation for time complexity and asymptotic notation Analyze recursive algorithms complexity Understand divide-and-conquer, dynamic programming and greedy approaches. Understand graphs and graph related algorithms. Understand hashing and encryption operations input and outputs. C.Course Topics \u00b6 Algorithms Basics, Pseudocode Algorithms Analysis for Time Complexity and Asymptotic Notation Sorting Problems (Insertion and Merge Sorts) Recursive Algorithms Divide-and-Conquer Analysis (Merge Sort, Binary Search) Matrix Multiplication Problem Quicksort Analysis Heaps, Heap Sort and Priority Queues Linked Lists, Radix Sort, You should have a laptop for programming practices during this course and Counting Sort. Convex Hull Dynamic Programming Greedy Algorithms Graphs and Graphs Search Algorithms Breadth-First Search Depth-First Search and Topological Sort Graph Structure Algorithms Strongly Connected Components Minimum Spanning Tree Disjoint Set Operations Single Source Shortest Path Algorithm Q-Learning Shortest Path Implementation Network Flow and Applications Hashing and Encryption D.Textbooks and Required Hardware or Equipment \u00b6 This course does not require a coursebook. If necessary, you can use the following books and open-source online resources. Paul Deitel and Harvey Deitel. 2012. C How to Program (7 th . ed.). Prentice Hall Press, USA. Intro to Java Programming, Comprehensive Version (10 th Edition) 10 th Edition by Y. Daniel Liang Introduction to Algorithms, Third Edition By Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein Problem Solving and Program Design in C, J.R. Hanly, and E.B. Koffman, 6 th Edition. Robert Sedgewick and Kevin Wayne. 2011. Algorithms (4 th . ed.). Addison-Wesley Professional. Harvey M. Deitel and Paul J. Deitel. 2001. Java How to Program (4 th . ed.). Prentice Hall PTR, USA. Paul Deitel and Harvey Deitel. 2016. Visual C# How to Program (6 th . ed.). Pearson. Additional Books TBD During this course, you should have a laptop for programming practices. You will have your development environment, and you will use this for examination and assignments also classroom practices. E.Grading System \u00b6 Midterm and Final grades will be calculated with the weighted average of the project or homework-based examinations. Midterm grades will be calculated between term beginning to the midterm week, and Final grades will be calculated between Midterm and Final week home works or projects as follows. taught Algorithms and Programming I programming skills \\[ a_n=\\text{Homework or Project Weight} \\] \\[ HW_n=\\text{Homework or Project Points} \\] \\[ n=\\text{Number of Homework or Project} \\] \\[ Grade=(a_1HW_1+a_2HW_2+...+a_nHW_n)/n \\] Homework Weight Midterm %40 Final %60 \\[ \\text{Passing Grade}=(40*Midterm_{Grade}+60*Final_{Grade})/100 \\] F. Instructional Strategies and Methods \u00b6 The basic teaching method of this course will be planned to be face-to-face in the classroom, and support resources, home works, and announcements will be shared over google classroom. Students are expected to be in the university. This responsibility is very important to complete this course with success. If pandemic situation changes and distance education is required during this course, this course will be done using synchronous and asynchronous distance education methods. In this scenario, students are expected to be in the online platform, zoom, or meet at the time specified in the course schedule. Attendance will be taken. G. Late Homework \u00b6 Throughout the semester, assignments must be submitted as specified by the announced deadline. Your grade will be reduced by 10% of the full points for each calendar day for overdue assignments. Overdue assignments will not be accepted after three (3) days. Unexpected situations must be reported to the instructor for late home works by students. H. Course Platform and Communication \u00b6 Google Classroom will be used as a course learning management system. All electronic resources and announcements about the course will be shared on this platform. It is very important to check the course page daily, access the necessary resources and announcements, and communicate with the instructor as you need Algorithms and Programming I programming skills to complete the course with success I. Academic Integrity, Plagiarism & Cheating \u00b6 Academic Integrity is one of the most important principles of RTE\u00dc University. Anyone who breaches the principles of academic honesty is severely punished. It is natural to interact with classmates and others t.\"study together\". It may also be the case where a student asks to help from someone else, paid or unpaid, better understand a difficult topic or a whole course. However, what is the borderline between \"studying together\" or \"taking private lessons\" and \"academic dishonesty\"? When is it plagiarism, when is it cheating? It is obvious that looking at another student's paper or any source other than what is allowed during the exam is cheating and will be punished. However, it is known that many students come to university with very little experience concerning what is acceptable and what counts as \"copying,\"\" especially for assignments. The following are attempted as guidelines for the Faculty of Engineering and Architecture students to highlight the philosophy of academic honesty for assignments for which the student will be graded. Should a situation arise which is not described below, the student is advised to ask the instructor or assistant of the course whether what they intend to do would remain within the framework of academic honesty or not. a. What is acceptable when preparing an assignment? \u00b6 Communicating with classmates about the assignment to understand it better Putting ideas, quotes, paragraphs, small pieces of code (snippets) that you find online or elsewhere into your assignment, provided that these are not themselves the whole solution to the assignment, you cite the origins of these Asking sources for help in guiding you for the English language content of your assignment. Sharing small pieces of your assignment in the classroom to create a class discussion on some controversial topics. Turning to the web or elsewhere for instructions, references, and solutions to technical difficulties, but not for direct answers to the assignment Discuss solutions to assignments with others using diagrams or summarized statements but not actual text or code. Working with (and even paying) a tutor to help you with the course, provided the tutor does not do your assignment for you. b. What is not acceptable? \u00b6 Ask a classmate to see their solution to a problem before submitting your own. Failing to cite the origins of any text (or code for programming courses) that you discover outside of the course's lessons and integrate into your work You are giving or showing a classmate your solution to a problem when the classmate is struggling to solve it. J. Expectations \u00b6 You are expected to attend classes on time by completing weekly course requirements (readings and assignments) during the semester. The main communication channel between the instructor and the students email emailed. Please send your questions to the instructor's email address about the course via the email address provided to you by the university. Ensure that you include the course name in the subject field of your message and your name in the text field . In addition, the instructor will contact you via email if necessary. For this reason, it is very important to check your email address every day for healthy communication. K. Lecture Content and Syllabus Updates \u00b6 If deemed necessary, changes in the lecture content or course schedule can be made. If any changes are made in the scope of this document, the instructor will inform you about this. Course Schedule Overview \u00b6 Weeks Dates Subjects Other Tasks Week 1 TBD Course Plan and Communication Grading System, Assignments and Exams. Algorithms Basics, Pseudocode,iv. RAM (Random Access Machine Model), Algorithm Cost Calculation for Time Complexity. Worst, Average and Best Case Summary Sorting Problem (Insertion and Merge Sort Analysis), 4. Asymptotic Notation(Big O, Big Teta,Big Omega, Small o, Small omega Notations) TBD Week 2 TBD Solving Recurrences (Recursion Tree, Master Method and Back-Substitution) Divide-and-Conquer Analysis (Merge Sort, Binary Search) Recurrence Solution TBD Week 3 TBD Matrix Multiplication(Traditional,Recursive,Strassen),Quicksort(Hoare and Lomuto Partitioning,Recursive Sorting),Quicksort Analysis,Randomized Quicksort, Randomized Selection(Recursive,Medians) TBD Week 4 TBD Heaps (Max / Min Heap, Heap Data Structure, Iterative and Recursive Heapify, Extract-Max, Build Heap) Heap Sort, Priority Queues, Linked Lists, Radix Sort,Counting Sort TBD Week 5 TBD Convex Hull (Divide & Conquer) Dynamic Programming (Fibonacci Numbers) Divide-and-Conquer (DAC) vs Dynamic Programming (DP) Development of a DP Algorithms Matrix-Chain Multiplication and Analysis TBD Week-6 TBD Elements of Dynamic Programming Recursive Matrix Chain Order Memoization (Top-Down Approach, RMC, MemoizedMatrixChain, LookupC) Dynamic Programming vs. Memoization Longest Common Subsequence (LCS) Most Common Dynamic Programming Interview Questions TBD Week-7 TBD Greedy Algorithms and Dynamic Programming Differences Greedy Algorithms (Activity Selection Problem, Knapsack Problems) TBD Week-8 TBD Midterm TBD Week-9 TBD Heap Data Structure, Heap Sort, Huffman Coding TBD Week-10 TBD Introduction to Graphs, Gr,aphs and Representation, BFS (Breath-First Search), DFS (Depth-First Search), Topological Order, SCC (Strongly Connected Components), MST, Prim, Kruskal TBD Week-11 TBD Disjoint Sets and Kruskal Relationships,Single-Source Shortest Path,(Bellman- Ford,Dijkstra),Q-Learning Shortest Path,Max-Flow Min-Cut (Ford-Fulkerson,Edmond\u2019s Karp,Dinic) TBD Week-12 TBD Crypto++ Library Usage, Hashing and Integrity Control, Cryptographic Hash Functions (SHA-1,SHA-256,SHA-512,H-MAC), Checksums(MD5,CRC32) TBD Week-13 TBD Symmetric Encryption Algorithms (AES, DES, TDES), Symmetric Encryption Modes (ECB, CBC), Asymmetric Encryption, Key Pairs (Public-Private Key Pairs), Signature Generation and Validation TBD Week-14 TBD OTP Calculation(Time-based, Counter-based),File Encryption and Decryption and Integrity Control Operations TBD Week-15 TBD Review TBD Week-16 TBD Final TBD Bologna Bilgileri \u00b6 \\(CE100 Ders \u0130zlencesi Sonu\\)","title":"Syllabus"},{"location":"tr/syllabus/syllabus/#recep-tayyip-erdogan-university","text":"","title":"Recep Tayyip Erdogan University"},{"location":"tr/syllabus/syllabus/#faculty-of-engineering-and-architecture","text":"","title":"Faculty of Engineering and Architecture"},{"location":"tr/syllabus/syllabus/#computer-engineering","text":"","title":"Computer Engineering"},{"location":"tr/syllabus/syllabus/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming-II"},{"location":"tr/syllabus/syllabus/#syllabus","text":"","title":"Syllabus"},{"location":"tr/syllabus/syllabus/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX Download WORD , PDF Instructor Asst. Prof. Dr. U\u011fur CORUH Contact Information ugur.coruh@erdogan.edu.tr Office No F-301 Google Classroom Code bafwwt6 Lecture Hours and Days TBD Lecture Classroom \u0130BBF 402 Level-4 Office Hours Meetings will be scheduled over Google Meet with your university account and email and performed via demand emails. Please send emails with the subject starting with [CE100] tag for the fast response and write formal, clear, and short emails Lecture and Communication Language English Theory/Laboratory Course Hour Per Week 3/2 Hours Credit 4 Prerequisite CE103- Algorithms and Programming I Corequisite TBD Requirement TBD *TBD: To Be Defined.","title":"Spring Semester, 2021-2022"},{"location":"tr/syllabus/syllabus/#acourse-description","text":"This course continues the CE103 Algorithms and Programming I course. This course taught programming skills in Algorithms and Programming I course met. This course taught programming skills in Algorithms and Programming I with common problems and their solution algorithms. This lecture is about analyzing and understanding how algorithms work for common issues. The class will be based on expertise sharing and guiding students to find learning methods and practice for algorithm and programming topics. By making programming applications and projects in the courses, the learning process will be strengthened by practicing rather than theory.","title":"A.Course Description"},{"location":"tr/syllabus/syllabus/#bcourse-learning-outcomes","text":"After completing this course satisfactorily, a student will be able to: Interpret a computational problem specification and algorithmic solution and implement a C/C++, Java or C# application to solve that problem. Argue the correctness of algorithms using inductive proofs and invariants. Understand algorithm design steps Argue algorithm cost calculation for time complexity and asymptotic notation Analyze recursive algorithms complexity Understand divide-and-conquer, dynamic programming and greedy approaches. Understand graphs and graph related algorithms. Understand hashing and encryption operations input and outputs.","title":"B.Course Learning Outcomes"},{"location":"tr/syllabus/syllabus/#ccourse-topics","text":"Algorithms Basics, Pseudocode Algorithms Analysis for Time Complexity and Asymptotic Notation Sorting Problems (Insertion and Merge Sorts) Recursive Algorithms Divide-and-Conquer Analysis (Merge Sort, Binary Search) Matrix Multiplication Problem Quicksort Analysis Heaps, Heap Sort and Priority Queues Linked Lists, Radix Sort, You should have a laptop for programming practices during this course and Counting Sort. Convex Hull Dynamic Programming Greedy Algorithms Graphs and Graphs Search Algorithms Breadth-First Search Depth-First Search and Topological Sort Graph Structure Algorithms Strongly Connected Components Minimum Spanning Tree Disjoint Set Operations Single Source Shortest Path Algorithm Q-Learning Shortest Path Implementation Network Flow and Applications Hashing and Encryption","title":"C.Course Topics"},{"location":"tr/syllabus/syllabus/#dtextbooks-and-required-hardware-or-equipment","text":"This course does not require a coursebook. If necessary, you can use the following books and open-source online resources. Paul Deitel and Harvey Deitel. 2012. C How to Program (7 th . ed.). Prentice Hall Press, USA. Intro to Java Programming, Comprehensive Version (10 th Edition) 10 th Edition by Y. Daniel Liang Introduction to Algorithms, Third Edition By Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein Problem Solving and Program Design in C, J.R. Hanly, and E.B. Koffman, 6 th Edition. Robert Sedgewick and Kevin Wayne. 2011. Algorithms (4 th . ed.). Addison-Wesley Professional. Harvey M. Deitel and Paul J. Deitel. 2001. Java How to Program (4 th . ed.). Prentice Hall PTR, USA. Paul Deitel and Harvey Deitel. 2016. Visual C# How to Program (6 th . ed.). Pearson. Additional Books TBD During this course, you should have a laptop for programming practices. You will have your development environment, and you will use this for examination and assignments also classroom practices.","title":"D.Textbooks and Required Hardware or Equipment"},{"location":"tr/syllabus/syllabus/#egrading-system","text":"Midterm and Final grades will be calculated with the weighted average of the project or homework-based examinations. Midterm grades will be calculated between term beginning to the midterm week, and Final grades will be calculated between Midterm and Final week home works or projects as follows. taught Algorithms and Programming I programming skills \\[ a_n=\\text{Homework or Project Weight} \\] \\[ HW_n=\\text{Homework or Project Points} \\] \\[ n=\\text{Number of Homework or Project} \\] \\[ Grade=(a_1HW_1+a_2HW_2+...+a_nHW_n)/n \\] Homework Weight Midterm %40 Final %60 \\[ \\text{Passing Grade}=(40*Midterm_{Grade}+60*Final_{Grade})/100 \\]","title":"E.Grading System"},{"location":"tr/syllabus/syllabus/#f-instructional-strategies-and-methods","text":"The basic teaching method of this course will be planned to be face-to-face in the classroom, and support resources, home works, and announcements will be shared over google classroom. Students are expected to be in the university. This responsibility is very important to complete this course with success. If pandemic situation changes and distance education is required during this course, this course will be done using synchronous and asynchronous distance education methods. In this scenario, students are expected to be in the online platform, zoom, or meet at the time specified in the course schedule. Attendance will be taken.","title":"F. Instructional Strategies and Methods"},{"location":"tr/syllabus/syllabus/#g-late-homework","text":"Throughout the semester, assignments must be submitted as specified by the announced deadline. Your grade will be reduced by 10% of the full points for each calendar day for overdue assignments. Overdue assignments will not be accepted after three (3) days. Unexpected situations must be reported to the instructor for late home works by students.","title":"G. Late Homework"},{"location":"tr/syllabus/syllabus/#h-course-platform-and-communication","text":"Google Classroom will be used as a course learning management system. All electronic resources and announcements about the course will be shared on this platform. It is very important to check the course page daily, access the necessary resources and announcements, and communicate with the instructor as you need Algorithms and Programming I programming skills to complete the course with success","title":"H. Course Platform and Communication"},{"location":"tr/syllabus/syllabus/#i-academic-integrity-plagiarism-cheating","text":"Academic Integrity is one of the most important principles of RTE\u00dc University. Anyone who breaches the principles of academic honesty is severely punished. It is natural to interact with classmates and others t.\"study together\". It may also be the case where a student asks to help from someone else, paid or unpaid, better understand a difficult topic or a whole course. However, what is the borderline between \"studying together\" or \"taking private lessons\" and \"academic dishonesty\"? When is it plagiarism, when is it cheating? It is obvious that looking at another student's paper or any source other than what is allowed during the exam is cheating and will be punished. However, it is known that many students come to university with very little experience concerning what is acceptable and what counts as \"copying,\"\" especially for assignments. The following are attempted as guidelines for the Faculty of Engineering and Architecture students to highlight the philosophy of academic honesty for assignments for which the student will be graded. Should a situation arise which is not described below, the student is advised to ask the instructor or assistant of the course whether what they intend to do would remain within the framework of academic honesty or not.","title":"I. Academic Integrity, Plagiarism &amp; Cheating"},{"location":"tr/syllabus/syllabus/#a-what-is-acceptable-when-preparing-an-assignment","text":"Communicating with classmates about the assignment to understand it better Putting ideas, quotes, paragraphs, small pieces of code (snippets) that you find online or elsewhere into your assignment, provided that these are not themselves the whole solution to the assignment, you cite the origins of these Asking sources for help in guiding you for the English language content of your assignment. Sharing small pieces of your assignment in the classroom to create a class discussion on some controversial topics. Turning to the web or elsewhere for instructions, references, and solutions to technical difficulties, but not for direct answers to the assignment Discuss solutions to assignments with others using diagrams or summarized statements but not actual text or code. Working with (and even paying) a tutor to help you with the course, provided the tutor does not do your assignment for you.","title":"a. What is acceptable when preparing an assignment?"},{"location":"tr/syllabus/syllabus/#b-what-is-not-acceptable","text":"Ask a classmate to see their solution to a problem before submitting your own. Failing to cite the origins of any text (or code for programming courses) that you discover outside of the course's lessons and integrate into your work You are giving or showing a classmate your solution to a problem when the classmate is struggling to solve it.","title":"b. What is not acceptable?"},{"location":"tr/syllabus/syllabus/#j-expectations","text":"You are expected to attend classes on time by completing weekly course requirements (readings and assignments) during the semester. The main communication channel between the instructor and the students email emailed. Please send your questions to the instructor's email address about the course via the email address provided to you by the university. Ensure that you include the course name in the subject field of your message and your name in the text field . In addition, the instructor will contact you via email if necessary. For this reason, it is very important to check your email address every day for healthy communication.","title":"J. Expectations"},{"location":"tr/syllabus/syllabus/#k-lecture-content-and-syllabus-updates","text":"If deemed necessary, changes in the lecture content or course schedule can be made. If any changes are made in the scope of this document, the instructor will inform you about this.","title":"K. Lecture Content and Syllabus Updates"},{"location":"tr/syllabus/syllabus/#course-schedule-overview","text":"Weeks Dates Subjects Other Tasks Week 1 TBD Course Plan and Communication Grading System, Assignments and Exams. Algorithms Basics, Pseudocode,iv. RAM (Random Access Machine Model), Algorithm Cost Calculation for Time Complexity. Worst, Average and Best Case Summary Sorting Problem (Insertion and Merge Sort Analysis), 4. Asymptotic Notation(Big O, Big Teta,Big Omega, Small o, Small omega Notations) TBD Week 2 TBD Solving Recurrences (Recursion Tree, Master Method and Back-Substitution) Divide-and-Conquer Analysis (Merge Sort, Binary Search) Recurrence Solution TBD Week 3 TBD Matrix Multiplication(Traditional,Recursive,Strassen),Quicksort(Hoare and Lomuto Partitioning,Recursive Sorting),Quicksort Analysis,Randomized Quicksort, Randomized Selection(Recursive,Medians) TBD Week 4 TBD Heaps (Max / Min Heap, Heap Data Structure, Iterative and Recursive Heapify, Extract-Max, Build Heap) Heap Sort, Priority Queues, Linked Lists, Radix Sort,Counting Sort TBD Week 5 TBD Convex Hull (Divide & Conquer) Dynamic Programming (Fibonacci Numbers) Divide-and-Conquer (DAC) vs Dynamic Programming (DP) Development of a DP Algorithms Matrix-Chain Multiplication and Analysis TBD Week-6 TBD Elements of Dynamic Programming Recursive Matrix Chain Order Memoization (Top-Down Approach, RMC, MemoizedMatrixChain, LookupC) Dynamic Programming vs. Memoization Longest Common Subsequence (LCS) Most Common Dynamic Programming Interview Questions TBD Week-7 TBD Greedy Algorithms and Dynamic Programming Differences Greedy Algorithms (Activity Selection Problem, Knapsack Problems) TBD Week-8 TBD Midterm TBD Week-9 TBD Heap Data Structure, Heap Sort, Huffman Coding TBD Week-10 TBD Introduction to Graphs, Gr,aphs and Representation, BFS (Breath-First Search), DFS (Depth-First Search), Topological Order, SCC (Strongly Connected Components), MST, Prim, Kruskal TBD Week-11 TBD Disjoint Sets and Kruskal Relationships,Single-Source Shortest Path,(Bellman- Ford,Dijkstra),Q-Learning Shortest Path,Max-Flow Min-Cut (Ford-Fulkerson,Edmond\u2019s Karp,Dinic) TBD Week-12 TBD Crypto++ Library Usage, Hashing and Integrity Control, Cryptographic Hash Functions (SHA-1,SHA-256,SHA-512,H-MAC), Checksums(MD5,CRC32) TBD Week-13 TBD Symmetric Encryption Algorithms (AES, DES, TDES), Symmetric Encryption Modes (ECB, CBC), Asymmetric Encryption, Key Pairs (Public-Private Key Pairs), Signature Generation and Validation TBD Week-14 TBD OTP Calculation(Time-based, Counter-based),File Encryption and Decryption and Integrity Control Operations TBD Week-15 TBD Review TBD Week-16 TBD Final TBD","title":"Course Schedule Overview"},{"location":"tr/syllabus/syllabus/#bologna-bilgileri","text":"\\(CE100 Ders \u0130zlencesi Sonu\\)","title":"Bologna Bilgileri"},{"location":"tr/week-1/ce100-week-1-intro/","text":"CE100 Algorithms and Programming II \u00b6 Week-1 (Introduction to Analysis of Algorithms) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Brief Description of Course and Rules \u00b6 We will first talk about, Course Plan and Communication Grading System, Homeworks, and Exams please read the syllabus carefully. Outline (1) \u00b6 Introduction to Analysis of Algorithms Algorithm Basics Flowgorithm Pseudocode Outline (2) \u00b6 RAM (Random Access Machine Model) Sorting Problem Insertion Sort Analysis Algorithm Cost Calculation for Time Complexity Worst, Average, and Best Case Summary Merge Sort Analysis Outline (3) \u00b6 Asymptotic Notation Big O Notation Big Teta Notation Big Omega Notation Small o Notation Small omega Notation We Need Mathematical Proofs (1) \u00b6 Direct proof Proof by mathematical induction Proof by contraposition Proof by contradiction Proof by construction Proof by exhaustion We Need Mathematical Proofs (2) \u00b6 Probabilistic proof Combinatorial proof Nonconstructive proof Statistical proofs in pure mathematics Computer-assisted proofs Mathematical proof - Wikipedia Introduction to Analysis of Algorithms \u00b6 Study two sorting algorithms as examples Insertion sort: Incremental algorithm Merge sort: Divide-and-conquer Introduction to runtime analysis Best vs. worst vs. average case Asymptotic analysis What is Algorithm \u00b6 Algorithm : A sequence of computational steps that transform the input to the desired output Procedure vs. algorithm An algorithm must halt within finite time with the right output We Need to Measure Performance Metrics Processing Time Allocated Memory Network Congestion Power Usage etc. Example Sorting Algorithms Input : a sequence of n numbers \\[ \\langle a_1,a_2,...,a_n \\rangle \\] Algorithm : Sorting / Permutation \\[ \\prod = \\langle \\prod_{(1)},\\prod_{(2)},...,\\prod_{(n)} \\rangle \\] Output : sorted permutation of the input sequence \\[ \\langle a_{\\prod_{(1)}} \\leqslant a_{\\prod_{(2)}} \\leqslant,...,a_{\\prod_{(n)}} \\rangle \\] Pseudo-code notation (1) \u00b6 Objective: Express algorithms to humans in a clear and concise way Liberal use of English Indentation for block structures Omission of error handling and other details (needed in real programs) You can use Flowgorithm application to understand concept easily. Pseudo-code notation (2) \u00b6 Links and Examples \u00b6 Wikipedia CS50 University of North Florida GeeksforGeeks Correctness (1) \u00b6 We often use a loop invariant to help us to understand why an algorithm gives the correct answer. Example: (Insertion Sort) at the start of each iteration of the \"outer\" for loop - the loop indexed by \\(j\\) - the subarray \\(A[1 \\dots j-1]\\) consist of the elements originally in \\(A[1\\dots j-1]\\) but in sorted order. Correctness (2) \u00b6 To use a loop invariant to prove correctness, we must show 3 things about it. Initialization: It is true to the first iteration of the loop. Maintaince: If it is true before an iteration of the loop, it remains true before the next iteration. Termination: When the loop terminates, the invariant - usually along with the reason that the loop terminated - gives us a usefull property that helps show that the algorithm is correct. RAM (Random Access Machine Model) \\(\\Longrightarrow \\Theta(1)\\) (1) \u00b6 Operations Single Step Sequential No Concurrent Arithmetic add, subtract, multiply, divide, remainder, floor, ceiling, shift left/shift right (good by multiply/dividing \\(2^k\\) ) RAM (Random Access Machine Model) \\(\\Longrightarrow \\Theta(1)\\) (2) \u00b6 Data Movement load, store, copy Control conditional / unconditional branch subroutine calls returns RAM (Random Access Machine Model) \\(\\Longrightarrow \\Theta(1)\\) (3) \u00b6 Each instruction take a constant amount of time Integer will be represented by \\(clogn\\) \\(c \\geq 1\\) \\(T(n)\\) the running time of the algorithm: \\[ \\sum \\limits_{\\text{all statement}}^{}(\\text{cost of statement})*(\\text{number of times statement is executed}) = T(n) \\] What is the processing time ? \u00b6 section{ font-size: 25px; } Insertion Sort Algorithm (1) \u00b6 Insertion sort is a simple sorting algorithm that works similar to the way you sort playing cards in your hands The array is virtually split into a sorted and an unsorted part Values from the unsorted part are picked and placed at the correct position in the sorted part. Assume input array : \\(A[1..n]\\) Iterate \\(j\\) from \\(2\\) to \\(n\\) Insertion Sort Algorithm (2) \u00b6 Insertion Sort Algorithm (Pseudo-Code) (3) \u00b6 Insertion - Sort ( A ) 1 . for j = 2 to A.length 2 . key = A [ j ] 3 . // insert A [ j ] into the sorted sequence A [ 1 ...j -1 ] 4 . i = j - 1 5 . while i > 0 and A [ i ] > key 6 . A [ i +1 ] = A [ i ] 7 . i = i - 1 8 . A [ i +1 ] = key Insertion Sort Step-By-Step Description (1) \u00b6 Insertion Sort Step-By-Step Description (2) \u00b6 Insertion Sort Step-By-Step Description (3) \u00b6 Insertion Sort Example \u00b6 Insertion Sort Step-1 (initial) \u00b6 Insertion Sort Step-2 (j=2) \u00b6 Insertion Sort Step-3 (j=3) \u00b6 Insertion Sort Step-4 (j=3) \u00b6 Insertion Sort Step-5 (j=4) \u00b6 Insertion Sort Step-6 (j=5) \u00b6 Insertion Sort Step-7 (j=5) \u00b6 Insertion Sort Step-8 (j=6) \u00b6 Insertion Sort Review (1) \u00b6 Items sorted in-place Elements are rearranged within the array. At a most constant number of items stored outside the array at any time (e.,g. the variable key) Input array \\(A\\) contains a sorted output sequence when the algorithm ends Insertion Sort Review (2) \u00b6 Incremental approach Having sorted \\(A[1..j-1]\\) , place \\(A[j]\\) correctly so that \\(A[1..j]\\) is sorted Running Time It depends on Input Size (5 elements or 5 billion elements) and Input Itself (partially sorted) Algorithm approach to upper bound of overall performance analysis Visualization of Insertion Sort \u00b6 Sorting (Bubble, Selection, Insertion, Merge, Quick, Counting, Radix) - VisuAlgo https://www.cs.usfca.edu/~galles/visualization/ComparisonSort.html https://algorithm-visualizer.org/ HMvHTs - Online C++ Compiler & Debugging Tool - Ideone.com Kinds of Running Time Analysis (Time Complexity) \u00b6 Worst Case (Big-O Notation) \\(T(n)\\) = maximum processing time of any input \\(n\\) Presentation of Big-O : \\(O(n)\\) Average Case (Teta Notation) \\(T(n)\\) = average time over all inputs of size \\(n\\) , inputs can have a uniform distribution Presentation of Big-Theta : \\(\\Theta(n)\\) Best Case (Omega Notation) \\(T(n)\\) = min time on any input of size \\(n\\) , for example sorted array Presentation of Big-Omega : \\(\\Omega(n)\\) Array Sorting Algorithms Time and Space Complexity \u00b6 Comparison of Time Analysis Cases \u00b6 For insertion sort, worst-case time depends on the speed of primitive operations such as Relative Speed (on the same machine) Absolute Speed (on different machines) Asymptotic Analysis Ignore machine-dependent constants Look at the growth of \\(T(n) | n\\rightarrow\\infty\\) Asymptotic Analysis (1) \u00b6 Asymptotic Analysis (2) \u00b6 Theta-Notation (Average-Case) \u00b6 Drop low order terms Ignore leading constants e.g \\[ \\begin{align*} 2n^2+5n+3 &= \\Theta(n^2) \\\\ 3n^3+90n^2-2n+5 &= \\Theta(n^3) \\end{align*} \\] As \\(n\\) gets large, a \\(\\Theta(n^2)\\) algorithm runs faster than a \\(\\Theta(n^3)\\) algorithm Asymptotic Analysis (3) \u00b6 section{ font-size: 25px; } For both algorithms, we can see a minimum item size in the following chart. After this point, we can see performance differences. Some algorithms for small item size can be run faster than others but if you increase item size you will see a reference point that notation proof performance metrics. section{ font-size: 25px; } Insertion Sort - Runtime Analysis (1) \u00b6 Cost Times Insertion - Sort ( A ) ---- ----- --------------------- c1 n 1 . for j = 2 to A.length c2 n -1 2 . key = A [ j ] c3 n -1 3 . // insert A [ j ] into the sorted sequence A [ 1 ...j -1 ] c4 n -1 4 . i = j - 1 c5 k5 5 . while i > 0 and A [ i ] > key do c6 k6 6 . A [ i +1 ] = A [ i ] c7 k6 7 . i = i - 1 c8 n -1 8 . A [ i +1 ] = key we have two loops here, if we sum up costs as follow we can see big-O worst case notation. \\(k_5 = \\sum \\limits_{j=2}^n{t_j}\\) and \\(k_6 = \\sum \\limits_{j=2}^n{t_i-1}\\) for operation counts Insertion Sort - Runtime Analysis (2) \u00b6 cost function can be evaluated as follow; \\[ \\begin{align*} T(n) &=c_1n+c_2(n-1)+ 0(n-1)+c_4(n-1) \\\\ & +c_5\\sum \\limits_{j=2}^n{t_j}+c_6\\sum \\limits_{j=2}^n{t_i-1} \\\\ & +c_7\\sum \\limits_{j=2}^n{t_i-1}+c_8(n-1) \\end{align*} \\] Insertion Sort - Runtime Analysis (3) \u00b6 \\[ \\begin{align*} \\sum \\limits_{j=2}^n j &= (n(n+1)/2)- 1 \\\\ & \\text{ and } \\\\ \\sum \\limits_{j=2}^n {j-1} &= n(n-1)/2 \\end{align*} \\] Insertion Sort - Runtime Analysis (4) \u00b6 \\[ \\begin{align*} T(n) & =(c_5/2 + c_6/2 + c_7/2)n^2 \\\\ & + (c_1+c_2+c_4+c_5/2-c_6/2-c_7/2+c_8)n \\\\ & -(c_2 + c_4 + c_5 + c_6) \\end{align*} \\] Insertion Sort - Runtime Analysis (5) \u00b6 \\[ \\begin{align*} T(n) &= an^2 + bn + c \\\\ &=O(n^2) \\end{align*} \\] section{ font-size: 25px; } Best-Case Scenario (Sorted Array) (1) \u00b6 Problem-1, If \\(A[1...j]\\) is already sorted, what will be \\(t_j=?\\) \\(t_j=1\\) section{ font-size: 25px; } Best-Case Scenario (Sorted Array) (2) \u00b6 Parameters are taken from image \\[ \\begin{align*} T(n) &=c_1n+c_2(n-1)+c_3(n-1) \\\\ & +c_4\\sum \\limits_{j=2}^nt_j+c_5\\sum \\limits_{j=2}^n(t_j-1) \\\\ & +c_6\\sum \\limits_{j=2}^n(t_j-1)+c_7(n-1) \\end{align*} \\] \\(t_j=1\\) for all \\(j\\) \\[ \\begin{align*} T(n) &= (c_1+c_2+c_3+c_4+c_7)n \\\\ &-(c_2+c_3+c_4+c_7) \\\\ T(n) &=an-b \\\\ &=\\Omega(n) \\end{align*} \\] section{ font-size: 25px; } Worst-Case Scenario (Reversed Array) (1) \u00b6 Problem-2 If \\(A[j]\\) is smaller than every entry in \\(A[1...j-1]\\) , what will be \\(t_j=?\\) \\(t_j=?\\) Worst-Case Scenario (Reversed Array) (2) \u00b6 The input array is reverse sorted \\(t_j=j\\) for all \\(j\\) after calculation worst case runtime will be \\[ \\begin{align*} T(n) &=1/2(c_4+c_5+c_6)n^2 \\\\ & +(c_1+c_2+c_3+1/2(c_4-c_5-c_6)+c_7)n -(c_2+c_3+c_4+c_7) \\\\ T(n) &=1/2an^2+bn-c \\\\ &= O(n^2) \\end{align*} \\] Asymptotic Runtime Analysis of Insertion-Sort \u00b6 Insertion-Sort Worst-case (input reverse sorted) \u00b6 Inner Loop is \\(\\Theta(j)\\) \\[ \\begin{align*} T(n) &=\\sum \\limits_{j=2}^n\\Theta(j) \\\\ &=\\Theta(\\sum \\limits_{j=2}^nj) \\\\ &=\\Theta(n^2) \\end{align*} \\] Insertion-Sort Average-case (all permutations uniformly distributed) \u00b6 Inner Loop is \\(\\Theta(j/2)\\) \\[ \\begin{align*} T(n) &=\\sum \\limits_{j=2}^n\\Theta(j/2) \\\\ &=\\sum \\limits_{j=2}^n\\Theta(j) \\\\ &=\\Theta(n^2) \\end{align*} \\] Array Sorting Algorithms Time/Space Complexities \u00b6 To compare this sorting algorithm please check the following map again. Merge Sort : Divide / Conquer / Combine (1) \u00b6 Merge Sort : Divide / Conquer / Combine (2) \u00b6 Divide : we divide the problem into a number of subproblems Conquer : We solve the subproblems recursively Base-Case : Solve by Brute-Force Combine : Subproblem solutions to the original problem Merge Sort Example \u00b6 Merge Sort Algorithm (initial setup) \u00b6 Merge Sort is a recursive sorting algorithm, for initial case we need to call Merge-Sort(A,1,n) for sorting \\(A[1..n]\\) initial case A : Array p : 1 ( offset ) r : n ( length ) Merge - Sort ( A , 1 , n ) Merge Sort Algorithm (internal iterations) \u00b6 internal iterations A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif section{ font-size: 25px; } Merge Sort Algorithm (Combine-1) \u00b6 \\(p = start-point\\) \\(q = mid-point\\) \\(r = end-point\\) Merge Sort Algorithm (Combine-2) \u00b6 brute-force task, merging two sorted subarrays The pseudo-code in the textbook (Sec. 2.3.1) Merge Sort Combine Algorithm (1) \u00b6 Merge ( A , p , q , r ) n1 = q - p +1 n2 = r - q // allocate left and right arrays // increment will be from left to right // left part will be bigger than right part L [ 1 ...n1 +1 ] // left array R [ 1 ...n2 +1 ] // right array // copy left part of array for i = 1 to n1 L [ i ] = A [ p + i -1 ] // copy right part of array for j = 1 to n2 R [ j ] = A [ q + j ] // put end items maximum values for termination L [ n1 +1 ] = inf R [ n2 +1 ] = inf i = 1 , j = 1 for k = p to r if L [ i ] <= R [ j ] A [ k ] = L [ i ] i = i +1 else A [ k ] = R [ j ] j = j +1 What is the complexity of merge operation? \u00b6 You can find by counting loops will provide you base constant nested level will provide you exponent of this constant, if you drop constants you will have complexity we have 3 for loops it will look like \\(3n\\) and \\(\\Theta(n)\\) will be merge complexity Merge Sort Correctness \u00b6 Base case \\(p = r\\) (Trivially correct) Inductive hypothesis MERGE-SORT is correct for any subarray that is a strict (smaller) subset of \\(A[p, q]\\) . General Case MERGE-SORT is correct for \\(A[p, q]\\) . From inductive hypothesis and correctness of Merge. Merge Sort Algorithm (Pseudo-Code) \u00b6 A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif Merge Sort Algorithm Complexity \u00b6 A : Array p : offset r : length Merge - Sort ( A , p , r ) -------------> T ( n ) if p = r then ---------------> Theta ( 1 ) return else q = floor (( p + r ) / 2 ) ----> Theta ( 1 ) Merge - Sort ( A , p , q ) -----> T ( n / 2 ) Merge - Sort ( A , q +1 , r ) ---> T ( n / 2 ) Merge ( A , p , q , r ) --------> Theta ( n ) endif Merge Sort Algorithm Recurrence \u00b6 We can describe a function recursively in terms of itself, to analyze the performance of recursive algorithms \\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\] How To Solve Recurrence (1) \u00b6 \\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\] How To Solve Recurrence (2) \u00b6 We will assume \\(T(n)= \\Theta(1)\\) for sufficiently small \\(n\\) to rewrite equation as \\[ T(n)=2T(n/2)+\\Theta(n) \\] Solution for this equation will be \\(\\Theta(nlgn)\\) with following recursion tree. How To Solve Recurrence (3) \u00b6 Multiply by height \\(\\Theta(lgn)\\) with each level cost \\(\\Theta(n)\\) we can found \\(\\Theta(nlgn)\\) How To Solve Recurrence (4) \u00b6 This tree is binary-tree and binary-tree height is related with item size. How Height of a Binary Tree is Equal to \\(logn\\) ? (1) \u00b6 Merge-Sort recursion tree is a perfect binary tree, a binary tree is a tree which every node has at most two children, A perfect binary tree is binary tree in which all internal nodes have exactly two children and all leaves are at the same level. How Height of a Binary Tree is Equal to \\(logn\\) ? (2) \u00b6 Let \\(n\\) be the number of nodes in the tree and let \\(l_k\\) denote the number of nodes on level k. According to this; \\(l_k = 2l_{k-1}\\) i.e. each level has exactly twice as many nodes as the previous level \\(l_0 = 1\\) , i.e. on the first level we have only one node (the root node) The leaves are at the last level, \\(l_h\\) where \\(h\\) is the height of the tree. How Height of a Binary Tree is Equal to \\(logn\\) ? (3) \u00b6 The total number of nodes in the tree is equal to the sum of the nodes on all the levels: nodes \\(n\\) \\[ \\begin{align*} 1+2^1+2^2+2^3+...+2^h &= n \\\\ 1+2^1+2^2+2^3+...+2^h &= 2^{h+1}-1 \\\\ 2^{h+1}-1 &= n\\\\ 2^{h+1} &= n+1\\\\ log_2{2^{h+1}} &= log_2{(n+1)} \\\\ h+1 &= log_2{(n+1)} \\\\ h &= log_2{(n+1)}-1 \\end{align*} \\] How Height of a Binary Tree is Equal to \\(logn\\) ? (3) \u00b6 If we write it as asymptotic approach, we will have the following result \\[ \\text{height of tree is }h = log_2{(n+1)}-1 = O(logn) \\] also \\[ \\text{number of leaves is } l_h = (n+1)/2 \\] nearly half of the nodes are at the leaves Review \u00b6 \\(\\Theta(nlgn)\\) grows more slowly than \\(\\Theta(n^2)\\) Therefore Merge-Sort beats Insertion-Sort in the worst case In practice Merge-Sort beats Insertion-Sort for \\(n>30\\) or so Asymptotic Notations \u00b6 Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (1) \u00b6 \\(f(n)=O(g(n))\\) if \\(\\exists\\) positive constants \\(c\\) , \\(n_0\\) such that \\[ 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\] Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (2) \u00b6 Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (3) \u00b6 Asymptotic running times of algorithms are usually defined by functions whose domain are \\(N={0, 1, 2, \u2026}\\) (natural numbers) Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (4) \u00b6 Example-1 \u00b6 Show that \\(2n^2 = O(n^3)\\) we need to find two positive constant \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq 2n^2 \\leq cn^3 \\text{ for all } n \\geq n_0 \\] Choose \\(c=2\\) and \\(n_0 = 1\\) \\[ 2n^2 \\leq 2n^3 \\text{ for all } n \\geq 1 \\] Or, choose \\(c=1\\) and \\(n_0=2\\) \\[ 2n^2 \\leq n^3 \\text{ for all } n \\geq 2 \\] Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (5) \u00b6 Example-2 \u00b6 Show that \\(2n^2 + n = O(n^2)\\) We need to find two positive constant \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq {2n^2+n} \\leq cn^2 \\text{ for all } n \\geq n_0 \\] \\[ 2 + (1/n) \\leq c \\text{ for all } n \\geq n_0 \\] Choose \\(c=3\\) and \\(n_0=1\\) \\[ 2n^2 + n \\leq 3n^2 \\text{ for all } n \\geq 1 \\] Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (6) \u00b6 We can say the followings about \\(f(n)=O(g(n))\\) equation The notation is a little sloppy One-way equation, e.q. \\(n^2 = O(n^3)\\) but we cannot say \\(O(n^3)=n^2\\) Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (7) \u00b6 \\(O(g(n))\\) is in fact a set of functions as follow \\(O(g(n)) = \\{ f(n) : \\exists \\text{ positive constant } c, n_0 \\text{ such that } 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\}\\) Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (8) \u00b6 In other words \\(O(g(n))\\) is in fact, the set of functions that have asymptotic upper bound \\(g(n)\\) e.q \\(2n^2 = O(n^3)\\) means \\(2n^2 \\in O(n^3)\\) Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (9) \u00b6 Example-1 \u00b6 \\(10^9n^2 = O(n^2)\\) \\(0 \\leq 10^9n^2 \\leq cn^2 \\text{ for } n \\geq n_0\\) choose \\(c=10^9\\) and \\(n_0=1\\) \\(0 \\leq 10^9n^2 \\leq 10^9n^2 \\text{ for } n \\geq 1\\) CORRECT Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (10) \u00b6 Example-2 \u00b6 \\(100n^{1.9999}=O(n^2)\\) \\(0 \\leq 100n^{1.9999} \\leq cn^2 \\text{ for } n \\geq n_0\\) choose \\(c=100\\) and \\(n_0=1\\) \\(0 \\leq 100n^{1.9999} \\leq 100n^2 \\text{ for } n \\geq 1\\) CORRECT Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (11) \u00b6 Example-3 \u00b6 \\(10^{-9}n^{2.0001} = O(n^2)\\) \\(0 \\leq 10^{-9}n^{2.0001} \\leq cn^2 \\text{ for } n \\geq n_0\\) \\(10^{-9}n^{0.0001} \\leq c \\text{ for } n \\geq n_0\\) INCORRECT (Contradiction) Big-O / \\(O\\) - Notation : Asymptotic Upper Bound (Worst-Case) (12) \u00b6 If we analysis \\(O(n^2)\\) case, \\(O\\) -notation is an upper bound notation and the runtime \\(T(n)\\) of algorithm A is at least \\(O(n^2 )\\) . \\(O(n^2)\\) : The set of functions with asymptotic upper bound \\(n^2\\) \\(T(n) \\geq O(n^2)\\) means \\(T(n) \\geq h(n)\\) for some \\(h(n) \\in O(n^2)\\) \\(h(n)=0\\) function is also in \\(O(n^2)\\) . Hence : \\(T(n) \\geq 0\\) , runtime must be nonnegative. Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (1) \u00b6 \\(f(n)=\\Omega(g(n))\\) if \\(\\exists\\) positive constants \\(c,n_0\\) such that \\(0 \\leq cg(n) \\leq f(n) , \\forall n \\geq n_0\\) Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (2) \u00b6 Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (3) \u00b6 Example-1 \u00b6 Show that \\(2n^3 = \\Omega(n^2)\\) We need to find two positive constants \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq cn^2 \\leq 2n^3 \\text{ for all } n \\geq n_0 \\] Choose \\(c=1\\) and \\(n_0=1\\) \\[ n^2 \\leq 2n^3 \\text{ for all } n \\geq 1 \\] Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (4) \u00b6 Example-4 \u00b6 Show that \\(\\sqrt{n}=\\Omega(lgn)\\) We need to find two positive constants \\(c\\) and \\(n_0\\) such that: \\[ clgn \\leq \\sqrt{n} \\text{ for all } n \\geq n_0 \\] Choose \\(c=1\\) and \\(n_0=16\\) \\[ lgn \\leq \\sqrt{n} \\text{ for all } n \\geq 16 \\] Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (5) \u00b6 \\(\\Omega(g(n))\\) is the set of functions that have asymptotic lower bound \\(g(n)\\) \\[ \\begin{align*} \\Omega(g(n)) &=\\{ f(n):\\exists \\text{ positive constants } c,n_0 \\text{ such that } \\\\ & 0 \\leq cg(n) \\leq f(n), \\forall n \\geq n_0 \\} \\end{align*} \\] Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (6) \u00b6 Example-1 \u00b6 \\(10^9n^2 = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 10^9n^2 \\text{ for } n\\geq n_0\\) Choose \\(c=10^9\\) and \\(n_0=1\\) \\(0 \\leq 10^9n^2 \\leq 10^9n^2 \\text{ for } n\\geq 1\\) CORRECT Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (7) \u00b6 Example-2 \u00b6 \\(100n^{1.9999} = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 100n^{1.9999} \\text{ for } n \\geq n_0\\) \\(n^{0.0001} \\leq (100/c) \\text{ for } n \\geq n_0\\) INCORRECT (Contradiction) Big-Omega / \\(\\Omega\\) -Notation : Asymptotic Lower Bound (Best-Case) (8) \u00b6 Example-3 \u00b6 \\(10^{-9}n^{2.0001} = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 10^{-9}n^{2.0001} \\text{ for } n \\geq n_0\\) Choose \\(c=10^{-9}\\) and \\(n_0=1\\) \\(0 \\leq 10^{-9}n^2 \\leq 10^{-9}n^{2.0001} \\text{ for } n \\geq 1\\) CORRECT Comparison of Notations (1) \u00b6 Comparison of Notations (2) \u00b6 Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (1) \u00b6 \\[ \\begin{align*} f(n) &=\\Theta(g(n)) \\ if \\ \\exists \\ \\text{positive constants} \\ c_1,c_2,n_0 \\text{such that} \\\\ & 0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n), \\forall n \\geq n_0 \\end{align*} \\] Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (2) \u00b6 Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (3) \u00b6 Example-1 \u00b6 Show that \\(2n^2 + n = \\Theta(n^2)\\) We need to find 3 positive constants \\(c_1,c_2\\) and \\(n_0\\) such that: \\(0 \\leq c_1n^2 \\leq 2n^2+n \\leq c_2n^2\\) for all \\(n \\geq n_0\\) \\(c_1 \\leq 2 + (1/n) \\leq c_2\\) for all \\(n \\geq n_0\\) Choose \\(c_1=2, c_2=3\\) and \\(n_0=1\\) \\(2n^2 \\leq 2n^2+n \\leq 3n^2\\) for all \\(n \\geq 1\\) Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (4) \u00b6 Example-2.1 \u00b6 Show that \\(1/2n^2-2n=\\Theta(n^2)\\) We need to find 3 positive constants \\(c_1,c_2\\) and \\(n_0\\) such that: \\(0 \\leq c_1n^2 \\leq 1/2n^2-2n \\leq c_2n^2 \\text{ for all } n \\geq n_0\\) \\(c_1 \\leq 1/2 - 2 / n \\leq c_2 \\text{ for all } n \\geq n_0\\) Choose 3 positive constants \\(c_1,c_2, n_0\\) that satisfy \\(c_1 \\leq 1/2 - 2/n \\leq c_2\\) for all \\(n \\geq n_0\\) Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (5) \u00b6 Example-2.2 \u00b6 Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (6) \u00b6 Example-2.3 \u00b6 \\[ 1/10 \\leq 1/2 - 2/n \\text{ for } n \\geq 5 \\] \\[ 1/2 - 2/n \\leq 1/2 \\text{ for } n \\geq 0 \\] Therefore we can choose \\(c_1 = 1/10, c_2=1/2, n_0=5\\) Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (7) \u00b6 Theorem : leading constants & low-order terms don\u2019t matter Justification : can choose the leading constant large enough to make high-order term dominate other terms Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (8) \u00b6 Example-1 \u00b6 \\(10^9n^2 = \\Theta(n^2)\\) CORRECT \\(100n^{1.9999} = \\Theta(n^2)\\) INCORRECT \\(10^9n^{2.0001} = \\Theta(n^2)\\) INCORRECT Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (9) \u00b6 \\(\\Theta(g(n))\\) is the set of functions that have asymptotically tight bound \\(g(n)\\) \\(\\Theta(g(n))=\\{ f(n):\\) \\(\\exists\\) positive constants \\(c_1,c_2, n_0\\) such that \\(0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n), \\forall n \\geq n_0 \\}\\) Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (10) \u00b6 Theorem : \\(f(n)=\\Theta(g(n))\\) if and only if \\(f(n)=O(g(n))\\) and \\(f(n)=\\Omega(g(n))\\) \\(\\Theta\\) is stronger than both \\(O\\) and \\(\\Omega\\) \\(\\Theta(g(n)) \\subseteq O(g(n)) \\text{ and } \\Theta(g(n)) \\subseteq \\Omega(g(n))\\) Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (11) \u00b6 Example-1.1 \u00b6 Prove that \\(10^{-8}n^2 \\neq \\Theta(n)\\) We can check that \\(10^{-8}n^2 = \\Omega(n)\\) and \\(10^{-8}n^2 \\neq O(n)\\) Proof by contradiction for \\(O(n)\\) notation \\[ \\begin{align*} O(g(n)) &= \\{ f(n) : \\exists \\text{ positive constant } c, n_0 \\text{ such that } \\\\ & 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\} \\end{align*} \\] Big-Theta / \\(\\Theta\\) -Notation : Asymptotically tight bound (Average Case) (12) \u00b6 Example-1.2 \u00b6 Suppose positive constants \\(c_2\\) and \\(n_0\\) exist such that: \\(10^{-8}n^2 \\leq c_2n, \\forall n \\geq n_0\\) \\(10^{-8}n \\leq c_2, \\forall n \\geq n_0\\) Contradiction : \\(c_2\\) is a constant Summary of \\(O,\\Omega\\) and \\(\\Theta\\) notations (1) \u00b6 \\(O(g(n))\\) : The set of functions with asymptotic upper bound \\(g(n)\\) \\(\\Omega(g(n))\\) : The set of functions with asymptotic lower bound \\(g(n)\\) \\(\\Theta(n)\\) : The set of functions with asymptotically tight bound \\(g(n)\\) \\(f(n)=\\Theta(g(n)) \\Leftrightarrow f(n)=O(g(n)) \\text{ and } f(n)=\\Omega(g(n))\\) Summary of \\(O,\\Omega\\) and \\(\\Theta\\) notations (2) \u00b6 Small-o / \\(o\\) -Notation : Asymptotic upper bound that is not tight (1) \u00b6 Remember, upper bound provided by big- \\(O\\) notation can be tight or not tight Tight mean values are close the original function e.g. followings are true \\(2n^2 = O(n^2)\\) is asymptotically tight \\(2n = O(n^2)\\) is not asymptotically tight According to this small- \\(o\\) notation is an upper bound that is not asymptotically tight Small-o / \\(o\\) -Notation : Asymptotic upper bound that is not tight (2) \u00b6 Note that in equations equality is removed in small notations \\[ \\begin{align*} o(g(n)) &=\\{ f(n): \\text{ for any constant} c > 0, \\exists \\text{ a constant } n_0 > 0, \\\\ & \\text{ such that } 0 \\leq f(n) < cg(n), \\\\ & \\forall n \\geq n_0 \\} \\end{align*} \\] \\[ \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0 \\] e.g \\(2n=o(n^2)\\) any positive \\(c\\) satisfies but \\(2n^2 \\neq o(n^2)\\) \\(c=2\\) does not satisfy Small-omega / \\(\\omega\\) -Notation: Asymptotic lower bound that is not tight (1) \u00b6 \\[ \\begin{align*} \\omega(g(n)) &= \\{ f(n): \\text{ for any constant } c > 0, \\exists \\text{ a constant } n_0>0, \\\\ & \\text{ such that } 0 \\leq cg(n) < f(n), \\\\ & \\forall n \\geq n_0 \\end{align*} \\] \\[ \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = \\infty \\] e.g. \\(n^2/2=\\omega(n)\\) , any positive \\(c\\) satisfies but \\(n^2/2 \\neq \\omega(n^2)\\) , \\(c=1/2\\) does not satisfy (Important) Analogy to compare of two real numbers (1) \u00b6 \\[ \\begin{align*} f(n) &= O(g(n)) \\leftrightarrow a \\leq b \\\\ f(n) &= \\Omega(g(n)) \\leftrightarrow a \\geq b \\\\ f(n) &= \\Theta(g(n)) \\leftrightarrow a = b \\\\ f(n) &= o(g(n)) \\leftrightarrow a < b \\\\ f(n) &= \\omega(g(n)) \\leftrightarrow a > b \\\\ \\end{align*} \\] (Important) Analogy to compare of two real numbers (2) \u00b6 \\[ \\begin{align*} O \\approx \\leq \\\\ \\Theta \\approx = \\\\ \\Omega \\approx \\geq \\\\ \\omega \\approx > \\\\ o \\approx < \\end{align*} \\] (Important) Trichotomy property for real numbers \u00b6 For any two real numbers \\(a\\) and \\(b\\) , we have either \\(a<b\\) , or \\(a=b\\) , or \\(a>b\\) Trichotomy property does not hold for asymptotic notation, for two functions \\(f(n)\\) and \\(g(n)\\) , it may be the case that neither \\(f(n)=O(g(n))\\) nor \\(f(n)=\\Omega(g(n))\\) holds. e.g. \\(n\\) and \\(n^{1+sin(n)}\\) cannot be compared asymptotically Examples \u00b6 \\(5n^2=O(n^2)\\) TRUE \\(n^2lgn = O(n^2)\\) FALSE \\(5n^2=\\Omega(n^2)\\) TRUE \\(n^2lgn = \\Omega(n^2)\\) TRUE \\(5n^2=\\Theta(n^2)\\) TRUE \\(n^2lgn = \\Theta(n^2)\\) FALSE \\(5n^2=o(n^2)\\) FALSE \\(n^2lgn = o(n^2)\\) FALSE \\(5n^2=\\omega(n^2)\\) FALSE \\(n^2lgn = \\omega(n^2)\\) TRUE \\(2^n = O(3^n)\\) TRUE \\(2^n = \\Omega(3^n)\\) FALSE \\(2^n=o(3^n)\\) TRUE \\(2^n = \\Theta(3^n)\\) FALSE \\(2^n = \\omega(3^n)\\) FALSE section{ font-size: 25px; } Asymptotic Function Properties \u00b6 Transitivity : holds for all e.g. \\(f(n) = \\Theta(g(n)) \\& g(n)=\\Theta(h(n)) \\Rightarrow f(n)=\\Theta(h(n))\\) Reflexivity : holds for \\(\\Theta,O,\\Omega\\) e.g. \\(f(n)=O(f(n))\\) Symmetry : hold only for \\(\\Theta\\) e.g. \\(f(n)=\\Theta(g(n)) \\Leftrightarrow g(n)=\\Theta(f(n))\\) Transpose Symmetry : holds for \\((O \\leftrightarrow \\Omega)\\) and \\((o \\leftrightarrow \\omega)\\) e.g. \\(f(n)=O(g(n))\\Leftrightarrow g(n)=\\Omega(f(n))\\) section{ font-size: 25px; } Using \\(O\\) -Notation to Describe Running Times (1) \u00b6 Used to bound worst-case running times, Implies an upper bound runtime for arbitrary inputs as well Example: Insertion sort has worst-case runtime of \\(O(n^2 )\\) Note: This \\(O(n^2)\\) upper bound also applies to its running time on every input Abuse to say \u201crunning time of insertion sort is \\(O(n^2)\\) \" For a given \\(n\\) , the actual running time depends on the particular input of size \\(n\\) i.e., running time is not only a function of \\(n\\) However, worst-case running time is only a function of \\(n\\) Using \\(O\\) -Notation to Describe Running Times (2) \u00b6 When we say: Running time of insertion sort is \\(O(n^2)\\) What we really mean is Worst-case running time of insertion sort is \\(O(n^2)\\) or equivalently No matter what particular input of size n is chosen, the running time on that set of inputs is \\(O(n^2)\\) Using \\(\\Omega\\) -Notation to Describe Running Times (1) \u00b6 Used to bound best-case running times, Implies a lower bound runtime for arbitrary inputs as well Example: Insertion sort has best-case runtime of \\(\\Omega(n)\\) Note : This \\(\\Omega(n)\\) lower bound also applies to its running time on every input Using \\(\\Omega\\) -Notation to Describe Running Times (2) \u00b6 When we say Running time of algorithm A is \\(\\Omega(g(n))\\) What we mean is For any input of size \\(n\\) , the runtime of A is at least a constant times \\(g(n)\\) for sufficiently large \\(n\\) It\u2019s not contradictory to say worst-case running time of insertion sort is \\(\\Omega(n^2)\\) Because there exists an input that causes the algorithm to take \\(\\Omega(n^2)\\) Using \\(\\Theta\\) -Notation to Describe Running Times (1) \u00b6 Consider 2 cases about the runtime of an algorithm Case 1: Worst-case and best-case not asymptotically equal Use \\(\\Theta\\) -notation to bound worst-case and best-case runtimes separately Case 2: Worst-case and best-case asymptotically equal Use \\(\\Theta\\) -notation to bound the runtime for any input Using \\(\\Theta\\) -Notation to Describe Running Times (2) \u00b6 Case 1: Worst-case and best-case not asymptotically equal Use \\(\\Theta\\) -notation to bound the worst-case and best-case runtimes separately We can say: \"The worst-case runtime of insertion sort is \\(\\Theta(n^2)\\) \" \"The best-case runtime of insertion sort is \\(\\Theta(n)\\) \" But, we can\u2019t say: \"The runtime of insertion sort is \\(\\Theta(n^2)\\) for every input\" A \\(\\Theta\\) -bound on worst/best-case running time does not apply to its running time on arbitrary inputs Worst-Case and Best-Case Equation for Merge-Sort \u00b6 e.g. for merge-sort, we have: \\[ T(n)=\\Theta(nlgn)\\begin{cases} T(n)=O(nlgn)\\\\ T(n)=\\Omega(nlgn)\\end{cases} \\] Using Asymptotic Notation to Describe Runtimes Summary (1) \u00b6 \"The worst case runtime of Insertion Sort is \\(O(n^2)\\) \" Also implies: \"The runtime of Insertion Sort is \\(O(n^2)\\) \" \"The best-case runtime of Insertion Sort is \\(\\Omega(n)\\) \" Also implies: \"The runtime of Insertion Sort is \\(\\Omega(n)\\) \" Using Asymptotic Notation to Describe Runtimes Summary (2) \u00b6 \"The worst case runtime of Insertion Sort is \\(\\Theta(n^2)\\) \" But: \"The runtime of Insertion Sort is not \\(\\Theta(n^2)\\) \" \"The best case runtime of Insertion Sort is \\(\\Theta(n)\\) \" But: \"The runtime of Insertion Sort is not \\(\\Theta(n)\\) \" Using Asymptotic Notation to Describe Runtimes Summary (3) \u00b6 Which one is true? \u00b6 FALSE \"The worst case runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" FALSE \"The best case runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" TRUE \"The runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" This is true, because the best and worst case runtimes have asymptotically the same tight bound \\(\\Theta(nlgn)\\) Asymptotic Notation in Equations (RHS) \u00b6 Asymptotic notation appears alone on the RHS of an equation: implies set membership e.g., \\(n = O(n^2)\\) means \\(n \\in O(n^2)\\) Asymptotic notation appears on the RHS of an equation stands for some anonymous function in the set e.g., \\(2n^2 + 3n + 1 = 2n^2 + \\Theta(n)\\) means: \\(2n^2 + 3n + 1 = 2n^2 + h(n)\\) , for some \\(h(n) \\in \\Theta(n)\\) i.e., \\(h(n) = 3n + 1\\) Asymptotic Notation in Equations (LHS) \u00b6 Asymptotic notation appears on the LHS of an equation: stands for any anonymous function in the set e.g., \\(2n^2 + \\Theta(n) = \\Theta(n^2)\\) means: for any function \\(g(n) \\in \\Theta(n)\\) \\(\\exists\\) some function \\(h(n)\\in \\Theta(n^2)\\) such that \\(2n^2+g(n) = h(n)\\) RHS provides coarser level of detail than LHS References \u00b6 Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) Insertion Sort - GeeksforGeeks NIST Dictionary of Algorithms and Data Structures NIST - Dictionary of Algorithms and Data Structures NIST - big-O notation NIST - big-Omega notation \\(-End-Of-Week-1-Course-Module-\\)","title":"Week-1 (Intro, Asymptotic Notations)"},{"location":"tr/week-1/ce100-week-1-intro/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"tr/week-1/ce100-week-1-intro/#week-1-introduction-to-analysis-of-algorithms","text":"","title":"Week-1 (Introduction to Analysis of Algorithms)"},{"location":"tr/week-1/ce100-week-1-intro/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"tr/week-1/ce100-week-1-intro/#brief-description-of-course-and-rules","text":"We will first talk about, Course Plan and Communication Grading System, Homeworks, and Exams please read the syllabus carefully.","title":"Brief Description of Course and Rules"},{"location":"tr/week-1/ce100-week-1-intro/#outline-1","text":"Introduction to Analysis of Algorithms Algorithm Basics Flowgorithm Pseudocode","title":"Outline (1)"},{"location":"tr/week-1/ce100-week-1-intro/#outline-2","text":"RAM (Random Access Machine Model) Sorting Problem Insertion Sort Analysis Algorithm Cost Calculation for Time Complexity Worst, Average, and Best Case Summary Merge Sort Analysis","title":"Outline (2)"},{"location":"tr/week-1/ce100-week-1-intro/#outline-3","text":"Asymptotic Notation Big O Notation Big Teta Notation Big Omega Notation Small o Notation Small omega Notation","title":"Outline (3)"},{"location":"tr/week-1/ce100-week-1-intro/#we-need-mathematical-proofs-1","text":"Direct proof Proof by mathematical induction Proof by contraposition Proof by contradiction Proof by construction Proof by exhaustion","title":"We Need Mathematical Proofs (1)"},{"location":"tr/week-1/ce100-week-1-intro/#we-need-mathematical-proofs-2","text":"Probabilistic proof Combinatorial proof Nonconstructive proof Statistical proofs in pure mathematics Computer-assisted proofs Mathematical proof - Wikipedia","title":"We Need Mathematical Proofs (2)"},{"location":"tr/week-1/ce100-week-1-intro/#introduction-to-analysis-of-algorithms","text":"Study two sorting algorithms as examples Insertion sort: Incremental algorithm Merge sort: Divide-and-conquer Introduction to runtime analysis Best vs. worst vs. average case Asymptotic analysis","title":"Introduction to Analysis of Algorithms"},{"location":"tr/week-1/ce100-week-1-intro/#what-is-algorithm","text":"Algorithm : A sequence of computational steps that transform the input to the desired output Procedure vs. algorithm An algorithm must halt within finite time with the right output We Need to Measure Performance Metrics Processing Time Allocated Memory Network Congestion Power Usage etc. Example Sorting Algorithms Input : a sequence of n numbers \\[ \\langle a_1,a_2,...,a_n \\rangle \\] Algorithm : Sorting / Permutation \\[ \\prod = \\langle \\prod_{(1)},\\prod_{(2)},...,\\prod_{(n)} \\rangle \\] Output : sorted permutation of the input sequence \\[ \\langle a_{\\prod_{(1)}} \\leqslant a_{\\prod_{(2)}} \\leqslant,...,a_{\\prod_{(n)}} \\rangle \\]","title":"What is Algorithm"},{"location":"tr/week-1/ce100-week-1-intro/#pseudo-code-notation-1","text":"Objective: Express algorithms to humans in a clear and concise way Liberal use of English Indentation for block structures Omission of error handling and other details (needed in real programs) You can use Flowgorithm application to understand concept easily.","title":"Pseudo-code notation (1)"},{"location":"tr/week-1/ce100-week-1-intro/#pseudo-code-notation-2","text":"","title":"Pseudo-code notation (2)"},{"location":"tr/week-1/ce100-week-1-intro/#links-and-examples","text":"Wikipedia CS50 University of North Florida GeeksforGeeks","title":"Links and Examples"},{"location":"tr/week-1/ce100-week-1-intro/#correctness-1","text":"We often use a loop invariant to help us to understand why an algorithm gives the correct answer. Example: (Insertion Sort) at the start of each iteration of the \"outer\" for loop - the loop indexed by \\(j\\) - the subarray \\(A[1 \\dots j-1]\\) consist of the elements originally in \\(A[1\\dots j-1]\\) but in sorted order.","title":"Correctness (1)"},{"location":"tr/week-1/ce100-week-1-intro/#correctness-2","text":"To use a loop invariant to prove correctness, we must show 3 things about it. Initialization: It is true to the first iteration of the loop. Maintaince: If it is true before an iteration of the loop, it remains true before the next iteration. Termination: When the loop terminates, the invariant - usually along with the reason that the loop terminated - gives us a usefull property that helps show that the algorithm is correct.","title":"Correctness (2)"},{"location":"tr/week-1/ce100-week-1-intro/#ram-random-access-machine-model-longrightarrow-theta1-1","text":"Operations Single Step Sequential No Concurrent Arithmetic add, subtract, multiply, divide, remainder, floor, ceiling, shift left/shift right (good by multiply/dividing \\(2^k\\) )","title":"RAM (Random Access Machine Model)  \\(\\Longrightarrow \\Theta(1)\\) (1)"},{"location":"tr/week-1/ce100-week-1-intro/#ram-random-access-machine-model-longrightarrow-theta1-2","text":"Data Movement load, store, copy Control conditional / unconditional branch subroutine calls returns","title":"RAM (Random Access Machine Model)  \\(\\Longrightarrow \\Theta(1)\\) (2)"},{"location":"tr/week-1/ce100-week-1-intro/#ram-random-access-machine-model-longrightarrow-theta1-3","text":"Each instruction take a constant amount of time Integer will be represented by \\(clogn\\) \\(c \\geq 1\\) \\(T(n)\\) the running time of the algorithm: \\[ \\sum \\limits_{\\text{all statement}}^{}(\\text{cost of statement})*(\\text{number of times statement is executed}) = T(n) \\]","title":"RAM (Random Access Machine Model)  \\(\\Longrightarrow \\Theta(1)\\) (3)"},{"location":"tr/week-1/ce100-week-1-intro/#what-is-the-processing-time","text":"section{ font-size: 25px; }","title":"What is the processing time ?"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-algorithm-1","text":"Insertion sort is a simple sorting algorithm that works similar to the way you sort playing cards in your hands The array is virtually split into a sorted and an unsorted part Values from the unsorted part are picked and placed at the correct position in the sorted part. Assume input array : \\(A[1..n]\\) Iterate \\(j\\) from \\(2\\) to \\(n\\)","title":"Insertion Sort Algorithm (1)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-algorithm-2","text":"","title":"Insertion Sort Algorithm (2)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-algorithm-pseudo-code-3","text":"Insertion - Sort ( A ) 1 . for j = 2 to A.length 2 . key = A [ j ] 3 . // insert A [ j ] into the sorted sequence A [ 1 ...j -1 ] 4 . i = j - 1 5 . while i > 0 and A [ i ] > key 6 . A [ i +1 ] = A [ i ] 7 . i = i - 1 8 . A [ i +1 ] = key","title":"Insertion Sort Algorithm (Pseudo-Code) (3)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-step-by-step-description-1","text":"","title":"Insertion Sort Step-By-Step Description (1)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-step-by-step-description-2","text":"","title":"Insertion Sort Step-By-Step Description (2)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-step-by-step-description-3","text":"","title":"Insertion Sort Step-By-Step Description (3)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-example","text":"","title":"Insertion Sort Example"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-step-1-initial","text":"","title":"Insertion Sort Step-1 (initial)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-step-2-j2","text":"","title":"Insertion Sort Step-2 (j=2)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-step-3-j3","text":"","title":"Insertion Sort Step-3 (j=3)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-step-4-j3","text":"","title":"Insertion Sort Step-4 (j=3)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-step-5-j4","text":"","title":"Insertion Sort Step-5 (j=4)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-step-6-j5","text":"","title":"Insertion Sort Step-6 (j=5)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-step-7-j5","text":"","title":"Insertion Sort Step-7 (j=5)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-step-8-j6","text":"","title":"Insertion Sort Step-8 (j=6)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-review-1","text":"Items sorted in-place Elements are rearranged within the array. At a most constant number of items stored outside the array at any time (e.,g. the variable key) Input array \\(A\\) contains a sorted output sequence when the algorithm ends","title":"Insertion Sort Review (1)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-review-2","text":"Incremental approach Having sorted \\(A[1..j-1]\\) , place \\(A[j]\\) correctly so that \\(A[1..j]\\) is sorted Running Time It depends on Input Size (5 elements or 5 billion elements) and Input Itself (partially sorted) Algorithm approach to upper bound of overall performance analysis","title":"Insertion Sort Review (2)"},{"location":"tr/week-1/ce100-week-1-intro/#visualization-of-insertion-sort","text":"Sorting (Bubble, Selection, Insertion, Merge, Quick, Counting, Radix) - VisuAlgo https://www.cs.usfca.edu/~galles/visualization/ComparisonSort.html https://algorithm-visualizer.org/ HMvHTs - Online C++ Compiler & Debugging Tool - Ideone.com","title":"Visualization of Insertion Sort"},{"location":"tr/week-1/ce100-week-1-intro/#kinds-of-running-time-analysis-time-complexity","text":"Worst Case (Big-O Notation) \\(T(n)\\) = maximum processing time of any input \\(n\\) Presentation of Big-O : \\(O(n)\\) Average Case (Teta Notation) \\(T(n)\\) = average time over all inputs of size \\(n\\) , inputs can have a uniform distribution Presentation of Big-Theta : \\(\\Theta(n)\\) Best Case (Omega Notation) \\(T(n)\\) = min time on any input of size \\(n\\) , for example sorted array Presentation of Big-Omega : \\(\\Omega(n)\\)","title":"Kinds of Running Time Analysis (Time Complexity)"},{"location":"tr/week-1/ce100-week-1-intro/#array-sorting-algorithms-time-and-space-complexity","text":"","title":"Array Sorting Algorithms Time and Space Complexity"},{"location":"tr/week-1/ce100-week-1-intro/#comparison-of-time-analysis-cases","text":"For insertion sort, worst-case time depends on the speed of primitive operations such as Relative Speed (on the same machine) Absolute Speed (on different machines) Asymptotic Analysis Ignore machine-dependent constants Look at the growth of \\(T(n) | n\\rightarrow\\infty\\)","title":"Comparison of Time Analysis Cases"},{"location":"tr/week-1/ce100-week-1-intro/#asymptotic-analysis-1","text":"","title":"Asymptotic Analysis (1)"},{"location":"tr/week-1/ce100-week-1-intro/#asymptotic-analysis-2","text":"","title":"Asymptotic Analysis (2)"},{"location":"tr/week-1/ce100-week-1-intro/#theta-notation-average-case","text":"Drop low order terms Ignore leading constants e.g \\[ \\begin{align*} 2n^2+5n+3 &= \\Theta(n^2) \\\\ 3n^3+90n^2-2n+5 &= \\Theta(n^3) \\end{align*} \\] As \\(n\\) gets large, a \\(\\Theta(n^2)\\) algorithm runs faster than a \\(\\Theta(n^3)\\) algorithm","title":"Theta-Notation (Average-Case)"},{"location":"tr/week-1/ce100-week-1-intro/#asymptotic-analysis-3","text":"section{ font-size: 25px; } For both algorithms, we can see a minimum item size in the following chart. After this point, we can see performance differences. Some algorithms for small item size can be run faster than others but if you increase item size you will see a reference point that notation proof performance metrics. section{ font-size: 25px; }","title":"Asymptotic Analysis (3)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-runtime-analysis-1","text":"Cost Times Insertion - Sort ( A ) ---- ----- --------------------- c1 n 1 . for j = 2 to A.length c2 n -1 2 . key = A [ j ] c3 n -1 3 . // insert A [ j ] into the sorted sequence A [ 1 ...j -1 ] c4 n -1 4 . i = j - 1 c5 k5 5 . while i > 0 and A [ i ] > key do c6 k6 6 . A [ i +1 ] = A [ i ] c7 k6 7 . i = i - 1 c8 n -1 8 . A [ i +1 ] = key we have two loops here, if we sum up costs as follow we can see big-O worst case notation. \\(k_5 = \\sum \\limits_{j=2}^n{t_j}\\) and \\(k_6 = \\sum \\limits_{j=2}^n{t_i-1}\\) for operation counts","title":"Insertion Sort - Runtime Analysis (1)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-runtime-analysis-2","text":"cost function can be evaluated as follow; \\[ \\begin{align*} T(n) &=c_1n+c_2(n-1)+ 0(n-1)+c_4(n-1) \\\\ & +c_5\\sum \\limits_{j=2}^n{t_j}+c_6\\sum \\limits_{j=2}^n{t_i-1} \\\\ & +c_7\\sum \\limits_{j=2}^n{t_i-1}+c_8(n-1) \\end{align*} \\]","title":"Insertion Sort - Runtime Analysis (2)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-runtime-analysis-3","text":"\\[ \\begin{align*} \\sum \\limits_{j=2}^n j &= (n(n+1)/2)- 1 \\\\ & \\text{ and } \\\\ \\sum \\limits_{j=2}^n {j-1} &= n(n-1)/2 \\end{align*} \\]","title":"Insertion Sort - Runtime Analysis (3)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-runtime-analysis-4","text":"\\[ \\begin{align*} T(n) & =(c_5/2 + c_6/2 + c_7/2)n^2 \\\\ & + (c_1+c_2+c_4+c_5/2-c_6/2-c_7/2+c_8)n \\\\ & -(c_2 + c_4 + c_5 + c_6) \\end{align*} \\]","title":"Insertion Sort - Runtime Analysis (4)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-runtime-analysis-5","text":"\\[ \\begin{align*} T(n) &= an^2 + bn + c \\\\ &=O(n^2) \\end{align*} \\] section{ font-size: 25px; }","title":"Insertion Sort - Runtime Analysis (5)"},{"location":"tr/week-1/ce100-week-1-intro/#best-case-scenario-sorted-array-1","text":"Problem-1, If \\(A[1...j]\\) is already sorted, what will be \\(t_j=?\\) \\(t_j=1\\) section{ font-size: 25px; }","title":"Best-Case Scenario (Sorted Array) (1)"},{"location":"tr/week-1/ce100-week-1-intro/#best-case-scenario-sorted-array-2","text":"Parameters are taken from image \\[ \\begin{align*} T(n) &=c_1n+c_2(n-1)+c_3(n-1) \\\\ & +c_4\\sum \\limits_{j=2}^nt_j+c_5\\sum \\limits_{j=2}^n(t_j-1) \\\\ & +c_6\\sum \\limits_{j=2}^n(t_j-1)+c_7(n-1) \\end{align*} \\] \\(t_j=1\\) for all \\(j\\) \\[ \\begin{align*} T(n) &= (c_1+c_2+c_3+c_4+c_7)n \\\\ &-(c_2+c_3+c_4+c_7) \\\\ T(n) &=an-b \\\\ &=\\Omega(n) \\end{align*} \\] section{ font-size: 25px; }","title":"Best-Case Scenario (Sorted Array) (2)"},{"location":"tr/week-1/ce100-week-1-intro/#worst-case-scenario-reversed-array-1","text":"Problem-2 If \\(A[j]\\) is smaller than every entry in \\(A[1...j-1]\\) , what will be \\(t_j=?\\) \\(t_j=?\\)","title":"Worst-Case Scenario (Reversed Array) (1)"},{"location":"tr/week-1/ce100-week-1-intro/#worst-case-scenario-reversed-array-2","text":"The input array is reverse sorted \\(t_j=j\\) for all \\(j\\) after calculation worst case runtime will be \\[ \\begin{align*} T(n) &=1/2(c_4+c_5+c_6)n^2 \\\\ & +(c_1+c_2+c_3+1/2(c_4-c_5-c_6)+c_7)n -(c_2+c_3+c_4+c_7) \\\\ T(n) &=1/2an^2+bn-c \\\\ &= O(n^2) \\end{align*} \\]","title":"Worst-Case Scenario (Reversed Array) (2)"},{"location":"tr/week-1/ce100-week-1-intro/#asymptotic-runtime-analysis-of-insertion-sort","text":"","title":"Asymptotic Runtime Analysis of Insertion-Sort"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-worst-case-input-reverse-sorted","text":"Inner Loop is \\(\\Theta(j)\\) \\[ \\begin{align*} T(n) &=\\sum \\limits_{j=2}^n\\Theta(j) \\\\ &=\\Theta(\\sum \\limits_{j=2}^nj) \\\\ &=\\Theta(n^2) \\end{align*} \\]","title":"Insertion-Sort Worst-case (input reverse sorted)"},{"location":"tr/week-1/ce100-week-1-intro/#insertion-sort-average-case-all-permutations-uniformly-distributed","text":"Inner Loop is \\(\\Theta(j/2)\\) \\[ \\begin{align*} T(n) &=\\sum \\limits_{j=2}^n\\Theta(j/2) \\\\ &=\\sum \\limits_{j=2}^n\\Theta(j) \\\\ &=\\Theta(n^2) \\end{align*} \\]","title":"Insertion-Sort Average-case (all permutations uniformly distributed)"},{"location":"tr/week-1/ce100-week-1-intro/#array-sorting-algorithms-timespace-complexities","text":"To compare this sorting algorithm please check the following map again.","title":"Array Sorting Algorithms Time/Space Complexities"},{"location":"tr/week-1/ce100-week-1-intro/#merge-sort-divide-conquer-combine-1","text":"","title":"Merge Sort : Divide / Conquer / Combine (1)"},{"location":"tr/week-1/ce100-week-1-intro/#merge-sort-divide-conquer-combine-2","text":"Divide : we divide the problem into a number of subproblems Conquer : We solve the subproblems recursively Base-Case : Solve by Brute-Force Combine : Subproblem solutions to the original problem","title":"Merge Sort : Divide / Conquer / Combine (2)"},{"location":"tr/week-1/ce100-week-1-intro/#merge-sort-example","text":"","title":"Merge Sort Example"},{"location":"tr/week-1/ce100-week-1-intro/#merge-sort-algorithm-initial-setup","text":"Merge Sort is a recursive sorting algorithm, for initial case we need to call Merge-Sort(A,1,n) for sorting \\(A[1..n]\\) initial case A : Array p : 1 ( offset ) r : n ( length ) Merge - Sort ( A , 1 , n )","title":"Merge Sort Algorithm (initial setup)"},{"location":"tr/week-1/ce100-week-1-intro/#merge-sort-algorithm-internal-iterations","text":"internal iterations A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif section{ font-size: 25px; }","title":"Merge Sort Algorithm (internal iterations)"},{"location":"tr/week-1/ce100-week-1-intro/#merge-sort-algorithm-combine-1","text":"\\(p = start-point\\) \\(q = mid-point\\) \\(r = end-point\\)","title":"Merge Sort Algorithm (Combine-1)"},{"location":"tr/week-1/ce100-week-1-intro/#merge-sort-algorithm-combine-2","text":"brute-force task, merging two sorted subarrays The pseudo-code in the textbook (Sec. 2.3.1)","title":"Merge Sort Algorithm (Combine-2)"},{"location":"tr/week-1/ce100-week-1-intro/#merge-sort-combine-algorithm-1","text":"Merge ( A , p , q , r ) n1 = q - p +1 n2 = r - q // allocate left and right arrays // increment will be from left to right // left part will be bigger than right part L [ 1 ...n1 +1 ] // left array R [ 1 ...n2 +1 ] // right array // copy left part of array for i = 1 to n1 L [ i ] = A [ p + i -1 ] // copy right part of array for j = 1 to n2 R [ j ] = A [ q + j ] // put end items maximum values for termination L [ n1 +1 ] = inf R [ n2 +1 ] = inf i = 1 , j = 1 for k = p to r if L [ i ] <= R [ j ] A [ k ] = L [ i ] i = i +1 else A [ k ] = R [ j ] j = j +1","title":"Merge Sort Combine Algorithm (1)"},{"location":"tr/week-1/ce100-week-1-intro/#what-is-the-complexity-of-merge-operation","text":"You can find by counting loops will provide you base constant nested level will provide you exponent of this constant, if you drop constants you will have complexity we have 3 for loops it will look like \\(3n\\) and \\(\\Theta(n)\\) will be merge complexity","title":"What is the complexity of merge operation?"},{"location":"tr/week-1/ce100-week-1-intro/#merge-sort-correctness","text":"Base case \\(p = r\\) (Trivially correct) Inductive hypothesis MERGE-SORT is correct for any subarray that is a strict (smaller) subset of \\(A[p, q]\\) . General Case MERGE-SORT is correct for \\(A[p, q]\\) . From inductive hypothesis and correctness of Merge.","title":"Merge Sort Correctness"},{"location":"tr/week-1/ce100-week-1-intro/#merge-sort-algorithm-pseudo-code","text":"A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif","title":"Merge Sort Algorithm (Pseudo-Code)"},{"location":"tr/week-1/ce100-week-1-intro/#merge-sort-algorithm-complexity","text":"A : Array p : offset r : length Merge - Sort ( A , p , r ) -------------> T ( n ) if p = r then ---------------> Theta ( 1 ) return else q = floor (( p + r ) / 2 ) ----> Theta ( 1 ) Merge - Sort ( A , p , q ) -----> T ( n / 2 ) Merge - Sort ( A , q +1 , r ) ---> T ( n / 2 ) Merge ( A , p , q , r ) --------> Theta ( n ) endif","title":"Merge Sort Algorithm Complexity"},{"location":"tr/week-1/ce100-week-1-intro/#merge-sort-algorithm-recurrence","text":"We can describe a function recursively in terms of itself, to analyze the performance of recursive algorithms \\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\]","title":"Merge Sort Algorithm Recurrence"},{"location":"tr/week-1/ce100-week-1-intro/#how-to-solve-recurrence-1","text":"\\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\]","title":"How To Solve Recurrence (1)"},{"location":"tr/week-1/ce100-week-1-intro/#how-to-solve-recurrence-2","text":"We will assume \\(T(n)= \\Theta(1)\\) for sufficiently small \\(n\\) to rewrite equation as \\[ T(n)=2T(n/2)+\\Theta(n) \\] Solution for this equation will be \\(\\Theta(nlgn)\\) with following recursion tree.","title":"How To Solve Recurrence (2)"},{"location":"tr/week-1/ce100-week-1-intro/#how-to-solve-recurrence-3","text":"Multiply by height \\(\\Theta(lgn)\\) with each level cost \\(\\Theta(n)\\) we can found \\(\\Theta(nlgn)\\)","title":"How To Solve Recurrence (3)"},{"location":"tr/week-1/ce100-week-1-intro/#how-to-solve-recurrence-4","text":"This tree is binary-tree and binary-tree height is related with item size.","title":"How To Solve Recurrence (4)"},{"location":"tr/week-1/ce100-week-1-intro/#how-height-of-a-binary-tree-is-equal-to-logn-1","text":"Merge-Sort recursion tree is a perfect binary tree, a binary tree is a tree which every node has at most two children, A perfect binary tree is binary tree in which all internal nodes have exactly two children and all leaves are at the same level.","title":"How Height of a Binary Tree is Equal to \\(logn\\) ? (1)"},{"location":"tr/week-1/ce100-week-1-intro/#how-height-of-a-binary-tree-is-equal-to-logn-2","text":"Let \\(n\\) be the number of nodes in the tree and let \\(l_k\\) denote the number of nodes on level k. According to this; \\(l_k = 2l_{k-1}\\) i.e. each level has exactly twice as many nodes as the previous level \\(l_0 = 1\\) , i.e. on the first level we have only one node (the root node) The leaves are at the last level, \\(l_h\\) where \\(h\\) is the height of the tree.","title":"How Height of a Binary Tree is Equal to \\(logn\\) ? (2)"},{"location":"tr/week-1/ce100-week-1-intro/#how-height-of-a-binary-tree-is-equal-to-logn-3","text":"The total number of nodes in the tree is equal to the sum of the nodes on all the levels: nodes \\(n\\) \\[ \\begin{align*} 1+2^1+2^2+2^3+...+2^h &= n \\\\ 1+2^1+2^2+2^3+...+2^h &= 2^{h+1}-1 \\\\ 2^{h+1}-1 &= n\\\\ 2^{h+1} &= n+1\\\\ log_2{2^{h+1}} &= log_2{(n+1)} \\\\ h+1 &= log_2{(n+1)} \\\\ h &= log_2{(n+1)}-1 \\end{align*} \\]","title":"How Height of a Binary Tree is Equal to \\(logn\\) ? (3)"},{"location":"tr/week-1/ce100-week-1-intro/#how-height-of-a-binary-tree-is-equal-to-logn-3_1","text":"If we write it as asymptotic approach, we will have the following result \\[ \\text{height of tree is }h = log_2{(n+1)}-1 = O(logn) \\] also \\[ \\text{number of leaves is } l_h = (n+1)/2 \\] nearly half of the nodes are at the leaves","title":"How Height of a Binary Tree is Equal to \\(logn\\) ? (3)"},{"location":"tr/week-1/ce100-week-1-intro/#review","text":"\\(\\Theta(nlgn)\\) grows more slowly than \\(\\Theta(n^2)\\) Therefore Merge-Sort beats Insertion-Sort in the worst case In practice Merge-Sort beats Insertion-Sort for \\(n>30\\) or so","title":"Review"},{"location":"tr/week-1/ce100-week-1-intro/#asymptotic-notations","text":"","title":"Asymptotic Notations"},{"location":"tr/week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-1","text":"\\(f(n)=O(g(n))\\) if \\(\\exists\\) positive constants \\(c\\) , \\(n_0\\) such that \\[ 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\]","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (1)"},{"location":"tr/week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-2","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (2)"},{"location":"tr/week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-3","text":"Asymptotic running times of algorithms are usually defined by functions whose domain are \\(N={0, 1, 2, \u2026}\\) (natural numbers)","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (3)"},{"location":"tr/week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-4","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (4)"},{"location":"tr/week-1/ce100-week-1-intro/#example-1","text":"Show that \\(2n^2 = O(n^3)\\) we need to find two positive constant \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq 2n^2 \\leq cn^3 \\text{ for all } n \\geq n_0 \\] Choose \\(c=2\\) and \\(n_0 = 1\\) \\[ 2n^2 \\leq 2n^3 \\text{ for all } n \\geq 1 \\] Or, choose \\(c=1\\) and \\(n_0=2\\) \\[ 2n^2 \\leq n^3 \\text{ for all } n \\geq 2 \\]","title":"Example-1"},{"location":"tr/week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-5","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (5)"},{"location":"tr/week-1/ce100-week-1-intro/#example-2","text":"Show that \\(2n^2 + n = O(n^2)\\) We need to find two positive constant \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq {2n^2+n} \\leq cn^2 \\text{ for all } n \\geq n_0 \\] \\[ 2 + (1/n) \\leq c \\text{ for all } n \\geq n_0 \\] Choose \\(c=3\\) and \\(n_0=1\\) \\[ 2n^2 + n \\leq 3n^2 \\text{ for all } n \\geq 1 \\]","title":"Example-2"},{"location":"tr/week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-6","text":"We can say the followings about \\(f(n)=O(g(n))\\) equation The notation is a little sloppy One-way equation, e.q. \\(n^2 = O(n^3)\\) but we cannot say \\(O(n^3)=n^2\\)","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (6)"},{"location":"tr/week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-7","text":"\\(O(g(n))\\) is in fact a set of functions as follow \\(O(g(n)) = \\{ f(n) : \\exists \\text{ positive constant } c, n_0 \\text{ such that } 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\}\\)","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (7)"},{"location":"tr/week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-8","text":"In other words \\(O(g(n))\\) is in fact, the set of functions that have asymptotic upper bound \\(g(n)\\) e.q \\(2n^2 = O(n^3)\\) means \\(2n^2 \\in O(n^3)\\)","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (8)"},{"location":"tr/week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-9","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (9)"},{"location":"tr/week-1/ce100-week-1-intro/#example-1_1","text":"\\(10^9n^2 = O(n^2)\\) \\(0 \\leq 10^9n^2 \\leq cn^2 \\text{ for } n \\geq n_0\\) choose \\(c=10^9\\) and \\(n_0=1\\) \\(0 \\leq 10^9n^2 \\leq 10^9n^2 \\text{ for } n \\geq 1\\) CORRECT","title":"Example-1"},{"location":"tr/week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-10","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (10)"},{"location":"tr/week-1/ce100-week-1-intro/#example-2_1","text":"\\(100n^{1.9999}=O(n^2)\\) \\(0 \\leq 100n^{1.9999} \\leq cn^2 \\text{ for } n \\geq n_0\\) choose \\(c=100\\) and \\(n_0=1\\) \\(0 \\leq 100n^{1.9999} \\leq 100n^2 \\text{ for } n \\geq 1\\) CORRECT","title":"Example-2"},{"location":"tr/week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-11","text":"","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (11)"},{"location":"tr/week-1/ce100-week-1-intro/#example-3","text":"\\(10^{-9}n^{2.0001} = O(n^2)\\) \\(0 \\leq 10^{-9}n^{2.0001} \\leq cn^2 \\text{ for } n \\geq n_0\\) \\(10^{-9}n^{0.0001} \\leq c \\text{ for } n \\geq n_0\\) INCORRECT (Contradiction)","title":"Example-3"},{"location":"tr/week-1/ce100-week-1-intro/#big-o-o-notation-asymptotic-upper-bound-worst-case-12","text":"If we analysis \\(O(n^2)\\) case, \\(O\\) -notation is an upper bound notation and the runtime \\(T(n)\\) of algorithm A is at least \\(O(n^2 )\\) . \\(O(n^2)\\) : The set of functions with asymptotic upper bound \\(n^2\\) \\(T(n) \\geq O(n^2)\\) means \\(T(n) \\geq h(n)\\) for some \\(h(n) \\in O(n^2)\\) \\(h(n)=0\\) function is also in \\(O(n^2)\\) . Hence : \\(T(n) \\geq 0\\) , runtime must be nonnegative.","title":"Big-O / \\(O\\)- Notation : Asymptotic Upper Bound (Worst-Case) (12)"},{"location":"tr/week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-1","text":"\\(f(n)=\\Omega(g(n))\\) if \\(\\exists\\) positive constants \\(c,n_0\\) such that \\(0 \\leq cg(n) \\leq f(n) , \\forall n \\geq n_0\\)","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (1)"},{"location":"tr/week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-2","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (2)"},{"location":"tr/week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-3","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (3)"},{"location":"tr/week-1/ce100-week-1-intro/#example-1_2","text":"Show that \\(2n^3 = \\Omega(n^2)\\) We need to find two positive constants \\(c\\) and \\(n_0\\) such that: \\[ 0 \\leq cn^2 \\leq 2n^3 \\text{ for all } n \\geq n_0 \\] Choose \\(c=1\\) and \\(n_0=1\\) \\[ n^2 \\leq 2n^3 \\text{ for all } n \\geq 1 \\]","title":"Example-1"},{"location":"tr/week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-4","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (4)"},{"location":"tr/week-1/ce100-week-1-intro/#example-4","text":"Show that \\(\\sqrt{n}=\\Omega(lgn)\\) We need to find two positive constants \\(c\\) and \\(n_0\\) such that: \\[ clgn \\leq \\sqrt{n} \\text{ for all } n \\geq n_0 \\] Choose \\(c=1\\) and \\(n_0=16\\) \\[ lgn \\leq \\sqrt{n} \\text{ for all } n \\geq 16 \\]","title":"Example-4"},{"location":"tr/week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-5","text":"\\(\\Omega(g(n))\\) is the set of functions that have asymptotic lower bound \\(g(n)\\) \\[ \\begin{align*} \\Omega(g(n)) &=\\{ f(n):\\exists \\text{ positive constants } c,n_0 \\text{ such that } \\\\ & 0 \\leq cg(n) \\leq f(n), \\forall n \\geq n_0 \\} \\end{align*} \\]","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (5)"},{"location":"tr/week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-6","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (6)"},{"location":"tr/week-1/ce100-week-1-intro/#example-1_3","text":"\\(10^9n^2 = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 10^9n^2 \\text{ for } n\\geq n_0\\) Choose \\(c=10^9\\) and \\(n_0=1\\) \\(0 \\leq 10^9n^2 \\leq 10^9n^2 \\text{ for } n\\geq 1\\) CORRECT","title":"Example-1"},{"location":"tr/week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-7","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (7)"},{"location":"tr/week-1/ce100-week-1-intro/#example-2_2","text":"\\(100n^{1.9999} = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 100n^{1.9999} \\text{ for } n \\geq n_0\\) \\(n^{0.0001} \\leq (100/c) \\text{ for } n \\geq n_0\\) INCORRECT (Contradiction)","title":"Example-2"},{"location":"tr/week-1/ce100-week-1-intro/#big-omega-omega-notation-asymptotic-lower-bound-best-case-8","text":"","title":"Big-Omega / \\(\\Omega\\)-Notation : Asymptotic Lower Bound (Best-Case) (8)"},{"location":"tr/week-1/ce100-week-1-intro/#example-3_1","text":"\\(10^{-9}n^{2.0001} = \\Omega(n^2)\\) \\(0 \\leq cn^2 \\leq 10^{-9}n^{2.0001} \\text{ for } n \\geq n_0\\) Choose \\(c=10^{-9}\\) and \\(n_0=1\\) \\(0 \\leq 10^{-9}n^2 \\leq 10^{-9}n^{2.0001} \\text{ for } n \\geq 1\\) CORRECT","title":"Example-3"},{"location":"tr/week-1/ce100-week-1-intro/#comparison-of-notations-1","text":"","title":"Comparison of Notations (1)"},{"location":"tr/week-1/ce100-week-1-intro/#comparison-of-notations-2","text":"","title":"Comparison of Notations (2)"},{"location":"tr/week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-1","text":"\\[ \\begin{align*} f(n) &=\\Theta(g(n)) \\ if \\ \\exists \\ \\text{positive constants} \\ c_1,c_2,n_0 \\text{such that} \\\\ & 0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n), \\forall n \\geq n_0 \\end{align*} \\]","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (1)"},{"location":"tr/week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-2","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (2)"},{"location":"tr/week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-3","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (3)"},{"location":"tr/week-1/ce100-week-1-intro/#example-1_4","text":"Show that \\(2n^2 + n = \\Theta(n^2)\\) We need to find 3 positive constants \\(c_1,c_2\\) and \\(n_0\\) such that: \\(0 \\leq c_1n^2 \\leq 2n^2+n \\leq c_2n^2\\) for all \\(n \\geq n_0\\) \\(c_1 \\leq 2 + (1/n) \\leq c_2\\) for all \\(n \\geq n_0\\) Choose \\(c_1=2, c_2=3\\) and \\(n_0=1\\) \\(2n^2 \\leq 2n^2+n \\leq 3n^2\\) for all \\(n \\geq 1\\)","title":"Example-1"},{"location":"tr/week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-4","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (4)"},{"location":"tr/week-1/ce100-week-1-intro/#example-21","text":"Show that \\(1/2n^2-2n=\\Theta(n^2)\\) We need to find 3 positive constants \\(c_1,c_2\\) and \\(n_0\\) such that: \\(0 \\leq c_1n^2 \\leq 1/2n^2-2n \\leq c_2n^2 \\text{ for all } n \\geq n_0\\) \\(c_1 \\leq 1/2 - 2 / n \\leq c_2 \\text{ for all } n \\geq n_0\\) Choose 3 positive constants \\(c_1,c_2, n_0\\) that satisfy \\(c_1 \\leq 1/2 - 2/n \\leq c_2\\) for all \\(n \\geq n_0\\)","title":"Example-2.1"},{"location":"tr/week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-5","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (5)"},{"location":"tr/week-1/ce100-week-1-intro/#example-22","text":"","title":"Example-2.2"},{"location":"tr/week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-6","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (6)"},{"location":"tr/week-1/ce100-week-1-intro/#example-23","text":"\\[ 1/10 \\leq 1/2 - 2/n \\text{ for } n \\geq 5 \\] \\[ 1/2 - 2/n \\leq 1/2 \\text{ for } n \\geq 0 \\] Therefore we can choose \\(c_1 = 1/10, c_2=1/2, n_0=5\\)","title":"Example-2.3"},{"location":"tr/week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-7","text":"Theorem : leading constants & low-order terms don\u2019t matter Justification : can choose the leading constant large enough to make high-order term dominate other terms","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (7)"},{"location":"tr/week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-8","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (8)"},{"location":"tr/week-1/ce100-week-1-intro/#example-1_5","text":"\\(10^9n^2 = \\Theta(n^2)\\) CORRECT \\(100n^{1.9999} = \\Theta(n^2)\\) INCORRECT \\(10^9n^{2.0001} = \\Theta(n^2)\\) INCORRECT","title":"Example-1"},{"location":"tr/week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-9","text":"\\(\\Theta(g(n))\\) is the set of functions that have asymptotically tight bound \\(g(n)\\) \\(\\Theta(g(n))=\\{ f(n):\\) \\(\\exists\\) positive constants \\(c_1,c_2, n_0\\) such that \\(0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n), \\forall n \\geq n_0 \\}\\)","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (9)"},{"location":"tr/week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-10","text":"Theorem : \\(f(n)=\\Theta(g(n))\\) if and only if \\(f(n)=O(g(n))\\) and \\(f(n)=\\Omega(g(n))\\) \\(\\Theta\\) is stronger than both \\(O\\) and \\(\\Omega\\) \\(\\Theta(g(n)) \\subseteq O(g(n)) \\text{ and } \\Theta(g(n)) \\subseteq \\Omega(g(n))\\)","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (10)"},{"location":"tr/week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-11","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (11)"},{"location":"tr/week-1/ce100-week-1-intro/#example-11","text":"Prove that \\(10^{-8}n^2 \\neq \\Theta(n)\\) We can check that \\(10^{-8}n^2 = \\Omega(n)\\) and \\(10^{-8}n^2 \\neq O(n)\\) Proof by contradiction for \\(O(n)\\) notation \\[ \\begin{align*} O(g(n)) &= \\{ f(n) : \\exists \\text{ positive constant } c, n_0 \\text{ such that } \\\\ & 0 \\leq f(n) \\leq cg(n), \\forall n \\geq n_0 \\} \\end{align*} \\]","title":"Example-1.1"},{"location":"tr/week-1/ce100-week-1-intro/#big-theta-theta-notation-asymptotically-tight-bound-average-case-12","text":"","title":"Big-Theta /\\(\\Theta\\)-Notation : Asymptotically tight bound (Average Case) (12)"},{"location":"tr/week-1/ce100-week-1-intro/#example-12","text":"Suppose positive constants \\(c_2\\) and \\(n_0\\) exist such that: \\(10^{-8}n^2 \\leq c_2n, \\forall n \\geq n_0\\) \\(10^{-8}n \\leq c_2, \\forall n \\geq n_0\\) Contradiction : \\(c_2\\) is a constant","title":"Example-1.2"},{"location":"tr/week-1/ce100-week-1-intro/#summary-of-oomega-and-theta-notations-1","text":"\\(O(g(n))\\) : The set of functions with asymptotic upper bound \\(g(n)\\) \\(\\Omega(g(n))\\) : The set of functions with asymptotic lower bound \\(g(n)\\) \\(\\Theta(n)\\) : The set of functions with asymptotically tight bound \\(g(n)\\) \\(f(n)=\\Theta(g(n)) \\Leftrightarrow f(n)=O(g(n)) \\text{ and } f(n)=\\Omega(g(n))\\)","title":"Summary of \\(O,\\Omega\\) and \\(\\Theta\\) notations (1)"},{"location":"tr/week-1/ce100-week-1-intro/#summary-of-oomega-and-theta-notations-2","text":"","title":"Summary of \\(O,\\Omega\\) and \\(\\Theta\\) notations (2)"},{"location":"tr/week-1/ce100-week-1-intro/#small-o-o-notation-asymptotic-upper-bound-that-is-not-tight-1","text":"Remember, upper bound provided by big- \\(O\\) notation can be tight or not tight Tight mean values are close the original function e.g. followings are true \\(2n^2 = O(n^2)\\) is asymptotically tight \\(2n = O(n^2)\\) is not asymptotically tight According to this small- \\(o\\) notation is an upper bound that is not asymptotically tight","title":"Small-o / \\(o\\)-Notation : Asymptotic upper bound that is not tight (1)"},{"location":"tr/week-1/ce100-week-1-intro/#small-o-o-notation-asymptotic-upper-bound-that-is-not-tight-2","text":"Note that in equations equality is removed in small notations \\[ \\begin{align*} o(g(n)) &=\\{ f(n): \\text{ for any constant} c > 0, \\exists \\text{ a constant } n_0 > 0, \\\\ & \\text{ such that } 0 \\leq f(n) < cg(n), \\\\ & \\forall n \\geq n_0 \\} \\end{align*} \\] \\[ \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0 \\] e.g \\(2n=o(n^2)\\) any positive \\(c\\) satisfies but \\(2n^2 \\neq o(n^2)\\) \\(c=2\\) does not satisfy","title":"Small-o / \\(o\\)-Notation : Asymptotic upper bound that is not tight (2)"},{"location":"tr/week-1/ce100-week-1-intro/#small-omega-omega-notation-asymptotic-lower-bound-that-is-not-tight-1","text":"\\[ \\begin{align*} \\omega(g(n)) &= \\{ f(n): \\text{ for any constant } c > 0, \\exists \\text{ a constant } n_0>0, \\\\ & \\text{ such that } 0 \\leq cg(n) < f(n), \\\\ & \\forall n \\geq n_0 \\end{align*} \\] \\[ \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = \\infty \\] e.g. \\(n^2/2=\\omega(n)\\) , any positive \\(c\\) satisfies but \\(n^2/2 \\neq \\omega(n^2)\\) , \\(c=1/2\\) does not satisfy","title":"Small-omega / \\(\\omega\\)-Notation: Asymptotic lower bound that is not tight (1)"},{"location":"tr/week-1/ce100-week-1-intro/#important-analogy-to-compare-of-two-real-numbers-1","text":"\\[ \\begin{align*} f(n) &= O(g(n)) \\leftrightarrow a \\leq b \\\\ f(n) &= \\Omega(g(n)) \\leftrightarrow a \\geq b \\\\ f(n) &= \\Theta(g(n)) \\leftrightarrow a = b \\\\ f(n) &= o(g(n)) \\leftrightarrow a < b \\\\ f(n) &= \\omega(g(n)) \\leftrightarrow a > b \\\\ \\end{align*} \\]","title":"(Important) Analogy to compare of two real numbers (1)"},{"location":"tr/week-1/ce100-week-1-intro/#important-analogy-to-compare-of-two-real-numbers-2","text":"\\[ \\begin{align*} O \\approx \\leq \\\\ \\Theta \\approx = \\\\ \\Omega \\approx \\geq \\\\ \\omega \\approx > \\\\ o \\approx < \\end{align*} \\]","title":"(Important) Analogy to compare of two real numbers (2)"},{"location":"tr/week-1/ce100-week-1-intro/#important-trichotomy-property-for-real-numbers","text":"For any two real numbers \\(a\\) and \\(b\\) , we have either \\(a<b\\) , or \\(a=b\\) , or \\(a>b\\) Trichotomy property does not hold for asymptotic notation, for two functions \\(f(n)\\) and \\(g(n)\\) , it may be the case that neither \\(f(n)=O(g(n))\\) nor \\(f(n)=\\Omega(g(n))\\) holds. e.g. \\(n\\) and \\(n^{1+sin(n)}\\) cannot be compared asymptotically","title":"(Important) Trichotomy property for real numbers"},{"location":"tr/week-1/ce100-week-1-intro/#examples","text":"\\(5n^2=O(n^2)\\) TRUE \\(n^2lgn = O(n^2)\\) FALSE \\(5n^2=\\Omega(n^2)\\) TRUE \\(n^2lgn = \\Omega(n^2)\\) TRUE \\(5n^2=\\Theta(n^2)\\) TRUE \\(n^2lgn = \\Theta(n^2)\\) FALSE \\(5n^2=o(n^2)\\) FALSE \\(n^2lgn = o(n^2)\\) FALSE \\(5n^2=\\omega(n^2)\\) FALSE \\(n^2lgn = \\omega(n^2)\\) TRUE \\(2^n = O(3^n)\\) TRUE \\(2^n = \\Omega(3^n)\\) FALSE \\(2^n=o(3^n)\\) TRUE \\(2^n = \\Theta(3^n)\\) FALSE \\(2^n = \\omega(3^n)\\) FALSE section{ font-size: 25px; }","title":"Examples"},{"location":"tr/week-1/ce100-week-1-intro/#asymptotic-function-properties","text":"Transitivity : holds for all e.g. \\(f(n) = \\Theta(g(n)) \\& g(n)=\\Theta(h(n)) \\Rightarrow f(n)=\\Theta(h(n))\\) Reflexivity : holds for \\(\\Theta,O,\\Omega\\) e.g. \\(f(n)=O(f(n))\\) Symmetry : hold only for \\(\\Theta\\) e.g. \\(f(n)=\\Theta(g(n)) \\Leftrightarrow g(n)=\\Theta(f(n))\\) Transpose Symmetry : holds for \\((O \\leftrightarrow \\Omega)\\) and \\((o \\leftrightarrow \\omega)\\) e.g. \\(f(n)=O(g(n))\\Leftrightarrow g(n)=\\Omega(f(n))\\) section{ font-size: 25px; }","title":"Asymptotic Function Properties"},{"location":"tr/week-1/ce100-week-1-intro/#using-o-notation-to-describe-running-times-1","text":"Used to bound worst-case running times, Implies an upper bound runtime for arbitrary inputs as well Example: Insertion sort has worst-case runtime of \\(O(n^2 )\\) Note: This \\(O(n^2)\\) upper bound also applies to its running time on every input Abuse to say \u201crunning time of insertion sort is \\(O(n^2)\\) \" For a given \\(n\\) , the actual running time depends on the particular input of size \\(n\\) i.e., running time is not only a function of \\(n\\) However, worst-case running time is only a function of \\(n\\)","title":"Using \\(O\\)-Notation to Describe Running Times (1)"},{"location":"tr/week-1/ce100-week-1-intro/#using-o-notation-to-describe-running-times-2","text":"When we say: Running time of insertion sort is \\(O(n^2)\\) What we really mean is Worst-case running time of insertion sort is \\(O(n^2)\\) or equivalently No matter what particular input of size n is chosen, the running time on that set of inputs is \\(O(n^2)\\)","title":"Using \\(O\\)-Notation to Describe Running Times (2)"},{"location":"tr/week-1/ce100-week-1-intro/#using-omega-notation-to-describe-running-times-1","text":"Used to bound best-case running times, Implies a lower bound runtime for arbitrary inputs as well Example: Insertion sort has best-case runtime of \\(\\Omega(n)\\) Note : This \\(\\Omega(n)\\) lower bound also applies to its running time on every input","title":"Using \\(\\Omega\\)-Notation to Describe Running Times (1)"},{"location":"tr/week-1/ce100-week-1-intro/#using-omega-notation-to-describe-running-times-2","text":"When we say Running time of algorithm A is \\(\\Omega(g(n))\\) What we mean is For any input of size \\(n\\) , the runtime of A is at least a constant times \\(g(n)\\) for sufficiently large \\(n\\) It\u2019s not contradictory to say worst-case running time of insertion sort is \\(\\Omega(n^2)\\) Because there exists an input that causes the algorithm to take \\(\\Omega(n^2)\\)","title":"Using \\(\\Omega\\)-Notation to Describe Running Times (2)"},{"location":"tr/week-1/ce100-week-1-intro/#using-theta-notation-to-describe-running-times-1","text":"Consider 2 cases about the runtime of an algorithm Case 1: Worst-case and best-case not asymptotically equal Use \\(\\Theta\\) -notation to bound worst-case and best-case runtimes separately Case 2: Worst-case and best-case asymptotically equal Use \\(\\Theta\\) -notation to bound the runtime for any input","title":"Using \\(\\Theta\\)-Notation to Describe Running Times (1)"},{"location":"tr/week-1/ce100-week-1-intro/#using-theta-notation-to-describe-running-times-2","text":"Case 1: Worst-case and best-case not asymptotically equal Use \\(\\Theta\\) -notation to bound the worst-case and best-case runtimes separately We can say: \"The worst-case runtime of insertion sort is \\(\\Theta(n^2)\\) \" \"The best-case runtime of insertion sort is \\(\\Theta(n)\\) \" But, we can\u2019t say: \"The runtime of insertion sort is \\(\\Theta(n^2)\\) for every input\" A \\(\\Theta\\) -bound on worst/best-case running time does not apply to its running time on arbitrary inputs","title":"Using \\(\\Theta\\)-Notation to Describe Running Times (2)"},{"location":"tr/week-1/ce100-week-1-intro/#worst-case-and-best-case-equation-for-merge-sort","text":"e.g. for merge-sort, we have: \\[ T(n)=\\Theta(nlgn)\\begin{cases} T(n)=O(nlgn)\\\\ T(n)=\\Omega(nlgn)\\end{cases} \\]","title":"Worst-Case and Best-Case Equation for Merge-Sort"},{"location":"tr/week-1/ce100-week-1-intro/#using-asymptotic-notation-to-describe-runtimes-summary-1","text":"\"The worst case runtime of Insertion Sort is \\(O(n^2)\\) \" Also implies: \"The runtime of Insertion Sort is \\(O(n^2)\\) \" \"The best-case runtime of Insertion Sort is \\(\\Omega(n)\\) \" Also implies: \"The runtime of Insertion Sort is \\(\\Omega(n)\\) \"","title":"Using Asymptotic Notation to Describe Runtimes Summary (1)"},{"location":"tr/week-1/ce100-week-1-intro/#using-asymptotic-notation-to-describe-runtimes-summary-2","text":"\"The worst case runtime of Insertion Sort is \\(\\Theta(n^2)\\) \" But: \"The runtime of Insertion Sort is not \\(\\Theta(n^2)\\) \" \"The best case runtime of Insertion Sort is \\(\\Theta(n)\\) \" But: \"The runtime of Insertion Sort is not \\(\\Theta(n)\\) \"","title":"Using Asymptotic Notation to Describe Runtimes Summary (2)"},{"location":"tr/week-1/ce100-week-1-intro/#using-asymptotic-notation-to-describe-runtimes-summary-3","text":"","title":"Using Asymptotic Notation to Describe Runtimes Summary (3)"},{"location":"tr/week-1/ce100-week-1-intro/#which-one-is-true","text":"FALSE \"The worst case runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" FALSE \"The best case runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" TRUE \"The runtime of Merge Sort is \\(\\Theta(nlgn)\\) \" This is true, because the best and worst case runtimes have asymptotically the same tight bound \\(\\Theta(nlgn)\\)","title":"Which one is true?"},{"location":"tr/week-1/ce100-week-1-intro/#asymptotic-notation-in-equations-rhs","text":"Asymptotic notation appears alone on the RHS of an equation: implies set membership e.g., \\(n = O(n^2)\\) means \\(n \\in O(n^2)\\) Asymptotic notation appears on the RHS of an equation stands for some anonymous function in the set e.g., \\(2n^2 + 3n + 1 = 2n^2 + \\Theta(n)\\) means: \\(2n^2 + 3n + 1 = 2n^2 + h(n)\\) , for some \\(h(n) \\in \\Theta(n)\\) i.e., \\(h(n) = 3n + 1\\)","title":"Asymptotic Notation in Equations (RHS)"},{"location":"tr/week-1/ce100-week-1-intro/#asymptotic-notation-in-equations-lhs","text":"Asymptotic notation appears on the LHS of an equation: stands for any anonymous function in the set e.g., \\(2n^2 + \\Theta(n) = \\Theta(n^2)\\) means: for any function \\(g(n) \\in \\Theta(n)\\) \\(\\exists\\) some function \\(h(n)\\in \\Theta(n^2)\\) such that \\(2n^2+g(n) = h(n)\\) RHS provides coarser level of detail than LHS","title":"Asymptotic Notation in Equations (LHS)"},{"location":"tr/week-1/ce100-week-1-intro/#references","text":"Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) Insertion Sort - GeeksforGeeks NIST Dictionary of Algorithms and Data Structures NIST - Dictionary of Algorithms and Data Structures NIST - big-O notation NIST - big-Omega notation \\(-End-Of-Week-1-Course-Module-\\)","title":"References"},{"location":"tr/week-10/ce100-week-10-graphs/","text":"CE100 Algorithms and Programming II \u00b6 Week-10 (Graphs) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Graphs \u00b6 Outline \u00b6 Introduction to Graphs Graphs and Representation BFS (Breath-First Search) DFS (Depth-First Search) in-order post-order pre-order Topological Order SCC (Strongly Connected Components) MST Prim Kruskal References \u00b6 TODO","title":"Week-10 (Graphs)"},{"location":"tr/week-10/ce100-week-10-graphs/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"tr/week-10/ce100-week-10-graphs/#week-10-graphs","text":"","title":"Week-10 (Graphs)"},{"location":"tr/week-10/ce100-week-10-graphs/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"tr/week-10/ce100-week-10-graphs/#graphs","text":"","title":"Graphs"},{"location":"tr/week-10/ce100-week-10-graphs/#outline","text":"Introduction to Graphs Graphs and Representation BFS (Breath-First Search) DFS (Depth-First Search) in-order post-order pre-order Topological Order SCC (Strongly Connected Components) MST Prim Kruskal","title":"Outline"},{"location":"tr/week-10/ce100-week-10-graphs/#references","text":"TODO","title":"References"},{"location":"tr/week-11/ce100-week-11-shortestpath/","text":"CE100 Algorithms and Programming II \u00b6 Week-11 (Shortest Path) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Shortest Path \u00b6 Outline \u00b6 Disjoint Sets and Kruskal Relationships Single-Source Shortest Paths Bellman-Ford Dijkstra Q-Learning Shortest Path Max-Flow Min-Cut Ford-Fulkerson Edmond\u2019s Karp Dinic References \u00b6 TODO","title":"Week-11 (Shortest Path)"},{"location":"tr/week-11/ce100-week-11-shortestpath/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"tr/week-11/ce100-week-11-shortestpath/#week-11-shortest-path","text":"","title":"Week-11 (Shortest Path)"},{"location":"tr/week-11/ce100-week-11-shortestpath/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"tr/week-11/ce100-week-11-shortestpath/#shortest-path","text":"","title":"Shortest Path"},{"location":"tr/week-11/ce100-week-11-shortestpath/#outline","text":"Disjoint Sets and Kruskal Relationships Single-Source Shortest Paths Bellman-Ford Dijkstra Q-Learning Shortest Path Max-Flow Min-Cut Ford-Fulkerson Edmond\u2019s Karp Dinic","title":"Outline"},{"location":"tr/week-11/ce100-week-11-shortestpath/#references","text":"TODO","title":"References"},{"location":"tr/week-12/ce100-week-12-crypto/","text":"CE100 Algorithms and Programming II \u00b6 Week-12 (Hashing and Encryption) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Hashing and Encryption \u00b6 Outline \u00b6 Crypto++ Library Usage Hashing and Encryption Integrity Control Hash Values Cryptographic Hash Functions SHA-1 SHA-256 SHA-512 Checksums MD5 CRC32 Hash Algorithms SHA-1 SHA-256 SHA-512 H-MAC References \u00b6 TODO","title":"Week-12 (Hashing)"},{"location":"tr/week-12/ce100-week-12-crypto/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"tr/week-12/ce100-week-12-crypto/#week-12-hashing-and-encryption","text":"","title":"Week-12 (Hashing and Encryption)"},{"location":"tr/week-12/ce100-week-12-crypto/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"tr/week-12/ce100-week-12-crypto/#hashing-and-encryption","text":"","title":"Hashing and Encryption"},{"location":"tr/week-12/ce100-week-12-crypto/#outline","text":"Crypto++ Library Usage Hashing and Encryption Integrity Control Hash Values Cryptographic Hash Functions SHA-1 SHA-256 SHA-512 Checksums MD5 CRC32 Hash Algorithms SHA-1 SHA-256 SHA-512 H-MAC","title":"Outline"},{"location":"tr/week-12/ce100-week-12-crypto/#references","text":"TODO","title":"References"},{"location":"tr/week-13/ce100-week-13-symenc/","text":"CE100 Algorithms and Programming II \u00b6 Week-13 (Symmetric and Asymmetric Encryption) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Symmetric and Asymmetric Encryption \u00b6 Outline \u00b6 Symmetric Encryption Algorithms AES https://formaestudio.com/portfolio/aes-animation/ DES http://desalgorithm.yolasite.com/ TDES https://en.wikipedia.org/wiki/Triple_DES Symmetric Encryption Modes https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation ECB CBC Asymmetric Encryption Key Pairs (Public-Private Key Pairs) Signature Generation and Validation References \u00b6 TODO","title":"Week-13 (Encryption)"},{"location":"tr/week-13/ce100-week-13-symenc/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"tr/week-13/ce100-week-13-symenc/#week-13-symmetric-and-asymmetric-encryption","text":"","title":"Week-13 (Symmetric and  Asymmetric Encryption)"},{"location":"tr/week-13/ce100-week-13-symenc/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"tr/week-13/ce100-week-13-symenc/#symmetric-and-asymmetric-encryption","text":"","title":"Symmetric and Asymmetric Encryption"},{"location":"tr/week-13/ce100-week-13-symenc/#outline","text":"Symmetric Encryption Algorithms AES https://formaestudio.com/portfolio/aes-animation/ DES http://desalgorithm.yolasite.com/ TDES https://en.wikipedia.org/wiki/Triple_DES Symmetric Encryption Modes https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation ECB CBC Asymmetric Encryption Key Pairs (Public-Private Key Pairs) Signature Generation and Validation","title":"Outline"},{"location":"tr/week-13/ce100-week-13-symenc/#references","text":"TODO","title":"References"},{"location":"tr/week-14/ce100-week-14-otp/","text":"CE100 Algorithms and Programming II \u00b6 Week-14 (OTP Calculation, File Encryption) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX OTP Calculation, File Encryption \u00b6 Outline \u00b6 1.OTP Calculation a.Time-based b.Counter-based File Encryption and Decryption and Integrity Control Operations References \u00b6 TODO","title":"Week-14 (One-Time-Password / File Enc.)"},{"location":"tr/week-14/ce100-week-14-otp/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"tr/week-14/ce100-week-14-otp/#week-14-otp-calculation-file-encryption","text":"","title":"Week-14 (OTP Calculation, File Encryption)"},{"location":"tr/week-14/ce100-week-14-otp/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"tr/week-14/ce100-week-14-otp/#otp-calculation-file-encryption","text":"","title":"OTP Calculation, File Encryption"},{"location":"tr/week-14/ce100-week-14-otp/#outline","text":"1.OTP Calculation a.Time-based b.Counter-based File Encryption and Decryption and Integrity Control Operations","title":"Outline"},{"location":"tr/week-14/ce100-week-14-otp/#references","text":"TODO","title":"References"},{"location":"tr/week-15/ce100-week-15-review/","text":"CE100 Algorithms and Programming II \u00b6 Week-15 (Review) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Review \u00b6 Outline \u00b6 References \u00b6 TODO","title":"Week-15 (Review)"},{"location":"tr/week-15/ce100-week-15-review/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"tr/week-15/ce100-week-15-review/#week-15-review","text":"","title":"Week-15 (Review)"},{"location":"tr/week-15/ce100-week-15-review/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"tr/week-15/ce100-week-15-review/#review","text":"","title":"Review"},{"location":"tr/week-15/ce100-week-15-review/#outline","text":"","title":"Outline"},{"location":"tr/week-15/ce100-week-15-review/#references","text":"TODO","title":"References"},{"location":"tr/week-16/ce100-week-16-final/","text":"CE100 Algorithms and Programming II \u00b6 Week-16 (Final) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Final \u00b6 Outline \u00b6 References \u00b6 TODO","title":"Week-16 (Final)"},{"location":"tr/week-16/ce100-week-16-final/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"tr/week-16/ce100-week-16-final/#week-16-final","text":"","title":"Week-16 (Final)"},{"location":"tr/week-16/ce100-week-16-final/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"tr/week-16/ce100-week-16-final/#final","text":"","title":"Final"},{"location":"tr/week-16/ce100-week-16-final/#outline","text":"","title":"Outline"},{"location":"tr/week-16/ce100-week-16-final/#references","text":"TODO","title":"References"},{"location":"tr/week-2/ce100-week-2-recurrence/","text":"CE100 Algorithms and Programming II \u00b6 Week-2 (Solving Recurrences / The Divide-and-Conquer) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Solving Recurrences \u00b6 Outline (1) \u00b6 Solving Recurrences Recursion Tree Master Method Back-Substitution Outline (2) \u00b6 Divide-and-Conquer Analysis Merge Sort Binary Search Merge Sort Analysis Complexity Outline (3) \u00b6 Recurrence Solution Solving Recurrences (1) \u00b6 Reminder: Runtime \\((T(n))\\) of MergeSort was expressed as a recurrence \\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\] Solving recurrences is like solving differential equations, integrals, etc. Need to learn a few tricks Solving Recurrences (2) \u00b6 Recurrence: An equation or inequality that describes a function in terms of its value on smaller inputs. Example : \\[ T(n)= \\begin{cases} 1 &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ 1 &\\text{if n>1} \\end{cases} \\] section{ font-size: 25px; } Recurrence Example \u00b6 \\[ T(n)= \\begin{cases} 1 &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ 1 &\\text{if n>1} \\end{cases} \\] Simplification: Assume \\(n=2^k\\) Claimed answer : \\(T(n)=lgn+1\\) Substitute claimed answer in the recurrence: \\[ lgn+1= \\begin{cases} 1 &\\text{if n=1} \\\\ lg(\\lceil{n/2}\\rceil)+ 2 &\\text{if n>1} \\end{cases} \\] True when \\(n=2^k\\) Technicalities: Floor / Ceiling \u00b6 Technically, should be careful about the floor and ceiling functions (as in the book). e.g. For merge sort, the recurrence should in fact be:, \\[ T(n)= \\begin{cases} \\Theta(1) &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ T(\\lfloor{n/2}\\rfloor) +\\Theta(n) &\\text{if n>1} \\end{cases} \\] But, it's usually ok to: ignore floor/ceiling solve for the exact power of 2 (or another number) Technicalities: Boundary Conditions \u00b6 Usually assume: \\(T(n) = \\Theta(1)\\) for sufficiently small \\(n\\) Changes the exact solution, but usually the asymptotic solution is not affected (e.g. if polynomially bounded) For convenience, the boundary conditions generally implicitly stated in a recurrence \\(T(n) = 2T(n/2) + \\Theta(n)\\) assuming that \\(T(n)=\\Theta(1)\\) for sufficiently small \\(n\\) section{ font-size: 25px; } Example: When Boundary Conditions Matter \u00b6 Exponential function: \\(T(n) = (T(n/2))2\\) Assume \\(T(1) = c \\text{ (where c is a positive constant)}\\) \\(T(2) = (T(1))^2 = c^2\\) \\(T(4) = (T(2))^2 = c^4\\) \\(T(n) = \\Theta(c^n)\\) e.g. \\[ \\text{ However } \\Theta(2^n) \\neq \\Theta(3^n) \\begin{cases} T(1)= 2 &\\Rightarrow & T(n)= \\Theta(2^n) \\\\ T(1)= 3 &\\Rightarrow & T(n)= \\Theta(3^n) \\end{cases} \\] The difference in solution more dramatic when: \\[ T(1) = 1 \\Rightarrow T(n) = \\Theta(1^n) = \\Theta(1) \\] Solving Recurrences Methods \u00b6 We will focus on 3 techniques Substitution method Recursion tree approach Master method Substitution Method \u00b6 The most general method: Guess Prove by induction Solve for constants Substitution Method: Example (1) \u00b6 Solve \\(T(n)=4T(n/2)+n\\) (assume \\(T(1)= \\Theta(1)\\) ) Guess \\(T(n) = O(n^3)\\) (need to prove \\(O\\) and \\(\\Omega\\) separately) Prove by induction that \\(T(n) \\leq cn^3\\) for large \\(n\\) (i.e. \\(n \\geq n_0\\) ) Inductive hypothesis: \\(T(k) \\leq ck^3\\) for any \\(k < n\\) Assuming ind. hyp. holds, prove \\(T(n) \\leq cn^3\\) Substitution Method: Example (2) \u00b6 Original recurrence: \\(T(n) = 4T(n/2) + n\\) From inductive hypothesis: \\(T(n/2) \\leq c(n/2)^3\\) Substitute this into the original recurrence: \\(T(n) \\leq 4c(n/2)^3 + n\\) \\(= (c/2)n^3 + n\\) \\(= cn^3 \u2013 ((c/2)n^3 \u2013 n)\\) \\(\\Longleftarrow\\) desired - residual \\(\\leq cn^3\\) when \\(((c/2)n^3 \u2013 n) \\geq 0\\) Substitution Method: Example (3) \u00b6 So far, we have shown: \\[ T(n) \\leq cn^3 \\text{ when } ((c/2)n^3 \u2013 n) \\geq 0 \\] We can choose \\(c \\geq 2\\) and \\(n_0 \\geq 1\\) But, the proof is not complete yet. Reminder: Proof by induction: 1.Prove the base cases \\(\\Longleftarrow\\) haven\u2019t proved the base cases yet 2.Inductive hypothesis for smaller sizes 3.Prove the general case Substitution Method: Example (4) \u00b6 We need to prove the base cases Base: \\(T(n) = \\Theta(1)\\) for small \\(n\\) (e.g. for \\(n = n_0\\) ) We should show that: \\(\\Theta(1) \\leq cn^3\\) for \\(n = n_0\\) , This holds if we pick \\(c\\) big enough So, the proof of \\(T(n) = O(n^3)\\) is complete But, is this a tight bound? Example: A tighter upper bound? (1) \u00b6 Original recurrence: \\(T(n) = 4T(n/2) + n\\) Try to prove that \\(T(n) = O(n^2)\\) , i.e. \\(T(n) \\leq cn^2\\) for all \\(n \\geq n_0\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\) Example: A tighter upper bound? (2) \u00b6 Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\) \\[ \\begin{align*} T(n) & = 4T(n/2) + n \\\\ & \\leq 4c(n/2)^2 + n \\\\ & = cn^2 + n \\\\ & = O(n2) \\Longleftarrow \\text{ Wrong! We must prove exactly} \\end{align*} \\] Example: A tighter upper bound? (3) \u00b6 Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\) So far, we have: \\(T(n) \\leq cn^2 + n\\) No matter which positive c value we choose, this does not show that \\(T(n) \\leq cn^2\\) Proof failed? Example: A tighter upper bound? (4) \u00b6 What was the problem? The inductive hypothesis was not strong enough Idea: Start with a stronger inductive hypothesis Subtract a low-order term Inductive hypothesis: \\(T(k) \\leq c_1k^2 \u2013 c_2k\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq c_1n^2 - c_2n\\) Example: A tighter upper bound? (5) \u00b6 Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq c_1k^2 \u2013 c_2k\\) for \\(k < n\\) Prove the general case: \\(T(n) \u2264 c_1n^2 \u2013 c_2n\\) \\[ \\begin{align*} T(n) & = 4T(n/2) + n \\\\ & \\leq 4 (c_1(n/2)^2 \u2013 c_2(n/2)) + n \\\\ & = c_1n^2 \u2013 2c_2n + n \\\\ & = c_1n^2 \u2013 c_2n \u2013 (c_2n \u2013 n) \\\\ & \\leq c_1n^2 \u2013 c_2n \\text{ for } n(c_2 \u2013 1) \\geq 0 \\\\ & \\text{choose } c2 \\geq 1 \\end{align*} \\] Example: A tighter upper bound? (6) \u00b6 We now need to prove $$ T(n) \\leq c_1n^2 \u2013 c_2n $$ for the base cases. \\(T(n) = \\Theta(1) \\text{ for } 1 \\leq n \\leq n_0\\) (implicit assumption) \\(\\Theta(1) \\leq c_1n^2 \u2013 c_2n\\) for \\(n\\) small enough (e.g. \\(n = n_0\\) ) We can choose c1 large enough to make this hold We have proved that \\(T(n) = O(n^2)\\) section{ font-size: 25px; } Substitution Method: Example 2 (1) \u00b6 For the recurrence \\(T(n) = 4T(n/2) + n\\) , prove that \\(T(n) = \\Omega(n^2)\\) i.e. \\(T(n) \u2265 cn^2\\) for any \\(n \\geq n_0\\) Ind. hyp: \\(T(k) \\geq ck^2\\) for any \\(k < n\\) Prove general case: \\(T(n) \\geq cn^2\\) \\(T(n) = 4T(n/2) + n\\) \\(\\geq 4c (n/2)^2 + n\\) \\(= cn^2 + n\\) \\(\\geq cn^2\\) since \\(n > 0\\) Proof succeeded \u2013 no need to strengthen the ind. hyp as in the last example Substitution Method: Example 2 (2) \u00b6 We now need to prove that \\(T(n) \u2265 cn^2\\) for the base cases \\(T(n) = \\Theta(1)\\) for \\(1 \\leq n \\leq n_0\\) (implicit assumption) \\(\\Theta(1) \\geq cn^2\\) for \\(n = n_0\\) \\(n_0\\) is sufficiently small (i.e. constant) We can choose \\(c\\) small enough for this to hold We have proved that \\(T(n) = \\Omega(n^2)\\) Substitution Method - Summary \u00b6 Guess the asymptotic complexity Prove your guess using induction Assume inductive hypothesis holds for \\(k < n\\) Try to prove the general case for \\(n\\) Note: \\(MUST\\) prove the \\(EXACT\\) inequality \\(CANNOT\\) ignore lower order terms, If the proof fails, strengthen the ind. hyp. and try again Prove the base cases (usually straightforward) Recursion Tree Method \u00b6 A recursion tree models the runtime costs of a recursive execution of an algorithm. The recursion tree method is good for generating guesses for the substitution method. The recursion-tree method can be unreliable. Not suitable for formal proofs The recursion-tree method promotes intuition, however. Solve Recurrence (1) : \\(T(n)=2T(n/2)+\\Theta(n)\\) \u00b6 Solve Recurrence (2) : \\(T(n)=2T(n/2)+\\Theta(n)\\) \u00b6 Solve Recurrence (3) : \\(T(n)=2T(n/2)+\\Theta(n)\\) \u00b6 Example of Recursion Tree (1) \u00b6 Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\) Example of Recursion Tree (2) \u00b6 Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\) Example of Recursion Tree (3) \u00b6 Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\) The Master Method \u00b6 A powerful black-box method to solve recurrences. The master method applies to recurrences of the form \\(T(n) = aT(n/b) + f (n)\\) where \\(a \\geq 1, b > 1\\) , and \\(f\\) is asymptotically positive . The Master Method: 3 Cases \u00b6 (TODO : Add Notes ) Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Compare \\(f(n)\\) with \\(n^{log_b^a}\\) Intuitively: Case 1: \\(f(n)\\) grows polynomially slower than \\(n^{log_b^a}\\) Case 2: \\(f(n)\\) grows at the same rate as \\(n^{log_b^a}\\) Case 3: \\(f(n)\\) grows polynomially faster than \\(n^{log_b^a}\\) The Master Method: Case 1 (Bigger) \u00b6 Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon})\\) for some constant \\(\\varepsilon>0\\) i.e., \\(f(n)\\) grows polynomialy slower than \\(n^{log_b^a}\\) (by an \\(n^{\\varepsilon}\\) factor) Solution: \\(T(n)=\\Theta(n^{log_b^a})\\) The Master Method: Case 2 (Simple Version) (Equal) \u00b6 Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(1)\\) i.e., \\(f(n)\\) and \\(n^{log_b^a}\\) grow at similar rates Solution: \\(T(n)=\\Theta(n^{log_b^a}lgn)\\) The Master Method: Case 3 (Smaller) \u00b6 Case 3: \\(\\frac{f(n)}{n^{log_b^a}}=\\Omega(n^{\\varepsilon})\\) for some constant \\(\\varepsilon > 0\\) i.e., \\(f(n)\\) grows polynomialy faster than \\(n^{log_b^a}\\) (by an \\(n^{\\varepsilon}\\) factor) and the following regularity condition holds: \\(af(n/b) \\leq cf(n)\\) for some constant \\(c<1\\) Solution: \\(T(n)=\\Theta(f(n))\\) The Master Method Example (case-1) : \\(T(n)=4T(n/2)+n\\) \u00b6 \\(a=4\\) \\(b=2\\) \\(f(n)=n\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n\\) grows polynomially slower than \\(n^{log_b^a}=n^2\\) \\(\\frac{n^{log_b^a}}{f(n)}=\\frac{n^2}{n}=n=\\Omega(n^{\\varepsilon})\\) CASE-1: \\(T(n)=\\Theta(n^{log_b^a})=\\Theta(n^{log_2^4})=\\Theta(n^2)\\) The Master Method Example (case-2) : \\(T(n)=4T(n/2)+n^2\\) \u00b6 \\(a=4\\) \\(b=2\\) \\(f(n)=n^2\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^2\\) grows at similar rate as \\(n^{log_b^a}=n^2\\) \\(f(n)=\\Theta(n^{log_b^a})=n^2\\) CASE-2: \\(T(n)=\\Theta(n^{log_b^a}lgn)=\\Theta(n^{log_2^4}lgn)=\\Theta(n^2lgn)\\) The Master Method Example (case-3) (1) : \\(T(n)=4T(n/2)+n^3\\) \u00b6 \\(a=4\\) \\(b=2\\) \\(f(n)=n^3\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^3\\) grows polynomially faster than \\(n^{log_b^a}=n^2\\) \\(\\frac{f(n)}{n^{log_b^a}}=\\frac{n^3}{n^2}=n=\\Omega(n^{\\varepsilon})\\) The Master Method Example (case-3) (2) : \\(T(n)=4T(n/2)+n^3\\) (con't) \u00b6 Seems like CASE 3, but need to check the regularity condition Regularity condition \\(af(n/b) \\leq cf(n)\\) for some constant \\(c<1\\) \\(4(n/2)^3 \\leq cn^3\\) for \\(c=1/2\\) CASE-3: \\(T(n)=\\Theta(f(n))\\) \\(\\Longrightarrow\\) \\(T(n)=\\Theta(n^3)\\) The Master Method Example (N/A case) : \\(T(n)=4T(n/2)+n^2lgn\\) \u00b6 \\(a=4\\) \\(b=2\\) \\(f(n)=n^2lgn\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^2lgn\\) grows slower than \\(n^{log_b^a}=n^2\\) but is it polynomially slower? \\(\\frac{n^{log_b^a}{f(n)}}=\\frac{n^2}{\\frac{n^2}{lgn}}=lgn \\neq \\Omega(n^{\\varepsilon})\\) for any \\(\\varepsilon>0\\) is not CASE-1 Master Method does not apply! The Master Method : Case 2 (General Version) \u00b6 Recurrence : \\(T(n) = aT(n/b) + f(n)\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn)\\) for some constant \\(k \\geq 0\\) Solution : \\(T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) General Method (Akra-Bazzi) \u00b6 \\(T(n)=\\sum \\limits_{i=1}^k{a_iT(n/b_i)}+f(n)\\) Let \\(p\\) be the unique solution to \\(\\sum \\limits_{i=1}^k{(a_i/b^p_i)}=1\\) Then, the answers are the same as for the master method, but with \\(n^p\\) instead of \\(n^{log_b^a}\\) (Akra and Bazzi also prove an even more general result.) Idea of Master Theorem (1) \u00b6 Recursion Tree: Idea of Master Theorem (2) \u00b6 CASE 1 : The weight increases geometrically from the root to the leaves. The leaves hold a constant fraction of the total weight. \\(n^{log_b^a}T(1)=\\Theta(n^{log_b^a})\\) Idea of Master Theorem (3) \u00b6 CASE 2 : \\((k = 0)\\) The weight is approximately the same on each of the \\(log_bn\\) levels. \\(n^{log_b^a}T(1)=\\Theta(n^{log_b^a}lgn)\\) Idea of Master Theorem (4) \u00b6 CASE 3 : The weight decreases geometrically from the root to the leaves. The root holds a constant fraction of the total weight. \\(n^{log_b^a}T(1)=\\Theta(f(n))\\) Proof of Master Theorem: Case 1 and Case 2 \u00b6 Recall from the recursion tree (note \\(h = lg_bn =\\text{tree height}\\) ) \\(\\text{Leaf Cost}=\\Theta(n^{log_b^a})\\) \\(\\text{Non-leaf Cost}=g(n)=\\sum \\limits_{i=0}^{h-1}a^if(n/{b^i})\\) \\(T(n)=\\text{Leaf Cost} + \\text{Non-leaf Cost}\\) \\(T(n)=\\Theta(n^{log_b^a}) + \\sum \\limits_{i=0}^{h-1}a^if(n/{b^i})\\) Proof of Master Theorem Case 1 (1) \u00b6 \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon})\\) for some \\(\\varepsilon>0\\) \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow O(n^{-\\varepsilon}) \\Longrightarrow f(n) = O(n^{log_b^{a-\\varepsilon}})\\) \\(g(n)=\\sum \\limits_{i=0}^{h-1}a^iO((n/{b^i})^{log_b^{a-\\varepsilon}})=O(\\sum \\limits_{i=0}^{h-1}a^i(n/{b^i})^{log_b^{a-\\varepsilon}})\\) \\(O(n^{log_b^{a-\\varepsilon}}\\sum \\limits_{i=0}^{h-1}a^ib^{i\\varepsilon}/b^{ilog_b^{a-\\varepsilon}})\\) Proof of Master Theorem Case 1 (2) \u00b6 \\(\\sum \\limits_{i=0}^{h-1} \\frac{a^ib^{i\\varepsilon}}{b^{ilog_b^a}} =\\sum \\limits_{i=0}^{h-1} a^i\\frac{(b^\\varepsilon)^i}{(b^{log_b^a})^i} =\\sum a^i\\frac{b^{i\\varepsilon}}{a^i}=\\sum \\limits_{i=0}^{h-1}(b^{\\varepsilon})^i\\) = An increasing geometric series since \\(b > 1\\) \\(\\frac{b^{h\\varepsilon}-1}{b^{\\varepsilon}-1}=\\frac{(b^h)^{\\varepsilon}-1}{b^{\\varepsilon}-1} = \\frac{(b^{log_b^n})^{\\varepsilon}-1}{b^{\\varepsilon}-1}=\\frac{n^{\\varepsilon}-1}{b^{\\varepsilon}-1} = O(n^{\\varepsilon})\\) Proof of Master Theorem Case 1 (3) \u00b6 \\(g(n)=O(n^{log_b{a-\\varepsilon}}O(n^{\\varepsilon}))=O(\\frac{n^{log_b^a}}{n^{\\varepsilon}}O(n^{\\varepsilon}))=O(n^{log_b^a})\\) \\(T(n)=\\Theta(n^{log_b^a})+g(n)=\\Theta(n^{log_b^a})+O(n^{log_b^a})=\\Theta(n^{log_b^a})\\) Q.E.D. (Quod Erat Demonstrandum) section{ font-size: 22px; } Proof of Master Theorem Case 2 (limited to k=0) \u00b6 \\(\\frac{f(n)}{n^log_b^a}=\\Theta(lg^0n)=\\Theta(1) \\Longrightarrow f(n)=\\Theta(n^{log_b^a}) \\Longrightarrow f(n/b^i)=\\Theta((n/b^i)^{log_b^a})\\) \\(g(n)=\\sum \\limits_{i=0}^{h-1}a^i\\Theta((n/b^i)^{log_b^a})\\) \\(= \\Theta(\\sum \\limits_{i=0}^{h-1}a^i\\frac{n^{log_b^a}}{b^{ilog_b^a}})\\) \\(=\\Theta(n^{log_b^a}\\sum \\limits_{i=0}^{h-1}a^i\\frac{1}{(b^{log_b^a})^i})\\) \\(=\\Theta(n^{log_b^a}\\sum \\limits_{i=0}^{h-1}a^i\\frac{1}{a^i})\\) \\(=\\Theta(n^{log_b^a}\\sum \\limits_{i=0}^{log_b^{n-1}}1) = \\Theta(n^{log_b^a}log_bn)=\\Theta(n^{log_b^a}lgn)\\) \\(T(n)=n^{log_b^a}+\\Theta(n^{log_b^a}lgn)\\) \\(=\\Theta(n^{log_b^a}lgn)\\) Q.E.D. The Divide-and-Conquer Design Paradigm (1) \u00b6 The Divide-and-Conquer Design Paradigm (2) \u00b6 Divide we divide the problem into a number of subproblems. Conquer we solve the subproblems recursively. BaseCase solve by Brute-Force Combine subproblem solutions to the original problem. The Divide-and-Conquer Design Paradigm (3) \u00b6 \\(a=\\text{subproblem}\\) \\(1/b=\\text{each size of the problem}\\) \\[ T(n)=\\begin{cases} \\Theta(1) & \\text{if} & n \\leq c & (basecase) \\\\ aT(n/b)+D(n)+C(n) & \\text{otherwise} \\end{cases} \\] Merge-Sort \\[ T(n)=\\begin{cases} \\Theta(1) & & n = 1 \\\\ 2T(n/2)+\\Theta(n) & \\text{if} & n>1 \\end{cases} \\] \\(T(n)=\\Theta(nlgn)\\) Selection Sort Algorithm \u00b6 SELECTION - SORT ( A ) n = A.length ; for j = 1 to n -1 smallest = j ; for i = j +1 to n if A [ i ] < A [ smallest ] smallest = i ; endfor exchange A [ j ] with A [ smallest ] endfor Selection Sort Algorithm \u00b6 \\[ T(n)=\\begin{cases} \\Theta(1) & & n = 1 \\\\ T(n-1)+\\Theta(n) & \\text{if} & n>1 \\end{cases} \\] Sequential Series \\[ cost = n(n+1)/2 = {1/2}n^2 +{1/2}n \\] Drop low-order terms Ignore the constant coefficient in the leading term \\[ T(n)=\\Theta(n^2) \\] Merge Sort Algorithm (initial setup) \u00b6 Merge Sort is a recursive sorting algorithm, for initial case we need to call Merge-Sort(A,1,n) for sorting \\(A[1..n]\\) initial case A : Array p : 1 ( offset ) r : n ( length ) Merge - Sort ( A , 1 , n ) section{ font-size: 25px; } Merge Sort Algorithm (internal iterations) \u00b6 internal iterations \\(p = start-point\\) \\(q = mid-point\\) \\(r = end-point\\) A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif Merge Sort Combine Algorithm (1) \u00b6 Merge ( A , p , q , r ) n1 = q - p +1 n2 = r - q // allocate left and right arrays // increment will be from left to right // left part will be bigger than right part L [ 1 ...n1 +1 ] // left array R [ 1 ...n2 +1 ] // right array // copy left part of array for i = 1 to n1 L [ i ] = A [ p + i -1 ] // copy right part of array for j = 1 to n2 R [ j ] = A [ q + j ] // put end items maximum values for termination L [ n1 +1 ] = inf R [ n2 +1 ] = inf i = 1 , j = 1 for k = p to r if L [ i ] <= R [ j ] A [ k ] = L [ i ] i = i +1 else A [ k ] = R [ j ] j = j +1 Example : Merge Sort \u00b6 Divide: Trivial. Conquer: Recursively sort 2 subarrays. Combine: Linear- time merge. \\(T(n)=2T(n/2)+\\Theta(n)\\) Subproblems \\(\\Longrightarrow 2\\) Subproblemsize \\(\\Longrightarrow n/2\\) Work dividing and combining \\(\\Longrightarrow\\Theta(n)\\) Master Theorem: Reminder \u00b6 \\(T(n) = aT(n/b) + f(n)\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(n^{log_b^a})\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) Case 3: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(f(n))\\) and \\(af(n/b) \\leq cf(n)\\) for \\(c<1\\) Merge Sort: Solving the Recurrence \u00b6 \\(T(n)=2T(n/2)+\\Theta(n)\\) \\(a=2,b=2,f(n)=\\Theta(n),n^{log_b^a}=n\\) Case-2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(nlgn)\\) Binary Search (1) \u00b6 Find an element in a sorted array: 1. Divide: Check middle element. 2. Conquer: Recursively search 1 subarray. 3. Combine: Trivial. Binary Search (2) \u00b6 \\[ \\text{PARENT} = \\lfloor i/2 \\rfloor \\] \\[ \\text{LEFT-CHILD} = 2i, \\text{ 2i>n} \\] \\[ \\text{RIGHT-CHILD} = 2i+1, \\text{ 2i>n} \\] Binary Search (3) : Iterative \u00b6 ITERATIVE - BINARY - SEARCH ( A , V , low , high ) while low <= high mid = floor (( low + high ) / 2 ); if v == A [ mid ] return mid ; elseif v > A [ mid ] low = mid + 1 ; else high = mid - 1 ; endwhile return NIL Binary Search (4): Recursive \u00b6 RECURSIVE - BINARY - SEARCH ( A , V , low , high ) if low > high return NIL ; endif mid = floor (( low + high ) / 2 ); if v == A [ mid ] return mid ; elseif v > A [ mid ] return RECURSIVE - BINARY - SEARCH ( A , V , mid +1 , high ); else return RECURSIVE - BINARY - SEARCH ( A , V , low , mid -1 ); endif Binary Search (5): Recursive \u00b6 \\[ T(n)=T(n/2)+\\Theta(1) \\Longrightarrow T(n)=\\Theta(lgn) \\] Binary Search (6): Example (Find 9) \u00b6 Recurrence for Binary Search (7) \u00b6 \\(T(n)=1T(n/2)+\\Theta(1)\\) Subproblems \\(\\Longrightarrow 1\\) Subproblemsize \\(\\Longrightarrow n/2\\) Work dividing and combining \\(\\Longrightarrow\\Theta(1)\\) Binary Search: Solving the Recurrence (8) \u00b6 \\(T(n) = T(n/2) + \\Theta(1)\\) \\(a = 1,b = 2,f(n) = \\Theta(1) \\Longrightarrow n^{log_b^a} = n^0=1\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(lgn)\\) Powering a Number: Divide & Conquer (1) \u00b6 Problem : Compute an, where n is a natural number NAIVE - POWER ( a , n ) powerVal = 1 ; for i = 1 to n powerVal = powerVal * a ; endfor return powerVal ; What is the complexity? \\(\\Longrightarrow T(n)=\\Theta(n)\\) Powering a Number: Divide & Conquer (2) \u00b6 Basic Idea: \\[ a^n=\\begin{cases} a^{n/2}*a^{n/2} & \\text{if n is even} \\\\ a^{(n-1)/2}*a^{(n-1)/2}*a & \\text{if n is odd} \\end{cases} \\] Powering a Number: Divide & Conquer (3) \u00b6 POWER ( a , n ) if n = 0 then return 1 ; else if n is even then val = POWER ( a , n / 2 ); return val * val ; else if n is odd then val = POWER ( a ,( n -1 ) / 2 ) return val * val * a ; endif Powering a Number: Solving the Recurrence (4) \u00b6 \\(T(n) = T(n/2) + \\Theta(1)\\) \\(a = 1,b = 2,f(n) = \\Theta(1) \\Longrightarrow n^{log_b^a} = n^0=1\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(lgn)\\) Correctness Proofs for Divide and Conquer Algorithms \u00b6 Proof by induction commonly used for Divide and Conquer Algorithms Base case: Show that the algorithm is correct when the recursion bottoms out (i.e., for sufficiently small n) Inductive hypothesis: Assume the alg. is correct for any recursive call on any smaller subproblem of size \\(k\\) , \\((k < n)\\) General case: Based on the inductive hypothesis, prove that the alg. is correct for any input of size n section{ font-size: 25px; } Example Correctness Proof: Powering a Number \u00b6 Base Case: \\(POWER(a, 0)\\) is correct, because it returns \\(1\\) Ind. Hyp: Assume \\(POWER(a, k)\\) is correct for any \\(k<n\\) General Case: In \\(POWER(a,n)\\) function: If \\(n\\) is \\(even\\) : \\(val = a^{n/2}\\) (due to ind. hyp.) it returns \\(val*val = a^n\\) If \\(n\\) is \\(odd\\) : \\(val = a^{(n-1)/2}\\) (due to ind. hyp.) it returns \\(val*val*a = a^n\\) The correctness proof is complete References \u00b6 Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) Insertion Sort - GeeksforGeeks NIST Dictionary of Algorithms and Data Structures NIST - Dictionary of Algorithms and Data Structures NIST - big-O notation NIST - big-Omega notation \\(-End-Of-Week-2-Course-Module-\\)","title":"Week-2 (Solving Recurrences)"},{"location":"tr/week-2/ce100-week-2-recurrence/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"tr/week-2/ce100-week-2-recurrence/#week-2-solving-recurrences-the-divide-and-conquer","text":"","title":"Week-2 (Solving Recurrences / The Divide-and-Conquer)"},{"location":"tr/week-2/ce100-week-2-recurrence/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"tr/week-2/ce100-week-2-recurrence/#solving-recurrences","text":"","title":"Solving Recurrences"},{"location":"tr/week-2/ce100-week-2-recurrence/#outline-1","text":"Solving Recurrences Recursion Tree Master Method Back-Substitution","title":"Outline (1)"},{"location":"tr/week-2/ce100-week-2-recurrence/#outline-2","text":"Divide-and-Conquer Analysis Merge Sort Binary Search Merge Sort Analysis Complexity","title":"Outline (2)"},{"location":"tr/week-2/ce100-week-2-recurrence/#outline-3","text":"Recurrence Solution","title":"Outline (3)"},{"location":"tr/week-2/ce100-week-2-recurrence/#solving-recurrences-1","text":"Reminder: Runtime \\((T(n))\\) of MergeSort was expressed as a recurrence \\[ T(n)=\\begin{cases} \\Theta(1)&\\text{if n=1} \\\\ 2T(n/2)+\\Theta(n)&otherwise \\end{cases} \\] Solving recurrences is like solving differential equations, integrals, etc. Need to learn a few tricks","title":"Solving Recurrences (1)"},{"location":"tr/week-2/ce100-week-2-recurrence/#solving-recurrences-2","text":"Recurrence: An equation or inequality that describes a function in terms of its value on smaller inputs. Example : \\[ T(n)= \\begin{cases} 1 &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ 1 &\\text{if n>1} \\end{cases} \\] section{ font-size: 25px; }","title":"Solving Recurrences (2)"},{"location":"tr/week-2/ce100-week-2-recurrence/#recurrence-example","text":"\\[ T(n)= \\begin{cases} 1 &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ 1 &\\text{if n>1} \\end{cases} \\] Simplification: Assume \\(n=2^k\\) Claimed answer : \\(T(n)=lgn+1\\) Substitute claimed answer in the recurrence: \\[ lgn+1= \\begin{cases} 1 &\\text{if n=1} \\\\ lg(\\lceil{n/2}\\rceil)+ 2 &\\text{if n>1} \\end{cases} \\] True when \\(n=2^k\\)","title":"Recurrence Example"},{"location":"tr/week-2/ce100-week-2-recurrence/#technicalities-floor-ceiling","text":"Technically, should be careful about the floor and ceiling functions (as in the book). e.g. For merge sort, the recurrence should in fact be:, \\[ T(n)= \\begin{cases} \\Theta(1) &\\text{if n=1} \\\\ T(\\lceil{n/2}\\rceil)+ T(\\lfloor{n/2}\\rfloor) +\\Theta(n) &\\text{if n>1} \\end{cases} \\] But, it's usually ok to: ignore floor/ceiling solve for the exact power of 2 (or another number)","title":"Technicalities: Floor / Ceiling"},{"location":"tr/week-2/ce100-week-2-recurrence/#technicalities-boundary-conditions","text":"Usually assume: \\(T(n) = \\Theta(1)\\) for sufficiently small \\(n\\) Changes the exact solution, but usually the asymptotic solution is not affected (e.g. if polynomially bounded) For convenience, the boundary conditions generally implicitly stated in a recurrence \\(T(n) = 2T(n/2) + \\Theta(n)\\) assuming that \\(T(n)=\\Theta(1)\\) for sufficiently small \\(n\\) section{ font-size: 25px; }","title":"Technicalities: Boundary Conditions"},{"location":"tr/week-2/ce100-week-2-recurrence/#example-when-boundary-conditions-matter","text":"Exponential function: \\(T(n) = (T(n/2))2\\) Assume \\(T(1) = c \\text{ (where c is a positive constant)}\\) \\(T(2) = (T(1))^2 = c^2\\) \\(T(4) = (T(2))^2 = c^4\\) \\(T(n) = \\Theta(c^n)\\) e.g. \\[ \\text{ However } \\Theta(2^n) \\neq \\Theta(3^n) \\begin{cases} T(1)= 2 &\\Rightarrow & T(n)= \\Theta(2^n) \\\\ T(1)= 3 &\\Rightarrow & T(n)= \\Theta(3^n) \\end{cases} \\] The difference in solution more dramatic when: \\[ T(1) = 1 \\Rightarrow T(n) = \\Theta(1^n) = \\Theta(1) \\]","title":"Example: When Boundary Conditions Matter"},{"location":"tr/week-2/ce100-week-2-recurrence/#solving-recurrences-methods","text":"We will focus on 3 techniques Substitution method Recursion tree approach Master method","title":"Solving Recurrences Methods"},{"location":"tr/week-2/ce100-week-2-recurrence/#substitution-method","text":"The most general method: Guess Prove by induction Solve for constants","title":"Substitution Method"},{"location":"tr/week-2/ce100-week-2-recurrence/#substitution-method-example-1","text":"Solve \\(T(n)=4T(n/2)+n\\) (assume \\(T(1)= \\Theta(1)\\) ) Guess \\(T(n) = O(n^3)\\) (need to prove \\(O\\) and \\(\\Omega\\) separately) Prove by induction that \\(T(n) \\leq cn^3\\) for large \\(n\\) (i.e. \\(n \\geq n_0\\) ) Inductive hypothesis: \\(T(k) \\leq ck^3\\) for any \\(k < n\\) Assuming ind. hyp. holds, prove \\(T(n) \\leq cn^3\\)","title":"Substitution Method: Example (1)"},{"location":"tr/week-2/ce100-week-2-recurrence/#substitution-method-example-2","text":"Original recurrence: \\(T(n) = 4T(n/2) + n\\) From inductive hypothesis: \\(T(n/2) \\leq c(n/2)^3\\) Substitute this into the original recurrence: \\(T(n) \\leq 4c(n/2)^3 + n\\) \\(= (c/2)n^3 + n\\) \\(= cn^3 \u2013 ((c/2)n^3 \u2013 n)\\) \\(\\Longleftarrow\\) desired - residual \\(\\leq cn^3\\) when \\(((c/2)n^3 \u2013 n) \\geq 0\\)","title":"Substitution Method: Example (2)"},{"location":"tr/week-2/ce100-week-2-recurrence/#substitution-method-example-3","text":"So far, we have shown: \\[ T(n) \\leq cn^3 \\text{ when } ((c/2)n^3 \u2013 n) \\geq 0 \\] We can choose \\(c \\geq 2\\) and \\(n_0 \\geq 1\\) But, the proof is not complete yet. Reminder: Proof by induction: 1.Prove the base cases \\(\\Longleftarrow\\) haven\u2019t proved the base cases yet 2.Inductive hypothesis for smaller sizes 3.Prove the general case","title":"Substitution Method: Example (3)"},{"location":"tr/week-2/ce100-week-2-recurrence/#substitution-method-example-4","text":"We need to prove the base cases Base: \\(T(n) = \\Theta(1)\\) for small \\(n\\) (e.g. for \\(n = n_0\\) ) We should show that: \\(\\Theta(1) \\leq cn^3\\) for \\(n = n_0\\) , This holds if we pick \\(c\\) big enough So, the proof of \\(T(n) = O(n^3)\\) is complete But, is this a tight bound?","title":"Substitution Method: Example (4)"},{"location":"tr/week-2/ce100-week-2-recurrence/#example-a-tighter-upper-bound-1","text":"Original recurrence: \\(T(n) = 4T(n/2) + n\\) Try to prove that \\(T(n) = O(n^2)\\) , i.e. \\(T(n) \\leq cn^2\\) for all \\(n \\geq n_0\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\)","title":"Example: A tighter upper bound? (1)"},{"location":"tr/week-2/ce100-week-2-recurrence/#example-a-tighter-upper-bound-2","text":"Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\) \\[ \\begin{align*} T(n) & = 4T(n/2) + n \\\\ & \\leq 4c(n/2)^2 + n \\\\ & = cn^2 + n \\\\ & = O(n2) \\Longleftarrow \\text{ Wrong! We must prove exactly} \\end{align*} \\]","title":"Example: A tighter upper bound? (2)"},{"location":"tr/week-2/ce100-week-2-recurrence/#example-a-tighter-upper-bound-3","text":"Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq ck^2\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq cn^2\\) So far, we have: \\(T(n) \\leq cn^2 + n\\) No matter which positive c value we choose, this does not show that \\(T(n) \\leq cn^2\\) Proof failed?","title":"Example: A tighter upper bound? (3)"},{"location":"tr/week-2/ce100-week-2-recurrence/#example-a-tighter-upper-bound-4","text":"What was the problem? The inductive hypothesis was not strong enough Idea: Start with a stronger inductive hypothesis Subtract a low-order term Inductive hypothesis: \\(T(k) \\leq c_1k^2 \u2013 c_2k\\) for \\(k < n\\) Prove the general case: \\(T(n) \\leq c_1n^2 - c_2n\\)","title":"Example: A tighter upper bound? (4)"},{"location":"tr/week-2/ce100-week-2-recurrence/#example-a-tighter-upper-bound-5","text":"Original recurrence: \\(T(n) = 4T(n/2) + n\\) Ind. hyp: Assume that \\(T(k) \\leq c_1k^2 \u2013 c_2k\\) for \\(k < n\\) Prove the general case: \\(T(n) \u2264 c_1n^2 \u2013 c_2n\\) \\[ \\begin{align*} T(n) & = 4T(n/2) + n \\\\ & \\leq 4 (c_1(n/2)^2 \u2013 c_2(n/2)) + n \\\\ & = c_1n^2 \u2013 2c_2n + n \\\\ & = c_1n^2 \u2013 c_2n \u2013 (c_2n \u2013 n) \\\\ & \\leq c_1n^2 \u2013 c_2n \\text{ for } n(c_2 \u2013 1) \\geq 0 \\\\ & \\text{choose } c2 \\geq 1 \\end{align*} \\]","title":"Example: A tighter upper bound? (5)"},{"location":"tr/week-2/ce100-week-2-recurrence/#example-a-tighter-upper-bound-6","text":"We now need to prove $$ T(n) \\leq c_1n^2 \u2013 c_2n $$ for the base cases. \\(T(n) = \\Theta(1) \\text{ for } 1 \\leq n \\leq n_0\\) (implicit assumption) \\(\\Theta(1) \\leq c_1n^2 \u2013 c_2n\\) for \\(n\\) small enough (e.g. \\(n = n_0\\) ) We can choose c1 large enough to make this hold We have proved that \\(T(n) = O(n^2)\\) section{ font-size: 25px; }","title":"Example: A tighter upper bound? (6)"},{"location":"tr/week-2/ce100-week-2-recurrence/#substitution-method-example-2-1","text":"For the recurrence \\(T(n) = 4T(n/2) + n\\) , prove that \\(T(n) = \\Omega(n^2)\\) i.e. \\(T(n) \u2265 cn^2\\) for any \\(n \\geq n_0\\) Ind. hyp: \\(T(k) \\geq ck^2\\) for any \\(k < n\\) Prove general case: \\(T(n) \\geq cn^2\\) \\(T(n) = 4T(n/2) + n\\) \\(\\geq 4c (n/2)^2 + n\\) \\(= cn^2 + n\\) \\(\\geq cn^2\\) since \\(n > 0\\) Proof succeeded \u2013 no need to strengthen the ind. hyp as in the last example","title":"Substitution Method: Example 2 (1)"},{"location":"tr/week-2/ce100-week-2-recurrence/#substitution-method-example-2-2","text":"We now need to prove that \\(T(n) \u2265 cn^2\\) for the base cases \\(T(n) = \\Theta(1)\\) for \\(1 \\leq n \\leq n_0\\) (implicit assumption) \\(\\Theta(1) \\geq cn^2\\) for \\(n = n_0\\) \\(n_0\\) is sufficiently small (i.e. constant) We can choose \\(c\\) small enough for this to hold We have proved that \\(T(n) = \\Omega(n^2)\\)","title":"Substitution Method: Example 2 (2)"},{"location":"tr/week-2/ce100-week-2-recurrence/#substitution-method-summary","text":"Guess the asymptotic complexity Prove your guess using induction Assume inductive hypothesis holds for \\(k < n\\) Try to prove the general case for \\(n\\) Note: \\(MUST\\) prove the \\(EXACT\\) inequality \\(CANNOT\\) ignore lower order terms, If the proof fails, strengthen the ind. hyp. and try again Prove the base cases (usually straightforward)","title":"Substitution Method - Summary"},{"location":"tr/week-2/ce100-week-2-recurrence/#recursion-tree-method","text":"A recursion tree models the runtime costs of a recursive execution of an algorithm. The recursion tree method is good for generating guesses for the substitution method. The recursion-tree method can be unreliable. Not suitable for formal proofs The recursion-tree method promotes intuition, however.","title":"Recursion Tree Method"},{"location":"tr/week-2/ce100-week-2-recurrence/#solve-recurrence-1-tn2tn2thetan","text":"","title":"Solve Recurrence (1) : \\(T(n)=2T(n/2)+\\Theta(n)\\)"},{"location":"tr/week-2/ce100-week-2-recurrence/#solve-recurrence-2-tn2tn2thetan","text":"","title":"Solve Recurrence (2) : \\(T(n)=2T(n/2)+\\Theta(n)\\)"},{"location":"tr/week-2/ce100-week-2-recurrence/#solve-recurrence-3-tn2tn2thetan","text":"","title":"Solve Recurrence (3) : \\(T(n)=2T(n/2)+\\Theta(n)\\)"},{"location":"tr/week-2/ce100-week-2-recurrence/#example-of-recursion-tree-1","text":"Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\)","title":"Example of Recursion Tree (1)"},{"location":"tr/week-2/ce100-week-2-recurrence/#example-of-recursion-tree-2","text":"Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\)","title":"Example of Recursion Tree (2)"},{"location":"tr/week-2/ce100-week-2-recurrence/#example-of-recursion-tree-3","text":"Solve \\(T(n) = T(n/4) + T(n/2) + n^2\\)","title":"Example of Recursion Tree (3)"},{"location":"tr/week-2/ce100-week-2-recurrence/#the-master-method","text":"A powerful black-box method to solve recurrences. The master method applies to recurrences of the form \\(T(n) = aT(n/b) + f (n)\\) where \\(a \\geq 1, b > 1\\) , and \\(f\\) is asymptotically positive .","title":"The Master Method"},{"location":"tr/week-2/ce100-week-2-recurrence/#the-master-method-3-cases","text":"(TODO : Add Notes ) Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Compare \\(f(n)\\) with \\(n^{log_b^a}\\) Intuitively: Case 1: \\(f(n)\\) grows polynomially slower than \\(n^{log_b^a}\\) Case 2: \\(f(n)\\) grows at the same rate as \\(n^{log_b^a}\\) Case 3: \\(f(n)\\) grows polynomially faster than \\(n^{log_b^a}\\)","title":"The Master Method: 3 Cases"},{"location":"tr/week-2/ce100-week-2-recurrence/#the-master-method-case-1-bigger","text":"Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon})\\) for some constant \\(\\varepsilon>0\\) i.e., \\(f(n)\\) grows polynomialy slower than \\(n^{log_b^a}\\) (by an \\(n^{\\varepsilon}\\) factor) Solution: \\(T(n)=\\Theta(n^{log_b^a})\\)","title":"The Master Method: Case 1 (Bigger)"},{"location":"tr/week-2/ce100-week-2-recurrence/#the-master-method-case-2-simple-version-equal","text":"Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(1)\\) i.e., \\(f(n)\\) and \\(n^{log_b^a}\\) grow at similar rates Solution: \\(T(n)=\\Theta(n^{log_b^a}lgn)\\)","title":"The Master Method: Case 2 (Simple Version) (Equal)"},{"location":"tr/week-2/ce100-week-2-recurrence/#the-master-method-case-3-smaller","text":"Case 3: \\(\\frac{f(n)}{n^{log_b^a}}=\\Omega(n^{\\varepsilon})\\) for some constant \\(\\varepsilon > 0\\) i.e., \\(f(n)\\) grows polynomialy faster than \\(n^{log_b^a}\\) (by an \\(n^{\\varepsilon}\\) factor) and the following regularity condition holds: \\(af(n/b) \\leq cf(n)\\) for some constant \\(c<1\\) Solution: \\(T(n)=\\Theta(f(n))\\)","title":"The Master Method: Case 3 (Smaller)"},{"location":"tr/week-2/ce100-week-2-recurrence/#the-master-method-example-case-1-tn4tn2n","text":"\\(a=4\\) \\(b=2\\) \\(f(n)=n\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n\\) grows polynomially slower than \\(n^{log_b^a}=n^2\\) \\(\\frac{n^{log_b^a}}{f(n)}=\\frac{n^2}{n}=n=\\Omega(n^{\\varepsilon})\\) CASE-1: \\(T(n)=\\Theta(n^{log_b^a})=\\Theta(n^{log_2^4})=\\Theta(n^2)\\)","title":"The Master Method Example (case-1) : \\(T(n)=4T(n/2)+n\\)"},{"location":"tr/week-2/ce100-week-2-recurrence/#the-master-method-example-case-2-tn4tn2n2","text":"\\(a=4\\) \\(b=2\\) \\(f(n)=n^2\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^2\\) grows at similar rate as \\(n^{log_b^a}=n^2\\) \\(f(n)=\\Theta(n^{log_b^a})=n^2\\) CASE-2: \\(T(n)=\\Theta(n^{log_b^a}lgn)=\\Theta(n^{log_2^4}lgn)=\\Theta(n^2lgn)\\)","title":"The Master Method Example (case-2) : \\(T(n)=4T(n/2)+n^2\\)"},{"location":"tr/week-2/ce100-week-2-recurrence/#the-master-method-example-case-3-1-tn4tn2n3","text":"\\(a=4\\) \\(b=2\\) \\(f(n)=n^3\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^3\\) grows polynomially faster than \\(n^{log_b^a}=n^2\\) \\(\\frac{f(n)}{n^{log_b^a}}=\\frac{n^3}{n^2}=n=\\Omega(n^{\\varepsilon})\\)","title":"The Master Method Example (case-3) (1) : \\(T(n)=4T(n/2)+n^3\\)"},{"location":"tr/week-2/ce100-week-2-recurrence/#the-master-method-example-case-3-2-tn4tn2n3-cont","text":"Seems like CASE 3, but need to check the regularity condition Regularity condition \\(af(n/b) \\leq cf(n)\\) for some constant \\(c<1\\) \\(4(n/2)^3 \\leq cn^3\\) for \\(c=1/2\\) CASE-3: \\(T(n)=\\Theta(f(n))\\) \\(\\Longrightarrow\\) \\(T(n)=\\Theta(n^3)\\)","title":"The Master Method Example (case-3) (2) : \\(T(n)=4T(n/2)+n^3\\) (con't)"},{"location":"tr/week-2/ce100-week-2-recurrence/#the-master-method-example-na-case-tn4tn2n2lgn","text":"\\(a=4\\) \\(b=2\\) \\(f(n)=n^2lgn\\) \\(n^{log_b^a}=n^{log_2^4}=n^{log_2^{2^2}}=n^{2log_2^2}=n^2\\) \\(f(n)=n^2lgn\\) grows slower than \\(n^{log_b^a}=n^2\\) but is it polynomially slower? \\(\\frac{n^{log_b^a}{f(n)}}=\\frac{n^2}{\\frac{n^2}{lgn}}=lgn \\neq \\Omega(n^{\\varepsilon})\\) for any \\(\\varepsilon>0\\) is not CASE-1 Master Method does not apply!","title":"The Master Method Example (N/A case) : \\(T(n)=4T(n/2)+n^2lgn\\)"},{"location":"tr/week-2/ce100-week-2-recurrence/#the-master-method-case-2-general-version","text":"Recurrence : \\(T(n) = aT(n/b) + f(n)\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn)\\) for some constant \\(k \\geq 0\\) Solution : \\(T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\)","title":"The Master Method : Case 2 (General Version)"},{"location":"tr/week-2/ce100-week-2-recurrence/#general-method-akra-bazzi","text":"\\(T(n)=\\sum \\limits_{i=1}^k{a_iT(n/b_i)}+f(n)\\) Let \\(p\\) be the unique solution to \\(\\sum \\limits_{i=1}^k{(a_i/b^p_i)}=1\\) Then, the answers are the same as for the master method, but with \\(n^p\\) instead of \\(n^{log_b^a}\\) (Akra and Bazzi also prove an even more general result.)","title":"General Method (Akra-Bazzi)"},{"location":"tr/week-2/ce100-week-2-recurrence/#idea-of-master-theorem-1","text":"Recursion Tree:","title":"Idea of Master Theorem (1)"},{"location":"tr/week-2/ce100-week-2-recurrence/#idea-of-master-theorem-2","text":"CASE 1 : The weight increases geometrically from the root to the leaves. The leaves hold a constant fraction of the total weight. \\(n^{log_b^a}T(1)=\\Theta(n^{log_b^a})\\)","title":"Idea of Master Theorem (2)"},{"location":"tr/week-2/ce100-week-2-recurrence/#idea-of-master-theorem-3","text":"CASE 2 : \\((k = 0)\\) The weight is approximately the same on each of the \\(log_bn\\) levels. \\(n^{log_b^a}T(1)=\\Theta(n^{log_b^a}lgn)\\)","title":"Idea of Master Theorem (3)"},{"location":"tr/week-2/ce100-week-2-recurrence/#idea-of-master-theorem-4","text":"CASE 3 : The weight decreases geometrically from the root to the leaves. The root holds a constant fraction of the total weight. \\(n^{log_b^a}T(1)=\\Theta(f(n))\\)","title":"Idea of Master Theorem (4)"},{"location":"tr/week-2/ce100-week-2-recurrence/#proof-of-master-theorem-case-1-and-case-2","text":"Recall from the recursion tree (note \\(h = lg_bn =\\text{tree height}\\) ) \\(\\text{Leaf Cost}=\\Theta(n^{log_b^a})\\) \\(\\text{Non-leaf Cost}=g(n)=\\sum \\limits_{i=0}^{h-1}a^if(n/{b^i})\\) \\(T(n)=\\text{Leaf Cost} + \\text{Non-leaf Cost}\\) \\(T(n)=\\Theta(n^{log_b^a}) + \\sum \\limits_{i=0}^{h-1}a^if(n/{b^i})\\)","title":"Proof of Master Theorem: Case 1 and Case 2"},{"location":"tr/week-2/ce100-week-2-recurrence/#proof-of-master-theorem-case-1-1","text":"\\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon})\\) for some \\(\\varepsilon>0\\) \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow O(n^{-\\varepsilon}) \\Longrightarrow f(n) = O(n^{log_b^{a-\\varepsilon}})\\) \\(g(n)=\\sum \\limits_{i=0}^{h-1}a^iO((n/{b^i})^{log_b^{a-\\varepsilon}})=O(\\sum \\limits_{i=0}^{h-1}a^i(n/{b^i})^{log_b^{a-\\varepsilon}})\\) \\(O(n^{log_b^{a-\\varepsilon}}\\sum \\limits_{i=0}^{h-1}a^ib^{i\\varepsilon}/b^{ilog_b^{a-\\varepsilon}})\\)","title":"Proof of Master Theorem Case 1 (1)"},{"location":"tr/week-2/ce100-week-2-recurrence/#proof-of-master-theorem-case-1-2","text":"\\(\\sum \\limits_{i=0}^{h-1} \\frac{a^ib^{i\\varepsilon}}{b^{ilog_b^a}} =\\sum \\limits_{i=0}^{h-1} a^i\\frac{(b^\\varepsilon)^i}{(b^{log_b^a})^i} =\\sum a^i\\frac{b^{i\\varepsilon}}{a^i}=\\sum \\limits_{i=0}^{h-1}(b^{\\varepsilon})^i\\) = An increasing geometric series since \\(b > 1\\) \\(\\frac{b^{h\\varepsilon}-1}{b^{\\varepsilon}-1}=\\frac{(b^h)^{\\varepsilon}-1}{b^{\\varepsilon}-1} = \\frac{(b^{log_b^n})^{\\varepsilon}-1}{b^{\\varepsilon}-1}=\\frac{n^{\\varepsilon}-1}{b^{\\varepsilon}-1} = O(n^{\\varepsilon})\\)","title":"Proof of Master Theorem Case 1 (2)"},{"location":"tr/week-2/ce100-week-2-recurrence/#proof-of-master-theorem-case-1-3","text":"\\(g(n)=O(n^{log_b{a-\\varepsilon}}O(n^{\\varepsilon}))=O(\\frac{n^{log_b^a}}{n^{\\varepsilon}}O(n^{\\varepsilon}))=O(n^{log_b^a})\\) \\(T(n)=\\Theta(n^{log_b^a})+g(n)=\\Theta(n^{log_b^a})+O(n^{log_b^a})=\\Theta(n^{log_b^a})\\) Q.E.D. (Quod Erat Demonstrandum) section{ font-size: 22px; }","title":"Proof of Master Theorem Case 1 (3)"},{"location":"tr/week-2/ce100-week-2-recurrence/#proof-of-master-theorem-case-2-limited-to-k0","text":"\\(\\frac{f(n)}{n^log_b^a}=\\Theta(lg^0n)=\\Theta(1) \\Longrightarrow f(n)=\\Theta(n^{log_b^a}) \\Longrightarrow f(n/b^i)=\\Theta((n/b^i)^{log_b^a})\\) \\(g(n)=\\sum \\limits_{i=0}^{h-1}a^i\\Theta((n/b^i)^{log_b^a})\\) \\(= \\Theta(\\sum \\limits_{i=0}^{h-1}a^i\\frac{n^{log_b^a}}{b^{ilog_b^a}})\\) \\(=\\Theta(n^{log_b^a}\\sum \\limits_{i=0}^{h-1}a^i\\frac{1}{(b^{log_b^a})^i})\\) \\(=\\Theta(n^{log_b^a}\\sum \\limits_{i=0}^{h-1}a^i\\frac{1}{a^i})\\) \\(=\\Theta(n^{log_b^a}\\sum \\limits_{i=0}^{log_b^{n-1}}1) = \\Theta(n^{log_b^a}log_bn)=\\Theta(n^{log_b^a}lgn)\\) \\(T(n)=n^{log_b^a}+\\Theta(n^{log_b^a}lgn)\\) \\(=\\Theta(n^{log_b^a}lgn)\\) Q.E.D.","title":"Proof of Master Theorem Case 2 (limited to k=0)"},{"location":"tr/week-2/ce100-week-2-recurrence/#the-divide-and-conquer-design-paradigm-1","text":"","title":"The Divide-and-Conquer Design Paradigm (1)"},{"location":"tr/week-2/ce100-week-2-recurrence/#the-divide-and-conquer-design-paradigm-2","text":"Divide we divide the problem into a number of subproblems. Conquer we solve the subproblems recursively. BaseCase solve by Brute-Force Combine subproblem solutions to the original problem.","title":"The Divide-and-Conquer Design Paradigm (2)"},{"location":"tr/week-2/ce100-week-2-recurrence/#the-divide-and-conquer-design-paradigm-3","text":"\\(a=\\text{subproblem}\\) \\(1/b=\\text{each size of the problem}\\) \\[ T(n)=\\begin{cases} \\Theta(1) & \\text{if} & n \\leq c & (basecase) \\\\ aT(n/b)+D(n)+C(n) & \\text{otherwise} \\end{cases} \\] Merge-Sort \\[ T(n)=\\begin{cases} \\Theta(1) & & n = 1 \\\\ 2T(n/2)+\\Theta(n) & \\text{if} & n>1 \\end{cases} \\] \\(T(n)=\\Theta(nlgn)\\)","title":"The Divide-and-Conquer Design Paradigm (3)"},{"location":"tr/week-2/ce100-week-2-recurrence/#selection-sort-algorithm","text":"SELECTION - SORT ( A ) n = A.length ; for j = 1 to n -1 smallest = j ; for i = j +1 to n if A [ i ] < A [ smallest ] smallest = i ; endfor exchange A [ j ] with A [ smallest ] endfor","title":"Selection Sort Algorithm"},{"location":"tr/week-2/ce100-week-2-recurrence/#selection-sort-algorithm_1","text":"\\[ T(n)=\\begin{cases} \\Theta(1) & & n = 1 \\\\ T(n-1)+\\Theta(n) & \\text{if} & n>1 \\end{cases} \\] Sequential Series \\[ cost = n(n+1)/2 = {1/2}n^2 +{1/2}n \\] Drop low-order terms Ignore the constant coefficient in the leading term \\[ T(n)=\\Theta(n^2) \\]","title":"Selection Sort Algorithm"},{"location":"tr/week-2/ce100-week-2-recurrence/#merge-sort-algorithm-initial-setup","text":"Merge Sort is a recursive sorting algorithm, for initial case we need to call Merge-Sort(A,1,n) for sorting \\(A[1..n]\\) initial case A : Array p : 1 ( offset ) r : n ( length ) Merge - Sort ( A , 1 , n ) section{ font-size: 25px; }","title":"Merge Sort Algorithm (initial setup)"},{"location":"tr/week-2/ce100-week-2-recurrence/#merge-sort-algorithm-internal-iterations","text":"internal iterations \\(p = start-point\\) \\(q = mid-point\\) \\(r = end-point\\) A : Array p : offset r : length Merge - Sort ( A , p , r ) if p = r then ( CHECK FOR BASE - CASE ) return else q = floor (( p + r ) / 2 ) ( DIVIDE ) Merge - Sort ( A , p , q ) ( CONQUER ) Merge - Sort ( A , q +1 , r ) ( CONQUER ) Merge ( A , p , q , r ) ( COMBINE ) endif","title":"Merge Sort Algorithm (internal iterations)"},{"location":"tr/week-2/ce100-week-2-recurrence/#merge-sort-combine-algorithm-1","text":"Merge ( A , p , q , r ) n1 = q - p +1 n2 = r - q // allocate left and right arrays // increment will be from left to right // left part will be bigger than right part L [ 1 ...n1 +1 ] // left array R [ 1 ...n2 +1 ] // right array // copy left part of array for i = 1 to n1 L [ i ] = A [ p + i -1 ] // copy right part of array for j = 1 to n2 R [ j ] = A [ q + j ] // put end items maximum values for termination L [ n1 +1 ] = inf R [ n2 +1 ] = inf i = 1 , j = 1 for k = p to r if L [ i ] <= R [ j ] A [ k ] = L [ i ] i = i +1 else A [ k ] = R [ j ] j = j +1","title":"Merge Sort Combine Algorithm (1)"},{"location":"tr/week-2/ce100-week-2-recurrence/#example-merge-sort","text":"Divide: Trivial. Conquer: Recursively sort 2 subarrays. Combine: Linear- time merge. \\(T(n)=2T(n/2)+\\Theta(n)\\) Subproblems \\(\\Longrightarrow 2\\) Subproblemsize \\(\\Longrightarrow n/2\\) Work dividing and combining \\(\\Longrightarrow\\Theta(n)\\)","title":"Example : Merge Sort"},{"location":"tr/week-2/ce100-week-2-recurrence/#master-theorem-reminder","text":"\\(T(n) = aT(n/b) + f(n)\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(n^{log_b^a})\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) Case 3: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(f(n))\\) and \\(af(n/b) \\leq cf(n)\\) for \\(c<1\\)","title":"Master Theorem: Reminder"},{"location":"tr/week-2/ce100-week-2-recurrence/#merge-sort-solving-the-recurrence","text":"\\(T(n)=2T(n/2)+\\Theta(n)\\) \\(a=2,b=2,f(n)=\\Theta(n),n^{log_b^a}=n\\) Case-2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(nlgn)\\)","title":"Merge Sort: Solving the Recurrence"},{"location":"tr/week-2/ce100-week-2-recurrence/#binary-search-1","text":"Find an element in a sorted array: 1. Divide: Check middle element. 2. Conquer: Recursively search 1 subarray. 3. Combine: Trivial.","title":"Binary Search (1)"},{"location":"tr/week-2/ce100-week-2-recurrence/#binary-search-2","text":"\\[ \\text{PARENT} = \\lfloor i/2 \\rfloor \\] \\[ \\text{LEFT-CHILD} = 2i, \\text{ 2i>n} \\] \\[ \\text{RIGHT-CHILD} = 2i+1, \\text{ 2i>n} \\]","title":"Binary Search (2)"},{"location":"tr/week-2/ce100-week-2-recurrence/#binary-search-3-iterative","text":"ITERATIVE - BINARY - SEARCH ( A , V , low , high ) while low <= high mid = floor (( low + high ) / 2 ); if v == A [ mid ] return mid ; elseif v > A [ mid ] low = mid + 1 ; else high = mid - 1 ; endwhile return NIL","title":"Binary Search (3) : Iterative"},{"location":"tr/week-2/ce100-week-2-recurrence/#binary-search-4-recursive","text":"RECURSIVE - BINARY - SEARCH ( A , V , low , high ) if low > high return NIL ; endif mid = floor (( low + high ) / 2 ); if v == A [ mid ] return mid ; elseif v > A [ mid ] return RECURSIVE - BINARY - SEARCH ( A , V , mid +1 , high ); else return RECURSIVE - BINARY - SEARCH ( A , V , low , mid -1 ); endif","title":"Binary Search (4): Recursive"},{"location":"tr/week-2/ce100-week-2-recurrence/#binary-search-5-recursive","text":"\\[ T(n)=T(n/2)+\\Theta(1) \\Longrightarrow T(n)=\\Theta(lgn) \\]","title":"Binary Search (5): Recursive"},{"location":"tr/week-2/ce100-week-2-recurrence/#binary-search-6-example-find-9","text":"","title":"Binary Search (6): Example (Find 9)"},{"location":"tr/week-2/ce100-week-2-recurrence/#recurrence-for-binary-search-7","text":"\\(T(n)=1T(n/2)+\\Theta(1)\\) Subproblems \\(\\Longrightarrow 1\\) Subproblemsize \\(\\Longrightarrow n/2\\) Work dividing and combining \\(\\Longrightarrow\\Theta(1)\\)","title":"Recurrence for Binary Search (7)"},{"location":"tr/week-2/ce100-week-2-recurrence/#binary-search-solving-the-recurrence-8","text":"\\(T(n) = T(n/2) + \\Theta(1)\\) \\(a = 1,b = 2,f(n) = \\Theta(1) \\Longrightarrow n^{log_b^a} = n^0=1\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(lgn)\\)","title":"Binary Search: Solving the Recurrence (8)"},{"location":"tr/week-2/ce100-week-2-recurrence/#powering-a-number-divide-conquer-1","text":"Problem : Compute an, where n is a natural number NAIVE - POWER ( a , n ) powerVal = 1 ; for i = 1 to n powerVal = powerVal * a ; endfor return powerVal ; What is the complexity? \\(\\Longrightarrow T(n)=\\Theta(n)\\)","title":"Powering a Number: Divide &amp; Conquer (1)"},{"location":"tr/week-2/ce100-week-2-recurrence/#powering-a-number-divide-conquer-2","text":"Basic Idea: \\[ a^n=\\begin{cases} a^{n/2}*a^{n/2} & \\text{if n is even} \\\\ a^{(n-1)/2}*a^{(n-1)/2}*a & \\text{if n is odd} \\end{cases} \\]","title":"Powering a Number: Divide &amp; Conquer (2)"},{"location":"tr/week-2/ce100-week-2-recurrence/#powering-a-number-divide-conquer-3","text":"POWER ( a , n ) if n = 0 then return 1 ; else if n is even then val = POWER ( a , n / 2 ); return val * val ; else if n is odd then val = POWER ( a ,( n -1 ) / 2 ) return val * val * a ; endif","title":"Powering a Number: Divide &amp; Conquer (3)"},{"location":"tr/week-2/ce100-week-2-recurrence/#powering-a-number-solving-the-recurrence-4","text":"\\(T(n) = T(n/2) + \\Theta(1)\\) \\(a = 1,b = 2,f(n) = \\Theta(1) \\Longrightarrow n^{log_b^a} = n^0=1\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(lg^kn) \\Longrightarrow T(n)=\\Theta(n^{log_b^a}lg^{k+1}n)\\) holds for \\(k=0\\) \\(T(n)=\\Theta(lgn)\\)","title":"Powering a Number: Solving the Recurrence (4)"},{"location":"tr/week-2/ce100-week-2-recurrence/#correctness-proofs-for-divide-and-conquer-algorithms","text":"Proof by induction commonly used for Divide and Conquer Algorithms Base case: Show that the algorithm is correct when the recursion bottoms out (i.e., for sufficiently small n) Inductive hypothesis: Assume the alg. is correct for any recursive call on any smaller subproblem of size \\(k\\) , \\((k < n)\\) General case: Based on the inductive hypothesis, prove that the alg. is correct for any input of size n section{ font-size: 25px; }","title":"Correctness Proofs for Divide and Conquer Algorithms"},{"location":"tr/week-2/ce100-week-2-recurrence/#example-correctness-proof-powering-a-number","text":"Base Case: \\(POWER(a, 0)\\) is correct, because it returns \\(1\\) Ind. Hyp: Assume \\(POWER(a, k)\\) is correct for any \\(k<n\\) General Case: In \\(POWER(a,n)\\) function: If \\(n\\) is \\(even\\) : \\(val = a^{n/2}\\) (due to ind. hyp.) it returns \\(val*val = a^n\\) If \\(n\\) is \\(odd\\) : \\(val = a^{(n-1)/2}\\) (due to ind. hyp.) it returns \\(val*val*a = a^n\\) The correctness proof is complete","title":"Example Correctness Proof: Powering a Number"},{"location":"tr/week-2/ce100-week-2-recurrence/#references","text":"Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) Insertion Sort - GeeksforGeeks NIST Dictionary of Algorithms and Data Structures NIST - Dictionary of Algorithms and Data Structures NIST - big-O notation NIST - big-Omega notation \\(-End-Of-Week-2-Course-Module-\\)","title":"References"},{"location":"tr/week-3/ce100-week-3-matrix/","text":"CE100 Algorithms and Programming II \u00b6 Week-3 (Matrix Multiplication/ Quick Sort) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Matrix Multiplication / Quick Sort \u00b6 Outline (1) \u00b6 Matrix Multiplication Traditional Recursive Strassen Outline (2) \u00b6 Quicksort Hoare Partitioning Lomuto Partitioning Recursive Sorting Outline (3) \u00b6 Quicksort Analysis Randomized Quicksort Randomized Selection Recursive Medians Matrix Multiplication (1) \u00b6 Input: \\(A=[a_{ij}],B=[b_{ij}]\\) Output: \\(C=[c_{ij}]=A \\cdot B\\) \\(\\Longrightarrow i,j=1,2,3, \\dots, n\\) \\[ \\begin{bmatrix} c_{11} & c_{12} & \\dots & c_{1n} \\\\ c_{21} & c_{22} & \\dots & c_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ c_{n1} & c_{n2} & \\dots & c_{nn} \\\\ \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ a_{n1} & a_{n2} & \\dots & a_{nn} \\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} b_{11} & b_{12} & \\dots & b_{1n} \\\\ b_{21} & b_{22} & \\dots & b_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ b_{n1} & a_{n2} & \\dots & b_{nn} \\\\ \\end{bmatrix} \\] Matrix Multiplication (2) \u00b6 \\(c_{ij}=\\sum \\limits_{1\\leq k \\leq n}^{}a_{ik}.b_{kj}\\) Matrix Multiplication: Standard Algorithm \u00b6 Running Time: \\(\\Theta(n^3)\\) for i = 1 to n do for j = 1 to n do C [ i , j ] = 0 for k = 1 to n do C [ i , j ] = C [ i , j ] + A [ i , k ] + B [ k , j ] endfor endfor endfor Matrix Multiplication: Divide & Conquer (1) \u00b6 IDEA: Divide the \\(nxn\\) matrix into \\(2x2\\) matrix of \\((n/2)x(n/2)\\) submatrices. Matrix Multiplication: Divide & Conquer (2) \u00b6 \\[ \\begin{bmatrix} c_{11} & c_{12} \\\\ c_{21} & c_{22} \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} \\cdot \\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{bmatrix} \\] \\[ \\text{8 mults and 4 adds of (n/2)*(n/2) submatrices}= \\begin{cases} c_{11}=a_{11}b_{11}+a_{12}b_{21} \\\\ c_{21}=a_{21}b_{11}+a_{22}b_{21} \\\\ c_{12}=a_{11}b_{12}+a_{12}b_{22} \\\\ c_{22}=a_{21}b_{12}+a_{22}b_{22} \\end{cases} \\] Matrix Multiplication: Divide & Conquer (3) \u00b6 MATRIX - MULTIPLY ( A , B ) // Assuming that both A and B are nxn matrices if n == 1 then return A * B else // partition A , B , and C as shown before C [ 1 , 1 ] = MATRIX - MULTIPLY ( A [ 1 , 1 ], B [ 1 , 1 ]) + MATRIX - MULTIPLY ( A [ 1 , 2 ], B [ 2 , 1 ]); C [ 1 , 2 ] = MATRIX - MULTIPLY ( A [ 1 , 1 ], B [ 1 , 2 ]) + MATRIX - MULTIPLY ( A [ 1 , 2 ], B [ 2 , 2 ]); C [ 2 , 1 ] = MATRIX - MULTIPLY ( A [ 2 , 1 ], B [ 1 , 1 ]) + MATRIX - MULTIPLY ( A [ 2 , 2 ], B [ 2 , 1 ]); C [ 2 , 2 ] = MATRIX - MULTIPLY ( A [ 2 , 1 ], B [ 1 , 2 ]) + MATRIX - MULTIPLY ( A [ 2 , 2 ], B [ 2 , 2 ]); endif return C Matrix Multiplication: Divide & Conquer Analysis \u00b6 \\(T(n) = 8T(n/2) + \\Theta(n^2)\\) \\(8\\) recursive calls \\(\\Longrightarrow 8T(\\cdots)\\) each problem has size \\(n/2\\) \\(\\Longrightarrow \\cdots T(n/2)\\) Submatrix addition \\(\\Longrightarrow \\Theta(n^2)\\) Matrix Multiplication: Solving the Recurrence \u00b6 \\(T(n) = 8T(n/2) + \\Theta(n^2)\\) \\(a=8\\) , \\(b=2\\) \\(f(n)=\\Theta(n^2)\\) \\(n^{log_b^a}=n^3\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(n^{log_b^a})\\) Similar with ordinary (iterative) algorithm. Matrix Multiplication: Strassen\u2019s Idea (1) \u00b6 Compute \\(c_{11},c_{12},c_{21},c_{22}\\) using \\(7\\) recursive multiplications. In normal case we need \\(8\\) as below. \\[ \\begin{bmatrix} c_{11} & c_{12} \\\\ c_{21} & c_{22} \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} \\cdot \\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{bmatrix} \\] \\[ \\text{8 mults and 4 adds of (n/2)*(n/2) submatrices}= \\begin{cases} c_{11}=a_{11}b_{11}+a_{12}b_{21} \\\\ c_{21}=a_{21}b_{11}+a_{22}b_{21} \\\\ c_{12}=a_{11}b_{12}+a_{12}b_{22} \\\\ c_{22}=a_{21}b_{12}+a_{22}b_{22} \\end{cases} \\] section{ font-size: 25px; } Matrix Multiplication: Strassen\u2019s Idea (2) \u00b6 Reminder: Each submatrix is of size \\((n/2)*(n/2)\\) Each add/sub operation takes \\(\\Theta(n^2)\\) time Compute \\(P1 \\dots P7\\) using \\(7\\) recursive calls to matrix-multiply \\[ \\begin{align*} P_1 & = a_{11} * (b_{12} - b_{22} ) \\\\ P_2 & = (a_{11} + a_{12} ) * b_{22} \\\\ P_3 & = (a_{21} + a_{22} ) * b_{11} \\\\ P_4 & = a_{22} * (b_{21} - b_{11} ) \\\\ P_5 & = (a_{11} + a_{22} ) * (b_{11} + b_{22} ) \\\\ P_6 & = (a_{12} - a_{22} ) * (b_{21} + b_{22} ) \\\\ P_7 & = ( a_{11} - a_{21} ) * (b_{11} + b_{12} ) \\end{align*} \\] section{ font-size: 25px; } Matrix Multiplication: Strassen\u2019s Idea (3) \u00b6 \\[ \\begin{align*} P_1 &= a_{11} * (b_{12} - b_{22} ) \\\\ P_2 &= (a_{11} + a_{12} ) * b_{22} \\\\ P_3 &= (a_{21} + a_{22} ) * b_{11} \\\\ P_4 &= a_{22} * (b_{21} - b_{11} ) \\\\ P_5 &= (a_{11} + a_{22} ) * (b_{11} + b_{22} ) \\\\ P_6 &= (a_{12} - a_{22} ) * (b_{21} + b_{22} ) \\\\ P_7 &= ( a_{11} - a_{21} ) * (b_{11} + b_{12} ) \\end{align*} \\] How to compute \\(c_{ij}\\) using \\(P1 \\dots P7\\) ? \\[ \\begin{align*} c_{11} & = P_5 + P_4 \u2013 P_2 + P_6 \\\\ c_{12} & = P_1 + P_2 \\\\ c_{21} & = P_3 + P_4 \\\\ c_{22} & = P_5 + P_1 \u2013 P_3 \u2013 P_7 \\end{align*} \\] Matrix Multiplication: Strassen\u2019s Idea (4) \u00b6 \\(7\\) recursive multiply calls \\(18\\) add/sub operations Matrix Multiplication: Strassen\u2019s Idea (5) \u00b6 e.g. Show that \\(c_{12} = P_1+P_2\\) : \\[ \\begin{align*} c_{12} & = P_1 + P_2 \\\\ &= a_{11}(b_{12}\u2013b_{22})+(a_{11}+a_{12})b_{22} \\\\ &= a_{11}b_{12}-a_{11}b_{22}+a_{11}b_{22}+a_{12}b_{22} \\\\ &= a_{11}b_{12}+a_{12}b_{22} \\end{align*} \\] Strassen\u2019s Algorithm \u00b6 Divide: Partition \\(A\\) and \\(B\\) into \\((n/2)*(n/2)\\) submatrices. Form terms to be multiplied using \\(+\\) and \\(-\\) . Conquer: Perform \\(7\\) multiplications of \\((n/2)*(n/2)\\) submatrices recursively. Combine: Form \\(C\\) using \\(+\\) and \\(\u2013\\) on \\((n/2)*(n/2)\\) submatrices. Recurrence: \\(T(n) = 7T(n/2) + \\Theta(n^2)\\) Strassen\u2019s Algorithm: Solving the Recurrence (1) \u00b6 \\(T(n) = 7T(n/2) + \\Theta(n^2)\\) \\(a=7\\) , \\(b=2\\) \\(f(n)=\\Theta(n^2)\\) \\(n^{log_b^a}=n^{lg7}\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(n^{log_b^a})\\) \\(T(n)=\\Theta(n^{log_2^7})\\) \\(2^3 = 8, 2^2=4\\) so \\(\\Longrightarrow log_2^7 \\approx 2.81\\) or use https://www.omnicalculator.com/math/log Strassen\u2019s Algorithm: Solving the Recurrence (2) \u00b6 The number \\(2.81\\) may not seem much smaller than \\(3\\) But, it is significant because the difference is in the exponent. Strassen\u2019s algorithm beats the ordinary algorithm on today\u2019s machines for \\(n \\geq 30\\) or so. Best to date: \\(\\Theta(n^{2.376 \\dots})\\) (of theoretical interest only) Maximum Subarray Problem \u00b6 Input: An array of values Output: The contiguous subarray that has the largest sum of elements Input array: \\([13][-3][-25][20][-3][-16][-23]\\overbrace{[18][20][-7][12]}^{\\textrm{max. contiguous subarray}}[-22][-4][7]\\) Maximum Subarray Problem: Divide & Conquer (1) \u00b6 Basic idea: Divide the input array into 2 from the middle Pick the best solution among the following: The max subarray of the left half The max subarray of the right half The max subarray crossing the mid-point Maximum Subarray Problem: Divide & Conquer (2) \u00b6 Maximum Subarray Problem: Divide & Conquer (3) \u00b6 Divide: Trivial (divide the array from the middle) Conquer: Recursively compute the max subarrays of the left and right halves Combine: Compute the max-subarray crossing the \\(mid-point\\) (can be done in \\(\\Theta(n)\\) time). Return the max among the following: the max subarray of the \\(\\text{left-subarray}\\) the max subarray of the \\(\\text{rightsubarray}\\) the max subarray crossing the \\(\\text{mid-point}\\) TODO : detailed solution in textbook... Conclusion : Divide & Conquer \u00b6 Divide and conquer is just one of several powerful techniques for algorithm design. Divide-and-conquer algorithms can be analyzed using recurrences and the master method (so practice this math). Can lead to more efficient algorithms Quicksort (1) \u00b6 One of the most-used algorithms in practice Proposed by C.A.R. Hoare in 1962. Divide-and-conquer algorithm In-place algorithm The additional space needed is O(1) The sorted array is returned in the input array Reminder: Insertion-sort is also an in-place algorithm, but Merge-Sort is not in-place. Very practical Quicksort (2) \u00b6 Divide: Partition the array into 2 subarrays such that elements in the lower part \\(\\leq\\) elements in the higher part Conquer: Recursively sort 2 subarrays Combine: Trivial (because in-place) Key: Linear-time \\((\\Theta(n))\\) partitioning algorithm section{ font-size: 25px; } Divide: Partition the array around a pivot element \u00b6 Choose a pivot element \\(x\\) Rearrange the array such that: Left subarray: All elements \\(\\leq x\\) Right subarray: All elements \\(\\geq x\\) Conquer: Recursively Sort the Subarrays \u00b6 Note: Everything in the left subarray \u2264 everything in the right subarray Note: Combine is trivial after conquer. Array already sorted. section{ font-size: 25px; } Two partitioning algorithms \u00b6 Hoare\u2019s algorithm: Partitions around the first element of subarray \\((pivot = x = A[p])\\) Lomuto\u2019s algorithm: Partitions around the last element of subarray \\((pivot = x = A[r])\\) Hoare\u2019s Partitioning Algorithm (1) \u00b6 Choose a pivot element: \\(pivot = x = A[p]\\) Grow two regions: from left to right: \\(A[p \\dots i]\\) from right to left: \\(A[j \\dots r]\\) such that: every element in \\(A[p \\dots i] \\leq\\) pivot every element in \\(A[p \\dots i] \\geq\\) pivot Hoare\u2019s Partitioning Algorithm (2) \u00b6 section{ font-size: 25px; } Hoare\u2019s Partitioning Algorithm (3) \u00b6 Elements are exchanged when \\(A[i]\\) is too large to belong to the left region \\(A[j]\\) is too small to belong to the right region assuming that the inequality is strict The two regions \\(A[p \\dots i]\\) and \\(A[j \\dots r]\\) grow until \\(A[i] \\geq pivot \\geq A[j]\\) H - PARTITION ( A , p , r ) pivot = A [ p ] i = p - 1 j = r - 1 while true do repeat j = j - 1 until A [ j ] <= pivot repeat i = i - 1 until A [ i ] <= pivot if i < j then exchange A [ i ] with A [ j ] else return j Hoare\u2019s Partitioning Algorithm Example (Step-1) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-2) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-3) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-4) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-5) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-6) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-7) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-8) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-9) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-10) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-11) \u00b6 Hoare\u2019s Partitioning Algorithm Example (Step-12) \u00b6 section{ font-size: 25px; } Hoare\u2019s Partitioning Algorithm - Notes \u00b6 Elements are exchanged when \\(A[i]\\) is too large to belong to the left region \\(A[j]\\) is too small to belong to the right region assuming that the inequality is strict The two regions \\(A[p \\dots i]\\) and \\(A[j \\dots r]\\) grow until \\(A[i] \\geq pivot \\geq A[j]\\) The asymptotic runtime of Hoare\u2019s partitioning algorithm \\(\\Theta(n)\\) H - PARTITION ( A , p , r ) pivot = A [ p ] i = p - 1 j = r - 1 while true do repeat j = j - 1 until A [ j ] <= pivot repeat i = i - 1 until A [ i ] <= pivot if i < j then exchange A [ i ] with A [ j ] else return j section{ font-size: 25px; } Quicksort with Hoare\u2019s Partitioning Algorithm \u00b6 QUICKSORT ( A , p , r ) if p < r then q = H - PARTITION ( A , p , r ) QUICKSORT ( A , p , q ) QUICKSORT ( A , q + 1 , r ) endif Initial invocation: QUICKSORT(A,1,n) section{ font-size: 25px; } Hoare\u2019s Partitioning Algorithm: Pivot Selection \u00b6 if we select pivot to be \\(A[r]\\) instead of \\(A[p]\\) in H-PARTITION Consider the example where \\(A[r]\\) is the largest element in the array: End of H-PARTITION: \\(i = j = r\\) In QUICKSORT: \\(q = r\\) So, recursive call to: QUICKSORT(A, p, q=r) infinite loop Correctness of Hoare\u2019s Algorithm (1) \u00b6 We need to prove \\(3\\) claims to show correctness: Indices \\(i\\) and \\(j\\) never reference \\(A\\) outside the interval \\(A[p \\dots r]\\) Split is always non-trivial; i.e., \\(j \\neq r\\) at termination Every element in \\(A[p \\dots j] \\leq\\) every element in \\(A[j+1 \\dots r]\\) at termination Correctness of Hoare\u2019s Algorithm (2) \u00b6 Notations: \\(k\\) : \\(\\#\\) of times the while-loop iterates until termination \\(i_m\\) : the value of index i at the end of iteration \\(m\\) \\(j_m\\) : the value of index j at the end of iteration \\(m\\) \\(x\\) : the value of the pivot element Note : We always have \\(i_1= p\\) and \\(p \\leq j_1 \\leq r\\) because \\(x = A[p]\\) Correctness of Hoare\u2019s Algorithm (3) \u00b6 Lemma 1: Either \\(i_k = j_k\\) or \\(i_k = j_k+1\\) at termination Proof of Lemma 1: The algorithm terminates when \\(i \\geq j\\) (the else condition). So, it is sufficient to prove that \\(i_k \u2013 j_k \\leq 1\\) There are \\(2\\) cases to consider: Case 1: \\(k = 1\\) , i.e. the algorithm terminates in a single iteration Case 2: \\(k > 1\\) , i.e. the alg. does not terminate in a single iter. By contradiction , assume there is a run with \\(i_k \u2013 j_k > 1\\) section{ font-size: 25px; } Correctness of Hoare\u2019s Algorithm (4) \u00b6 Original correctness claims: Indices \\(i\\) and \\(j\\) never reference A outside the interval \\(A[p \\dots r]\\) Split is always non-trivial; i.e., \\(j \\neq r\\) at termination Proof: For \\(k = 1\\) : Trivial because \\(i_1 = j_1 = p\\) ( see Case 1 in proof of Lemma 2 ) For \\(k > 1\\) : \\(i_k > p\\) and \\(j_k < r\\) ( due to the repeat-until loops moving indices ) \\(i_k \\leq r\\) and \\(j_k \\geq p\\) ( due to Lemma 1 and the statement above ) The proof of claims (a) and (b) complete section{ font-size: 25px; } Correctness of Hoare\u2019s Algorithm (5) \u00b6 Lemma 2: At the end of iteration \\(m\\) , where \\(m<k\\) ( i.e. m is not the last iteration ), we must have: \\(A[p \\dots i_m] \\leq x\\) and \\(A[j_m \\dots r] \\geq x\\) Proof of Lemma 2: Base case: \\(m=1\\) and \\(k>1\\) ( i.e. the alg. does not terminate in the first iter. ) Ind. Hyp.: At the end of iteration \\(m-1\\) , where \\(m<k\\) ( i.e. m is not the last iteration ), we must have: \\(A[p \\dots i_m-1] \\leq x\\) and \\(A[j_m-1 \\dots r] \\geq x\\) General case: The lemma holds for \\(m\\) , where \\(m < k\\) Proof of base case complete! Correctness of Hoare\u2019s Algorithm (6) \u00b6 Original correctness claim: \u00a9 Every element in \\(A[ \\dots j] \\leq\\) every element in \\(A[j+ \\dots r]\\) at termination Proof of claim \u00a9 There are \\(3\\) cases to consider: Case 1: \\(k=1\\) , i.e. the algorithm terminates in a single iteration Case 2: \\(k>1\\) and \\(i_k = j_k\\) Case 3: \\(k>1\\) and \\(i_k = j_k + 1\\) Lomuto\u2019s Partitioning Algorithm (1) \u00b6 Choose a pivot element: \\(pivot = x = A[r]\\) Grow two regions: from left to right: \\(A[p \\dots i]\\) from left to right: \\(A[i+1 \\dots j]\\) such that: every element in \\(A[p \\dots i] \\leq pivot\\) every element in \\(A[i+1 \\dots j] > pivot\\) Lomuto\u2019s Partitioning Algorithm (2) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-1) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-2) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-3) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-4) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-5) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-6) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-7) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-8) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-9) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-10) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-11) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-12) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-13) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-14) \u00b6 Lomuto\u2019s Partitioning Algorithm Ex. (Step-15) \u00b6 Quicksort with Lomuto\u2019s Partitioning Algorithm \u00b6 QUICKSORT ( A , p , r ) if p < r then q = L - PARTITION ( A , p , r ) QUICKSORT ( A , p , q - 1 ) QUICKSORT ( A , q + 1 , r ) endif Initial invocation: QUICKSORT(A,1,n) Comparison of Hoare\u2019s & Lomuto\u2019s Algorithms (1) \u00b6 Notation: \\(n=r-p+1\\) \\(pivot=A[p]\\) ( Hoare ) \\(pivot=A[r]\\) ( Lomuto ) \\(\\#\\) of element exchanges: \\(e(n)\\) Hoare: \\(0 \\geq e(n) \\geq \\lfloor \\frac{n}{2} \\rfloor\\) Best : \\(k=1\\) with \\(i_1=j_1=p\\) (i.e., \\(A[p+1 \\dots r]>pivot\\) ) Worst : \\(A[p+1 \\dots p+ \\lfloor \\frac{n}{2} \\rfloor - 1] \\geq pivot \\geq A[p+ \\lceil \\frac{n}{2} \\rceil \\dots r]\\) Lomuto : \\(1 \\leq e(n) \\leq n\\) Best : \\(A[p \\dots r -1]>pivot\\) Worst : \\(A[p \\dots r-1] \\leq pivot\\) Comparison of Hoare\u2019s & Lomuto\u2019s Algorithms (2) \u00b6 \\(\\#\\) of element comparisons: \\(c_e(n)\\) Hoare : \\(n+1 \\leq c_e(n) \\leq n+2\\) Best : \\(i_k=j_k\\) Worst : \\(i_k=j_k+1\\) Lomuto : \\(c_e(n)=n-1\\) \\(\\#\\) of index comparisons: \\(c_i(n)\\) Hoare : \\(1 \\leq c_i(n) \\leq \\lfloor \\frac{n}{2} \\rfloor + 1 | (c_i(n)=e(n)+1)\\) Lomuto : \\(c_i(n)=n-1\\) Comparison of Hoare\u2019s & Lomuto\u2019s Algorithms (3) \u00b6 \\(\\#\\) of index increment/decrement operations: \\(a(n)\\) Hoare : \\(n+1 \\leq a(n) \\leq n+2 | (a(n)=c_e(n))\\) Lomuto : \\(n \\leq a(n) \\leq 2n-1 | (a(n)=e(n)+(n-1))\\) Hoare\u2019s algorithm is in general faster Hoare behaves better when pivot is repeated in \\(A[p \\dots r]\\) Hoare : Evenly distributes them between left & right regions Lomuto : Puts all of them to the left region Analysis of Quicksort (1) \u00b6 QUICKSORT ( A , p , r ) if p < r then q = H - PARTITION ( A , p , r ) QUICKSORT ( A , p , q ) QUICKSORT ( A , q + 1 , r ) endif Initial invocation: QUICKSORT(A,1,n) Assume all elements are distinct in the following analysis Analysis of Quicksort (2) \u00b6 H-PARTITION always chooses \\(A[p]\\) (the first element) as the pivot. The runtime of QUICKSORT on an already-sorted array is \\(\\Theta(n^2)\\) Example: An Already Sorted Array \u00b6 Partitioning always leads to \\(2\\) parts of size \\(1\\) and \\(n-1\\) Worst Case Analysis of Quicksort \u00b6 Worst case is when the PARTITION algorithm always returns imbalanced partitions (of size \\(1\\) and \\(n-1\\) ) in every recursive call. This happens when the pivot is selected to be either the min or max element. This happens for H-PARTITION when the input array is already sorted or reverse sorted \\[ \\begin{align*} T(n) &= T(1) + T(n-1) + \u0398(n) \\\\ &= T(n-1) + \u0398(n) \\\\ &= \u0398(n2) \\end{align*} \\] Worst Case Recursion Tree \u00b6 \\[ T(n) = T(1) + T(n-1) + cn \\] Best Case Analysis (for intuition only) \u00b6 If we\u2019re extremely lucky, H-PARTITION splits the array evenly at every recursive call \\[ \\begin{align*} T(n) &= 2T(n/2) + \\Theta(n) \\\\ &= \\Theta(nlgn) \\end{align*} \\] (same as merge sort) Instead of splitting \\(0.5:0.5\\) , if we split \\(0.1:0.9\\) then we need solve following equation. \\[ \\begin{align*} T(n) &= T(n/10) + T(9n/10) + \\Theta(n) \\\\ &= \\Theta(nlgn) \\end{align*} \\] \u201cAlmost-Best\u201d Case Analysis \u00b6 Balanced Partitioning (1) \u00b6 We have seen that if H-PARTITION always splits the array with \\(0.1-to-0.9\\) ratio, the runtime will be \\(\\Theta(nlgn)\\) . Same is true with a split ratio of \\(0.01-to-0.99\\) , etc. Possible to show that if the split has always constant \\((\\Theta(1))\\) proportionality, then the runtime will be \\(\\Theta(nlgn)\\) . In other words, for a constant \\(\\alpha | (0 < \\alpha \u2264 0.5)\\) : \\(\\alpha\u2013to\u2013(1-\\alpha)\\) proportional split yields \\(\\Theta(nlgn)\\) total runtime Balanced Partitioning (2) \u00b6 In the rest of the analysis, assume that all input permutations are equally likely. This is only to gain some intuition We cannot make this assumption for average case analysis We will revisit this assumption later Also, assume that all input elements are distinct. Balanced Partitioning (3) \u00b6 Question: What is the probability that H-PARTITION returns a split that is more balanced than \\(0.1-to-0.9\\) ? Balanced Partitioning (4) \u00b6 Reminder: H-PARTITION will place the pivot in the right partition unless the pivot is the smallest element in the arrays. Question: If the pivot selected is the mth smallest value \\((1 < m \u2264 n)\\) in the input array, what is the size of the left region after partitioning? Balanced Partitioning (5) \u00b6 Question: What is the probability that the pivot selected is the \\(m^{th}\\) smallest value in the array of size \\(n\\) ? \\(1/n\\) ( since all input permutations are equally likely ) Question: What is the probability that the left partition returned by H-PARTITION has size \\(m\\) , where \\(1<m<n\\) ? \\(1/n\\) ( due to the answers to the previous 2 questions ) Balanced Partitioning (6) \u00b6 section{ font-size: 25px; } Question: What is the probability that H-PARTITION returns a split that is more balanced than \\(0.1-to-0.9\\) ? \\[ \\begin{align*} Probability &=\\sum \\limits_{q=0.1n+1}^{0.9n-1}\\frac{1}{n} \\\\ &=\\frac{1}{n}(0.9n-1-0.1n-1+1) \\\\ &= 0.8-\\frac{1}{n} \\\\ & \\approx 0.8 \\text{ for large n} \\end{align*} \\] Balanced Partitioning (7) \u00b6 The probability that H-PARTITION yields a split that is more balanced than \\(0.1-to-0.9\\) is \\(80\\%\\) on a random array. Let \\(P_{\\alpha>}\\) be the probability that H-PARTITION yields a split more balanced than \\(\\alpha-to-(1-\\alpha)\\) , where \\(0 < \\alpha \\leq 0.5\\) Repeat the analysis to generalize the previous result section{ font-size: 25px; } Balanced Partitioning (8) \u00b6 Question: What is the probability that H-PARTITION returns a split that is more balanced than \\(\\alpha-to-(1-\\alpha)\\) ? \\[ \\begin{align*} Probability & =\\sum \\limits_{q=\\alpha n+1}^{(1-\\alpha)n-1}\\frac{1}{n} \\\\ & =\\frac{1}{n}((1-\\alpha)n-1- \\alpha n-1+1) \\\\ & = (1-2\\alpha)-\\frac{1}{n} \\\\ & \\approx (1-2\\alpha) \\text{ for large n} \\end{align*} \\] Balanced Partitioning (9) \u00b6 We found \\(P_{\\alpha >}=1-2\\alpha\\) Ex: \\(P_{0.1>}=0.8\\) and \\(P_{0.01>}=0.98\\) Hence, H-PARTITION produces a split more balanced than a \\(0.1-to-0.9\\) split \\(80\\%\\) of the time \\(0.01-to-0.99\\) split \\(98\\%\\) of the time less balanced than a \\(0.1-to-0.9\\) split \\(20\\%\\) of the time \\(0.01-to-0.99\\) split \\(2\\%\\) of the time Intuition for the Average Case (1) \u00b6 Assumption: All permutations are equally likely Only for intuition; we\u2019ll revisit this assumption later Unlikely: Splits always the same way at every level Expectation: Some splits will be reasonably balanced Some splits will be fairly unbalanced Average case: A mix of good and bad splits Good and bad splits distributed randomly thru the tree Intuition for the Average Case (2) \u00b6 Assume for intuition: Good and bad splits occur in the alternate levels of the tree Good split: Best case split Bad split: Worst case split Intuition for the Average Case (3) \u00b6 Compare 2-successive levels of avg case vs. 1 level of best case Intuition for the Average Case (4) \u00b6 In terms of the remaining subproblems, two levels of avg case is slightly better than the single level of the best case The avg case has extra divide cost of \\(\\Theta(n)\\) at alternate levels The extra divide cost \\(\\Theta(n)\\) of bad splits absorbed into the \\(\\Theta(n)\\) of good splits. Running time is still \\(\\Theta(nlgn)\\) But, slightly larger hidden constants, because the height of the recursion tree is about twice of that of best case. Intuition for the Average Case (5) \u00b6 Another way of looking at it: Suppose we alternate lucky, unlucky, lucky, unlucky, \\(\\dots\\) We can write the recurrence as: \\(L(n) = 2U(n/2) + \\Theta(n)\\) lucky split (best) \\(U(n) = L(n-1) + \\Theta(n)\\) unlucky split (worst) Solving: $$ \\begin{align*} L(n) & = 2(L(n/2-1) + \\Theta(n/2)) + \\Theta(n) \\ & = 2L(n/2-1) + \\Theta(n) \\ & = \u0398(nlgn) \\end{align*} $$ How can we make sure we are usually lucky for all inputs? Summary: Quicksort Runtime Analysis (1) \u00b6 Worst case: Unbalanced split at every recursive call \\[ \\begin{align*} T(n) & = T(1) + T(n-1) + \\Theta(n) \\\\ T(n) & = \\Theta(n2) \\end{align*} \\] Best case: Balanced split at every recursive call ( extremely lucky ) \\[ \\begin{align*} T(n) & = 2T(n/2) + \\Theta(n) \\\\ T(n) & = \\Theta(nlgn) \\end{align*} \\] Summary: Quicksort Runtime Analysis (2) \u00b6 Almost-best case: Almost-balanced split at every recursive call \\[ \\begin{align*} T(n) &=T(n/10)+T(9n/10)+ \\Theta(n) \\\\ \\text{or } T(n) &= T(n/100) + T(99n/100) + \u0398(n) \\\\ \\text{or } T(n) &= T(\\alpha n) + T((1-\\alpha n)+ \\Theta(n) \\end{align*} \\] for any constant \\(\\alpha, 0 < \\alpha \\leq 0.5\\) Summary: Quicksort Runtime Analysis (3) \u00b6 For a random input array, the probability of having a split more balanced than \\(0.1 \u2013 to \u2013 0.9 : 80\\%\\) more balanced than \\(0.01 \u2013 to \u2013 0.99 : 98\\%\\) more balanced than \\(\\alpha \u2013 to \u2013 (1-\\alpha) : 1 \u2013 2 \\alpha\\) for any constant \\(\\alpha, 0 < \\alpha \\leq 0.5\\) Summary: Quicksort Runtime Analysis (4) \u00b6 Avg case intuition: Different splits expected at different levels some balanced (good), some unbalanced (bad) Avg case intuition: Assume the good and bad splits alternate i.e. good split -> bad split -> good split -> \u2026 \\(T(n) = \\Theta(nlgn)\\) (informal analysis for intuition) Randomized Quicksort \u00b6 In the avg-case analysis, we assumed that all permutations of the input array are equally likely. But, this assumption does not always hold e.g. What if all the input arrays are reverse sorted ? Always worst-case behavior Ideally, the avg-case runtime should be independent of the input permutation . Randomness should be within the algorithm , not based on the distribution of the inputs. i.e. The avg case should hold for all possible inputs Randomized Algorithms (1) \u00b6 Alternative to assuming a uniform distribution: Impose a uniform distribution e.g. Choose a random pivot rather than the first element Typically useful when: there are many ways that an algorithm can proceed but, it\u2019s difficult to determine a way that is always guaranteed to be good . If there are many good alternatives ; simply choose one randomly . Randomized Algorithms (1) \u00b6 Ideally: Runtime should be independent of the specific inputs No specific input should cause worst-case behavior Worst-case should be determined only by output of a random number generator. section{ font-size: 25px; } Randomized Quicksort (1) \u00b6 Using Hoare\u2019s partitioning algorithm: R - QUICKSORT ( A , p , r ) if p < r then q = R - PARTITION ( A , p , r ) R - QUICKSORT ( A , p , q ) R - QUICKSORT ( A , q +1 , r ) R - PARTITION ( A , p , r ) s = RANDOM ( p , r ) exchange A [ p ] with A [ s ] return H - PARTITION ( A , p , r ) Alternatively, permuting the whole array would also work but, would be more difficult to analyze section{ font-size: 25px; } Randomized Quicksort (2) \u00b6 Using Lomuto\u2019s partitioning algorithm: R - QUICKSORT ( A , p , r ) if p < r then q = R - PARTITION ( A , p , r ) R - QUICKSORT ( A , p , q -1 ) R - QUICKSORT ( A , q +1 , r ) R - PARTITION ( A , p , r ) s = RANDOM ( p , r ) exchange A [ r ] with A [ s ] return L - PARTITION ( A , p , r ) Alternatively, permuting the whole array would also work but, would be more difficult to analyze Notations for Formal Analysis \u00b6 Assume all elements in \\(A[p \\dots r]\\) are distinct Let \\(n = r \u2013 p + 1\\) Let \\(rank(x) = |{A[i]: p \\leq i \\leq r \\text{ and } A[i] \\leq x}|\\) i.e. \\(rank(x)\\) is the number of array elements with value less than or equal to \\(x\\) \\(A=\\{5,9,7,6,8,1,4\\}\\) \\(p=5,r=4\\) \\(rank(5)=3\\) i.e. it is the \\(3^{rd}\\) smallest element in the array Formal Analysis for Average Case \u00b6 The following analysis will be for Quicksort using Hoare\u2019s partitioning algorithm. Reminder: The pivot is selected randomly and exchanged with \\(A[p]\\) before calling H-PARTITION Let \\(x\\) be the random pivot chosen. What is the probability that \\(rank(x) = i\\) for \\(i = 1, 2, \\dots n\\) ? \\(P(rank(x) = i) = 1/n\\) section{ font-size: 25px; } Various Outcomes of H-PARTITION (1) \u00b6 Assume that \\(rank(x)=1\\) i.e. the random pivot chosen is the smallest element What will be the size of the left partition \\((|L|)\\) ? Reminder: Only the elements less than or equal to \\(x\\) will be in the left partition. \\(A=\\{\\overbrace{2}^{p=x=pivot}\\underbrace{,}_{\\Longrightarrow|L|=1 } 9,7,6,8,5,\\overbrace{4}^r\\}\\) \\(p=2,r=4\\) \\(pivot=x=2\\) TODO: convert to image...S6_P9 section{ font-size: 25px; } Various Outcomes of H-PARTITION (2) \u00b6 Assume that \\(rank(x)>1\\) i.e. the random pivot chosen is not the smallest element What will be the size of the left partition \\((|L|)\\) ? Reminder: Only the elements less than or equal to \\(x\\) will be in the left partition. Reminder: The pivot will stay in the right region after H-PARTITION if \\(rank(x)>1\\) \\(A=\\{\\overbrace{2}^{p}, 4 \\underbrace{,}_{\\Longrightarrow|L|=rank(x)-1}7,6,8,\\overbrace{5,}^{pivot}\\overbrace{9}^r\\}\\) \\(p=2,r=4\\) \\(pivot=x=5\\) TODO: convert to image...S6_P10 Various Outcomes of H-PARTITION - Summary (1) \u00b6 \\(x: pivot\\) \\(|L|: \\text{size of left region}\\) \\(P(rank(x) = i) = 1/n \\text{ for } 1 \\leq i \\leq n\\) \\(\\text{if } rank(x) = 1 \\text{ then } |L| = 1\\) \\(\\text{if } rank(x) > 1 \\text{ then } |L| = rank(x) - 1\\) \\(P(|L| = 1) = P(rank(x) = 1) + P(rank(x) = 2)\\) \\(P(|L| = 1) = 2/n\\) \\(P(|L| = i) = P(rank(x) = i+1) \\text{ for } 1< i < n\\) \\(P(|L| = i) = 1/n \\text{ for } 1< i < n\\) Various Outcomes of H-PARTITION - Summary (2) \u00b6 section{ font-size: 25px; } Average - Case Analysis: Recurrence (1) \u00b6 \\(x=pivot\\) \\[ \\begin{align*} T(n) & = \\frac{1}{n}(T(1)+t(n-1) ) & rank:1 \\\\ & + \\frac{1}{n}(T(1)+t(n-1) ) & rank:2 \\\\ & + \\frac{1}{n}(T(2)+t(n-2) ) & rank:3 \\\\ & \\vdots & \\vdots \\\\ & + \\frac{1}{n}(T(i)+t(n-i) ) & rank:i+1 \\\\ & \\vdots & \\vdots \\\\ & + \\frac{1}{n}(T(n-1)+t(1) ) & rank:n \\\\ & + \\Theta(n) \\end{align*} \\] section{ font-size: 25px; } Average - Case Analysis: Recurrence (2) \u00b6 \\[ \\begin{align*} T(n) &= \\frac{1}{n}\\sum \\limits_{q=1}^{n-1}(T(q)+T(n-q))+\\frac{1}{n}(T(1)+T(n-1))+\\Theta(n)\\\\ & \\text{Note: } \\frac{1}{n}(T(1)+T(n-1))=\\frac{1}{n}(\\Theta(1)+O(n^2))=O(n) \\\\ T(n) &= \\frac{1}{n}\\sum \\limits_{q=1}^{n-1}(T(q)+T(n-q))+\\Theta(n) \\end{align*} \\] for \\(k=1,2,\\dots,n-1\\) each term \\(T(k)\\) appears twice once for \\(q = k\\) and once for \\(q = n\u2212k\\) \\[ T(n) = \\frac{2}{n}\\sum \\limits_{k=1}^{n-1} T(k)+\\Theta(n) \\] Average - Case Analysis -Solving Recurrence: Substitution \u00b6 Guess: \\(T(n)=O(nlgn)\\) \\(T(k) \u2264 aklgk\\) for \\(k<n\\) , for some constant \\(a > 0\\) \\[ \\begin{align*} T(n) &= \\frac{2}{n} \\sum \\limits_{k=1}^{n-1} T(k)+\\Theta(n) \\\\ & \\leq \\frac{2}{n} \\sum \\limits_{k=1}^{n-1} aklgk+\\Theta(n) \\\\ & \\leq \\frac{2a}{n} \\sum \\limits_{k=1}^{n-1} klgk+\\Theta(n) \\end{align*} \\] Need a tight bound for \\(\\sum klgk\\) Tight bound for \\(\\sum klgk\\) (1) \u00b6 Bounding the terms \\(\\ \\sum \\limits_{k=1}^{n-1}klgk \\leq \\sum \\limits_{k=1}^{n-1}nlgn = n(n-1)lgn \\leq n^2lgn\\) This bound is not strong enough because \\(T(n) \\leq \\frac{2a}{n}n^2lgn+\\Theta(n)\\) \\(=2anlgn+\\Theta(n)\\) \\(\\Longrightarrow\\) couldn\u2019t prove \\(T(n) \\leq anlgn\\) Tight bound for \\(\\sum klgk\\) (2) \u00b6 Splitting summations: ignore ceilings for simplicity $$ \\sum \\limits_{k=1}^{n-1}klgk \\leq \\sum \\limits_{k=1}^{n/2-1}klgk + \\sum \\limits_{k=n/2}^{n-1}klgk $$ First summation : \\(lgk < lg(n/2)=lgn-1\\) Second summation : \\(lgk < lgn\\) section{ font-size: 25px; } Splitting: \\(\\sum \\limits_{k=1}^{n-1}klgk \\leq \\sum \\limits_{k=1}^{n/2-1}klgk + \\sum \\limits_{k=n/2}^{n-1}klgk\\) (3) \u00b6 \\[ \\begin{align*} & \\sum \\limits_{k=1}^{n-1}klgk \\leq (lg(n-1))\\sum \\limits_{k=1}^{n/2-1}k + lgn \\sum \\limits_{k=n/2}^{n-1}k \\\\ &= lgn \\sum \\limits_{k=1}^{n-1}k- \\sum \\limits_{k=1}^{n/2-1}k \\\\ &= \\frac{1}{2}n(n-1)lgn - \\frac{1}{2} \\frac{n}{2}(\\frac{n}{2}-1) \\\\ &= \\frac{1}{2}n^2lgn - \\frac{1}{8}n^2 - \\frac{1}{2}n(lgn-1/2) \\\\ \\end{align*} \\] \\[ \\begin{align*} & \\sum \\limits_{k=1}^{n-1} klgk \\leq \\frac{1}{2}n^2lgn-\\frac{1}{8}n^2 \\ for \\ lgn \\geq 1/2 \\Longrightarrow n \\geq \\sqrt{2} \\end{align*} \\] Substituting: - \\(\\sum \\limits_{k=1}^{n-1}klgk \\leq \\frac{1}{2}n^2lgn-\\frac{1}{8}n^2\\) (4) \u00b6 \\[ \\begin{align*} T(n) & \\leq \\frac{2a}{n}\\sum \\limits_{k=1}^{n-1}klgk+\\Theta(n)\\\\ & \\leq \\frac{2a}{n}(\\frac{1}{2}n^2lgn-\\frac{1}{8}n^2)+\\Theta(n) \\\\ & = anlgn - (\\frac{a}{4}n-\\Theta(n)) \\end{align*} \\] We can choose a large enough so that \\(\\frac{a}{4}n \\geq \\Theta(n)\\) \\[ \\begin{align*} T(n) & \\leq anlgn \\\\ T(n) & = O(nlgn) \\end{align*} \\] Q.E.D. Medians and Order Statistics \u00b6 ith order statistic : \\(i^{th}\\) smallest element of a set of \\(n\\) elements minimum: first order statistic maximum: \\(n^{th}\\) order statistic median: \u201chalfway point\u201d of the set \\[ \\begin{align*} i & = \\lfloor \\frac{(n+1)}{2} \\rfloor \\\\ \\text{ or } \\\\ i & = \\lceil \\frac{(n+1)}{2} \\rceil \\end{align*} \\] Selection Problem \u00b6 Selection problem: Select the \\(i^{th}\\) smallest of \\(n\\) elements Na\u00efve algorithm: Sort the input array \\(A\\) ; then return \\(A[i]\\) \\(T(n) = \\theta(nlgn)\\) using e.g. merge sort (but not quicksort) Can we do any better? Selection in Expected Linear Time \u00b6 Randomized algorithm using divide and conquer Similar to randomized quicksort Like quicksort: Partitions input array recursively Unlike quicksort: Makes a single recursive call Reminder: Quicksort makes two recursive calls Expected runtime: \\(\\Theta(n)\\) Reminder: Expected runtime of quicksort: \\(\\Theta(nlgn)\\) Selection in Expected Linear Time: Example 1 \u00b6 Select the \\(2^{nd}\\) smallest element: \\[ \\begin{align*} A & = \\{6,10,13,5,8,3,2,11\\} \\\\ i & = 2 \\\\ \\end{align*} \\] Partition the input array: \\[ \\begin{align*} A & = \\{\\underbrace{2,3,5,}_{\\text{left subarray} }\\underbrace{13,8,10,6,11}_{\\text{right subarray}}\\} \\end{align*} \\] make a recursive call to select the \\(2^{nd}\\) smallest element in left subarray Selection in Expected Linear Time: Example 2 \u00b6 Select the \\(7^{th}\\) smallest element: \\[ \\begin{align*} A & = \\{6,10,13,5,8,3,2,11\\} \\\\ i & = 7 \\\\ \\end{align*} \\] Partition the input array: \\[ \\begin{align*} A & = \\{\\underbrace{2,3,5,}_{\\text{left subarray} }\\underbrace{13,8,10,6,11}_{\\text{right subarray}}\\} \\end{align*} \\] make a recursive call to select the \\(4^{th}\\) smallest element in right subarray Selection in Expected Linear Time (1) \u00b6 R - SELECT ( A , p , r , i ) if p == r then return A [ p ]; q = R - PARTITION ( A , p , r ) k = q \u2013 p +1 ; if i <= k then return R - SELECT ( A , p , q , i ); else return R - SELECT ( A , q +1 , r , i - k ); \\[ \\begin{align*} A & = \\{ \\underbrace{ | }_{p} \\dots \\leq x \\text{(k smallest elements)} \\dots \\underbrace{ | }_{q} \\dots \\geq x \\dots \\underbrace{ | }_{r} \\} \\\\ x & = pivot \\end{align*} \\] Selection in Expected Linear Time (2) \u00b6 \\[ \\begin{align*} A & = \\{ \\overbrace{ | }^{p} \\underbrace{ \\dots \\leq x \\dots }_{L} \\overbrace{ | }^{q} \\underbrace{ \\dots \\geq x \\dots }_{R} \\overbrace{ | }^{r} \\} \\\\ x & = pivot \\end{align*} \\] All elements in \\(L \\leq\\) all elements in \\(R\\) \\(L\\) contains: \\(|L| = q\u2013p+1\\) \\(=\\) k smallest elements of \\(A[p...r]\\) if \\(i \\leq |L| = k\\) then search \\(L\\) recursively for its \\(i^{th}\\) smallest element else search \\(R\\) recursively for its \\((i-k)^{th}\\) smallest element Runtime Analysis (1) \u00b6 Worst case: Imbalanced partitioning at every level and the recursive call always to the larger partition \\[ \\begin{align*} & = \\{1,\\underbrace{2,3,4,5,6,7,8}_{\\text{recursive call}} \\} & i & = 8 \\\\ & = \\{2,\\underbrace{3,4,5,6,7,8}_{\\text{recursive call}} \\} & i & = 7 \\end{align*} \\] Runtime Analysis (2) \u00b6 Worst case: Worse than the na\u00efve method (based on sorting) \\[ \\begin{align*} T(n) &= T(n-1) + \\Theta(n) \\\\ T(n) &= \\Theta(n^2) \\end{align*} \\] Best case: Balanced partitioning at every recursive level \\[ \\begin{align*} T(n) &= T(n/2) + \\Theta(n) \\\\ T(n) &= \\Theta(n) \\end{align*} \\] Avg case: Expected runtime \u2013 need analysis T.B.D. section{ font-size: 25px; } Reminder: Various Outcomes of H-PARTITION \u00b6 \\(x: pivot\\) \\(|L|: \\text{size of left region}\\) \\(P(rank(x) = i) = 1/n \\text{ for } 1 \\leq i \\leq n\\) \\(\\text{if } rank(x) = 1 \\text{ then } |L| = 1\\) \\(\\text{if } rank(x) > 1 \\text{ then } |L| = rank(x) - 1\\) \\(P(|L| = 1) = P(rank(x) = 1) + P(rank(x) = 2)\\) \\(P(|L| = 1) = 2/n\\) \\(P(|L| = i) = P(rank(x) = i+1) \\text{ for } 1< i < n\\) \\(P(|L| = i) = 1/n \\text{ for } 1< i < n\\) Average Case Analysis of Randomized Select \u00b6 To compute the upper bound for the avg case , assume that the \\(i^{th}\\) element always falls into the larger partition . \\[ \\begin{align*} A & = \\{ \\overbrace{ | }^{p} \\underbrace{ \\dots \\leq x \\dots }_{Left Partition} \\overbrace{ | }^{q} \\underbrace{ \\dots \\geq x \\dots }_{Right Partition} \\overbrace{ | }^{r} \\} \\\\ x & = pivot \\end{align*} \\] We will analyze the case where the recursive call is always made to the larger partition This will give us an upper bound for the avg case Various Outcomes of H-PARTITION \u00b6 Average-Case Analysis of Randomized Select (1) \u00b6 \\[ \\text{Recall:} P(|L|=i) = \\begin{cases} 2/n & \\text{for } i=1 \\\\ 1/n & \\text{for } i=2,3,\\dots,n-1 \\end{cases} \\] Upper bound: Assume \\(i^{th}\\) element always falls into the larger part. \\[ \\begin{align*} T(n) &\\leq \\frac{1}{n}T(max(1,n-1))+\\frac{1}{n}\\sum \\limits_{q=1}^{n-1}T(max(q,n-q))+O(n) \\\\ Note: & \\frac{1}{n}T(max(1,n-1)) = \\frac{1}{n}T(n-1)=\\frac{1}{n}O(n^2) = O(n) \\\\ \\therefore \\text{(3 dot mean therefore) } & T(n) \\leq \\frac{1}{n}\\sum \\limits_{q=1}^{n-1}T(max(q,n-q))+O(n) \\end{align*} \\] Average-Case Analysis of Randomized Select (2) \u00b6 \\[ \\begin{align*} \\therefore T(n) \\leq \\frac{1}{n}\\sum \\limits_{q=1}^{n-1}T(max(q,n-q))+O(n) \\end{align*} \\] \\[ max(q, n\u2013q) = \\begin{cases} q & \\text{ if } q \\geq \\lceil n/2 \\rceil \\\\ n-q & \\text{ if } q < \\lceil n/2 \\rceil \\\\ \\end{cases} \\] \\(n\\) is odd: \\(T(k)\\) appears twice for \\(k=\\lceil n/2 \\rceil+1,\\lceil n/2 \\rceil+2,\\dots,n\u20131\\) \\(n\\) is even: \\(T(\\lceil n/2 \\rceil)\\) appears once \\(T(k)\\) appears twice for \\(k = \\lceil n/2 \\rceil +1, \\lceil n/2 \\rceil+2,\\dots,n\u20131\\) Average-Case Analysis of Randomized Select (3) \u00b6 Hence, in both cases: \\[ \\begin{align*} \\sum \\limits_{q=1}^{n-1} T(max(q,n-q))+O(n) & \\leq 2\\sum \\limits_{q=\\lceil n/2 \\rceil}^{n-1} T(q)+O(n) \\\\ \\therefore T(n) & \\leq \\frac{2}{n} \\sum \\limits_{q=\\lceil n/2 \\rceil}^{n-1}T(q)+O(n) \\end{align*} \\] section{ font-size: 25px; } Average-Case Analysis of Randomized Select (4) \u00b6 \\[ \\begin{align*} T(n) & \\leq \\frac{2}{n} \\sum \\limits_{q=\\lceil n/2 \\rceil}^{n-1}T(q)+O(n) \\end{align*} \\] By substitution guess \\(T(n) = O(n)\\) Inductive hypothesis: \\(T(k) \\leq ck, \\forall k<n\\) \\[ \\begin{align*} T(n) & \\leq \\frac{2}{n} \\sum \\limits_{q=\\lceil n/2 \\rceil}^{n-1}ck+O(n) \\\\ & = \\frac{2c}{n} \\Bigg(\\sum \\limits_{k=1}^{n-1}k-\\sum \\limits_{k=1}^{\\lceil n/2 \\rceil-1}k \\Bigg)+ O(n) \\\\ & = \\frac{2c}{n} \\Bigg(\\frac{1}{2}n(n-1)-\\frac{1}{2} \\lceil \\frac{n}{2} \\rceil \\bigg( \\frac{n}{2}-1 \\bigg) \\Bigg)+ O(n) \\end{align*} \\] Average-Case Analysis of Randomized Select (5) \u00b6 \\[ \\begin{align*} T(n)& \\leq \\frac{2c}{n} \\Bigg(\\frac{1}{2}n(n-1)-\\frac{1}{2} \\lceil \\frac{n}{2} \\rceil \\bigg( \\frac{n}{2}-1 \\bigg) \\Bigg)+ O(n) \\\\ & \\leq c(n-1)-\\frac{c}{4}n+\\frac{c}{2}+O(n) \\\\ & = cn - \\frac{c}{4}n - \\frac{c}{2} + O(n) \\\\ & = cn - \\Bigg( \\bigg( \\frac{c}{4}n+\\frac{c}{2}\\bigg) + O(n) \\Bigg) \\\\ & \\leq cn \\end{align*} \\] since we can choose c large enough so that \\((cn/4+c/2 )\\) dominates \\(O(n)\\) Summary of Randomized Order-Statistic Selection \u00b6 Works fast: linear expected time Excellent algorithm in practise But, the worst case is very bad: \\(\\Theta(n^2)\\) Blum, Floyd, Pratt, Rivest & Tarjan[1973] algorithms are runs in linear time in the worst case . Generate a good pivot recursively section{ font-size: 25px; } Selection in Worst Case Linear Time \u00b6 // return i - th element in set S with n elements SELECT ( S , n , i ) if n <= 5 then SORT S and return the i - th element DIVIDE S into ceil ( n / 5 ) groups // first ceil ( n / 5 ) groups are of size 5 , last group is of size n mod 5 FIND median set M = { m , \u2026 , m_ceil ( n / 5 )} // m_j : median of j - th group x = SELECT ( M , ceil ( n / 5 ), floor (( ceil ( n / 5 ) +1 ) / 2 )) PARTITION set S around the pivot x into L and R if i <= | L | then return SELECT ( L , | L | , i ) else return SELECT ( R , n \u2013 | L | , i \u2013 | L | ) Selection in Worst Case Linear Time - Example (1) \u00b6 Input: Array \\(S\\) and index \\(i\\) Output: The \\(i^{th}\\) smallest value \\[ \\begin{array}{ccc} 25 & 9 & 16 & 8 & 11 & 27 & 39 & 42 & 15 & 6 32 & 14 & 36 & 20 & 33 & 22 & 31 & 4 & 17 & 3 & 30 & 41 \\\\ 2 & 13 & 19 & 7 & 21 & 10 & 34 & 1 & 37 & 23 & 40 & 5 & 29 & 18 & 24 & 12 & 38 & 28 & 26 & 35 & 43 \\end{array} \\] Selection in Worst Case Linear Time - Example (2) \u00b6 Step 1: Divide the input array into groups of size \\(5\\) \\[ \\overbrace{ \\begin{array}{ccc} 25 & 9 & 16 & 8 & 11 \\\\ 27 & 39 & 42 & 15 & 6 \\\\ 32 & 14 & 36 & 20 & 33 \\\\ 22 & 31 & 4 & 17 & 3 \\\\ 30 & 41 & 2 & 13 & 19 \\\\ 7 & 21 & 10 & 34 & 1 \\\\ 37 & 23 & 40 & 5 & 29 \\\\ 18 & 24 & 12 & 38 & 28 \\\\ 26 & 35 & 43 \\end{array} }^{\\text{group size}=5} \\] section{ font-size: 25px; } Selection in Worst Case Linear Time - Example (3) \u00b6 Step 2: Compute the median of each group ( \\(\\Theta(n)\\) ) \\[ \\begin{array}{ccc} 25 & 16 & \\overbrace{11}^{Medians} & 8 & 9 \\\\ 39 & 42 & 27 & 6 & 15 \\\\ 36 & 33 & 32 & 20 & 14 \\\\ 22 & 31 & 17 & 3 & 4 \\\\ 41 & 30 & 19 & 13 & 2 \\\\ 21 & 34 & 10 & 1 & 7 \\\\ 37 & 40 & 29 & 23 & 5 \\\\ 38 & 28 & 24 & 12 & 18 \\\\ & 26 & 35 & 43 \\end{array} \\] Let \\(M\\) be the set of the medians computed: \\(M = \\{11, 27, 32, 17, 19, 10, 29, 24, 35\\}\\) Selection in Worst Case Linear Time - Example (4) \u00b6 Step 3: Compute the median of the median group \\(M\\) \\(x \\leftarrow SELECT(M,|M|,\\lfloor (|M|+1)/2 \\rfloor)\\) where \\(|M|=\\lceil n/5 \\rceil\\) Let \\(M\\) be the set of the medians computed: \\(M = \\{11, 27, 32, 17, 19, 10, 29, \\overbrace{24}^{Median}, 35\\}\\) \\(Median = 24\\) The runtime of the recursive call: \\(T(|M|)=T(\\lceil n/5 \\rceil)\\) Selection in Worst Case Linear Time - Example (5) \u00b6 Step 4: Partition the input array \\(S\\) around the median-of-medians \\(x\\) \\[ \\begin{array}{ccc} 25 & 9 & 16 & 8 & 11 & 27 & 39 & 42 & 15 & 6 32 & 14 & 36 & 20 & 33 & 22 & 31 & 4 & 17 & 3 & 30 & 41 \\\\ 2 & 13 & 19 & 7 & 21 & 10 & 34 & 1 & 37 & 23 & 40 & 5 & 29 & 18 & 24 & 12 & 38 & 28 & 26 & 35 & 43 \\end{array} \\] Partition \\(S\\) around \\(x = 24\\) Claim: Partitioning around x is guaranteed to be well-balanced. section{ font-size: 25px; } Selection in Worst Case Linear Time - Example (6) \u00b6 \\(M\\) : Median, \\(M^*\\) : Median of Medians \\[ \\begin{array}{ccc} 41 & 30 & \\overbrace{19}^{M} & 13 & 2 \\\\ 21 & 34 & 10 & 1 & 7 \\\\ 22 & 31 & 17 & 3 & 4 \\\\ 25 & 16 & 11 & 8 & 9 \\\\ 38 & 28 & \\overbrace{24}^{M^*} & 12 & 18 \\\\ 36 & 33 & 32 & 20 & 14 \\\\ 37 & 40 & 29 & 23 & 5 \\\\ 39 & 42 & 27 & 6 & 15 \\\\ & 26 & 35 & 43 \\end{array} \\] About half of the medians greater than \\(x=24\\) (about \\(n/10\\) ) Selection in Worst Case Linear Time - Example (7) \u00b6 Selection in Worst Case Linear Time - Example (8) \u00b6 Selection in Worst Case Linear Time - Example (9) \u00b6 \\[ S = \\begin{array}{ccc} \\{ 25 & 9 & 16 & 8 & 11 & 27 & 39 & 42 & 15 & 6 32 & 14 & 36 & 20 & 33 & 22 & 31 & 4 & 17 & 3 & 30 & 41 \\\\ 2 & 13 & 19 & 7 & 21 & 10 & 34 & 1 & 37 & 23 & 40 & 5 & 29 & 18 & 24 & 12 & 38 & 28 & 26 & 35 & 43 \\} \\end{array} \\] Partitioning \\(S\\) around \\(x = 24\\) will lead to partitions of sizes \\(\\sim 3n/10\\) and \\(\\sim 7n/10\\) in the worst case . Step 5: Make a recursive call to one of the partitions if i <= | L | then return SELECT ( L , | L | , i ) else return SELECT ( R , n -| L | , i -| L | ) section{ font-size: 25px; } Selection in Worst Case Linear Time \u00b6 // return i - th element in set S with n elements SELECT ( S , n , i ) if n <= 5 then SORT S and return the i - th element DIVIDE S into ceil ( n / 5 ) groups // first ceil ( n / 5 ) groups are of size 5 , last group is of size n mod 5 FIND median set M = { m , \u2026 , m_ceil ( n / 5 )} // m_j : median of j - th group x = SELECT ( M , ceil ( n / 5 ), floor (( ceil ( n / 5 ) +1 ) / 2 )) PARTITION set S around the pivot x into L and R if i <= | L | then return SELECT ( L , | L | , i ) else return SELECT ( R , n \u2013 | L | , i \u2013 | L | ) Choosing the Pivot (1) \u00b6 Divide S into groups of size 5 Choosing the Pivot (2) \u00b6 Divide S into groups of size 5 Find the median of each group Choosing the Pivot (3) \u00b6 Divide S into groups of size 5 Find the median of each group Recursively select the median x of the medians Choosing the Pivot (4) \u00b6 At least half of the medians \\(\\geq x\\) Thus \\(m = \\lceil \\lceil n/5 \\rceil /2 \\rceil\\) groups contribute 3 elements to R except possibly the last group and the group that contains \\(x\\) , \\(|R|\\geq 3(m\u20132)\\geq \\frac{3n}{10}\u20136\\) Choosing the Pivot (5) \u00b6 Similarly \\(|L| \\geq \\frac{3n}{10}\u2013 6\\) Therefore, SELECT is recursively called on at most \\(n-(\\frac{3n}{10}-6)=\\frac{7n}{10}+6\\) elements Selection in Worst Case Linear Time (1) \u00b6 Selection in Worst Case Linear Time (2) \u00b6 Thus recurrence becomes \\(T(n) \\leq T \\big( \\lceil \\frac{n}{5} \\rceil \\big) + T\\big( \\frac{7n}{10}+6 \\big) + \\Theta(n)\\) Guess \\(T(n)=O(n)\\) and prove by induction Inductive step: \\[ \\begin{align*} T(n) & \\leq c \\lceil n/5 \\rceil + c(7n/10+6)+\\Theta(n) \\\\ & \\leq cn/5 + c + 7cn/10 + 6c + \\Theta(n) \\\\ & = 9cn/10 + 7c + \\Theta(n) \\\\ & = cn - [c(n/10-7)-\\Theta(n)] \\leq cn &\\text{( for large c)} \\end{align*} \\] Work at each level of recursion is a constant factor \\((9/10)\\) smaller References \u00b6 Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) Insertion Sort - GeeksforGeeks NIST Dictionary of Algorithms and Data Structures NIST - Dictionary of Algorithms and Data Structures NIST - big-O notation NIST - big-Omega notation \\(-End-Of-Week-3-Course-Module-\\)","title":"Week-3 (Matrix Multiplication/Quick Sort)"},{"location":"tr/week-3/ce100-week-3-matrix/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"tr/week-3/ce100-week-3-matrix/#week-3-matrix-multiplication-quick-sort","text":"","title":"Week-3 (Matrix Multiplication/ Quick Sort)"},{"location":"tr/week-3/ce100-week-3-matrix/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"tr/week-3/ce100-week-3-matrix/#matrix-multiplication-quick-sort","text":"","title":"Matrix Multiplication / Quick Sort"},{"location":"tr/week-3/ce100-week-3-matrix/#outline-1","text":"Matrix Multiplication Traditional Recursive Strassen","title":"Outline (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#outline-2","text":"Quicksort Hoare Partitioning Lomuto Partitioning Recursive Sorting","title":"Outline (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#outline-3","text":"Quicksort Analysis Randomized Quicksort Randomized Selection Recursive Medians","title":"Outline (3)"},{"location":"tr/week-3/ce100-week-3-matrix/#matrix-multiplication-1","text":"Input: \\(A=[a_{ij}],B=[b_{ij}]\\) Output: \\(C=[c_{ij}]=A \\cdot B\\) \\(\\Longrightarrow i,j=1,2,3, \\dots, n\\) \\[ \\begin{bmatrix} c_{11} & c_{12} & \\dots & c_{1n} \\\\ c_{21} & c_{22} & \\dots & c_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ c_{n1} & c_{n2} & \\dots & c_{nn} \\\\ \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ a_{n1} & a_{n2} & \\dots & a_{nn} \\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} b_{11} & b_{12} & \\dots & b_{1n} \\\\ b_{21} & b_{22} & \\dots & b_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ b_{n1} & a_{n2} & \\dots & b_{nn} \\\\ \\end{bmatrix} \\]","title":"Matrix Multiplication (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#matrix-multiplication-2","text":"\\(c_{ij}=\\sum \\limits_{1\\leq k \\leq n}^{}a_{ik}.b_{kj}\\)","title":"Matrix Multiplication (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#matrix-multiplication-standard-algorithm","text":"Running Time: \\(\\Theta(n^3)\\) for i = 1 to n do for j = 1 to n do C [ i , j ] = 0 for k = 1 to n do C [ i , j ] = C [ i , j ] + A [ i , k ] + B [ k , j ] endfor endfor endfor","title":"Matrix Multiplication: Standard Algorithm"},{"location":"tr/week-3/ce100-week-3-matrix/#matrix-multiplication-divide-conquer-1","text":"IDEA: Divide the \\(nxn\\) matrix into \\(2x2\\) matrix of \\((n/2)x(n/2)\\) submatrices.","title":"Matrix Multiplication: Divide &amp; Conquer (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#matrix-multiplication-divide-conquer-2","text":"\\[ \\begin{bmatrix} c_{11} & c_{12} \\\\ c_{21} & c_{22} \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} \\cdot \\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{bmatrix} \\] \\[ \\text{8 mults and 4 adds of (n/2)*(n/2) submatrices}= \\begin{cases} c_{11}=a_{11}b_{11}+a_{12}b_{21} \\\\ c_{21}=a_{21}b_{11}+a_{22}b_{21} \\\\ c_{12}=a_{11}b_{12}+a_{12}b_{22} \\\\ c_{22}=a_{21}b_{12}+a_{22}b_{22} \\end{cases} \\]","title":"Matrix Multiplication: Divide &amp; Conquer (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#matrix-multiplication-divide-conquer-3","text":"MATRIX - MULTIPLY ( A , B ) // Assuming that both A and B are nxn matrices if n == 1 then return A * B else // partition A , B , and C as shown before C [ 1 , 1 ] = MATRIX - MULTIPLY ( A [ 1 , 1 ], B [ 1 , 1 ]) + MATRIX - MULTIPLY ( A [ 1 , 2 ], B [ 2 , 1 ]); C [ 1 , 2 ] = MATRIX - MULTIPLY ( A [ 1 , 1 ], B [ 1 , 2 ]) + MATRIX - MULTIPLY ( A [ 1 , 2 ], B [ 2 , 2 ]); C [ 2 , 1 ] = MATRIX - MULTIPLY ( A [ 2 , 1 ], B [ 1 , 1 ]) + MATRIX - MULTIPLY ( A [ 2 , 2 ], B [ 2 , 1 ]); C [ 2 , 2 ] = MATRIX - MULTIPLY ( A [ 2 , 1 ], B [ 1 , 2 ]) + MATRIX - MULTIPLY ( A [ 2 , 2 ], B [ 2 , 2 ]); endif return C","title":"Matrix Multiplication: Divide &amp; Conquer (3)"},{"location":"tr/week-3/ce100-week-3-matrix/#matrix-multiplication-divide-conquer-analysis","text":"\\(T(n) = 8T(n/2) + \\Theta(n^2)\\) \\(8\\) recursive calls \\(\\Longrightarrow 8T(\\cdots)\\) each problem has size \\(n/2\\) \\(\\Longrightarrow \\cdots T(n/2)\\) Submatrix addition \\(\\Longrightarrow \\Theta(n^2)\\)","title":"Matrix Multiplication: Divide &amp; Conquer Analysis"},{"location":"tr/week-3/ce100-week-3-matrix/#matrix-multiplication-solving-the-recurrence","text":"\\(T(n) = 8T(n/2) + \\Theta(n^2)\\) \\(a=8\\) , \\(b=2\\) \\(f(n)=\\Theta(n^2)\\) \\(n^{log_b^a}=n^3\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(n^{log_b^a})\\) Similar with ordinary (iterative) algorithm.","title":"Matrix Multiplication: Solving the Recurrence"},{"location":"tr/week-3/ce100-week-3-matrix/#matrix-multiplication-strassens-idea-1","text":"Compute \\(c_{11},c_{12},c_{21},c_{22}\\) using \\(7\\) recursive multiplications. In normal case we need \\(8\\) as below. \\[ \\begin{bmatrix} c_{11} & c_{12} \\\\ c_{21} & c_{22} \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} \\cdot \\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{bmatrix} \\] \\[ \\text{8 mults and 4 adds of (n/2)*(n/2) submatrices}= \\begin{cases} c_{11}=a_{11}b_{11}+a_{12}b_{21} \\\\ c_{21}=a_{21}b_{11}+a_{22}b_{21} \\\\ c_{12}=a_{11}b_{12}+a_{12}b_{22} \\\\ c_{22}=a_{21}b_{12}+a_{22}b_{22} \\end{cases} \\] section{ font-size: 25px; }","title":"Matrix Multiplication: Strassen\u2019s Idea (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#matrix-multiplication-strassens-idea-2","text":"Reminder: Each submatrix is of size \\((n/2)*(n/2)\\) Each add/sub operation takes \\(\\Theta(n^2)\\) time Compute \\(P1 \\dots P7\\) using \\(7\\) recursive calls to matrix-multiply \\[ \\begin{align*} P_1 & = a_{11} * (b_{12} - b_{22} ) \\\\ P_2 & = (a_{11} + a_{12} ) * b_{22} \\\\ P_3 & = (a_{21} + a_{22} ) * b_{11} \\\\ P_4 & = a_{22} * (b_{21} - b_{11} ) \\\\ P_5 & = (a_{11} + a_{22} ) * (b_{11} + b_{22} ) \\\\ P_6 & = (a_{12} - a_{22} ) * (b_{21} + b_{22} ) \\\\ P_7 & = ( a_{11} - a_{21} ) * (b_{11} + b_{12} ) \\end{align*} \\] section{ font-size: 25px; }","title":"Matrix Multiplication: Strassen\u2019s Idea (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#matrix-multiplication-strassens-idea-3","text":"\\[ \\begin{align*} P_1 &= a_{11} * (b_{12} - b_{22} ) \\\\ P_2 &= (a_{11} + a_{12} ) * b_{22} \\\\ P_3 &= (a_{21} + a_{22} ) * b_{11} \\\\ P_4 &= a_{22} * (b_{21} - b_{11} ) \\\\ P_5 &= (a_{11} + a_{22} ) * (b_{11} + b_{22} ) \\\\ P_6 &= (a_{12} - a_{22} ) * (b_{21} + b_{22} ) \\\\ P_7 &= ( a_{11} - a_{21} ) * (b_{11} + b_{12} ) \\end{align*} \\] How to compute \\(c_{ij}\\) using \\(P1 \\dots P7\\) ? \\[ \\begin{align*} c_{11} & = P_5 + P_4 \u2013 P_2 + P_6 \\\\ c_{12} & = P_1 + P_2 \\\\ c_{21} & = P_3 + P_4 \\\\ c_{22} & = P_5 + P_1 \u2013 P_3 \u2013 P_7 \\end{align*} \\]","title":"Matrix Multiplication: Strassen\u2019s Idea (3)"},{"location":"tr/week-3/ce100-week-3-matrix/#matrix-multiplication-strassens-idea-4","text":"\\(7\\) recursive multiply calls \\(18\\) add/sub operations","title":"Matrix Multiplication: Strassen\u2019s Idea (4)"},{"location":"tr/week-3/ce100-week-3-matrix/#matrix-multiplication-strassens-idea-5","text":"e.g. Show that \\(c_{12} = P_1+P_2\\) : \\[ \\begin{align*} c_{12} & = P_1 + P_2 \\\\ &= a_{11}(b_{12}\u2013b_{22})+(a_{11}+a_{12})b_{22} \\\\ &= a_{11}b_{12}-a_{11}b_{22}+a_{11}b_{22}+a_{12}b_{22} \\\\ &= a_{11}b_{12}+a_{12}b_{22} \\end{align*} \\]","title":"Matrix Multiplication: Strassen\u2019s Idea (5)"},{"location":"tr/week-3/ce100-week-3-matrix/#strassens-algorithm","text":"Divide: Partition \\(A\\) and \\(B\\) into \\((n/2)*(n/2)\\) submatrices. Form terms to be multiplied using \\(+\\) and \\(-\\) . Conquer: Perform \\(7\\) multiplications of \\((n/2)*(n/2)\\) submatrices recursively. Combine: Form \\(C\\) using \\(+\\) and \\(\u2013\\) on \\((n/2)*(n/2)\\) submatrices. Recurrence: \\(T(n) = 7T(n/2) + \\Theta(n^2)\\)","title":"Strassen\u2019s Algorithm"},{"location":"tr/week-3/ce100-week-3-matrix/#strassens-algorithm-solving-the-recurrence-1","text":"\\(T(n) = 7T(n/2) + \\Theta(n^2)\\) \\(a=7\\) , \\(b=2\\) \\(f(n)=\\Theta(n^2)\\) \\(n^{log_b^a}=n^{lg7}\\) Case 1: \\(\\frac{n^{log_b^a}}{f(n)}=\\Omega(n^{\\varepsilon}) \\Longrightarrow T(n)=\\Theta(n^{log_b^a})\\) \\(T(n)=\\Theta(n^{log_2^7})\\) \\(2^3 = 8, 2^2=4\\) so \\(\\Longrightarrow log_2^7 \\approx 2.81\\) or use https://www.omnicalculator.com/math/log","title":"Strassen\u2019s Algorithm: Solving the Recurrence (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#strassens-algorithm-solving-the-recurrence-2","text":"The number \\(2.81\\) may not seem much smaller than \\(3\\) But, it is significant because the difference is in the exponent. Strassen\u2019s algorithm beats the ordinary algorithm on today\u2019s machines for \\(n \\geq 30\\) or so. Best to date: \\(\\Theta(n^{2.376 \\dots})\\) (of theoretical interest only)","title":"Strassen\u2019s Algorithm: Solving the Recurrence (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#maximum-subarray-problem","text":"Input: An array of values Output: The contiguous subarray that has the largest sum of elements Input array: \\([13][-3][-25][20][-3][-16][-23]\\overbrace{[18][20][-7][12]}^{\\textrm{max. contiguous subarray}}[-22][-4][7]\\)","title":"Maximum Subarray Problem"},{"location":"tr/week-3/ce100-week-3-matrix/#maximum-subarray-problem-divide-conquer-1","text":"Basic idea: Divide the input array into 2 from the middle Pick the best solution among the following: The max subarray of the left half The max subarray of the right half The max subarray crossing the mid-point","title":"Maximum Subarray Problem: Divide &amp; Conquer (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#maximum-subarray-problem-divide-conquer-2","text":"","title":"Maximum Subarray Problem: Divide &amp; Conquer (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#maximum-subarray-problem-divide-conquer-3","text":"Divide: Trivial (divide the array from the middle) Conquer: Recursively compute the max subarrays of the left and right halves Combine: Compute the max-subarray crossing the \\(mid-point\\) (can be done in \\(\\Theta(n)\\) time). Return the max among the following: the max subarray of the \\(\\text{left-subarray}\\) the max subarray of the \\(\\text{rightsubarray}\\) the max subarray crossing the \\(\\text{mid-point}\\) TODO : detailed solution in textbook...","title":"Maximum Subarray Problem: Divide &amp; Conquer (3)"},{"location":"tr/week-3/ce100-week-3-matrix/#conclusion-divide-conquer","text":"Divide and conquer is just one of several powerful techniques for algorithm design. Divide-and-conquer algorithms can be analyzed using recurrences and the master method (so practice this math). Can lead to more efficient algorithms","title":"Conclusion : Divide &amp; Conquer"},{"location":"tr/week-3/ce100-week-3-matrix/#quicksort-1","text":"One of the most-used algorithms in practice Proposed by C.A.R. Hoare in 1962. Divide-and-conquer algorithm In-place algorithm The additional space needed is O(1) The sorted array is returned in the input array Reminder: Insertion-sort is also an in-place algorithm, but Merge-Sort is not in-place. Very practical","title":"Quicksort (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#quicksort-2","text":"Divide: Partition the array into 2 subarrays such that elements in the lower part \\(\\leq\\) elements in the higher part Conquer: Recursively sort 2 subarrays Combine: Trivial (because in-place) Key: Linear-time \\((\\Theta(n))\\) partitioning algorithm section{ font-size: 25px; }","title":"Quicksort (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#divide-partition-the-array-around-a-pivot-element","text":"Choose a pivot element \\(x\\) Rearrange the array such that: Left subarray: All elements \\(\\leq x\\) Right subarray: All elements \\(\\geq x\\)","title":"Divide: Partition the array around a pivot element"},{"location":"tr/week-3/ce100-week-3-matrix/#conquer-recursively-sort-the-subarrays","text":"Note: Everything in the left subarray \u2264 everything in the right subarray Note: Combine is trivial after conquer. Array already sorted. section{ font-size: 25px; }","title":"Conquer: Recursively Sort the Subarrays"},{"location":"tr/week-3/ce100-week-3-matrix/#two-partitioning-algorithms","text":"Hoare\u2019s algorithm: Partitions around the first element of subarray \\((pivot = x = A[p])\\) Lomuto\u2019s algorithm: Partitions around the last element of subarray \\((pivot = x = A[r])\\)","title":"Two partitioning algorithms"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-1","text":"Choose a pivot element: \\(pivot = x = A[p]\\) Grow two regions: from left to right: \\(A[p \\dots i]\\) from right to left: \\(A[j \\dots r]\\) such that: every element in \\(A[p \\dots i] \\leq\\) pivot every element in \\(A[p \\dots i] \\geq\\) pivot","title":"Hoare\u2019s Partitioning Algorithm (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-2","text":"section{ font-size: 25px; }","title":"Hoare\u2019s Partitioning Algorithm (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-3","text":"Elements are exchanged when \\(A[i]\\) is too large to belong to the left region \\(A[j]\\) is too small to belong to the right region assuming that the inequality is strict The two regions \\(A[p \\dots i]\\) and \\(A[j \\dots r]\\) grow until \\(A[i] \\geq pivot \\geq A[j]\\) H - PARTITION ( A , p , r ) pivot = A [ p ] i = p - 1 j = r - 1 while true do repeat j = j - 1 until A [ j ] <= pivot repeat i = i - 1 until A [ i ] <= pivot if i < j then exchange A [ i ] with A [ j ] else return j","title":"Hoare\u2019s Partitioning Algorithm (3)"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-1","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-1)"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-2","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-2)"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-3","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-3)"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-4","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-4)"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-5","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-5)"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-6","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-6)"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-7","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-7)"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-8","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-8)"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-9","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-9)"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-10","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-10)"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-11","text":"","title":"Hoare\u2019s Partitioning Algorithm Example (Step-11)"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-example-step-12","text":"section{ font-size: 25px; }","title":"Hoare\u2019s Partitioning Algorithm Example (Step-12)"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-notes","text":"Elements are exchanged when \\(A[i]\\) is too large to belong to the left region \\(A[j]\\) is too small to belong to the right region assuming that the inequality is strict The two regions \\(A[p \\dots i]\\) and \\(A[j \\dots r]\\) grow until \\(A[i] \\geq pivot \\geq A[j]\\) The asymptotic runtime of Hoare\u2019s partitioning algorithm \\(\\Theta(n)\\) H - PARTITION ( A , p , r ) pivot = A [ p ] i = p - 1 j = r - 1 while true do repeat j = j - 1 until A [ j ] <= pivot repeat i = i - 1 until A [ i ] <= pivot if i < j then exchange A [ i ] with A [ j ] else return j section{ font-size: 25px; }","title":"Hoare\u2019s Partitioning Algorithm - Notes"},{"location":"tr/week-3/ce100-week-3-matrix/#quicksort-with-hoares-partitioning-algorithm","text":"QUICKSORT ( A , p , r ) if p < r then q = H - PARTITION ( A , p , r ) QUICKSORT ( A , p , q ) QUICKSORT ( A , q + 1 , r ) endif Initial invocation: QUICKSORT(A,1,n) section{ font-size: 25px; }","title":"Quicksort with Hoare\u2019s Partitioning Algorithm"},{"location":"tr/week-3/ce100-week-3-matrix/#hoares-partitioning-algorithm-pivot-selection","text":"if we select pivot to be \\(A[r]\\) instead of \\(A[p]\\) in H-PARTITION Consider the example where \\(A[r]\\) is the largest element in the array: End of H-PARTITION: \\(i = j = r\\) In QUICKSORT: \\(q = r\\) So, recursive call to: QUICKSORT(A, p, q=r) infinite loop","title":"Hoare\u2019s Partitioning Algorithm: Pivot Selection"},{"location":"tr/week-3/ce100-week-3-matrix/#correctness-of-hoares-algorithm-1","text":"We need to prove \\(3\\) claims to show correctness: Indices \\(i\\) and \\(j\\) never reference \\(A\\) outside the interval \\(A[p \\dots r]\\) Split is always non-trivial; i.e., \\(j \\neq r\\) at termination Every element in \\(A[p \\dots j] \\leq\\) every element in \\(A[j+1 \\dots r]\\) at termination","title":"Correctness of Hoare\u2019s Algorithm (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#correctness-of-hoares-algorithm-2","text":"Notations: \\(k\\) : \\(\\#\\) of times the while-loop iterates until termination \\(i_m\\) : the value of index i at the end of iteration \\(m\\) \\(j_m\\) : the value of index j at the end of iteration \\(m\\) \\(x\\) : the value of the pivot element Note : We always have \\(i_1= p\\) and \\(p \\leq j_1 \\leq r\\) because \\(x = A[p]\\)","title":"Correctness of Hoare\u2019s Algorithm (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#correctness-of-hoares-algorithm-3","text":"Lemma 1: Either \\(i_k = j_k\\) or \\(i_k = j_k+1\\) at termination Proof of Lemma 1: The algorithm terminates when \\(i \\geq j\\) (the else condition). So, it is sufficient to prove that \\(i_k \u2013 j_k \\leq 1\\) There are \\(2\\) cases to consider: Case 1: \\(k = 1\\) , i.e. the algorithm terminates in a single iteration Case 2: \\(k > 1\\) , i.e. the alg. does not terminate in a single iter. By contradiction , assume there is a run with \\(i_k \u2013 j_k > 1\\) section{ font-size: 25px; }","title":"Correctness of Hoare\u2019s Algorithm (3)"},{"location":"tr/week-3/ce100-week-3-matrix/#correctness-of-hoares-algorithm-4","text":"Original correctness claims: Indices \\(i\\) and \\(j\\) never reference A outside the interval \\(A[p \\dots r]\\) Split is always non-trivial; i.e., \\(j \\neq r\\) at termination Proof: For \\(k = 1\\) : Trivial because \\(i_1 = j_1 = p\\) ( see Case 1 in proof of Lemma 2 ) For \\(k > 1\\) : \\(i_k > p\\) and \\(j_k < r\\) ( due to the repeat-until loops moving indices ) \\(i_k \\leq r\\) and \\(j_k \\geq p\\) ( due to Lemma 1 and the statement above ) The proof of claims (a) and (b) complete section{ font-size: 25px; }","title":"Correctness of Hoare\u2019s Algorithm (4)"},{"location":"tr/week-3/ce100-week-3-matrix/#correctness-of-hoares-algorithm-5","text":"Lemma 2: At the end of iteration \\(m\\) , where \\(m<k\\) ( i.e. m is not the last iteration ), we must have: \\(A[p \\dots i_m] \\leq x\\) and \\(A[j_m \\dots r] \\geq x\\) Proof of Lemma 2: Base case: \\(m=1\\) and \\(k>1\\) ( i.e. the alg. does not terminate in the first iter. ) Ind. Hyp.: At the end of iteration \\(m-1\\) , where \\(m<k\\) ( i.e. m is not the last iteration ), we must have: \\(A[p \\dots i_m-1] \\leq x\\) and \\(A[j_m-1 \\dots r] \\geq x\\) General case: The lemma holds for \\(m\\) , where \\(m < k\\) Proof of base case complete!","title":"Correctness of Hoare\u2019s Algorithm (5)"},{"location":"tr/week-3/ce100-week-3-matrix/#correctness-of-hoares-algorithm-6","text":"Original correctness claim: \u00a9 Every element in \\(A[ \\dots j] \\leq\\) every element in \\(A[j+ \\dots r]\\) at termination Proof of claim \u00a9 There are \\(3\\) cases to consider: Case 1: \\(k=1\\) , i.e. the algorithm terminates in a single iteration Case 2: \\(k>1\\) and \\(i_k = j_k\\) Case 3: \\(k>1\\) and \\(i_k = j_k + 1\\)","title":"Correctness of Hoare\u2019s Algorithm (6)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-1","text":"Choose a pivot element: \\(pivot = x = A[r]\\) Grow two regions: from left to right: \\(A[p \\dots i]\\) from left to right: \\(A[i+1 \\dots j]\\) such that: every element in \\(A[p \\dots i] \\leq pivot\\) every element in \\(A[i+1 \\dots j] > pivot\\)","title":"Lomuto\u2019s Partitioning Algorithm (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-2","text":"","title":"Lomuto\u2019s Partitioning Algorithm (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-1","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-1)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-2","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-2)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-3","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-3)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-4","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-4)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-5","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-5)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-6","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-6)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-7","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-7)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-8","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-8)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-9","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-9)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-10","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-10)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-11","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-11)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-12","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-12)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-13","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-13)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-14","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-14)"},{"location":"tr/week-3/ce100-week-3-matrix/#lomutos-partitioning-algorithm-ex-step-15","text":"","title":"Lomuto\u2019s Partitioning Algorithm Ex. (Step-15)"},{"location":"tr/week-3/ce100-week-3-matrix/#quicksort-with-lomutos-partitioning-algorithm","text":"QUICKSORT ( A , p , r ) if p < r then q = L - PARTITION ( A , p , r ) QUICKSORT ( A , p , q - 1 ) QUICKSORT ( A , q + 1 , r ) endif Initial invocation: QUICKSORT(A,1,n)","title":"Quicksort with Lomuto\u2019s Partitioning Algorithm"},{"location":"tr/week-3/ce100-week-3-matrix/#comparison-of-hoares-lomutos-algorithms-1","text":"Notation: \\(n=r-p+1\\) \\(pivot=A[p]\\) ( Hoare ) \\(pivot=A[r]\\) ( Lomuto ) \\(\\#\\) of element exchanges: \\(e(n)\\) Hoare: \\(0 \\geq e(n) \\geq \\lfloor \\frac{n}{2} \\rfloor\\) Best : \\(k=1\\) with \\(i_1=j_1=p\\) (i.e., \\(A[p+1 \\dots r]>pivot\\) ) Worst : \\(A[p+1 \\dots p+ \\lfloor \\frac{n}{2} \\rfloor - 1] \\geq pivot \\geq A[p+ \\lceil \\frac{n}{2} \\rceil \\dots r]\\) Lomuto : \\(1 \\leq e(n) \\leq n\\) Best : \\(A[p \\dots r -1]>pivot\\) Worst : \\(A[p \\dots r-1] \\leq pivot\\)","title":"Comparison of Hoare\u2019s &amp; Lomuto\u2019s Algorithms (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#comparison-of-hoares-lomutos-algorithms-2","text":"\\(\\#\\) of element comparisons: \\(c_e(n)\\) Hoare : \\(n+1 \\leq c_e(n) \\leq n+2\\) Best : \\(i_k=j_k\\) Worst : \\(i_k=j_k+1\\) Lomuto : \\(c_e(n)=n-1\\) \\(\\#\\) of index comparisons: \\(c_i(n)\\) Hoare : \\(1 \\leq c_i(n) \\leq \\lfloor \\frac{n}{2} \\rfloor + 1 | (c_i(n)=e(n)+1)\\) Lomuto : \\(c_i(n)=n-1\\)","title":"Comparison of Hoare\u2019s &amp; Lomuto\u2019s Algorithms (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#comparison-of-hoares-lomutos-algorithms-3","text":"\\(\\#\\) of index increment/decrement operations: \\(a(n)\\) Hoare : \\(n+1 \\leq a(n) \\leq n+2 | (a(n)=c_e(n))\\) Lomuto : \\(n \\leq a(n) \\leq 2n-1 | (a(n)=e(n)+(n-1))\\) Hoare\u2019s algorithm is in general faster Hoare behaves better when pivot is repeated in \\(A[p \\dots r]\\) Hoare : Evenly distributes them between left & right regions Lomuto : Puts all of them to the left region","title":"Comparison of Hoare\u2019s &amp; Lomuto\u2019s Algorithms (3)"},{"location":"tr/week-3/ce100-week-3-matrix/#analysis-of-quicksort-1","text":"QUICKSORT ( A , p , r ) if p < r then q = H - PARTITION ( A , p , r ) QUICKSORT ( A , p , q ) QUICKSORT ( A , q + 1 , r ) endif Initial invocation: QUICKSORT(A,1,n) Assume all elements are distinct in the following analysis","title":"Analysis of Quicksort (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#analysis-of-quicksort-2","text":"H-PARTITION always chooses \\(A[p]\\) (the first element) as the pivot. The runtime of QUICKSORT on an already-sorted array is \\(\\Theta(n^2)\\)","title":"Analysis of Quicksort (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#example-an-already-sorted-array","text":"Partitioning always leads to \\(2\\) parts of size \\(1\\) and \\(n-1\\)","title":"Example: An Already Sorted Array"},{"location":"tr/week-3/ce100-week-3-matrix/#worst-case-analysis-of-quicksort","text":"Worst case is when the PARTITION algorithm always returns imbalanced partitions (of size \\(1\\) and \\(n-1\\) ) in every recursive call. This happens when the pivot is selected to be either the min or max element. This happens for H-PARTITION when the input array is already sorted or reverse sorted \\[ \\begin{align*} T(n) &= T(1) + T(n-1) + \u0398(n) \\\\ &= T(n-1) + \u0398(n) \\\\ &= \u0398(n2) \\end{align*} \\]","title":"Worst Case Analysis of Quicksort"},{"location":"tr/week-3/ce100-week-3-matrix/#worst-case-recursion-tree","text":"\\[ T(n) = T(1) + T(n-1) + cn \\]","title":"Worst Case Recursion Tree"},{"location":"tr/week-3/ce100-week-3-matrix/#best-case-analysis-for-intuition-only","text":"If we\u2019re extremely lucky, H-PARTITION splits the array evenly at every recursive call \\[ \\begin{align*} T(n) &= 2T(n/2) + \\Theta(n) \\\\ &= \\Theta(nlgn) \\end{align*} \\] (same as merge sort) Instead of splitting \\(0.5:0.5\\) , if we split \\(0.1:0.9\\) then we need solve following equation. \\[ \\begin{align*} T(n) &= T(n/10) + T(9n/10) + \\Theta(n) \\\\ &= \\Theta(nlgn) \\end{align*} \\]","title":"Best Case Analysis (for intuition only)"},{"location":"tr/week-3/ce100-week-3-matrix/#almost-best-case-analysis","text":"","title":"\u201cAlmost-Best\u201d Case Analysis"},{"location":"tr/week-3/ce100-week-3-matrix/#balanced-partitioning-1","text":"We have seen that if H-PARTITION always splits the array with \\(0.1-to-0.9\\) ratio, the runtime will be \\(\\Theta(nlgn)\\) . Same is true with a split ratio of \\(0.01-to-0.99\\) , etc. Possible to show that if the split has always constant \\((\\Theta(1))\\) proportionality, then the runtime will be \\(\\Theta(nlgn)\\) . In other words, for a constant \\(\\alpha | (0 < \\alpha \u2264 0.5)\\) : \\(\\alpha\u2013to\u2013(1-\\alpha)\\) proportional split yields \\(\\Theta(nlgn)\\) total runtime","title":"Balanced Partitioning (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#balanced-partitioning-2","text":"In the rest of the analysis, assume that all input permutations are equally likely. This is only to gain some intuition We cannot make this assumption for average case analysis We will revisit this assumption later Also, assume that all input elements are distinct.","title":"Balanced Partitioning (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#balanced-partitioning-3","text":"Question: What is the probability that H-PARTITION returns a split that is more balanced than \\(0.1-to-0.9\\) ?","title":"Balanced Partitioning (3)"},{"location":"tr/week-3/ce100-week-3-matrix/#balanced-partitioning-4","text":"Reminder: H-PARTITION will place the pivot in the right partition unless the pivot is the smallest element in the arrays. Question: If the pivot selected is the mth smallest value \\((1 < m \u2264 n)\\) in the input array, what is the size of the left region after partitioning?","title":"Balanced Partitioning (4)"},{"location":"tr/week-3/ce100-week-3-matrix/#balanced-partitioning-5","text":"Question: What is the probability that the pivot selected is the \\(m^{th}\\) smallest value in the array of size \\(n\\) ? \\(1/n\\) ( since all input permutations are equally likely ) Question: What is the probability that the left partition returned by H-PARTITION has size \\(m\\) , where \\(1<m<n\\) ? \\(1/n\\) ( due to the answers to the previous 2 questions )","title":"Balanced Partitioning (5)"},{"location":"tr/week-3/ce100-week-3-matrix/#balanced-partitioning-6","text":"section{ font-size: 25px; } Question: What is the probability that H-PARTITION returns a split that is more balanced than \\(0.1-to-0.9\\) ? \\[ \\begin{align*} Probability &=\\sum \\limits_{q=0.1n+1}^{0.9n-1}\\frac{1}{n} \\\\ &=\\frac{1}{n}(0.9n-1-0.1n-1+1) \\\\ &= 0.8-\\frac{1}{n} \\\\ & \\approx 0.8 \\text{ for large n} \\end{align*} \\]","title":"Balanced Partitioning (6)"},{"location":"tr/week-3/ce100-week-3-matrix/#balanced-partitioning-7","text":"The probability that H-PARTITION yields a split that is more balanced than \\(0.1-to-0.9\\) is \\(80\\%\\) on a random array. Let \\(P_{\\alpha>}\\) be the probability that H-PARTITION yields a split more balanced than \\(\\alpha-to-(1-\\alpha)\\) , where \\(0 < \\alpha \\leq 0.5\\) Repeat the analysis to generalize the previous result section{ font-size: 25px; }","title":"Balanced Partitioning (7)"},{"location":"tr/week-3/ce100-week-3-matrix/#balanced-partitioning-8","text":"Question: What is the probability that H-PARTITION returns a split that is more balanced than \\(\\alpha-to-(1-\\alpha)\\) ? \\[ \\begin{align*} Probability & =\\sum \\limits_{q=\\alpha n+1}^{(1-\\alpha)n-1}\\frac{1}{n} \\\\ & =\\frac{1}{n}((1-\\alpha)n-1- \\alpha n-1+1) \\\\ & = (1-2\\alpha)-\\frac{1}{n} \\\\ & \\approx (1-2\\alpha) \\text{ for large n} \\end{align*} \\]","title":"Balanced Partitioning (8)"},{"location":"tr/week-3/ce100-week-3-matrix/#balanced-partitioning-9","text":"We found \\(P_{\\alpha >}=1-2\\alpha\\) Ex: \\(P_{0.1>}=0.8\\) and \\(P_{0.01>}=0.98\\) Hence, H-PARTITION produces a split more balanced than a \\(0.1-to-0.9\\) split \\(80\\%\\) of the time \\(0.01-to-0.99\\) split \\(98\\%\\) of the time less balanced than a \\(0.1-to-0.9\\) split \\(20\\%\\) of the time \\(0.01-to-0.99\\) split \\(2\\%\\) of the time","title":"Balanced Partitioning (9)"},{"location":"tr/week-3/ce100-week-3-matrix/#intuition-for-the-average-case-1","text":"Assumption: All permutations are equally likely Only for intuition; we\u2019ll revisit this assumption later Unlikely: Splits always the same way at every level Expectation: Some splits will be reasonably balanced Some splits will be fairly unbalanced Average case: A mix of good and bad splits Good and bad splits distributed randomly thru the tree","title":"Intuition for the Average Case (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#intuition-for-the-average-case-2","text":"Assume for intuition: Good and bad splits occur in the alternate levels of the tree Good split: Best case split Bad split: Worst case split","title":"Intuition for the Average Case (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#intuition-for-the-average-case-3","text":"Compare 2-successive levels of avg case vs. 1 level of best case","title":"Intuition for the Average Case (3)"},{"location":"tr/week-3/ce100-week-3-matrix/#intuition-for-the-average-case-4","text":"In terms of the remaining subproblems, two levels of avg case is slightly better than the single level of the best case The avg case has extra divide cost of \\(\\Theta(n)\\) at alternate levels The extra divide cost \\(\\Theta(n)\\) of bad splits absorbed into the \\(\\Theta(n)\\) of good splits. Running time is still \\(\\Theta(nlgn)\\) But, slightly larger hidden constants, because the height of the recursion tree is about twice of that of best case.","title":"Intuition for the Average Case (4)"},{"location":"tr/week-3/ce100-week-3-matrix/#intuition-for-the-average-case-5","text":"Another way of looking at it: Suppose we alternate lucky, unlucky, lucky, unlucky, \\(\\dots\\) We can write the recurrence as: \\(L(n) = 2U(n/2) + \\Theta(n)\\) lucky split (best) \\(U(n) = L(n-1) + \\Theta(n)\\) unlucky split (worst) Solving: $$ \\begin{align*} L(n) & = 2(L(n/2-1) + \\Theta(n/2)) + \\Theta(n) \\ & = 2L(n/2-1) + \\Theta(n) \\ & = \u0398(nlgn) \\end{align*} $$ How can we make sure we are usually lucky for all inputs?","title":"Intuition for the Average Case (5)"},{"location":"tr/week-3/ce100-week-3-matrix/#summary-quicksort-runtime-analysis-1","text":"Worst case: Unbalanced split at every recursive call \\[ \\begin{align*} T(n) & = T(1) + T(n-1) + \\Theta(n) \\\\ T(n) & = \\Theta(n2) \\end{align*} \\] Best case: Balanced split at every recursive call ( extremely lucky ) \\[ \\begin{align*} T(n) & = 2T(n/2) + \\Theta(n) \\\\ T(n) & = \\Theta(nlgn) \\end{align*} \\]","title":"Summary: Quicksort Runtime Analysis (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#summary-quicksort-runtime-analysis-2","text":"Almost-best case: Almost-balanced split at every recursive call \\[ \\begin{align*} T(n) &=T(n/10)+T(9n/10)+ \\Theta(n) \\\\ \\text{or } T(n) &= T(n/100) + T(99n/100) + \u0398(n) \\\\ \\text{or } T(n) &= T(\\alpha n) + T((1-\\alpha n)+ \\Theta(n) \\end{align*} \\] for any constant \\(\\alpha, 0 < \\alpha \\leq 0.5\\)","title":"Summary: Quicksort Runtime Analysis (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#summary-quicksort-runtime-analysis-3","text":"For a random input array, the probability of having a split more balanced than \\(0.1 \u2013 to \u2013 0.9 : 80\\%\\) more balanced than \\(0.01 \u2013 to \u2013 0.99 : 98\\%\\) more balanced than \\(\\alpha \u2013 to \u2013 (1-\\alpha) : 1 \u2013 2 \\alpha\\) for any constant \\(\\alpha, 0 < \\alpha \\leq 0.5\\)","title":"Summary: Quicksort Runtime Analysis (3)"},{"location":"tr/week-3/ce100-week-3-matrix/#summary-quicksort-runtime-analysis-4","text":"Avg case intuition: Different splits expected at different levels some balanced (good), some unbalanced (bad) Avg case intuition: Assume the good and bad splits alternate i.e. good split -> bad split -> good split -> \u2026 \\(T(n) = \\Theta(nlgn)\\) (informal analysis for intuition)","title":"Summary: Quicksort Runtime Analysis (4)"},{"location":"tr/week-3/ce100-week-3-matrix/#randomized-quicksort","text":"In the avg-case analysis, we assumed that all permutations of the input array are equally likely. But, this assumption does not always hold e.g. What if all the input arrays are reverse sorted ? Always worst-case behavior Ideally, the avg-case runtime should be independent of the input permutation . Randomness should be within the algorithm , not based on the distribution of the inputs. i.e. The avg case should hold for all possible inputs","title":"Randomized Quicksort"},{"location":"tr/week-3/ce100-week-3-matrix/#randomized-algorithms-1","text":"Alternative to assuming a uniform distribution: Impose a uniform distribution e.g. Choose a random pivot rather than the first element Typically useful when: there are many ways that an algorithm can proceed but, it\u2019s difficult to determine a way that is always guaranteed to be good . If there are many good alternatives ; simply choose one randomly .","title":"Randomized Algorithms (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#randomized-algorithms-1_1","text":"Ideally: Runtime should be independent of the specific inputs No specific input should cause worst-case behavior Worst-case should be determined only by output of a random number generator. section{ font-size: 25px; }","title":"Randomized Algorithms (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#randomized-quicksort-1","text":"Using Hoare\u2019s partitioning algorithm: R - QUICKSORT ( A , p , r ) if p < r then q = R - PARTITION ( A , p , r ) R - QUICKSORT ( A , p , q ) R - QUICKSORT ( A , q +1 , r ) R - PARTITION ( A , p , r ) s = RANDOM ( p , r ) exchange A [ p ] with A [ s ] return H - PARTITION ( A , p , r ) Alternatively, permuting the whole array would also work but, would be more difficult to analyze section{ font-size: 25px; }","title":"Randomized Quicksort (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#randomized-quicksort-2","text":"Using Lomuto\u2019s partitioning algorithm: R - QUICKSORT ( A , p , r ) if p < r then q = R - PARTITION ( A , p , r ) R - QUICKSORT ( A , p , q -1 ) R - QUICKSORT ( A , q +1 , r ) R - PARTITION ( A , p , r ) s = RANDOM ( p , r ) exchange A [ r ] with A [ s ] return L - PARTITION ( A , p , r ) Alternatively, permuting the whole array would also work but, would be more difficult to analyze","title":"Randomized Quicksort (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#notations-for-formal-analysis","text":"Assume all elements in \\(A[p \\dots r]\\) are distinct Let \\(n = r \u2013 p + 1\\) Let \\(rank(x) = |{A[i]: p \\leq i \\leq r \\text{ and } A[i] \\leq x}|\\) i.e. \\(rank(x)\\) is the number of array elements with value less than or equal to \\(x\\) \\(A=\\{5,9,7,6,8,1,4\\}\\) \\(p=5,r=4\\) \\(rank(5)=3\\) i.e. it is the \\(3^{rd}\\) smallest element in the array","title":"Notations for Formal Analysis"},{"location":"tr/week-3/ce100-week-3-matrix/#formal-analysis-for-average-case","text":"The following analysis will be for Quicksort using Hoare\u2019s partitioning algorithm. Reminder: The pivot is selected randomly and exchanged with \\(A[p]\\) before calling H-PARTITION Let \\(x\\) be the random pivot chosen. What is the probability that \\(rank(x) = i\\) for \\(i = 1, 2, \\dots n\\) ? \\(P(rank(x) = i) = 1/n\\) section{ font-size: 25px; }","title":"Formal Analysis for Average Case"},{"location":"tr/week-3/ce100-week-3-matrix/#various-outcomes-of-h-partition-1","text":"Assume that \\(rank(x)=1\\) i.e. the random pivot chosen is the smallest element What will be the size of the left partition \\((|L|)\\) ? Reminder: Only the elements less than or equal to \\(x\\) will be in the left partition. \\(A=\\{\\overbrace{2}^{p=x=pivot}\\underbrace{,}_{\\Longrightarrow|L|=1 } 9,7,6,8,5,\\overbrace{4}^r\\}\\) \\(p=2,r=4\\) \\(pivot=x=2\\) TODO: convert to image...S6_P9 section{ font-size: 25px; }","title":"Various Outcomes of H-PARTITION (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#various-outcomes-of-h-partition-2","text":"Assume that \\(rank(x)>1\\) i.e. the random pivot chosen is not the smallest element What will be the size of the left partition \\((|L|)\\) ? Reminder: Only the elements less than or equal to \\(x\\) will be in the left partition. Reminder: The pivot will stay in the right region after H-PARTITION if \\(rank(x)>1\\) \\(A=\\{\\overbrace{2}^{p}, 4 \\underbrace{,}_{\\Longrightarrow|L|=rank(x)-1}7,6,8,\\overbrace{5,}^{pivot}\\overbrace{9}^r\\}\\) \\(p=2,r=4\\) \\(pivot=x=5\\) TODO: convert to image...S6_P10","title":"Various Outcomes of H-PARTITION (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#various-outcomes-of-h-partition-summary-1","text":"\\(x: pivot\\) \\(|L|: \\text{size of left region}\\) \\(P(rank(x) = i) = 1/n \\text{ for } 1 \\leq i \\leq n\\) \\(\\text{if } rank(x) = 1 \\text{ then } |L| = 1\\) \\(\\text{if } rank(x) > 1 \\text{ then } |L| = rank(x) - 1\\) \\(P(|L| = 1) = P(rank(x) = 1) + P(rank(x) = 2)\\) \\(P(|L| = 1) = 2/n\\) \\(P(|L| = i) = P(rank(x) = i+1) \\text{ for } 1< i < n\\) \\(P(|L| = i) = 1/n \\text{ for } 1< i < n\\)","title":"Various Outcomes of H-PARTITION - Summary (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#various-outcomes-of-h-partition-summary-2","text":"section{ font-size: 25px; }","title":"Various Outcomes of H-PARTITION - Summary (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#average-case-analysis-recurrence-1","text":"\\(x=pivot\\) \\[ \\begin{align*} T(n) & = \\frac{1}{n}(T(1)+t(n-1) ) & rank:1 \\\\ & + \\frac{1}{n}(T(1)+t(n-1) ) & rank:2 \\\\ & + \\frac{1}{n}(T(2)+t(n-2) ) & rank:3 \\\\ & \\vdots & \\vdots \\\\ & + \\frac{1}{n}(T(i)+t(n-i) ) & rank:i+1 \\\\ & \\vdots & \\vdots \\\\ & + \\frac{1}{n}(T(n-1)+t(1) ) & rank:n \\\\ & + \\Theta(n) \\end{align*} \\] section{ font-size: 25px; }","title":"Average - Case Analysis: Recurrence (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#average-case-analysis-recurrence-2","text":"\\[ \\begin{align*} T(n) &= \\frac{1}{n}\\sum \\limits_{q=1}^{n-1}(T(q)+T(n-q))+\\frac{1}{n}(T(1)+T(n-1))+\\Theta(n)\\\\ & \\text{Note: } \\frac{1}{n}(T(1)+T(n-1))=\\frac{1}{n}(\\Theta(1)+O(n^2))=O(n) \\\\ T(n) &= \\frac{1}{n}\\sum \\limits_{q=1}^{n-1}(T(q)+T(n-q))+\\Theta(n) \\end{align*} \\] for \\(k=1,2,\\dots,n-1\\) each term \\(T(k)\\) appears twice once for \\(q = k\\) and once for \\(q = n\u2212k\\) \\[ T(n) = \\frac{2}{n}\\sum \\limits_{k=1}^{n-1} T(k)+\\Theta(n) \\]","title":"Average - Case Analysis: Recurrence (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#average-case-analysis-solving-recurrence-substitution","text":"Guess: \\(T(n)=O(nlgn)\\) \\(T(k) \u2264 aklgk\\) for \\(k<n\\) , for some constant \\(a > 0\\) \\[ \\begin{align*} T(n) &= \\frac{2}{n} \\sum \\limits_{k=1}^{n-1} T(k)+\\Theta(n) \\\\ & \\leq \\frac{2}{n} \\sum \\limits_{k=1}^{n-1} aklgk+\\Theta(n) \\\\ & \\leq \\frac{2a}{n} \\sum \\limits_{k=1}^{n-1} klgk+\\Theta(n) \\end{align*} \\] Need a tight bound for \\(\\sum klgk\\)","title":"Average - Case Analysis -Solving Recurrence: Substitution"},{"location":"tr/week-3/ce100-week-3-matrix/#tight-bound-for-sum-klgk-1","text":"Bounding the terms \\(\\ \\sum \\limits_{k=1}^{n-1}klgk \\leq \\sum \\limits_{k=1}^{n-1}nlgn = n(n-1)lgn \\leq n^2lgn\\) This bound is not strong enough because \\(T(n) \\leq \\frac{2a}{n}n^2lgn+\\Theta(n)\\) \\(=2anlgn+\\Theta(n)\\) \\(\\Longrightarrow\\) couldn\u2019t prove \\(T(n) \\leq anlgn\\)","title":"Tight bound for \\(\\sum klgk\\) (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#tight-bound-for-sum-klgk-2","text":"Splitting summations: ignore ceilings for simplicity $$ \\sum \\limits_{k=1}^{n-1}klgk \\leq \\sum \\limits_{k=1}^{n/2-1}klgk + \\sum \\limits_{k=n/2}^{n-1}klgk $$ First summation : \\(lgk < lg(n/2)=lgn-1\\) Second summation : \\(lgk < lgn\\) section{ font-size: 25px; }","title":"Tight bound for \\(\\sum klgk\\) (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#splitting-sum-limits_k1n-1klgk-leq-sum-limits_k1n2-1klgk-sum-limits_kn2n-1klgk-3","text":"\\[ \\begin{align*} & \\sum \\limits_{k=1}^{n-1}klgk \\leq (lg(n-1))\\sum \\limits_{k=1}^{n/2-1}k + lgn \\sum \\limits_{k=n/2}^{n-1}k \\\\ &= lgn \\sum \\limits_{k=1}^{n-1}k- \\sum \\limits_{k=1}^{n/2-1}k \\\\ &= \\frac{1}{2}n(n-1)lgn - \\frac{1}{2} \\frac{n}{2}(\\frac{n}{2}-1) \\\\ &= \\frac{1}{2}n^2lgn - \\frac{1}{8}n^2 - \\frac{1}{2}n(lgn-1/2) \\\\ \\end{align*} \\] \\[ \\begin{align*} & \\sum \\limits_{k=1}^{n-1} klgk \\leq \\frac{1}{2}n^2lgn-\\frac{1}{8}n^2 \\ for \\ lgn \\geq 1/2 \\Longrightarrow n \\geq \\sqrt{2} \\end{align*} \\]","title":"Splitting: \\(\\sum \\limits_{k=1}^{n-1}klgk \\leq \\sum \\limits_{k=1}^{n/2-1}klgk + \\sum \\limits_{k=n/2}^{n-1}klgk\\) (3)"},{"location":"tr/week-3/ce100-week-3-matrix/#substituting-sum-limits_k1n-1klgk-leq-frac12n2lgn-frac18n2-4","text":"\\[ \\begin{align*} T(n) & \\leq \\frac{2a}{n}\\sum \\limits_{k=1}^{n-1}klgk+\\Theta(n)\\\\ & \\leq \\frac{2a}{n}(\\frac{1}{2}n^2lgn-\\frac{1}{8}n^2)+\\Theta(n) \\\\ & = anlgn - (\\frac{a}{4}n-\\Theta(n)) \\end{align*} \\] We can choose a large enough so that \\(\\frac{a}{4}n \\geq \\Theta(n)\\) \\[ \\begin{align*} T(n) & \\leq anlgn \\\\ T(n) & = O(nlgn) \\end{align*} \\] Q.E.D.","title":"Substituting: - \\(\\sum \\limits_{k=1}^{n-1}klgk \\leq \\frac{1}{2}n^2lgn-\\frac{1}{8}n^2\\) (4)"},{"location":"tr/week-3/ce100-week-3-matrix/#medians-and-order-statistics","text":"ith order statistic : \\(i^{th}\\) smallest element of a set of \\(n\\) elements minimum: first order statistic maximum: \\(n^{th}\\) order statistic median: \u201chalfway point\u201d of the set \\[ \\begin{align*} i & = \\lfloor \\frac{(n+1)}{2} \\rfloor \\\\ \\text{ or } \\\\ i & = \\lceil \\frac{(n+1)}{2} \\rceil \\end{align*} \\]","title":"Medians and Order Statistics"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-problem","text":"Selection problem: Select the \\(i^{th}\\) smallest of \\(n\\) elements Na\u00efve algorithm: Sort the input array \\(A\\) ; then return \\(A[i]\\) \\(T(n) = \\theta(nlgn)\\) using e.g. merge sort (but not quicksort) Can we do any better?","title":"Selection Problem"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-expected-linear-time","text":"Randomized algorithm using divide and conquer Similar to randomized quicksort Like quicksort: Partitions input array recursively Unlike quicksort: Makes a single recursive call Reminder: Quicksort makes two recursive calls Expected runtime: \\(\\Theta(n)\\) Reminder: Expected runtime of quicksort: \\(\\Theta(nlgn)\\)","title":"Selection in Expected Linear Time"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-expected-linear-time-example-1","text":"Select the \\(2^{nd}\\) smallest element: \\[ \\begin{align*} A & = \\{6,10,13,5,8,3,2,11\\} \\\\ i & = 2 \\\\ \\end{align*} \\] Partition the input array: \\[ \\begin{align*} A & = \\{\\underbrace{2,3,5,}_{\\text{left subarray} }\\underbrace{13,8,10,6,11}_{\\text{right subarray}}\\} \\end{align*} \\] make a recursive call to select the \\(2^{nd}\\) smallest element in left subarray","title":"Selection in Expected Linear Time: Example 1"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-expected-linear-time-example-2","text":"Select the \\(7^{th}\\) smallest element: \\[ \\begin{align*} A & = \\{6,10,13,5,8,3,2,11\\} \\\\ i & = 7 \\\\ \\end{align*} \\] Partition the input array: \\[ \\begin{align*} A & = \\{\\underbrace{2,3,5,}_{\\text{left subarray} }\\underbrace{13,8,10,6,11}_{\\text{right subarray}}\\} \\end{align*} \\] make a recursive call to select the \\(4^{th}\\) smallest element in right subarray","title":"Selection in Expected Linear Time: Example 2"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-expected-linear-time-1","text":"R - SELECT ( A , p , r , i ) if p == r then return A [ p ]; q = R - PARTITION ( A , p , r ) k = q \u2013 p +1 ; if i <= k then return R - SELECT ( A , p , q , i ); else return R - SELECT ( A , q +1 , r , i - k ); \\[ \\begin{align*} A & = \\{ \\underbrace{ | }_{p} \\dots \\leq x \\text{(k smallest elements)} \\dots \\underbrace{ | }_{q} \\dots \\geq x \\dots \\underbrace{ | }_{r} \\} \\\\ x & = pivot \\end{align*} \\]","title":"Selection in Expected Linear Time (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-expected-linear-time-2","text":"\\[ \\begin{align*} A & = \\{ \\overbrace{ | }^{p} \\underbrace{ \\dots \\leq x \\dots }_{L} \\overbrace{ | }^{q} \\underbrace{ \\dots \\geq x \\dots }_{R} \\overbrace{ | }^{r} \\} \\\\ x & = pivot \\end{align*} \\] All elements in \\(L \\leq\\) all elements in \\(R\\) \\(L\\) contains: \\(|L| = q\u2013p+1\\) \\(=\\) k smallest elements of \\(A[p...r]\\) if \\(i \\leq |L| = k\\) then search \\(L\\) recursively for its \\(i^{th}\\) smallest element else search \\(R\\) recursively for its \\((i-k)^{th}\\) smallest element","title":"Selection in Expected Linear Time (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#runtime-analysis-1","text":"Worst case: Imbalanced partitioning at every level and the recursive call always to the larger partition \\[ \\begin{align*} & = \\{1,\\underbrace{2,3,4,5,6,7,8}_{\\text{recursive call}} \\} & i & = 8 \\\\ & = \\{2,\\underbrace{3,4,5,6,7,8}_{\\text{recursive call}} \\} & i & = 7 \\end{align*} \\]","title":"Runtime Analysis (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#runtime-analysis-2","text":"Worst case: Worse than the na\u00efve method (based on sorting) \\[ \\begin{align*} T(n) &= T(n-1) + \\Theta(n) \\\\ T(n) &= \\Theta(n^2) \\end{align*} \\] Best case: Balanced partitioning at every recursive level \\[ \\begin{align*} T(n) &= T(n/2) + \\Theta(n) \\\\ T(n) &= \\Theta(n) \\end{align*} \\] Avg case: Expected runtime \u2013 need analysis T.B.D. section{ font-size: 25px; }","title":"Runtime Analysis (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#reminder-various-outcomes-of-h-partition","text":"\\(x: pivot\\) \\(|L|: \\text{size of left region}\\) \\(P(rank(x) = i) = 1/n \\text{ for } 1 \\leq i \\leq n\\) \\(\\text{if } rank(x) = 1 \\text{ then } |L| = 1\\) \\(\\text{if } rank(x) > 1 \\text{ then } |L| = rank(x) - 1\\) \\(P(|L| = 1) = P(rank(x) = 1) + P(rank(x) = 2)\\) \\(P(|L| = 1) = 2/n\\) \\(P(|L| = i) = P(rank(x) = i+1) \\text{ for } 1< i < n\\) \\(P(|L| = i) = 1/n \\text{ for } 1< i < n\\)","title":"Reminder: Various Outcomes of H-PARTITION"},{"location":"tr/week-3/ce100-week-3-matrix/#average-case-analysis-of-randomized-select","text":"To compute the upper bound for the avg case , assume that the \\(i^{th}\\) element always falls into the larger partition . \\[ \\begin{align*} A & = \\{ \\overbrace{ | }^{p} \\underbrace{ \\dots \\leq x \\dots }_{Left Partition} \\overbrace{ | }^{q} \\underbrace{ \\dots \\geq x \\dots }_{Right Partition} \\overbrace{ | }^{r} \\} \\\\ x & = pivot \\end{align*} \\] We will analyze the case where the recursive call is always made to the larger partition This will give us an upper bound for the avg case","title":"Average Case Analysis of Randomized Select"},{"location":"tr/week-3/ce100-week-3-matrix/#various-outcomes-of-h-partition","text":"","title":"Various Outcomes of H-PARTITION"},{"location":"tr/week-3/ce100-week-3-matrix/#average-case-analysis-of-randomized-select-1","text":"\\[ \\text{Recall:} P(|L|=i) = \\begin{cases} 2/n & \\text{for } i=1 \\\\ 1/n & \\text{for } i=2,3,\\dots,n-1 \\end{cases} \\] Upper bound: Assume \\(i^{th}\\) element always falls into the larger part. \\[ \\begin{align*} T(n) &\\leq \\frac{1}{n}T(max(1,n-1))+\\frac{1}{n}\\sum \\limits_{q=1}^{n-1}T(max(q,n-q))+O(n) \\\\ Note: & \\frac{1}{n}T(max(1,n-1)) = \\frac{1}{n}T(n-1)=\\frac{1}{n}O(n^2) = O(n) \\\\ \\therefore \\text{(3 dot mean therefore) } & T(n) \\leq \\frac{1}{n}\\sum \\limits_{q=1}^{n-1}T(max(q,n-q))+O(n) \\end{align*} \\]","title":"Average-Case Analysis of Randomized Select (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#average-case-analysis-of-randomized-select-2","text":"\\[ \\begin{align*} \\therefore T(n) \\leq \\frac{1}{n}\\sum \\limits_{q=1}^{n-1}T(max(q,n-q))+O(n) \\end{align*} \\] \\[ max(q, n\u2013q) = \\begin{cases} q & \\text{ if } q \\geq \\lceil n/2 \\rceil \\\\ n-q & \\text{ if } q < \\lceil n/2 \\rceil \\\\ \\end{cases} \\] \\(n\\) is odd: \\(T(k)\\) appears twice for \\(k=\\lceil n/2 \\rceil+1,\\lceil n/2 \\rceil+2,\\dots,n\u20131\\) \\(n\\) is even: \\(T(\\lceil n/2 \\rceil)\\) appears once \\(T(k)\\) appears twice for \\(k = \\lceil n/2 \\rceil +1, \\lceil n/2 \\rceil+2,\\dots,n\u20131\\)","title":"Average-Case Analysis of Randomized Select (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#average-case-analysis-of-randomized-select-3","text":"Hence, in both cases: \\[ \\begin{align*} \\sum \\limits_{q=1}^{n-1} T(max(q,n-q))+O(n) & \\leq 2\\sum \\limits_{q=\\lceil n/2 \\rceil}^{n-1} T(q)+O(n) \\\\ \\therefore T(n) & \\leq \\frac{2}{n} \\sum \\limits_{q=\\lceil n/2 \\rceil}^{n-1}T(q)+O(n) \\end{align*} \\] section{ font-size: 25px; }","title":"Average-Case Analysis of Randomized Select (3)"},{"location":"tr/week-3/ce100-week-3-matrix/#average-case-analysis-of-randomized-select-4","text":"\\[ \\begin{align*} T(n) & \\leq \\frac{2}{n} \\sum \\limits_{q=\\lceil n/2 \\rceil}^{n-1}T(q)+O(n) \\end{align*} \\] By substitution guess \\(T(n) = O(n)\\) Inductive hypothesis: \\(T(k) \\leq ck, \\forall k<n\\) \\[ \\begin{align*} T(n) & \\leq \\frac{2}{n} \\sum \\limits_{q=\\lceil n/2 \\rceil}^{n-1}ck+O(n) \\\\ & = \\frac{2c}{n} \\Bigg(\\sum \\limits_{k=1}^{n-1}k-\\sum \\limits_{k=1}^{\\lceil n/2 \\rceil-1}k \\Bigg)+ O(n) \\\\ & = \\frac{2c}{n} \\Bigg(\\frac{1}{2}n(n-1)-\\frac{1}{2} \\lceil \\frac{n}{2} \\rceil \\bigg( \\frac{n}{2}-1 \\bigg) \\Bigg)+ O(n) \\end{align*} \\]","title":"Average-Case Analysis of Randomized Select (4)"},{"location":"tr/week-3/ce100-week-3-matrix/#average-case-analysis-of-randomized-select-5","text":"\\[ \\begin{align*} T(n)& \\leq \\frac{2c}{n} \\Bigg(\\frac{1}{2}n(n-1)-\\frac{1}{2} \\lceil \\frac{n}{2} \\rceil \\bigg( \\frac{n}{2}-1 \\bigg) \\Bigg)+ O(n) \\\\ & \\leq c(n-1)-\\frac{c}{4}n+\\frac{c}{2}+O(n) \\\\ & = cn - \\frac{c}{4}n - \\frac{c}{2} + O(n) \\\\ & = cn - \\Bigg( \\bigg( \\frac{c}{4}n+\\frac{c}{2}\\bigg) + O(n) \\Bigg) \\\\ & \\leq cn \\end{align*} \\] since we can choose c large enough so that \\((cn/4+c/2 )\\) dominates \\(O(n)\\)","title":"Average-Case Analysis of Randomized Select (5)"},{"location":"tr/week-3/ce100-week-3-matrix/#summary-of-randomized-order-statistic-selection","text":"Works fast: linear expected time Excellent algorithm in practise But, the worst case is very bad: \\(\\Theta(n^2)\\) Blum, Floyd, Pratt, Rivest & Tarjan[1973] algorithms are runs in linear time in the worst case . Generate a good pivot recursively section{ font-size: 25px; }","title":"Summary of Randomized Order-Statistic Selection"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time","text":"// return i - th element in set S with n elements SELECT ( S , n , i ) if n <= 5 then SORT S and return the i - th element DIVIDE S into ceil ( n / 5 ) groups // first ceil ( n / 5 ) groups are of size 5 , last group is of size n mod 5 FIND median set M = { m , \u2026 , m_ceil ( n / 5 )} // m_j : median of j - th group x = SELECT ( M , ceil ( n / 5 ), floor (( ceil ( n / 5 ) +1 ) / 2 )) PARTITION set S around the pivot x into L and R if i <= | L | then return SELECT ( L , | L | , i ) else return SELECT ( R , n \u2013 | L | , i \u2013 | L | )","title":"Selection in Worst Case Linear Time"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-1","text":"Input: Array \\(S\\) and index \\(i\\) Output: The \\(i^{th}\\) smallest value \\[ \\begin{array}{ccc} 25 & 9 & 16 & 8 & 11 & 27 & 39 & 42 & 15 & 6 32 & 14 & 36 & 20 & 33 & 22 & 31 & 4 & 17 & 3 & 30 & 41 \\\\ 2 & 13 & 19 & 7 & 21 & 10 & 34 & 1 & 37 & 23 & 40 & 5 & 29 & 18 & 24 & 12 & 38 & 28 & 26 & 35 & 43 \\end{array} \\]","title":"Selection in Worst Case Linear Time - Example (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-2","text":"Step 1: Divide the input array into groups of size \\(5\\) \\[ \\overbrace{ \\begin{array}{ccc} 25 & 9 & 16 & 8 & 11 \\\\ 27 & 39 & 42 & 15 & 6 \\\\ 32 & 14 & 36 & 20 & 33 \\\\ 22 & 31 & 4 & 17 & 3 \\\\ 30 & 41 & 2 & 13 & 19 \\\\ 7 & 21 & 10 & 34 & 1 \\\\ 37 & 23 & 40 & 5 & 29 \\\\ 18 & 24 & 12 & 38 & 28 \\\\ 26 & 35 & 43 \\end{array} }^{\\text{group size}=5} \\] section{ font-size: 25px; }","title":"Selection in Worst Case Linear Time - Example (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-3","text":"Step 2: Compute the median of each group ( \\(\\Theta(n)\\) ) \\[ \\begin{array}{ccc} 25 & 16 & \\overbrace{11}^{Medians} & 8 & 9 \\\\ 39 & 42 & 27 & 6 & 15 \\\\ 36 & 33 & 32 & 20 & 14 \\\\ 22 & 31 & 17 & 3 & 4 \\\\ 41 & 30 & 19 & 13 & 2 \\\\ 21 & 34 & 10 & 1 & 7 \\\\ 37 & 40 & 29 & 23 & 5 \\\\ 38 & 28 & 24 & 12 & 18 \\\\ & 26 & 35 & 43 \\end{array} \\] Let \\(M\\) be the set of the medians computed: \\(M = \\{11, 27, 32, 17, 19, 10, 29, 24, 35\\}\\)","title":"Selection in Worst Case Linear Time - Example (3)"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-4","text":"Step 3: Compute the median of the median group \\(M\\) \\(x \\leftarrow SELECT(M,|M|,\\lfloor (|M|+1)/2 \\rfloor)\\) where \\(|M|=\\lceil n/5 \\rceil\\) Let \\(M\\) be the set of the medians computed: \\(M = \\{11, 27, 32, 17, 19, 10, 29, \\overbrace{24}^{Median}, 35\\}\\) \\(Median = 24\\) The runtime of the recursive call: \\(T(|M|)=T(\\lceil n/5 \\rceil)\\)","title":"Selection in Worst Case Linear Time - Example (4)"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-5","text":"Step 4: Partition the input array \\(S\\) around the median-of-medians \\(x\\) \\[ \\begin{array}{ccc} 25 & 9 & 16 & 8 & 11 & 27 & 39 & 42 & 15 & 6 32 & 14 & 36 & 20 & 33 & 22 & 31 & 4 & 17 & 3 & 30 & 41 \\\\ 2 & 13 & 19 & 7 & 21 & 10 & 34 & 1 & 37 & 23 & 40 & 5 & 29 & 18 & 24 & 12 & 38 & 28 & 26 & 35 & 43 \\end{array} \\] Partition \\(S\\) around \\(x = 24\\) Claim: Partitioning around x is guaranteed to be well-balanced. section{ font-size: 25px; }","title":"Selection in Worst Case Linear Time - Example (5)"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-6","text":"\\(M\\) : Median, \\(M^*\\) : Median of Medians \\[ \\begin{array}{ccc} 41 & 30 & \\overbrace{19}^{M} & 13 & 2 \\\\ 21 & 34 & 10 & 1 & 7 \\\\ 22 & 31 & 17 & 3 & 4 \\\\ 25 & 16 & 11 & 8 & 9 \\\\ 38 & 28 & \\overbrace{24}^{M^*} & 12 & 18 \\\\ 36 & 33 & 32 & 20 & 14 \\\\ 37 & 40 & 29 & 23 & 5 \\\\ 39 & 42 & 27 & 6 & 15 \\\\ & 26 & 35 & 43 \\end{array} \\] About half of the medians greater than \\(x=24\\) (about \\(n/10\\) )","title":"Selection in Worst Case Linear Time - Example (6)"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-7","text":"","title":"Selection in Worst Case Linear Time - Example (7)"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-8","text":"","title":"Selection in Worst Case Linear Time - Example (8)"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-example-9","text":"\\[ S = \\begin{array}{ccc} \\{ 25 & 9 & 16 & 8 & 11 & 27 & 39 & 42 & 15 & 6 32 & 14 & 36 & 20 & 33 & 22 & 31 & 4 & 17 & 3 & 30 & 41 \\\\ 2 & 13 & 19 & 7 & 21 & 10 & 34 & 1 & 37 & 23 & 40 & 5 & 29 & 18 & 24 & 12 & 38 & 28 & 26 & 35 & 43 \\} \\end{array} \\] Partitioning \\(S\\) around \\(x = 24\\) will lead to partitions of sizes \\(\\sim 3n/10\\) and \\(\\sim 7n/10\\) in the worst case . Step 5: Make a recursive call to one of the partitions if i <= | L | then return SELECT ( L , | L | , i ) else return SELECT ( R , n -| L | , i -| L | ) section{ font-size: 25px; }","title":"Selection in Worst Case Linear Time - Example (9)"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time_1","text":"// return i - th element in set S with n elements SELECT ( S , n , i ) if n <= 5 then SORT S and return the i - th element DIVIDE S into ceil ( n / 5 ) groups // first ceil ( n / 5 ) groups are of size 5 , last group is of size n mod 5 FIND median set M = { m , \u2026 , m_ceil ( n / 5 )} // m_j : median of j - th group x = SELECT ( M , ceil ( n / 5 ), floor (( ceil ( n / 5 ) +1 ) / 2 )) PARTITION set S around the pivot x into L and R if i <= | L | then return SELECT ( L , | L | , i ) else return SELECT ( R , n \u2013 | L | , i \u2013 | L | )","title":"Selection in Worst Case Linear Time"},{"location":"tr/week-3/ce100-week-3-matrix/#choosing-the-pivot-1","text":"Divide S into groups of size 5","title":"Choosing the Pivot (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#choosing-the-pivot-2","text":"Divide S into groups of size 5 Find the median of each group","title":"Choosing the Pivot (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#choosing-the-pivot-3","text":"Divide S into groups of size 5 Find the median of each group Recursively select the median x of the medians","title":"Choosing the Pivot (3)"},{"location":"tr/week-3/ce100-week-3-matrix/#choosing-the-pivot-4","text":"At least half of the medians \\(\\geq x\\) Thus \\(m = \\lceil \\lceil n/5 \\rceil /2 \\rceil\\) groups contribute 3 elements to R except possibly the last group and the group that contains \\(x\\) , \\(|R|\\geq 3(m\u20132)\\geq \\frac{3n}{10}\u20136\\)","title":"Choosing the Pivot (4)"},{"location":"tr/week-3/ce100-week-3-matrix/#choosing-the-pivot-5","text":"Similarly \\(|L| \\geq \\frac{3n}{10}\u2013 6\\) Therefore, SELECT is recursively called on at most \\(n-(\\frac{3n}{10}-6)=\\frac{7n}{10}+6\\) elements","title":"Choosing the Pivot (5)"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-1","text":"","title":"Selection in Worst Case Linear Time (1)"},{"location":"tr/week-3/ce100-week-3-matrix/#selection-in-worst-case-linear-time-2","text":"Thus recurrence becomes \\(T(n) \\leq T \\big( \\lceil \\frac{n}{5} \\rceil \\big) + T\\big( \\frac{7n}{10}+6 \\big) + \\Theta(n)\\) Guess \\(T(n)=O(n)\\) and prove by induction Inductive step: \\[ \\begin{align*} T(n) & \\leq c \\lceil n/5 \\rceil + c(7n/10+6)+\\Theta(n) \\\\ & \\leq cn/5 + c + 7cn/10 + 6c + \\Theta(n) \\\\ & = 9cn/10 + 7c + \\Theta(n) \\\\ & = cn - [c(n/10-7)-\\Theta(n)] \\leq cn &\\text{( for large c)} \\end{align*} \\] Work at each level of recursion is a constant factor \\((9/10)\\) smaller","title":"Selection in Worst Case Linear Time (2)"},{"location":"tr/week-3/ce100-week-3-matrix/#references","text":"Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) Insertion Sort - GeeksforGeeks NIST Dictionary of Algorithms and Data Structures NIST - Dictionary of Algorithms and Data Structures NIST - big-O notation NIST - big-Omega notation \\(-End-Of-Week-3-Course-Module-\\)","title":"References"},{"location":"tr/week-4/ce100-week-4-heap/","text":"CE100 Algorithms and Programming II \u00b6 Week-4 (Heap/Heap Sort) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Heap/Heap Sort \u00b6 Outline (1) \u00b6 Heaps Max / Min Heap Heap Data Structure Heapify Iterative Recursive Outline (2) \u00b6 Extract-Max Build Heap Outline (3) \u00b6 Heap Sort Priority Queues Linked Lists Radix Sort Counting Sort Heapsort \u00b6 Worst-case runtime: \\(O(nlgn)\\) Sorts in-place Uses a special data structure (heap) to manage information during execution of the algorithm Another design paradigm Heap Data Structure (1) \u00b6 Nearly complete binary tree Completely filled on all levels except possibly the lowest level Heap Data Structure (2) \u00b6 Height of node i: Length of the longest simple downward path from i to a leaf Height of the tree: height of the root Heap Data Structures (3) \u00b6 Depth of node i: Length of the simple downward path from the root to node i Heap Property: Min-Heap \u00b6 The smallest element in any subtree is the root element in a min-heap Min heap: For every node i other than root , \\(A[parent(i)] \\leq A[i]\\) Parent node is always smaller than the child nodes Heap Property: Max-Heap \u00b6 The largest element in any subtree is the root element in a max-heap We will focus on max-heaps Max heap: For every node i other than root , \\(A[parent(i)] \u2265 A[i]\\) Parent node is always larger than the child nodes Heap Data Structures (4) \u00b6 Heap Data Structures (5) \u00b6 Computing left child, right child, and parent indices very fast left(i) = 2i \\(\\Longrightarrow\\) binary left shift right(i) = 2i+1 \\(\\Longrightarrow\\) binary left shift, then set the lowest bit to 1 parent(i) = floor(i/2) \\(\\Longrightarrow\\) right shift in binary \\(A[1]\\) is always the root element Array \\(A\\) has two attributes: length(A): The number of elements in \\(A\\) n = heap-size(A): The number elements in \\(heap\\) \\(n \\leq length(A)\\) Heap Operations : EXTRACT-MAX (1) \u00b6 EXTRACT - MAX ( A , n ) max = A [ 1 ] A [ 1 ] = A [ n ] n = n - 1 HEAPIFY ( A , 1 , n ) return max Heap Operations : EXTRACT-MAX (2) \u00b6 Return the max element,and reorganize the heap to maintain heap property Heap Operations: HEAPIFY (1) \u00b6 Heap Operations: HEAPIFY (2) \u00b6 Maintaining heap property: Subtrees rooted at \\(left[i]\\) and \\(right[i]\\) are already heaps. But, \\(A[i]\\) may violate the heap property (i.e., may be smaller than its children) Idea: Float down the value at \\(A[i]\\) in the heap so that subtree rooted at \\(i\\) becomes a heap. Heap Operations: HEAPIFY (2) \u00b6 HEAPIFY ( A , i , n ) largest = i if 2i <= n and A [ 2i ] > A [ i ] then largest = 2i ; endif if 2i+1 <= n and A [ 2i+1 ] > A [ largest ] then largest = 2i+1 ; endif if largest != i then exchange A [ i ] with A [ largest ]; HEAPIFY ( A , largest , n ); endif Heap Operations: HEAPIFY (3) \u00b6 Heap Operations: HEAPIFY (4) \u00b6 Heap Operations: HEAPIFY (5) \u00b6 Heap Operations: HEAPIFY (6) \u00b6 Heap Operations: HEAPIFY (7) \u00b6 Heap Operations: HEAPIFY (8) \u00b6 Intuitive Analysis of HEAPIFY \u00b6 Consider \\(HEAPIFY(A, i, n)\\) let \\(h(i)\\) be the height of node \\(i\\) at most \\(h(i)\\) recursion levels Constant work at each level: \\(\\Theta(1)\\) Therefore \\(T(i)=O(h(i))\\) Heap is almost-complete binary tree \\(h(n)=O(lgn)\\) Thus \\(T(n)=O(lgn)\\) Formal Analysis of HEAPIFY \u00b6 What is the recurrence? Depends on the size of the subtree on which recursive call is made In the next, we try to compute an upper bound for this subtree . Reminder: Binary trees \u00b6 For a complete binary tree: \\(\\#\\) of nodes at depth \\(d\\) : \\(2^d\\) \\(\\#\\) of nodes with depths less than \\(d\\) : \\(2^d-1\\) Formal Analysis of HEAPIFY (1) \u00b6 Worst case occurs when last row of the subtree \\(S_i\\) rooted at node \\(i\\) is half full \\(T(n) \\leq T(|S_{L(i)}|) + \\Theta(1)\\) \\(S_{L(i)}\\) and \\(S_{R(i)}\\) are complete binary trees of heights \\(h(i)-1\\) and \\(h(i)-2\\) , respectively Formal Analysis of HEAPIFY (2) \u00b6 Let \\(m\\) be the number of leaf nodes in \\(S_{L(i)}\\) \\(|S_{L(i)}|=\\overbrace{m}^{ext.}+\\overbrace{(m\u20131)}^{int.}=2m\u20131\\) \\(|S_{R(i)}|=\\overbrace{\\frac{m}{2}}^{ext.}+\\overbrace{(\\frac{m}{2}\u20131)}^{int.}=m\u20131\\) \\(|S_{L(i)}|+|S_{R(i)}|+1=n\\) Formal Analysis of HEAPIFY (2) \u00b6 \\[ \\begin{align*} (2m\u20131)+(m\u20131)+1 &=n \\\\ m &= (n+1)/3 \\\\ |S_{L(i)}| &= 2m \u2013 1 \\\\ &=2(n+1)/3 \u2013 1 \\\\ &=(2n/3+2/3) \u20131 \\\\ &=\\frac{2n}{3}-\\frac{1}{3} \\leq \\frac{2n}{3} \\\\ T(n) & \\leq T(2n/3) + \\Theta(1) \\\\ T(n) &= O(lgn) \\end{align*} \\] By CASE-2 of Master Theorem \\(\\Longrightarrow\\) \\(T(n)=\\Theta(n^{log_b^a}lgn)\\) Formal Analysis of HEAPIFY (2) \u00b6 Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(1)\\) i.e., \\(f(n)\\) and \\(n^{log_b^a}\\) grow at similar rates Solution: \\(T(n)=\\Theta(n^{log_b^a}lgn)\\) \\(T(n) \\leq T(2n/3) + \\Theta(1)\\) (drop constants.) \\(T(n) \\leq \\Theta(n^{log_3^1}lgn)\\) \\(T(n) \\leq \\Theta(n^0lgn)\\) \\(T(n) = O(lgn)\\) HEAPIFY: Efficiency Issues \u00b6 Recursion vs Iteration: In the absence of tail recursion, iterative version is in general more efficient because of the pop/push operations to/from stack at each level of recursion . Heap Operations: HEAPIFY (1) \u00b6 Recursive HEAPIFY ( A , i , n ) largest = i if 2i <= n and A [ 2i ] > A [ i ] then largest = 2i if 2i+1 <= n and A [ 2i+1 ] > A [ largest ] then largest = 2i+1 if largest != i then exchange A [ i ] with A [ largest ] HEAPIFY ( A , largest , n ) Heap Operations: HEAPIFY (2) \u00b6 Iterative HEAPIFY ( A , i , n ) j = i while ( true ) do largest = j if 2 j <= n and A [ 2 j ] > A [ j ] then largest = 2 j if 2 j +1 <= n and A [ 2 j +1 ] > A [ largest ] then largest = 2 j +1 if largest != j then exchange A [ j ] with A [ largest ] j = largest else return Heap Operations: HEAPIFY (3) \u00b6 Heap Operations: Building Heap \u00b6 Given an arbitrary array, how to build a heap from scratch? Basic idea: Call \\(HEAPIFY\\) on each node bottom up Start from the leaves (which trivially satisfy the heap property) Process nodes in bottom up order. When \\(HEAPIFY\\) is called on node \\(i\\) , the subtrees connected to the \\(left\\) and \\(right\\) subtrees already satisfy the heap property. Storage of the leaves (Lemma) \u00b6 Lemma: The last \\(\\lceil \\frac{n}{2} \\rceil\\) nodes of a heap are all leaves. Storage of the leaves (Proof of Lemma) (1) \u00b6 Lemma: last \\(\\lceil n/2 \\rceil\\) nodes of a heap are all leaves Proof : \\(m=2^{d-1}\\) : \\(\\#\\) nodes at level \\(d-1\\) \\(f\\) : \\(\\#\\) nodes at level \\(d\\) (last level) \\(\\#\\) of nodes with depth \\(d-1\\) : \\(m\\) \\(\\#\\) of nodes with depth \\(<d-1\\) : \\(m-1\\) \\(\\#\\) of nodes with depth \\(d\\) : \\(f\\) Total \\(\\#\\) of nodes : \\(n=f+2m-1\\) Storage of the leaves (Proof of Lemma) (2) \u00b6 Total \\(\\#\\) of nodes : \\(f=n-2m+1\\) \\[ \\begin{align*} \\text{\\# of leaves: }&=f+m-\\lceil f/2 \\rceil \\\\ &= m+\\lfloor f/2 \\rfloor \\\\ &= m+\\lfloor (n-2m+1)/2 \\rfloor \\\\ &= \\lfloor (n+1)/2 \\rfloor \\\\ &= \\lceil n/2 \\rceil \\end{align*} \\] Proof is Completed Heap Operations: Building Heap \u00b6 BUILD - HEAP ( A , n ) for i = ceil ( n / 2 ) downto 1 do HEAPIFY ( A , i , n ) Reminder: The last \\(\\lceil n/2 \\rceil\\) nodes of a heap are all leaves , which trivially satisfy the heap property Build-Heap Example (Step-1) \u00b6 Build-Heap Example (Step-2) \u00b6 Build-Heap Example (Step-3) \u00b6 Build-Heap Example (Step-4) \u00b6 Build-Heap Example (Step-5) \u00b6 Build-Heap Example (Step-6) \u00b6 Build-Heap Example (Step-7) \u00b6 Build-Heap Example (Step-8) \u00b6 Build-Heap Example (Step-9) \u00b6 Build-Heap: Runtime Analysis \u00b6 Simple analysis: \\(O(n)\\) calls to \\(HEAPIFY\\) , each of which takes \\(O(lgn)\\) time \\(O(nlgn)\\) \\(\\Longrightarrow\\) loose bound In general, a good approach: Start by proving an easy bound Then, try to tighten it Is there a tighter bound? section{ font-size: 25px; } Build-Heap: Tighter Running Time Analysis \u00b6 If the heap is complete binary tree then \\(h_{\\ell} = d \u2013 \\ell\\) Otherwise, nodes at a given level do not all have the same height, But we have \\(d \u2013 \\ell \u2013 1 \\leq h_{\\ell} \\leq d \u2013 \\ell\\) section{ font-size: 25px; } Build-Heap: Tighter Running Time Analysis \u00b6 Assume that all nodes at level \\(\\ell= d \u2013 1\\) are processed \\[ \\begin{align*} T(n) &=\\sum \\limits_{\\ell=0}^{d-1}n_{\\ell}O(h_{\\ell})=O(\\sum \\limits_{\\ell=0}^{d-1}n_{\\ell}h_{\\ell}) \\begin{cases} n_{\\ell}=2^{\\ell} = \\# \\text{ of nodes at level }\\ell \\\\ h_{\\ell}=\\text{height of nodes at level } \\ell \\end{cases} \\\\ \\therefore T(n) &= O \\bigg( \\sum \\limits_{\\ell=0}^{d-1}2^{\\ell}(d-\\ell) \\bigg) \\\\ \\text{Let } & h=d-\\ell \\Longrightarrow \\ell = d-h \\text{ change of variables} \\\\ T(n) &= O\\bigg(\\sum \\limits_{h=1}^{d}h2^{d-h} \\bigg)=O\\bigg(\\sum \\limits_{h=1}^{d}h \\frac{2^d}{2^h} \\bigg) = O\\bigg(2^d\\sum \\limits_{h=1}^{d}h (1/2)^h\\bigg) \\\\ \\text{ but } & 2^d = \\Theta(n) \\Longrightarrow O\\bigg(n\\sum \\limits_{h=1}^{d}h (1/2)^h \\bigg) \\end{align*} \\] section{ font-size: 25px; } Build-Heap: Tighter Running Time Analysis \u00b6 \\[ \\sum \\limits_{h=1}^{d}h(1/2)^h \\leq \\sum \\limits_{h=0}^{d}h(1/2)^h \\leq \\sum \\limits_{h=0}^{\\infty}h(1/2)^h \\] recall infinite decreasing geometric series \\[ \\sum \\limits_{k=0}^{\\infty} x^k = \\frac{1}{1-x} \\text{ where } |x|<1 \\] differentiate both sides \\[ \\sum \\limits_{k=0}^{\\infty}kx^{k-1} = \\frac{1}{(1-x)^2} \\] Build-Heap: Tighter Running Time Analysis \u00b6 \\[ \\sum \\limits_{k=0}^{\\infty}kx^{k-1} = \\frac{1}{(1-x)^2} \\] then, multiply both sides by \\(x\\) \\[ \\sum \\limits_{k=0}^{\\infty}kx^k = \\frac{x}{(1-x)^2} \\] in our case: \\(x = 1/2\\) and \\(k = h\\) \\[ \\therefore \\sum \\limits_{h=0}^{\\infty}h(1/2)^h = \\frac{1/2}{(1-(1/2))^2}=2=O(1) \\\\ \\therefore T(n)=O(n\\sum \\limits_{h=1}^{d}h(1/2)^h)=O(n) \\] Heapsort Algorithm Steps \u00b6 (1) Build a heap on array \\(A[1 \\dots n]\\) by calling \\(BUILD-HEAP(A, n)\\) (2) The largest element is stored at the root \\(A[1]\\) Put it into its correct final position \\(A[n]\\) by \\(A[1] \\longleftrightarrow A[n]\\) (3) Discard node \\(n\\) from the heap (4) Subtrees \\((S2 \\& S3)\\) rooted at children of root remain as heaps, but the new root element may violate the heap property. Make \\(A[1 \\dots n-1]\\) a heap by calling \\(HEAPIFY(A,1,n-1)\\) (5) \\(n \\leftarrow n-1\\) (6) Repeat steps (2-4) until \\(n=2\\) Heapsort Algorithm Example (Step-1) \u00b6 Heapsort Algorithm Example (Step-2) \u00b6 Heapsort Algorithm Example (Step-3) \u00b6 Heapsort Algorithm Example (Step-4) \u00b6 Heapsort Algorithm Example (Step-5) \u00b6 Heapsort Algorithm Example (Step-6) \u00b6 Heapsort Algorithm Example (Step-7) \u00b6 Heapsort Algorithm Example (Step-8) \u00b6 Heapsort Algorithm Example (Step-9) \u00b6 Heapsort Algorithm Example (Step-10) \u00b6 Heapsort Algorithm Example (Step-11) \u00b6 Heapsort Algorithm Example (Step-12) \u00b6 Heapsort Algorithm Example (Step-13) \u00b6 Heapsort Algorithm Example (Step-14) \u00b6 Heapsort Algorithm Example (Step-15) \u00b6 Heapsort Algorithm Example (Step-16) \u00b6 Heapsort Algorithm Example (Step-17) \u00b6 Heapsort Algorithm Example (Step-18) \u00b6 Heapsort Algorithm Example (Step-19) \u00b6 Heapsort Algorithm: Runtime Analysis \u00b6 \\[ \\begin{align*} T(n) &= \\Theta(n)+\\sum \\limits_{i=2}^{n}O(lgi) \\\\ &= \\Theta(n)+O\\bigg( \\sum \\limits_{i=2}^{n}O(lgn) \\bigg) \\\\ &= O(nlgn) \\end{align*} \\] Heapsort - Notes \u00b6 Heapsort is a very good algorithm but, a good implementation of quicksort always beats heapsort in practice However, heap data structure has many popular applications, and it can be efficiently used for implementing priority queues Data structures for Dynamic Sets \u00b6 Consider sets of records having key and satellite data Operations on Dynamic Sets \u00b6 Queries: Simply return info; \\(MAX(S) / MIN(S):\\) (Query) return \\(x \\in S\\) with the largest/smallest \\(key\\) \\(SEARCH(S, k):\\) (Query) return \\(x \\in S\\) with \\(key[x]= k\\) \\(SUCCESSOR(S, x) / PREDECESSOR(S, x):\\) (Query) return \\(y \\in S\\) which is the next larger/smaller element after \\(x\\) Modifying operations: Change the set \\(INSERT(S, x):\\) (Modifying) \\(S \\leftarrow S \\cup \\{x\\}\\) \\(DELETE(S, x):\\) (Modifying) \\(S \\leftarrow S - \\{x\\}\\) \\(\\text{EXTRACT-MAX}(S) / \\text{EXTRACT-MIN}(S):\\) (Modifying) return and delete \\(x \\in S\\) with the largest/smallest \\(key\\) Different data structures support/optimize different operations Priority Queues (PQ) \u00b6 Supports \\(INSERT\\) \\(MAX / MIN\\) \\(\\text{EXTRACT-MAX} / \\text{EXTRACT-MIN}\\) Priority Queues (PQ) \u00b6 One application: Schedule jobs on a shared resource PQ keeps track of jobs and their relative priorities When a job is finished or interrupted, highest priority job is selected from those pending using \\(\\text{EXTRACT-MAX}\\) A new job can be added at any time using \\(INSERT\\) Priority Queues (PQ) \u00b6 Another application: Event-driven simulation Events to be simulated are the items in the PQ Each event is associated with a time of occurrence which serves as a \\(key\\) Simulation of an event can cause other events to be simulated in the future Use \\(\\text{EXTRACT-MIN}\\) at each step to choose the next event to simulate As new events are produced insert them into the PQ using \\(INSERT\\) Implementation of Priority Queue \u00b6 Sorted linked list: Simplest implementation \\(INSERT\\) \\(O(n)\\) time Scan the list to find place and splice in the new item \\(\\text{EXTRACT-MAX}\\) \\(O(1)\\) time Take the first element Fast extraction but slow insertion. Implementation of Priority Queue \u00b6 Unsorted linked list: Simplest implementation \\(INSERT\\) \\(O(1)\\) time Put the new item at front \\(\\text{EXTRACT-MAX}\\) \\(O(n)\\) time Scan the whole list Fast insertion but slow extraction. Sorted linked list is better on the average Sorted list: on the average, scans \\(n/2\\) element per insertion Unsorted list: always scans \\(n\\) element at each extraction Heap Implementation of PQ \u00b6 \\(INSERT\\) and \\(\\text{EXTRACT-MAX}\\) are both \\(O(lgn)\\) good compromise between fast insertion but slow extraction and vice versa \\(\\text{EXTRACT-MAX}\\) : already discussed \\(\\text{HEAP-EXTRACT-MAX}\\) \\(INSERT\\) : Insertion is like that of Insertion-Sort. HEAP - INSERT ( A , key , n ) n = n +1 i = n while i > 1 and A [ floor ( i / 2 )] < key do A [ i ] = A [ floor ( i / 2 )] i = floor ( i / 2 ) A [ i ] = key Heap Implementation of PQ \u00b6 Traverses \\(O(lgn)\\) nodes, as \\(HEAPIFY\\) does but makes fewer comparisons and assignments \\(HEAPIFY\\) : compares parent with both children \\(HEAP-INSERT\\) : with only one HEAP-INSERT Example (Step-1) \u00b6 HEAP-INSERT Example (Step-2) \u00b6 HEAP-INSERT Example (Step-3) \u00b6 HEAP-INSERT Example (Step-4) \u00b6 HEAP-INSERT Example (Step-5) \u00b6 Heap Increase Key \u00b6 Key value of \\(i^{th}\\) element of heap is increased from \\(A[i]\\) to \\(key\\) HEAP - INCREASE - KEY ( A , i , key ) if key < A [ i ] then return error while i > 1 and A [ floor ( i / 2 )] < key do A [ i ] = A [ floor ( i / 2 )] i = floor ( i / 2 ) A [ i ] = key HEAP-INCREASE-KEY Example (Step-1) \u00b6 HEAP-INCREASE-KEY Example (Step-2) \u00b6 HEAP-INCREASE-KEY Example (Step-3) \u00b6 HEAP-INCREASE-KEY Example (Step-4) \u00b6 HEAP-INCREASE-KEY Example (Step-5) \u00b6 Heap Implementation of Priority Queue (PQ) \u00b6 section{ font-size: 25px; } Summary: Max Heap \u00b6 Heapify(A, i) Works when both child subtrees of node i are heaps \" Floats down \" node i to satisfy the heap property Runtime: \\(O(lgn)\\) Max(A, n) Returns the max element of the heap (no modification) Runtime: \\(O(1)\\) Extract-Max(A, n) Returns and removes the max element of the heap Fills the gap in \\(A[1]\\) with \\(A[n]\\) , then calls Heapify(A,1) Runtime: \\(O(lgn)\\) section{ font-size: 25px; } Summary: Max Heap \u00b6 Build-Heap(A, n) Given an arbitrary array, builds a heap from scratch Runtime: \\(O(n)\\) Min(A, n) How to return the min element in a max-heap? Worst case runtime: \\(O(n)\\) because ~half of the heap elements are leaf nodes Instead, use a min-heap for efficient min operations Search(A, x) For an arbitrary \\(x\\) value, the worst-case runtime: \\(O(n)\\) Use a sorted array instead for efficient search operations Summary: Max Heap \u00b6 Increase-Key(A, i, x) Increase the key of node \\(i\\) (from \\(A[i]\\) to \\(x\\) ) \u201c Float up \u201d \\(x\\) until heap property is satisfied Runtime: \\(O(lgn)\\) Decrease-Key(A, i, x) Decrease the key of node \\(i\\) (from \\(A[i]\\) to \\(x\\) ) Call Heapify(A, i) Runtime: \\(O(lgn)\\) Phone Operator Problem \u00b6 A phone operator answering \\(n\\) phones Each phone \\(i\\) has \\(x_i\\) people waiting in line for their calls to be answered. Phone operator needs to answer the phone with the largest number of people waiting in line. New calls come continuously, and some people hang up after waiting. Phone Operator Solution \u00b6 Step 1: Define the following array: \\(A[i]\\) : the ith element in heap \\(A[i].id\\) : the index of the corresponding phone \\(A[i].key\\) : \\(\\#\\) of people waiting in line for phone with index \\(A[i].id\\) Phone Operator Solution \u00b6 Step 2: \\(\\text{Build-Max-Heap}(A, n)\\) Execution: When the operator wants to answer a phone: \\(id = A[1].id\\) \\(\\text{Decrease-Key}(A, 1, A[1].key-1)\\) answer phone with index \\(id\\) When a new call comes in to phone i: \\(\\text{Increase-Key}(A, i, A[i].key+1)\\) When a call drops from phone i: \\(\\text{Decrease-Key}(A, i, A[i].key-1)\\) Linked Lists \u00b6 Like arrays, Linked List is a linear data structure. Unlike arrays, linked list elements are not stored at a contiguous location; the elements are linked using pointers. Linked Lists - C Definition \u00b6 C // A linked list node struct Node { int data ; struct Node * next ; }; Linked Lists - Cpp Definition \u00b6 Cpp class Node { public : int data ; Node * next ; }; Linked Lists - Java Definition \u00b6 Java class LinkedList { Node head ; // head of the list /* Linked list Node*/ class Node { int data ; Node next ; // Constructor to create a new node // Next is by default initialized // as null Node ( int d ) { data = d ; } } } Linked Lists - Csharp Definition \u00b6 Csharp class LinkedList { // The first node(head) of the linked list // Will be an object of type Node (null by default) Node head ; class Node { int data ; Node next ; // Constructor to create a new node Node ( int d ) { data = d ; } } } Priority Queue using Linked List Methods \u00b6 Implement Priority Queue using Linked Lists. push(): This function is used to insert a new data into the queue. pop(): This function removes the element with the highest priority from the queue. peek()/top(): This function is used to get the highest priority element in the queue without removing it from the queue. Priority Queue using Linked List Algorithm \u00b6 PUSH ( HEAD , DATA , PRIORITY ) Create NEW.Data = DATA & NEW.Priority = PRIORITY If HEAD.priority < NEW.Priority NEW -> NEXT = HEAD HEAD = NEW Else Set TEMP to head of the list Endif WHILE TEMP -> NEXT != NULL and TEMP -> NEXT -> PRIORITY > PRIORITY THEN TEMP = TEMP -> NEXT ENDWHILE NEW -> NEXT = TEMP -> NEXT TEMP -> NEXT = NEW Priority Queue using Linked List Algorithm \u00b6 POP ( HEAD ) // Set the head of the list to the next node in the list. HEAD = HEAD -> NEXT. Free the node at the head of the list PEEK ( HEAD ) : Return HEAD -> DATA Priority Queue using Linked List Notes \u00b6 LinkedList is already sorted. Time Complexities and Comparison with Binary Heap peek() push() pop() Linked List \\(O(1)\\) \\(O(n)\\) \\(O(1)\\) Binary Heap \\(O(1)\\) \\(O(lgn)\\) \\(O(lgn)\\) Sorting in Linear Time \u00b6 How Fast Can We Sort? \u00b6 The algorithms we have seen so far: Based on comparison of elements We only care about the relative ordering between the elements (not the actual values) The smallest worst-case runtime we have seen so far: \\(O(nlgn)\\) Is \\(O(nlgn)\\) the best we can do? Comparison sorts: Only use comparisons to determine the relative order of elements. Decision Trees for Comparison Sorts \u00b6 Represent a sorting algorithm abstractly in terms of a decision tree A binary tree that represents the comparisons between elements in the sorting algorithm Control, data movement, and other aspects are ignored One decision tree corresponds to one sorting algorithm and one value of \\(n\\) ( input size ) Reminder: Insertion Sort Step-By-Step Description (1) \u00b6 Reminder: Insertion Sort Step-By-Step Description (2) \u00b6 Reminder: Insertion Sort Step-By-Step Description (3) \u00b6 Different Outcomes for Insertion Sort and n=3 \u00b6 Input : \\(<a_1,a_2,a_3>\\) Decision Tree for Insertion Sort and n=3 \u00b6 Decision Tree Model for Comparison Sorts \u00b6 Internal node \\((i:j)\\) : Comparison between elements \\(a_i\\) and \\(a_j\\) Leaf node: An output of the sorting algorithm Path from root to a leaf: The execution of the sorting algorithm for a given input All possible executions are captured by the decision tree All possible outcomes (permutations) are in the leaf nodes Decision Tree for Insertion Sort and n=3 \u00b6 Input: \\(<9, 4, 6>\\) Decision Tree Model \u00b6 A decision tree can model the execution of any comparison sort: One tree for each input size \\(n\\) View the algorithm as splitting whenever it compares two elements The tree contains the comparisons along all possible instruction traces The running time of the algorithm \\(=\\) the length of the path taken Worst case running time \\(=\\) height of the tree Counting Sort \u00b6 Lower Bound for Comparison Sorts \u00b6 Let \\(n\\) be the number of elements in the input array. What is the \\(min\\) number of leaves in the decision tree? \\(n!\\) (because there are n! permutations of the input array, and all possible outputs must be captured in the leaves) What is the max number of leaves in a binary tree of height \\(h\\) ? \\(\\Longrightarrow\\) \\(2^h\\) So, we must have: $$ 2^h \\geq n! $$ Lower Bound for Decision Tree Sorting \u00b6 Theorem: Any comparison sort algorithm requires \\(\\Omega(nlgn)\\) comparisons in the worst case. Proof: We\u2019ll prove that any decision tree corresponding to a comparison sort algorithm must have height \\(\\Omega(nlgn)\\) \\[ \\begin{align*} 2^h & \\geq n! \\\\ h & \\geq lg(n!) \\\\ & \\geq lg((n/e)^n) (Stirling Approximation) \\\\ & = nlgn - nlge \\\\ & = \\Omega(nlgn) \\end{align*} \\] Lower Bound for Decision Tree Sorting \u00b6 Corollary: Heapsort and merge sort are asymptotically optimal comparison sorts. Proof: The \\(O(nlgn)\\) upper bounds on the runtimes for heapsort and merge sort match the \\(\\Omega(nlgn)\\) worst-case lower bound from the previous theorem. Sorting in Linear Time \u00b6 Counting sort: No comparisons between elements Input: \\(A[1 \\dots n]\\) , where \\(A[j] \\in \\{1, 2,\\dots, k\\}\\) Output: \\(B[1 \\dots n]\\) , sorted Auxiliary storage: \\(C[1 \\dots k]\\) Counting Sort-1 \u00b6 Counting Sort-2 \u00b6 Step 1: Initialize all counts to 0 Counting Sort-3 \u00b6 Step 2: Count the number of occurrences of each value in the input array Counting Sort-4 \u00b6 Step 3: Compute the number of elements less than or equal to each value Counting Sort-5 \u00b6 Step 4: Populate the output array There are \\(C[3] = 3\\) elements that are \\(\\leq 3\\) Counting Sort-6 \u00b6 Step 4: Populate the output array There are \\(C[4]=5\\) elements that are \\(\\leq 4\\) Counting Sort-7 \u00b6 Step 4: Populate the output array There are \\(C[3]=2\\) elements that are \\(\\leq 3\\) Counting Sort-8 \u00b6 Step 4: Populate the output array There are \\(C[1]=1\\) elements that are \\(\\leq 1\\) Counting Sort-9 \u00b6 Step 4: Populate the output array There are \\(C[4]=4\\) elements that are \\(\\leq 4\\) Counting Sort: Runtime Analysis \u00b6 Total Runtime: \\(\\Theta(n+k)\\) \\(n\\) : size of the input array \\(k\\) : the range of input values Counting Sort: Runtime \u00b6 Runtime is \\(\\Theta(n+k)\\) If \\(k=O(n)\\) , then counting sort takes \\(\\Theta(n)\\) Question: We proved a lower bound of \\(\\Theta(nlgn)\\) before! Where is the fallacy? Answer: \\(\\Theta(nlgn)\\) lower bound is for comparison-based sorting Counting sort is not a comparison sort In fact, not a single comparison between elements occurs! Stable Sorting \u00b6 Counting sort is a stable sort: It preserves the input order among equal elements. i.e. The numbers with the same value appear in the output array in the same order as they do in the input array. Note : Which other sorting algorithms have this property? Radix Sort \u00b6 Origin: Herman Hollerith\u2019s card-sorting machine for the 1890 US Census. Basic idea: Digit-by-digit sorting Two variations: Sort from MSD to LSD (bad idea) Sort from LSD to MSD (good idea) ( LSD/MSD: Least/most significant digit ) Herman Hollerith (1860-1929) \u00b6 The 1880 U.S. Census took almost 10 years to process. While a lecturer at MIT, Hollerith prototyped punched-card technology . His machines, including a card sorter , allowed the 1890 census total to be reported in 6 weeks . He founded the Tabulating Machine Company in 1911, which merged with other companies in 1924 to form International Business Machines(IBM) . Hollerith Punched Card \u00b6 Punched card: A piece of stiff paper that contains digital information represented by the presence or absence of holes. 12 rows and 24 columns coded for age, state of residency, gender, etc. Modern IBM card \u00b6 One character per column So, that\u2019s why text windows have 80 columns! for more samples visit https://en.wikipedia.org/wiki/Punched_card Hollerith Tabulating Machine and Sorter \u00b6 Mechanically sorts the cards based on the hole locations. Sorting performed for one column at a time Human operator needed to load/retrieve/move cards at each stage Hollerith\u2019s MSD-First Radix Sort \u00b6 Sort starting from the most significant digit (MSD) Then, sort each of the resulting bins recursively At the end, combine the decks in order Hollerith\u2019s MSD-First Radix Sort \u00b6 To sort a subset of cards recursively: All the other cards need to be removed from the machine, because the machine can handle only one sorting problem at a time. The human operator needs to keep track of the intermediate card piles Hollerith\u2019s MSD-First Radix Sort \u00b6 MSD-first sorting may require: very large number of sorting passes very large number of intermediate card piles to maintain S(d): \\(\\#\\) of passes needed to sort d-digit numbers (worst-case) Recurrence: \\(S(d)=10S(d-1)+1\\) with \\(S(1)=1\\) Reminder: Recursive call made to each subset with the same most significant digit(MSD) Hollerith\u2019s MSD-First Radix Sort \u00b6 Recurrence: \\(S(d)=10S(d-1)+1\\) \\[ \\begin{align*} S(d) &= 10 S(d-1) + 1 \\\\ & = 10 \\bigg(10 S(d-2) + 1 \\bigg) + 1 \\\\ & = 10 \\Big(10 \\bigg(10 S(d-3) + 1\\bigg) + 1 \\Big) + 1 \\\\ & = 10i S(d-i) + 10i-1 + 10i-2 + \\dots + 101 + 100 \\\\ &=\\sum \\limits_{i=0}^{d-1}10^i \\end{align*} \\] Iteration terminates when \\(i = d-1\\) with \\(S(d-(d-1)) = S(1) = 1\\) Hollerith\u2019s MSD-First Radix Sort \u00b6 Recurrence: \\(S(d)=10S(d-1)+1\\) \\[ \\begin{align*} S(d) &=\\sum \\limits_{i=0}^{d-1}10^i \\\\ & = \\frac{10^d-1}{10-1} \\\\ & = \\frac{1}{9}(10^d-1)\\\\ & \\Downarrow \\\\ S(d)&=\\frac{1}{9}(10^d-1) \\end{align*} \\] Hollerith\u2019s MSD-First Radix Sort \u00b6 \\(P(d)\\) : \\(\\#\\) of intermediate card piles maintained (worst-case) Reminder: Each routing pass generates 9 intermediate piles except the sorting passes on least significant digits (LSDs) There are \\(10^{d-1}\\) sorting calls to LSDs \\[ \\begin{align*} P(d) &= 9(S(d)\u201310^{d-1}) \\\\ &= 9\\frac{(10^{d\u20131})}{9\u2013 10^{d-1}} \\\\ &= (10^{d\u20131}\u20139 * 10^{d-1}) \\\\ &= 10^{d-1} - 1 \\end{align*} \\] Hollerith\u2019s MSD-First Radix Sort \u00b6 \\[ \\begin{align*} P(d) &= 10^{d-1} - 1 \\end{align*} \\] Alternative solution: Solve the recurrence \\[ \\begin{align*} P(d) &= 10P(d-1)+9 \\\\ P(1) &= 0 \\\\ \\end{align*} \\] Hollerith\u2019s MSD-First Radix Sort \u00b6 Example: To sort \\(3\\) digit numbers, in the worst case: \\(S(d) = (1/9) (103-1) = 111\\) sorting passes needed \\(P(d) = 10d-1-1 = 99\\) intermediate card piles generated MSD-first approach has more recursive calls and intermediate storage requirement Expensive for a tabulating machine to sort punched cards Overhead of recursive calls in a modern computer section{ font-size: 25px; } LSD-First Radix Sort \u00b6 Least significant digit ( LSD )-first radix sort seems to be a folk invention originated by machine operators. It is the counter-intuitive, but the better algorithm. Basic Algorithm: Sort numbers on their LSD first ( Stable Sorting Needed ) Combine the cards into a single deck in order Continue this sorting process for the other digits from the LSD to MSD Requires only \\(d\\) sorting passes No intermediate card pile generated LSD-first Radix Sort Example \u00b6 Correctness of Radix Sort (LSD-first) \u00b6 Proof by induction: Base case: \\(d=1\\) is correct ( trivial ) Inductive hyp: Assume the first \\(d-1\\) digits are sorted correctly Prove that all \\(d\\) digits are sorted correctly after sorting digit \\(d\\) Two numbers that differ in digit \\(d\\) are correctly sorted ( e.g. 355 and 657 ) Two numbers equal in digit d are put in the same order as the input ( correct order ) Radix Sort Runtime \u00b6 Use counting-sort to sort each digit Reminder: Counting sort complexity: \\(\\Theta(n+k)\\) \\(n\\) : size of input array \\(k\\) : the range of the values Radix sort runtime: \\(\\Theta(d(n+k))\\) \\(d\\) : \\(\\#\\) of digits How to choose the \\(d\\) and \\(k\\) ? Radix Sort: Runtime \u2013 Example 1 \u00b6 We have flexibility in choosing \\(d\\) and \\(k\\) Assume we are trying to sort 32-bit words We can define each digit to be 4 bits Then, the range for each digit \\(k=2^4=16\\) So, counting sort will take \\(\\Theta(n+16)\\) The number of digits \\(d =32/4=8\\) Radix sort runtime: \\(\\Theta(8(n+16)) = \\Theta(n)\\) \\(\\overbrace{[4bits|4bits|4bits|4bits|4bits|4bits|4bits|4bits]}^{\\text{32-bits}}\\) Radix Sort: Runtime \u2013 Example 2 \u00b6 We have flexibility in choosing \\(d\\) and \\(k\\) Assume we are trying to sort 32-bit words Or, we can define each digit to be 8 bits Then, the range for each digit \\(k = 2^8 = 256\\) So, counting sort will take \\(\\Theta(n+256)\\) The number of digits \\(d = 32/8 = 4\\) Radix sort runtime: \\(\\Theta(4(n+256)) = \\Theta(n)\\) \\(\\overbrace{[8bits|8bits|8bits|8bits]}^{\\text{32-bits}}\\) section{ font-size: 25px; } Radix Sort: Runtime \u00b6 Assume we are trying to sort \\(b\\) -bit words Define each digit to be \\(r\\) bits Then, the range for each digit \\(k = 2^r\\) So, counting sort will take \\(\\Theta(n+2^r)\\) The number of digits \\(d = b/r\\) Radix sort runtime: \\[ \\begin{align*} T(n,b)&=\\Theta \\bigg( \\frac{b}{r}(n+2^r) \\bigg) \\end{align*} \\] \\(\\overbrace{[rbits|rbits|rbits|rbits]}^{b/r \\text{ bits}}\\) Radix Sort: Runtime Analysis \u00b6 \\[ \\begin{align*} T(n,b)&=\\Theta \\bigg( \\frac{b}{r}(n+2^r) \\bigg) \\end{align*} \\] Minimize \\(T(n,b)\\) by differentiating and setting to \\(0\\) Or, intuitively: We want to balance the terms \\((b/r)\\) and \\((n + 2^r)\\) Choose \\(r \\approx lgn\\) If we choose \\(r << lgn \\Longrightarrow (n + 2^r)\\) term doesn\u2019t improve If we choose \\(r >> lgn \\Longrightarrow (n + 2^r)\\) increases exponentially Radix Sort: Runtime Analysis \u00b6 \\[ \\begin{align*} T(n,b)&=\\Theta \\bigg( \\frac{b}{r}(n+2^r) \\bigg) \\end{align*} \\] \\[ \\begin{align*} \\text{Choose } r=lgn \\Longrightarrow T(n,b)=\\Theta(bn/lgn) \\end{align*} \\] For numbers in the range from \\(0\\) to \\(n^d \u2013 1\\) , we have: The number of bits \\(b = lg(nd ) = d lgn\\) Radix sort runs in \\(\\Theta(dn)\\) Radix Sort: Conclusions \u00b6 \\[ \\begin{align*} \\text{Choose } r=lgn \\Longrightarrow T(n,b)=\\Theta(bn/lgn) \\end{align*} \\] Example: Compare radix sort with merge sort/heapsort \\(1\\) million ( \\(2^{20}\\) ), \\(32\\) -bit numbers \\((n = 2^{20}, b = 32)\\) Radix sort: \\(\\lfloor 32/20 \\rfloor = 2\\) passes Merge sort/heap sort: \\(lgn = 20\\) passes Downsides: Radix sort has little locality of reference (more cache misses) The version that uses counting sort is not in-place On modern processors, a well-tuned quicksort implementation typically runs faster. References \u00b6 Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) Insertion Sort - GeeksforGeeks Priority Queue Using Linked List - GeeksforGeeks Priority Queue Using Linked List - JavatPoint NIST Dictionary of Algorithms and Data Structures NIST - Dictionary of Algorithms and Data Structures \\(-End-Of-Week-4-Course-Module-\\)","title":"Week-4 (Heap/Heap Sort)"},{"location":"tr/week-4/ce100-week-4-heap/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"tr/week-4/ce100-week-4-heap/#week-4-heapheap-sort","text":"","title":"Week-4 (Heap/Heap Sort)"},{"location":"tr/week-4/ce100-week-4-heap/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"tr/week-4/ce100-week-4-heap/#heapheap-sort","text":"","title":"Heap/Heap Sort"},{"location":"tr/week-4/ce100-week-4-heap/#outline-1","text":"Heaps Max / Min Heap Heap Data Structure Heapify Iterative Recursive","title":"Outline (1)"},{"location":"tr/week-4/ce100-week-4-heap/#outline-2","text":"Extract-Max Build Heap","title":"Outline (2)"},{"location":"tr/week-4/ce100-week-4-heap/#outline-3","text":"Heap Sort Priority Queues Linked Lists Radix Sort Counting Sort","title":"Outline (3)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort","text":"Worst-case runtime: \\(O(nlgn)\\) Sorts in-place Uses a special data structure (heap) to manage information during execution of the algorithm Another design paradigm","title":"Heapsort"},{"location":"tr/week-4/ce100-week-4-heap/#heap-data-structure-1","text":"Nearly complete binary tree Completely filled on all levels except possibly the lowest level","title":"Heap Data Structure (1)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-data-structure-2","text":"Height of node i: Length of the longest simple downward path from i to a leaf Height of the tree: height of the root","title":"Heap Data Structure (2)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-data-structures-3","text":"Depth of node i: Length of the simple downward path from the root to node i","title":"Heap Data Structures (3)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-property-min-heap","text":"The smallest element in any subtree is the root element in a min-heap Min heap: For every node i other than root , \\(A[parent(i)] \\leq A[i]\\) Parent node is always smaller than the child nodes","title":"Heap Property: Min-Heap"},{"location":"tr/week-4/ce100-week-4-heap/#heap-property-max-heap","text":"The largest element in any subtree is the root element in a max-heap We will focus on max-heaps Max heap: For every node i other than root , \\(A[parent(i)] \u2265 A[i]\\) Parent node is always larger than the child nodes","title":"Heap Property: Max-Heap"},{"location":"tr/week-4/ce100-week-4-heap/#heap-data-structures-4","text":"","title":"Heap Data Structures (4)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-data-structures-5","text":"Computing left child, right child, and parent indices very fast left(i) = 2i \\(\\Longrightarrow\\) binary left shift right(i) = 2i+1 \\(\\Longrightarrow\\) binary left shift, then set the lowest bit to 1 parent(i) = floor(i/2) \\(\\Longrightarrow\\) right shift in binary \\(A[1]\\) is always the root element Array \\(A\\) has two attributes: length(A): The number of elements in \\(A\\) n = heap-size(A): The number elements in \\(heap\\) \\(n \\leq length(A)\\)","title":"Heap Data Structures (5)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-operations-extract-max-1","text":"EXTRACT - MAX ( A , n ) max = A [ 1 ] A [ 1 ] = A [ n ] n = n - 1 HEAPIFY ( A , 1 , n ) return max","title":"Heap Operations : EXTRACT-MAX (1)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-operations-extract-max-2","text":"Return the max element,and reorganize the heap to maintain heap property","title":"Heap Operations : EXTRACT-MAX (2)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-operations-heapify-1","text":"","title":"Heap Operations: HEAPIFY (1)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-operations-heapify-2","text":"Maintaining heap property: Subtrees rooted at \\(left[i]\\) and \\(right[i]\\) are already heaps. But, \\(A[i]\\) may violate the heap property (i.e., may be smaller than its children) Idea: Float down the value at \\(A[i]\\) in the heap so that subtree rooted at \\(i\\) becomes a heap.","title":"Heap Operations: HEAPIFY (2)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-operations-heapify-2_1","text":"HEAPIFY ( A , i , n ) largest = i if 2i <= n and A [ 2i ] > A [ i ] then largest = 2i ; endif if 2i+1 <= n and A [ 2i+1 ] > A [ largest ] then largest = 2i+1 ; endif if largest != i then exchange A [ i ] with A [ largest ]; HEAPIFY ( A , largest , n ); endif","title":"Heap Operations: HEAPIFY (2)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-operations-heapify-3","text":"","title":"Heap Operations: HEAPIFY (3)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-operations-heapify-4","text":"","title":"Heap Operations: HEAPIFY (4)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-operations-heapify-5","text":"","title":"Heap Operations: HEAPIFY (5)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-operations-heapify-6","text":"","title":"Heap Operations: HEAPIFY (6)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-operations-heapify-7","text":"","title":"Heap Operations: HEAPIFY (7)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-operations-heapify-8","text":"","title":"Heap Operations: HEAPIFY (8)"},{"location":"tr/week-4/ce100-week-4-heap/#intuitive-analysis-of-heapify","text":"Consider \\(HEAPIFY(A, i, n)\\) let \\(h(i)\\) be the height of node \\(i\\) at most \\(h(i)\\) recursion levels Constant work at each level: \\(\\Theta(1)\\) Therefore \\(T(i)=O(h(i))\\) Heap is almost-complete binary tree \\(h(n)=O(lgn)\\) Thus \\(T(n)=O(lgn)\\)","title":"Intuitive Analysis of HEAPIFY"},{"location":"tr/week-4/ce100-week-4-heap/#formal-analysis-of-heapify","text":"What is the recurrence? Depends on the size of the subtree on which recursive call is made In the next, we try to compute an upper bound for this subtree .","title":"Formal Analysis of HEAPIFY"},{"location":"tr/week-4/ce100-week-4-heap/#reminder-binary-trees","text":"For a complete binary tree: \\(\\#\\) of nodes at depth \\(d\\) : \\(2^d\\) \\(\\#\\) of nodes with depths less than \\(d\\) : \\(2^d-1\\)","title":"Reminder: Binary trees"},{"location":"tr/week-4/ce100-week-4-heap/#formal-analysis-of-heapify-1","text":"Worst case occurs when last row of the subtree \\(S_i\\) rooted at node \\(i\\) is half full \\(T(n) \\leq T(|S_{L(i)}|) + \\Theta(1)\\) \\(S_{L(i)}\\) and \\(S_{R(i)}\\) are complete binary trees of heights \\(h(i)-1\\) and \\(h(i)-2\\) , respectively","title":"Formal Analysis of HEAPIFY (1)"},{"location":"tr/week-4/ce100-week-4-heap/#formal-analysis-of-heapify-2","text":"Let \\(m\\) be the number of leaf nodes in \\(S_{L(i)}\\) \\(|S_{L(i)}|=\\overbrace{m}^{ext.}+\\overbrace{(m\u20131)}^{int.}=2m\u20131\\) \\(|S_{R(i)}|=\\overbrace{\\frac{m}{2}}^{ext.}+\\overbrace{(\\frac{m}{2}\u20131)}^{int.}=m\u20131\\) \\(|S_{L(i)}|+|S_{R(i)}|+1=n\\)","title":"Formal Analysis of HEAPIFY (2)"},{"location":"tr/week-4/ce100-week-4-heap/#formal-analysis-of-heapify-2_1","text":"\\[ \\begin{align*} (2m\u20131)+(m\u20131)+1 &=n \\\\ m &= (n+1)/3 \\\\ |S_{L(i)}| &= 2m \u2013 1 \\\\ &=2(n+1)/3 \u2013 1 \\\\ &=(2n/3+2/3) \u20131 \\\\ &=\\frac{2n}{3}-\\frac{1}{3} \\leq \\frac{2n}{3} \\\\ T(n) & \\leq T(2n/3) + \\Theta(1) \\\\ T(n) &= O(lgn) \\end{align*} \\] By CASE-2 of Master Theorem \\(\\Longrightarrow\\) \\(T(n)=\\Theta(n^{log_b^a}lgn)\\)","title":"Formal Analysis of HEAPIFY (2)"},{"location":"tr/week-4/ce100-week-4-heap/#formal-analysis-of-heapify-2_2","text":"Recurrence: \\(T(n) = aT(n/b) + f(n)\\) Case 2: \\(\\frac{f(n)}{n^{log_b^a}}=\\Theta(1)\\) i.e., \\(f(n)\\) and \\(n^{log_b^a}\\) grow at similar rates Solution: \\(T(n)=\\Theta(n^{log_b^a}lgn)\\) \\(T(n) \\leq T(2n/3) + \\Theta(1)\\) (drop constants.) \\(T(n) \\leq \\Theta(n^{log_3^1}lgn)\\) \\(T(n) \\leq \\Theta(n^0lgn)\\) \\(T(n) = O(lgn)\\)","title":"Formal Analysis of HEAPIFY (2)"},{"location":"tr/week-4/ce100-week-4-heap/#heapify-efficiency-issues","text":"Recursion vs Iteration: In the absence of tail recursion, iterative version is in general more efficient because of the pop/push operations to/from stack at each level of recursion .","title":"HEAPIFY: Efficiency Issues"},{"location":"tr/week-4/ce100-week-4-heap/#heap-operations-heapify-1_1","text":"Recursive HEAPIFY ( A , i , n ) largest = i if 2i <= n and A [ 2i ] > A [ i ] then largest = 2i if 2i+1 <= n and A [ 2i+1 ] > A [ largest ] then largest = 2i+1 if largest != i then exchange A [ i ] with A [ largest ] HEAPIFY ( A , largest , n )","title":"Heap Operations: HEAPIFY (1)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-operations-heapify-2_2","text":"Iterative HEAPIFY ( A , i , n ) j = i while ( true ) do largest = j if 2 j <= n and A [ 2 j ] > A [ j ] then largest = 2 j if 2 j +1 <= n and A [ 2 j +1 ] > A [ largest ] then largest = 2 j +1 if largest != j then exchange A [ j ] with A [ largest ] j = largest else return","title":"Heap Operations: HEAPIFY (2)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-operations-heapify-3_1","text":"","title":"Heap Operations: HEAPIFY (3)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-operations-building-heap","text":"Given an arbitrary array, how to build a heap from scratch? Basic idea: Call \\(HEAPIFY\\) on each node bottom up Start from the leaves (which trivially satisfy the heap property) Process nodes in bottom up order. When \\(HEAPIFY\\) is called on node \\(i\\) , the subtrees connected to the \\(left\\) and \\(right\\) subtrees already satisfy the heap property.","title":"Heap Operations: Building Heap"},{"location":"tr/week-4/ce100-week-4-heap/#storage-of-the-leaves-lemma","text":"Lemma: The last \\(\\lceil \\frac{n}{2} \\rceil\\) nodes of a heap are all leaves.","title":"Storage of the leaves (Lemma)"},{"location":"tr/week-4/ce100-week-4-heap/#storage-of-the-leaves-proof-of-lemma-1","text":"Lemma: last \\(\\lceil n/2 \\rceil\\) nodes of a heap are all leaves Proof : \\(m=2^{d-1}\\) : \\(\\#\\) nodes at level \\(d-1\\) \\(f\\) : \\(\\#\\) nodes at level \\(d\\) (last level) \\(\\#\\) of nodes with depth \\(d-1\\) : \\(m\\) \\(\\#\\) of nodes with depth \\(<d-1\\) : \\(m-1\\) \\(\\#\\) of nodes with depth \\(d\\) : \\(f\\) Total \\(\\#\\) of nodes : \\(n=f+2m-1\\)","title":"Storage of the leaves (Proof of Lemma) (1)"},{"location":"tr/week-4/ce100-week-4-heap/#storage-of-the-leaves-proof-of-lemma-2","text":"Total \\(\\#\\) of nodes : \\(f=n-2m+1\\) \\[ \\begin{align*} \\text{\\# of leaves: }&=f+m-\\lceil f/2 \\rceil \\\\ &= m+\\lfloor f/2 \\rfloor \\\\ &= m+\\lfloor (n-2m+1)/2 \\rfloor \\\\ &= \\lfloor (n+1)/2 \\rfloor \\\\ &= \\lceil n/2 \\rceil \\end{align*} \\] Proof is Completed","title":"Storage of the leaves (Proof of Lemma) (2)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-operations-building-heap_1","text":"BUILD - HEAP ( A , n ) for i = ceil ( n / 2 ) downto 1 do HEAPIFY ( A , i , n ) Reminder: The last \\(\\lceil n/2 \\rceil\\) nodes of a heap are all leaves , which trivially satisfy the heap property","title":"Heap Operations: Building Heap"},{"location":"tr/week-4/ce100-week-4-heap/#build-heap-example-step-1","text":"","title":"Build-Heap Example (Step-1)"},{"location":"tr/week-4/ce100-week-4-heap/#build-heap-example-step-2","text":"","title":"Build-Heap Example (Step-2)"},{"location":"tr/week-4/ce100-week-4-heap/#build-heap-example-step-3","text":"","title":"Build-Heap Example (Step-3)"},{"location":"tr/week-4/ce100-week-4-heap/#build-heap-example-step-4","text":"","title":"Build-Heap Example (Step-4)"},{"location":"tr/week-4/ce100-week-4-heap/#build-heap-example-step-5","text":"","title":"Build-Heap Example (Step-5)"},{"location":"tr/week-4/ce100-week-4-heap/#build-heap-example-step-6","text":"","title":"Build-Heap Example (Step-6)"},{"location":"tr/week-4/ce100-week-4-heap/#build-heap-example-step-7","text":"","title":"Build-Heap Example (Step-7)"},{"location":"tr/week-4/ce100-week-4-heap/#build-heap-example-step-8","text":"","title":"Build-Heap Example (Step-8)"},{"location":"tr/week-4/ce100-week-4-heap/#build-heap-example-step-9","text":"","title":"Build-Heap Example (Step-9)"},{"location":"tr/week-4/ce100-week-4-heap/#build-heap-runtime-analysis","text":"Simple analysis: \\(O(n)\\) calls to \\(HEAPIFY\\) , each of which takes \\(O(lgn)\\) time \\(O(nlgn)\\) \\(\\Longrightarrow\\) loose bound In general, a good approach: Start by proving an easy bound Then, try to tighten it Is there a tighter bound? section{ font-size: 25px; }","title":"Build-Heap: Runtime Analysis"},{"location":"tr/week-4/ce100-week-4-heap/#build-heap-tighter-running-time-analysis","text":"If the heap is complete binary tree then \\(h_{\\ell} = d \u2013 \\ell\\) Otherwise, nodes at a given level do not all have the same height, But we have \\(d \u2013 \\ell \u2013 1 \\leq h_{\\ell} \\leq d \u2013 \\ell\\) section{ font-size: 25px; }","title":"Build-Heap: Tighter Running Time Analysis"},{"location":"tr/week-4/ce100-week-4-heap/#build-heap-tighter-running-time-analysis_1","text":"Assume that all nodes at level \\(\\ell= d \u2013 1\\) are processed \\[ \\begin{align*} T(n) &=\\sum \\limits_{\\ell=0}^{d-1}n_{\\ell}O(h_{\\ell})=O(\\sum \\limits_{\\ell=0}^{d-1}n_{\\ell}h_{\\ell}) \\begin{cases} n_{\\ell}=2^{\\ell} = \\# \\text{ of nodes at level }\\ell \\\\ h_{\\ell}=\\text{height of nodes at level } \\ell \\end{cases} \\\\ \\therefore T(n) &= O \\bigg( \\sum \\limits_{\\ell=0}^{d-1}2^{\\ell}(d-\\ell) \\bigg) \\\\ \\text{Let } & h=d-\\ell \\Longrightarrow \\ell = d-h \\text{ change of variables} \\\\ T(n) &= O\\bigg(\\sum \\limits_{h=1}^{d}h2^{d-h} \\bigg)=O\\bigg(\\sum \\limits_{h=1}^{d}h \\frac{2^d}{2^h} \\bigg) = O\\bigg(2^d\\sum \\limits_{h=1}^{d}h (1/2)^h\\bigg) \\\\ \\text{ but } & 2^d = \\Theta(n) \\Longrightarrow O\\bigg(n\\sum \\limits_{h=1}^{d}h (1/2)^h \\bigg) \\end{align*} \\] section{ font-size: 25px; }","title":"Build-Heap: Tighter Running Time Analysis"},{"location":"tr/week-4/ce100-week-4-heap/#build-heap-tighter-running-time-analysis_2","text":"\\[ \\sum \\limits_{h=1}^{d}h(1/2)^h \\leq \\sum \\limits_{h=0}^{d}h(1/2)^h \\leq \\sum \\limits_{h=0}^{\\infty}h(1/2)^h \\] recall infinite decreasing geometric series \\[ \\sum \\limits_{k=0}^{\\infty} x^k = \\frac{1}{1-x} \\text{ where } |x|<1 \\] differentiate both sides \\[ \\sum \\limits_{k=0}^{\\infty}kx^{k-1} = \\frac{1}{(1-x)^2} \\]","title":"Build-Heap: Tighter Running Time Analysis"},{"location":"tr/week-4/ce100-week-4-heap/#build-heap-tighter-running-time-analysis_3","text":"\\[ \\sum \\limits_{k=0}^{\\infty}kx^{k-1} = \\frac{1}{(1-x)^2} \\] then, multiply both sides by \\(x\\) \\[ \\sum \\limits_{k=0}^{\\infty}kx^k = \\frac{x}{(1-x)^2} \\] in our case: \\(x = 1/2\\) and \\(k = h\\) \\[ \\therefore \\sum \\limits_{h=0}^{\\infty}h(1/2)^h = \\frac{1/2}{(1-(1/2))^2}=2=O(1) \\\\ \\therefore T(n)=O(n\\sum \\limits_{h=1}^{d}h(1/2)^h)=O(n) \\]","title":"Build-Heap: Tighter Running Time Analysis"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-steps","text":"(1) Build a heap on array \\(A[1 \\dots n]\\) by calling \\(BUILD-HEAP(A, n)\\) (2) The largest element is stored at the root \\(A[1]\\) Put it into its correct final position \\(A[n]\\) by \\(A[1] \\longleftrightarrow A[n]\\) (3) Discard node \\(n\\) from the heap (4) Subtrees \\((S2 \\& S3)\\) rooted at children of root remain as heaps, but the new root element may violate the heap property. Make \\(A[1 \\dots n-1]\\) a heap by calling \\(HEAPIFY(A,1,n-1)\\) (5) \\(n \\leftarrow n-1\\) (6) Repeat steps (2-4) until \\(n=2\\)","title":"Heapsort Algorithm Steps"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-1","text":"","title":"Heapsort Algorithm Example (Step-1)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-2","text":"","title":"Heapsort Algorithm Example (Step-2)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-3","text":"","title":"Heapsort Algorithm Example (Step-3)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-4","text":"","title":"Heapsort Algorithm Example (Step-4)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-5","text":"","title":"Heapsort Algorithm Example (Step-5)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-6","text":"","title":"Heapsort Algorithm Example (Step-6)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-7","text":"","title":"Heapsort Algorithm Example (Step-7)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-8","text":"","title":"Heapsort Algorithm Example (Step-8)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-9","text":"","title":"Heapsort Algorithm Example (Step-9)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-10","text":"","title":"Heapsort Algorithm Example (Step-10)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-11","text":"","title":"Heapsort Algorithm Example (Step-11)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-12","text":"","title":"Heapsort Algorithm Example (Step-12)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-13","text":"","title":"Heapsort Algorithm Example (Step-13)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-14","text":"","title":"Heapsort Algorithm Example (Step-14)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-15","text":"","title":"Heapsort Algorithm Example (Step-15)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-16","text":"","title":"Heapsort Algorithm Example (Step-16)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-17","text":"","title":"Heapsort Algorithm Example (Step-17)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-18","text":"","title":"Heapsort Algorithm Example (Step-18)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-example-step-19","text":"","title":"Heapsort Algorithm Example (Step-19)"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-algorithm-runtime-analysis","text":"\\[ \\begin{align*} T(n) &= \\Theta(n)+\\sum \\limits_{i=2}^{n}O(lgi) \\\\ &= \\Theta(n)+O\\bigg( \\sum \\limits_{i=2}^{n}O(lgn) \\bigg) \\\\ &= O(nlgn) \\end{align*} \\]","title":"Heapsort Algorithm: Runtime Analysis"},{"location":"tr/week-4/ce100-week-4-heap/#heapsort-notes","text":"Heapsort is a very good algorithm but, a good implementation of quicksort always beats heapsort in practice However, heap data structure has many popular applications, and it can be efficiently used for implementing priority queues","title":"Heapsort - Notes"},{"location":"tr/week-4/ce100-week-4-heap/#data-structures-for-dynamic-sets","text":"Consider sets of records having key and satellite data","title":"Data structures for Dynamic Sets"},{"location":"tr/week-4/ce100-week-4-heap/#operations-on-dynamic-sets","text":"Queries: Simply return info; \\(MAX(S) / MIN(S):\\) (Query) return \\(x \\in S\\) with the largest/smallest \\(key\\) \\(SEARCH(S, k):\\) (Query) return \\(x \\in S\\) with \\(key[x]= k\\) \\(SUCCESSOR(S, x) / PREDECESSOR(S, x):\\) (Query) return \\(y \\in S\\) which is the next larger/smaller element after \\(x\\) Modifying operations: Change the set \\(INSERT(S, x):\\) (Modifying) \\(S \\leftarrow S \\cup \\{x\\}\\) \\(DELETE(S, x):\\) (Modifying) \\(S \\leftarrow S - \\{x\\}\\) \\(\\text{EXTRACT-MAX}(S) / \\text{EXTRACT-MIN}(S):\\) (Modifying) return and delete \\(x \\in S\\) with the largest/smallest \\(key\\) Different data structures support/optimize different operations","title":"Operations on Dynamic Sets"},{"location":"tr/week-4/ce100-week-4-heap/#priority-queues-pq","text":"Supports \\(INSERT\\) \\(MAX / MIN\\) \\(\\text{EXTRACT-MAX} / \\text{EXTRACT-MIN}\\)","title":"Priority Queues (PQ)"},{"location":"tr/week-4/ce100-week-4-heap/#priority-queues-pq_1","text":"One application: Schedule jobs on a shared resource PQ keeps track of jobs and their relative priorities When a job is finished or interrupted, highest priority job is selected from those pending using \\(\\text{EXTRACT-MAX}\\) A new job can be added at any time using \\(INSERT\\)","title":"Priority Queues (PQ)"},{"location":"tr/week-4/ce100-week-4-heap/#priority-queues-pq_2","text":"Another application: Event-driven simulation Events to be simulated are the items in the PQ Each event is associated with a time of occurrence which serves as a \\(key\\) Simulation of an event can cause other events to be simulated in the future Use \\(\\text{EXTRACT-MIN}\\) at each step to choose the next event to simulate As new events are produced insert them into the PQ using \\(INSERT\\)","title":"Priority Queues (PQ)"},{"location":"tr/week-4/ce100-week-4-heap/#implementation-of-priority-queue","text":"Sorted linked list: Simplest implementation \\(INSERT\\) \\(O(n)\\) time Scan the list to find place and splice in the new item \\(\\text{EXTRACT-MAX}\\) \\(O(1)\\) time Take the first element Fast extraction but slow insertion.","title":"Implementation of Priority Queue"},{"location":"tr/week-4/ce100-week-4-heap/#implementation-of-priority-queue_1","text":"Unsorted linked list: Simplest implementation \\(INSERT\\) \\(O(1)\\) time Put the new item at front \\(\\text{EXTRACT-MAX}\\) \\(O(n)\\) time Scan the whole list Fast insertion but slow extraction. Sorted linked list is better on the average Sorted list: on the average, scans \\(n/2\\) element per insertion Unsorted list: always scans \\(n\\) element at each extraction","title":"Implementation of Priority Queue"},{"location":"tr/week-4/ce100-week-4-heap/#heap-implementation-of-pq","text":"\\(INSERT\\) and \\(\\text{EXTRACT-MAX}\\) are both \\(O(lgn)\\) good compromise between fast insertion but slow extraction and vice versa \\(\\text{EXTRACT-MAX}\\) : already discussed \\(\\text{HEAP-EXTRACT-MAX}\\) \\(INSERT\\) : Insertion is like that of Insertion-Sort. HEAP - INSERT ( A , key , n ) n = n +1 i = n while i > 1 and A [ floor ( i / 2 )] < key do A [ i ] = A [ floor ( i / 2 )] i = floor ( i / 2 ) A [ i ] = key","title":"Heap Implementation of PQ"},{"location":"tr/week-4/ce100-week-4-heap/#heap-implementation-of-pq_1","text":"Traverses \\(O(lgn)\\) nodes, as \\(HEAPIFY\\) does but makes fewer comparisons and assignments \\(HEAPIFY\\) : compares parent with both children \\(HEAP-INSERT\\) : with only one","title":"Heap Implementation of PQ"},{"location":"tr/week-4/ce100-week-4-heap/#heap-insert-example-step-1","text":"","title":"HEAP-INSERT Example (Step-1)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-insert-example-step-2","text":"","title":"HEAP-INSERT Example (Step-2)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-insert-example-step-3","text":"","title":"HEAP-INSERT Example (Step-3)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-insert-example-step-4","text":"","title":"HEAP-INSERT Example (Step-4)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-insert-example-step-5","text":"","title":"HEAP-INSERT Example (Step-5)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-increase-key","text":"Key value of \\(i^{th}\\) element of heap is increased from \\(A[i]\\) to \\(key\\) HEAP - INCREASE - KEY ( A , i , key ) if key < A [ i ] then return error while i > 1 and A [ floor ( i / 2 )] < key do A [ i ] = A [ floor ( i / 2 )] i = floor ( i / 2 ) A [ i ] = key","title":"Heap Increase Key"},{"location":"tr/week-4/ce100-week-4-heap/#heap-increase-key-example-step-1","text":"","title":"HEAP-INCREASE-KEY Example (Step-1)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-increase-key-example-step-2","text":"","title":"HEAP-INCREASE-KEY Example (Step-2)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-increase-key-example-step-3","text":"","title":"HEAP-INCREASE-KEY Example (Step-3)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-increase-key-example-step-4","text":"","title":"HEAP-INCREASE-KEY Example (Step-4)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-increase-key-example-step-5","text":"","title":"HEAP-INCREASE-KEY Example (Step-5)"},{"location":"tr/week-4/ce100-week-4-heap/#heap-implementation-of-priority-queue-pq","text":"section{ font-size: 25px; }","title":"Heap Implementation of Priority Queue (PQ)"},{"location":"tr/week-4/ce100-week-4-heap/#summary-max-heap","text":"Heapify(A, i) Works when both child subtrees of node i are heaps \" Floats down \" node i to satisfy the heap property Runtime: \\(O(lgn)\\) Max(A, n) Returns the max element of the heap (no modification) Runtime: \\(O(1)\\) Extract-Max(A, n) Returns and removes the max element of the heap Fills the gap in \\(A[1]\\) with \\(A[n]\\) , then calls Heapify(A,1) Runtime: \\(O(lgn)\\) section{ font-size: 25px; }","title":"Summary: Max Heap"},{"location":"tr/week-4/ce100-week-4-heap/#summary-max-heap_1","text":"Build-Heap(A, n) Given an arbitrary array, builds a heap from scratch Runtime: \\(O(n)\\) Min(A, n) How to return the min element in a max-heap? Worst case runtime: \\(O(n)\\) because ~half of the heap elements are leaf nodes Instead, use a min-heap for efficient min operations Search(A, x) For an arbitrary \\(x\\) value, the worst-case runtime: \\(O(n)\\) Use a sorted array instead for efficient search operations","title":"Summary: Max Heap"},{"location":"tr/week-4/ce100-week-4-heap/#summary-max-heap_2","text":"Increase-Key(A, i, x) Increase the key of node \\(i\\) (from \\(A[i]\\) to \\(x\\) ) \u201c Float up \u201d \\(x\\) until heap property is satisfied Runtime: \\(O(lgn)\\) Decrease-Key(A, i, x) Decrease the key of node \\(i\\) (from \\(A[i]\\) to \\(x\\) ) Call Heapify(A, i) Runtime: \\(O(lgn)\\)","title":"Summary: Max Heap"},{"location":"tr/week-4/ce100-week-4-heap/#phone-operator-problem","text":"A phone operator answering \\(n\\) phones Each phone \\(i\\) has \\(x_i\\) people waiting in line for their calls to be answered. Phone operator needs to answer the phone with the largest number of people waiting in line. New calls come continuously, and some people hang up after waiting.","title":"Phone Operator Problem"},{"location":"tr/week-4/ce100-week-4-heap/#phone-operator-solution","text":"Step 1: Define the following array: \\(A[i]\\) : the ith element in heap \\(A[i].id\\) : the index of the corresponding phone \\(A[i].key\\) : \\(\\#\\) of people waiting in line for phone with index \\(A[i].id\\)","title":"Phone Operator Solution"},{"location":"tr/week-4/ce100-week-4-heap/#phone-operator-solution_1","text":"Step 2: \\(\\text{Build-Max-Heap}(A, n)\\) Execution: When the operator wants to answer a phone: \\(id = A[1].id\\) \\(\\text{Decrease-Key}(A, 1, A[1].key-1)\\) answer phone with index \\(id\\) When a new call comes in to phone i: \\(\\text{Increase-Key}(A, i, A[i].key+1)\\) When a call drops from phone i: \\(\\text{Decrease-Key}(A, i, A[i].key-1)\\)","title":"Phone Operator Solution"},{"location":"tr/week-4/ce100-week-4-heap/#linked-lists","text":"Like arrays, Linked List is a linear data structure. Unlike arrays, linked list elements are not stored at a contiguous location; the elements are linked using pointers.","title":"Linked Lists"},{"location":"tr/week-4/ce100-week-4-heap/#linked-lists-c-definition","text":"C // A linked list node struct Node { int data ; struct Node * next ; };","title":"Linked Lists - C Definition"},{"location":"tr/week-4/ce100-week-4-heap/#linked-lists-cpp-definition","text":"Cpp class Node { public : int data ; Node * next ; };","title":"Linked Lists - Cpp Definition"},{"location":"tr/week-4/ce100-week-4-heap/#linked-lists-java-definition","text":"Java class LinkedList { Node head ; // head of the list /* Linked list Node*/ class Node { int data ; Node next ; // Constructor to create a new node // Next is by default initialized // as null Node ( int d ) { data = d ; } } }","title":"Linked Lists - Java Definition"},{"location":"tr/week-4/ce100-week-4-heap/#linked-lists-csharp-definition","text":"Csharp class LinkedList { // The first node(head) of the linked list // Will be an object of type Node (null by default) Node head ; class Node { int data ; Node next ; // Constructor to create a new node Node ( int d ) { data = d ; } } }","title":"Linked Lists - Csharp Definition"},{"location":"tr/week-4/ce100-week-4-heap/#priority-queue-using-linked-list-methods","text":"Implement Priority Queue using Linked Lists. push(): This function is used to insert a new data into the queue. pop(): This function removes the element with the highest priority from the queue. peek()/top(): This function is used to get the highest priority element in the queue without removing it from the queue.","title":"Priority Queue using Linked List Methods"},{"location":"tr/week-4/ce100-week-4-heap/#priority-queue-using-linked-list-algorithm","text":"PUSH ( HEAD , DATA , PRIORITY ) Create NEW.Data = DATA & NEW.Priority = PRIORITY If HEAD.priority < NEW.Priority NEW -> NEXT = HEAD HEAD = NEW Else Set TEMP to head of the list Endif WHILE TEMP -> NEXT != NULL and TEMP -> NEXT -> PRIORITY > PRIORITY THEN TEMP = TEMP -> NEXT ENDWHILE NEW -> NEXT = TEMP -> NEXT TEMP -> NEXT = NEW","title":"Priority Queue using Linked List Algorithm"},{"location":"tr/week-4/ce100-week-4-heap/#priority-queue-using-linked-list-algorithm_1","text":"POP ( HEAD ) // Set the head of the list to the next node in the list. HEAD = HEAD -> NEXT. Free the node at the head of the list PEEK ( HEAD ) : Return HEAD -> DATA","title":"Priority Queue using Linked List Algorithm"},{"location":"tr/week-4/ce100-week-4-heap/#priority-queue-using-linked-list-notes","text":"LinkedList is already sorted. Time Complexities and Comparison with Binary Heap peek() push() pop() Linked List \\(O(1)\\) \\(O(n)\\) \\(O(1)\\) Binary Heap \\(O(1)\\) \\(O(lgn)\\) \\(O(lgn)\\)","title":"Priority Queue using Linked List Notes"},{"location":"tr/week-4/ce100-week-4-heap/#sorting-in-linear-time","text":"","title":"Sorting in Linear Time"},{"location":"tr/week-4/ce100-week-4-heap/#how-fast-can-we-sort","text":"The algorithms we have seen so far: Based on comparison of elements We only care about the relative ordering between the elements (not the actual values) The smallest worst-case runtime we have seen so far: \\(O(nlgn)\\) Is \\(O(nlgn)\\) the best we can do? Comparison sorts: Only use comparisons to determine the relative order of elements.","title":"How Fast Can We Sort?"},{"location":"tr/week-4/ce100-week-4-heap/#decision-trees-for-comparison-sorts","text":"Represent a sorting algorithm abstractly in terms of a decision tree A binary tree that represents the comparisons between elements in the sorting algorithm Control, data movement, and other aspects are ignored One decision tree corresponds to one sorting algorithm and one value of \\(n\\) ( input size )","title":"Decision Trees for Comparison Sorts"},{"location":"tr/week-4/ce100-week-4-heap/#reminder-insertion-sort-step-by-step-description-1","text":"","title":"Reminder: Insertion Sort Step-By-Step Description (1)"},{"location":"tr/week-4/ce100-week-4-heap/#reminder-insertion-sort-step-by-step-description-2","text":"","title":"Reminder: Insertion Sort Step-By-Step Description (2)"},{"location":"tr/week-4/ce100-week-4-heap/#reminder-insertion-sort-step-by-step-description-3","text":"","title":"Reminder: Insertion Sort Step-By-Step Description (3)"},{"location":"tr/week-4/ce100-week-4-heap/#different-outcomes-for-insertion-sort-and-n3","text":"Input : \\(<a_1,a_2,a_3>\\)","title":"Different Outcomes for Insertion Sort and n=3"},{"location":"tr/week-4/ce100-week-4-heap/#decision-tree-for-insertion-sort-and-n3","text":"","title":"Decision Tree for Insertion Sort and n=3"},{"location":"tr/week-4/ce100-week-4-heap/#decision-tree-model-for-comparison-sorts","text":"Internal node \\((i:j)\\) : Comparison between elements \\(a_i\\) and \\(a_j\\) Leaf node: An output of the sorting algorithm Path from root to a leaf: The execution of the sorting algorithm for a given input All possible executions are captured by the decision tree All possible outcomes (permutations) are in the leaf nodes","title":"Decision Tree Model for Comparison Sorts"},{"location":"tr/week-4/ce100-week-4-heap/#decision-tree-for-insertion-sort-and-n3_1","text":"Input: \\(<9, 4, 6>\\)","title":"Decision Tree for Insertion Sort and n=3"},{"location":"tr/week-4/ce100-week-4-heap/#decision-tree-model","text":"A decision tree can model the execution of any comparison sort: One tree for each input size \\(n\\) View the algorithm as splitting whenever it compares two elements The tree contains the comparisons along all possible instruction traces The running time of the algorithm \\(=\\) the length of the path taken Worst case running time \\(=\\) height of the tree","title":"Decision Tree Model"},{"location":"tr/week-4/ce100-week-4-heap/#counting-sort","text":"","title":"Counting Sort"},{"location":"tr/week-4/ce100-week-4-heap/#lower-bound-for-comparison-sorts","text":"Let \\(n\\) be the number of elements in the input array. What is the \\(min\\) number of leaves in the decision tree? \\(n!\\) (because there are n! permutations of the input array, and all possible outputs must be captured in the leaves) What is the max number of leaves in a binary tree of height \\(h\\) ? \\(\\Longrightarrow\\) \\(2^h\\) So, we must have: $$ 2^h \\geq n! $$","title":"Lower Bound for Comparison Sorts"},{"location":"tr/week-4/ce100-week-4-heap/#lower-bound-for-decision-tree-sorting","text":"Theorem: Any comparison sort algorithm requires \\(\\Omega(nlgn)\\) comparisons in the worst case. Proof: We\u2019ll prove that any decision tree corresponding to a comparison sort algorithm must have height \\(\\Omega(nlgn)\\) \\[ \\begin{align*} 2^h & \\geq n! \\\\ h & \\geq lg(n!) \\\\ & \\geq lg((n/e)^n) (Stirling Approximation) \\\\ & = nlgn - nlge \\\\ & = \\Omega(nlgn) \\end{align*} \\]","title":"Lower Bound for Decision Tree Sorting"},{"location":"tr/week-4/ce100-week-4-heap/#lower-bound-for-decision-tree-sorting_1","text":"Corollary: Heapsort and merge sort are asymptotically optimal comparison sorts. Proof: The \\(O(nlgn)\\) upper bounds on the runtimes for heapsort and merge sort match the \\(\\Omega(nlgn)\\) worst-case lower bound from the previous theorem.","title":"Lower Bound for Decision Tree Sorting"},{"location":"tr/week-4/ce100-week-4-heap/#sorting-in-linear-time_1","text":"Counting sort: No comparisons between elements Input: \\(A[1 \\dots n]\\) , where \\(A[j] \\in \\{1, 2,\\dots, k\\}\\) Output: \\(B[1 \\dots n]\\) , sorted Auxiliary storage: \\(C[1 \\dots k]\\)","title":"Sorting in Linear Time"},{"location":"tr/week-4/ce100-week-4-heap/#counting-sort-1","text":"","title":"Counting Sort-1"},{"location":"tr/week-4/ce100-week-4-heap/#counting-sort-2","text":"Step 1: Initialize all counts to 0","title":"Counting Sort-2"},{"location":"tr/week-4/ce100-week-4-heap/#counting-sort-3","text":"Step 2: Count the number of occurrences of each value in the input array","title":"Counting Sort-3"},{"location":"tr/week-4/ce100-week-4-heap/#counting-sort-4","text":"Step 3: Compute the number of elements less than or equal to each value","title":"Counting Sort-4"},{"location":"tr/week-4/ce100-week-4-heap/#counting-sort-5","text":"Step 4: Populate the output array There are \\(C[3] = 3\\) elements that are \\(\\leq 3\\)","title":"Counting Sort-5"},{"location":"tr/week-4/ce100-week-4-heap/#counting-sort-6","text":"Step 4: Populate the output array There are \\(C[4]=5\\) elements that are \\(\\leq 4\\)","title":"Counting Sort-6"},{"location":"tr/week-4/ce100-week-4-heap/#counting-sort-7","text":"Step 4: Populate the output array There are \\(C[3]=2\\) elements that are \\(\\leq 3\\)","title":"Counting Sort-7"},{"location":"tr/week-4/ce100-week-4-heap/#counting-sort-8","text":"Step 4: Populate the output array There are \\(C[1]=1\\) elements that are \\(\\leq 1\\)","title":"Counting Sort-8"},{"location":"tr/week-4/ce100-week-4-heap/#counting-sort-9","text":"Step 4: Populate the output array There are \\(C[4]=4\\) elements that are \\(\\leq 4\\)","title":"Counting Sort-9"},{"location":"tr/week-4/ce100-week-4-heap/#counting-sort-runtime-analysis","text":"Total Runtime: \\(\\Theta(n+k)\\) \\(n\\) : size of the input array \\(k\\) : the range of input values","title":"Counting Sort: Runtime Analysis"},{"location":"tr/week-4/ce100-week-4-heap/#counting-sort-runtime","text":"Runtime is \\(\\Theta(n+k)\\) If \\(k=O(n)\\) , then counting sort takes \\(\\Theta(n)\\) Question: We proved a lower bound of \\(\\Theta(nlgn)\\) before! Where is the fallacy? Answer: \\(\\Theta(nlgn)\\) lower bound is for comparison-based sorting Counting sort is not a comparison sort In fact, not a single comparison between elements occurs!","title":"Counting Sort: Runtime"},{"location":"tr/week-4/ce100-week-4-heap/#stable-sorting","text":"Counting sort is a stable sort: It preserves the input order among equal elements. i.e. The numbers with the same value appear in the output array in the same order as they do in the input array. Note : Which other sorting algorithms have this property?","title":"Stable Sorting"},{"location":"tr/week-4/ce100-week-4-heap/#radix-sort","text":"Origin: Herman Hollerith\u2019s card-sorting machine for the 1890 US Census. Basic idea: Digit-by-digit sorting Two variations: Sort from MSD to LSD (bad idea) Sort from LSD to MSD (good idea) ( LSD/MSD: Least/most significant digit )","title":"Radix Sort"},{"location":"tr/week-4/ce100-week-4-heap/#herman-hollerith-1860-1929","text":"The 1880 U.S. Census took almost 10 years to process. While a lecturer at MIT, Hollerith prototyped punched-card technology . His machines, including a card sorter , allowed the 1890 census total to be reported in 6 weeks . He founded the Tabulating Machine Company in 1911, which merged with other companies in 1924 to form International Business Machines(IBM) .","title":"Herman Hollerith (1860-1929)"},{"location":"tr/week-4/ce100-week-4-heap/#hollerith-punched-card","text":"Punched card: A piece of stiff paper that contains digital information represented by the presence or absence of holes. 12 rows and 24 columns coded for age, state of residency, gender, etc.","title":"Hollerith Punched Card"},{"location":"tr/week-4/ce100-week-4-heap/#modern-ibm-card","text":"One character per column So, that\u2019s why text windows have 80 columns! for more samples visit https://en.wikipedia.org/wiki/Punched_card","title":"Modern IBM card"},{"location":"tr/week-4/ce100-week-4-heap/#hollerith-tabulating-machine-and-sorter","text":"Mechanically sorts the cards based on the hole locations. Sorting performed for one column at a time Human operator needed to load/retrieve/move cards at each stage","title":"Hollerith Tabulating Machine and Sorter"},{"location":"tr/week-4/ce100-week-4-heap/#holleriths-msd-first-radix-sort","text":"Sort starting from the most significant digit (MSD) Then, sort each of the resulting bins recursively At the end, combine the decks in order","title":"Hollerith\u2019s MSD-First Radix Sort"},{"location":"tr/week-4/ce100-week-4-heap/#holleriths-msd-first-radix-sort_1","text":"To sort a subset of cards recursively: All the other cards need to be removed from the machine, because the machine can handle only one sorting problem at a time. The human operator needs to keep track of the intermediate card piles","title":"Hollerith\u2019s MSD-First Radix Sort"},{"location":"tr/week-4/ce100-week-4-heap/#holleriths-msd-first-radix-sort_2","text":"MSD-first sorting may require: very large number of sorting passes very large number of intermediate card piles to maintain S(d): \\(\\#\\) of passes needed to sort d-digit numbers (worst-case) Recurrence: \\(S(d)=10S(d-1)+1\\) with \\(S(1)=1\\) Reminder: Recursive call made to each subset with the same most significant digit(MSD)","title":"Hollerith\u2019s MSD-First Radix Sort"},{"location":"tr/week-4/ce100-week-4-heap/#holleriths-msd-first-radix-sort_3","text":"Recurrence: \\(S(d)=10S(d-1)+1\\) \\[ \\begin{align*} S(d) &= 10 S(d-1) + 1 \\\\ & = 10 \\bigg(10 S(d-2) + 1 \\bigg) + 1 \\\\ & = 10 \\Big(10 \\bigg(10 S(d-3) + 1\\bigg) + 1 \\Big) + 1 \\\\ & = 10i S(d-i) + 10i-1 + 10i-2 + \\dots + 101 + 100 \\\\ &=\\sum \\limits_{i=0}^{d-1}10^i \\end{align*} \\] Iteration terminates when \\(i = d-1\\) with \\(S(d-(d-1)) = S(1) = 1\\)","title":"Hollerith\u2019s MSD-First Radix Sort"},{"location":"tr/week-4/ce100-week-4-heap/#holleriths-msd-first-radix-sort_4","text":"Recurrence: \\(S(d)=10S(d-1)+1\\) \\[ \\begin{align*} S(d) &=\\sum \\limits_{i=0}^{d-1}10^i \\\\ & = \\frac{10^d-1}{10-1} \\\\ & = \\frac{1}{9}(10^d-1)\\\\ & \\Downarrow \\\\ S(d)&=\\frac{1}{9}(10^d-1) \\end{align*} \\]","title":"Hollerith\u2019s MSD-First Radix Sort"},{"location":"tr/week-4/ce100-week-4-heap/#holleriths-msd-first-radix-sort_5","text":"\\(P(d)\\) : \\(\\#\\) of intermediate card piles maintained (worst-case) Reminder: Each routing pass generates 9 intermediate piles except the sorting passes on least significant digits (LSDs) There are \\(10^{d-1}\\) sorting calls to LSDs \\[ \\begin{align*} P(d) &= 9(S(d)\u201310^{d-1}) \\\\ &= 9\\frac{(10^{d\u20131})}{9\u2013 10^{d-1}} \\\\ &= (10^{d\u20131}\u20139 * 10^{d-1}) \\\\ &= 10^{d-1} - 1 \\end{align*} \\]","title":"Hollerith\u2019s MSD-First Radix Sort"},{"location":"tr/week-4/ce100-week-4-heap/#holleriths-msd-first-radix-sort_6","text":"\\[ \\begin{align*} P(d) &= 10^{d-1} - 1 \\end{align*} \\] Alternative solution: Solve the recurrence \\[ \\begin{align*} P(d) &= 10P(d-1)+9 \\\\ P(1) &= 0 \\\\ \\end{align*} \\]","title":"Hollerith\u2019s MSD-First Radix Sort"},{"location":"tr/week-4/ce100-week-4-heap/#holleriths-msd-first-radix-sort_7","text":"Example: To sort \\(3\\) digit numbers, in the worst case: \\(S(d) = (1/9) (103-1) = 111\\) sorting passes needed \\(P(d) = 10d-1-1 = 99\\) intermediate card piles generated MSD-first approach has more recursive calls and intermediate storage requirement Expensive for a tabulating machine to sort punched cards Overhead of recursive calls in a modern computer section{ font-size: 25px; }","title":"Hollerith\u2019s MSD-First Radix Sort"},{"location":"tr/week-4/ce100-week-4-heap/#lsd-first-radix-sort","text":"Least significant digit ( LSD )-first radix sort seems to be a folk invention originated by machine operators. It is the counter-intuitive, but the better algorithm. Basic Algorithm: Sort numbers on their LSD first ( Stable Sorting Needed ) Combine the cards into a single deck in order Continue this sorting process for the other digits from the LSD to MSD Requires only \\(d\\) sorting passes No intermediate card pile generated","title":"LSD-First Radix Sort"},{"location":"tr/week-4/ce100-week-4-heap/#lsd-first-radix-sort-example","text":"","title":"LSD-first Radix Sort Example"},{"location":"tr/week-4/ce100-week-4-heap/#correctness-of-radix-sort-lsd-first","text":"Proof by induction: Base case: \\(d=1\\) is correct ( trivial ) Inductive hyp: Assume the first \\(d-1\\) digits are sorted correctly Prove that all \\(d\\) digits are sorted correctly after sorting digit \\(d\\) Two numbers that differ in digit \\(d\\) are correctly sorted ( e.g. 355 and 657 ) Two numbers equal in digit d are put in the same order as the input ( correct order )","title":"Correctness of Radix Sort (LSD-first)"},{"location":"tr/week-4/ce100-week-4-heap/#radix-sort-runtime","text":"Use counting-sort to sort each digit Reminder: Counting sort complexity: \\(\\Theta(n+k)\\) \\(n\\) : size of input array \\(k\\) : the range of the values Radix sort runtime: \\(\\Theta(d(n+k))\\) \\(d\\) : \\(\\#\\) of digits How to choose the \\(d\\) and \\(k\\) ?","title":"Radix Sort Runtime"},{"location":"tr/week-4/ce100-week-4-heap/#radix-sort-runtime-example-1","text":"We have flexibility in choosing \\(d\\) and \\(k\\) Assume we are trying to sort 32-bit words We can define each digit to be 4 bits Then, the range for each digit \\(k=2^4=16\\) So, counting sort will take \\(\\Theta(n+16)\\) The number of digits \\(d =32/4=8\\) Radix sort runtime: \\(\\Theta(8(n+16)) = \\Theta(n)\\) \\(\\overbrace{[4bits|4bits|4bits|4bits|4bits|4bits|4bits|4bits]}^{\\text{32-bits}}\\)","title":"Radix Sort: Runtime \u2013 Example 1"},{"location":"tr/week-4/ce100-week-4-heap/#radix-sort-runtime-example-2","text":"We have flexibility in choosing \\(d\\) and \\(k\\) Assume we are trying to sort 32-bit words Or, we can define each digit to be 8 bits Then, the range for each digit \\(k = 2^8 = 256\\) So, counting sort will take \\(\\Theta(n+256)\\) The number of digits \\(d = 32/8 = 4\\) Radix sort runtime: \\(\\Theta(4(n+256)) = \\Theta(n)\\) \\(\\overbrace{[8bits|8bits|8bits|8bits]}^{\\text{32-bits}}\\) section{ font-size: 25px; }","title":"Radix Sort: Runtime \u2013 Example 2"},{"location":"tr/week-4/ce100-week-4-heap/#radix-sort-runtime_1","text":"Assume we are trying to sort \\(b\\) -bit words Define each digit to be \\(r\\) bits Then, the range for each digit \\(k = 2^r\\) So, counting sort will take \\(\\Theta(n+2^r)\\) The number of digits \\(d = b/r\\) Radix sort runtime: \\[ \\begin{align*} T(n,b)&=\\Theta \\bigg( \\frac{b}{r}(n+2^r) \\bigg) \\end{align*} \\] \\(\\overbrace{[rbits|rbits|rbits|rbits]}^{b/r \\text{ bits}}\\)","title":"Radix Sort: Runtime"},{"location":"tr/week-4/ce100-week-4-heap/#radix-sort-runtime-analysis","text":"\\[ \\begin{align*} T(n,b)&=\\Theta \\bigg( \\frac{b}{r}(n+2^r) \\bigg) \\end{align*} \\] Minimize \\(T(n,b)\\) by differentiating and setting to \\(0\\) Or, intuitively: We want to balance the terms \\((b/r)\\) and \\((n + 2^r)\\) Choose \\(r \\approx lgn\\) If we choose \\(r << lgn \\Longrightarrow (n + 2^r)\\) term doesn\u2019t improve If we choose \\(r >> lgn \\Longrightarrow (n + 2^r)\\) increases exponentially","title":"Radix Sort: Runtime Analysis"},{"location":"tr/week-4/ce100-week-4-heap/#radix-sort-runtime-analysis_1","text":"\\[ \\begin{align*} T(n,b)&=\\Theta \\bigg( \\frac{b}{r}(n+2^r) \\bigg) \\end{align*} \\] \\[ \\begin{align*} \\text{Choose } r=lgn \\Longrightarrow T(n,b)=\\Theta(bn/lgn) \\end{align*} \\] For numbers in the range from \\(0\\) to \\(n^d \u2013 1\\) , we have: The number of bits \\(b = lg(nd ) = d lgn\\) Radix sort runs in \\(\\Theta(dn)\\)","title":"Radix Sort: Runtime Analysis"},{"location":"tr/week-4/ce100-week-4-heap/#radix-sort-conclusions","text":"\\[ \\begin{align*} \\text{Choose } r=lgn \\Longrightarrow T(n,b)=\\Theta(bn/lgn) \\end{align*} \\] Example: Compare radix sort with merge sort/heapsort \\(1\\) million ( \\(2^{20}\\) ), \\(32\\) -bit numbers \\((n = 2^{20}, b = 32)\\) Radix sort: \\(\\lfloor 32/20 \\rfloor = 2\\) passes Merge sort/heap sort: \\(lgn = 20\\) passes Downsides: Radix sort has little locality of reference (more cache misses) The version that uses counting sort is not in-place On modern processors, a well-tuned quicksort implementation typically runs faster.","title":"Radix Sort: Conclusions"},{"location":"tr/week-4/ce100-week-4-heap/#references","text":"Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) Insertion Sort - GeeksforGeeks Priority Queue Using Linked List - GeeksforGeeks Priority Queue Using Linked List - JavatPoint NIST Dictionary of Algorithms and Data Structures NIST - Dictionary of Algorithms and Data Structures \\(-End-Of-Week-4-Course-Module-\\)","title":"References"},{"location":"tr/week-5/ce100-week-5-dp/","text":"CE100 Algorithms and Programming II \u00b6 Week-5 (Dynamic Programming) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Quicksort Sort \u00b6 Outline \u00b6 Convex Hull (Divide & Conquer) Dynamic Programming Introduction Divide-and-Conquer (DAC) vs Dynamic Programming (DP) Fibonacci Numbers Recursive Solution Bottom-Up Solution Optimization Problems Development of a DP Algorithms Matrix-Chain Multiplication Matrix Multiplication and Row Columns Definitions Cost of Multiplication Operations (pxqxr) Counting the Number of Parenthesizations The Structure of Optimal Parenthesization Characterize the structure of an optimal solution A Recursive Solution Direct Recursion Inefficiency. Computing the optimal Cost of Matrix-Chain Multiplication Bottom-up Computation Algorithm for Computing the Optimal Costs MATRIX-CHAIN-ORDER Construction and Optimal Solution MATRIX-CHAIN-MULTIPLY Summary Dynamic Programming - Introduction \u00b6 An algorithm design paradigm like divide-and-conquer Programming: A tabular method (not writing computer code) Older sense of planning or scheduling, typically by filling in a table Divide-and-Conquer (DAC): subproblems are independent Dynamic Programming (DP): subproblems are not independent Overlapping subproblems: subproblems share sub-subproblems In solving problems with overlapping subproblems A DAC algorithm does redundant work Repeatedly solves common subproblems A DP algorithm solves each problem just once Saves its result in a table Problem 1: Fibonacci Numbers Recursive Solution \u00b6 Reminder: \\[ \\begin{align*} & F(0)=0 \\text{ and } F(1)=1 \\\\ & F(n)=F(n-1)+F(n-2) \\\\[10 pt] &\\text{REC-FIBO}(n) \\{ \\\\ & \\quad \\text{if} \\ n < 2 \\\\ & \\qquad \\text{return} \\ n \\\\ & \\quad \\text{else} \\\\ & \\qquad \\text{return} \\ \\text{REC-FIBO}(n-1) + \\text{REC-FIBO}(n-2) \\ \\} \\end{align*} \\] Overlapping subproblems in different recursive calls. Repeated work! Problem 1: Fibonacci Numbers Recursive Solution \u00b6 Recurrence: exponential runtime \\[ T(n) = T(n-1) + T(n-2) + 1 \\] Recursive algorithm inefficient because it recomputes the same \\(F(i)\\) repeatedly in different branches of the recursion tree. Problem 1: Fibonacci Numbers Bottom-up Computation \u00b6 Reminder: \\[ \\begin{align*} & F(0)=0 \\text{ and } F(1)=1 \\\\ & F(n)=F(n-1)+F(n-2) \\end{align*} \\] Runtime \\(\\Theta(n)\\) ITER - FIBO ( n ) F [ 0 ] = 0 F [ 1 ] = 1 for i = 2 to n do F [ i ] = F [ i -1 ] + F [ i -2 ] return F [ n ] Optimization Problems \u00b6 DP typically applied to optimization problems In an optimization problem There are many possible solutions (feasible solutions) Each solution has a value Want to find an optimal solution to the problem A solution with the optimal value (min or max value) Wrong to say the optimal solution to the problem There may be several solutions with the same optimal value Development of a DP Algorithm \u00b6 Step-1 . Characterize the structure of an optimal solution Step-2 . Recursively define the value of an optimal solution Step-3 . Compute the value of an optimal solution in a bottom-up fashion Step-4 . Construct an optimal solution from the information computed in Step 3 section{ font-size: 25px; } Problem 2: Matric Chain Multiplication \u00b6 Input: a sequence (chain) \\(\\langle A_1,A_2, \\dots , A_n\\rangle\\) of \\(n\\) matrices Aim: compute the product \\(A_1 \\cdot A_2 \\cdot \\dots A_n\\) A product of matrices is fully parenthesized if It is either a single matrix Or, the product of two fully parenthesized matrix products surrounded by a pair of parentheses. \\(\\bigg(A_i(A_{i+1}A_{i+2} \\dots A_j) \\bigg)\\) \\(\\bigg((A_iA_{i+1}A_{i+2} \\dots A_{j-1})A_j \\bigg)\\) \\(\\bigg((A_iA_{i+1}A_{i+2} \\dots A_k)(A_{k+1}A_{k+2} \\dots A_j)\\bigg)\\) for \\(i \\leq k < j\\) All parenthesizations yield the same product; matrix product is associative section{ font-size: 25px; } Matrix-chain Multiplication: An Example Parenthesization \u00b6 Input: \\(\\langle A_1,A_2,A_3,A_4\\rangle\\) ( \\(5\\) distinct ways of full parenthesization) \\[ \\begin{align*} & \\bigg(A_1\\Big(A_2(A_3A_4)\\Big)\\bigg) \\\\ & \\bigg(A_1\\Big((A_2A_3)A_4\\Big)\\bigg) \\\\ & \\bigg((A_1A_2)(A_3A_4)\\bigg) \\\\ & \\bigg(\\Big(A_1(A_2A_3)A_4\\Big)\\bigg) \\\\ & \\bigg(\\Big((A_1A_2)A_3\\Big)A_4\\bigg) \\end{align*} \\] The way we parenthesize a chain of matrices can have a dramatic effect on the cost of computing the product Matrix-chain Multiplication: Reminder \u00b6 MATRIX - MULTIPLY ( A , B ) if cols [ A ] != rows [ B ] then error ( \u201c incompatible dimensions \u201d ) for i = 1 to rows [ A ] do for j = 1 to cols [ B ] do C [ i , j ] = 0 for k = 1 to cols [ A ] do C [ i , j ] = C [ i , j ] + A [ i , k ] \u00b7 B [ k , j ] return C Matrix Chain Multiplication: Example \u00b6 \\(A1:10\\text{x}100\\) , \\(A2:100\\text{x}5\\) , \\(A3:5\\text{x}50\\) Which paranthesization is better? \\((A1A2)A3\\) or \\(A1(A2A3)\\) ? Matrix Chain Multiplication: Example \u00b6 \\(A1:10 \\times 100\\) , \\(A2:100 \\times 5\\) , \\(A3:5 \\times 50\\) Which paranthesization is better? \\((A1A2)A3\\) or \\(A1(A2A3)\\) ? Matrix Chain Multiplication: Example \u00b6 \\(A1:10 \\times 100\\) , \\(A2:100 \\times 5\\) , \\(A3:5 \\times 50\\) Which paranthesization is better? \\((A1A2)A3\\) or \\(A1(A2A3)\\) ? In summary: \\((A1A2)A3\\) = \\(\\#\\) of multiply-add ops: \\(7500\\) \\(A1(A2A3)\\) = \\(\\#\\) of multiple-add ops: \\(75000\\) First parenthesization yields 10x faster computation Matrix-chain Multiplication Problem \u00b6 Input: A chain \\(\\langle A_1,A_2, \\dots ,A_n\\rangle\\) of \\(n\\) matrices, where \\(A_i\\) is a \\(p_{i-1} \\times p_i\\) matrix Objective: Fully parenthesize the product \\(A_1 \\cdot A_2 \\dots A_n\\) such that the number of scalar mult-adds is minimized. section{ font-size: 25px; } Counting the Number of Parenthesizations \u00b6 Brute force approach: exhaustively check all parenthesizations \\(P(n)\\) : \\(\\#\\) of parenthesizations of a sequence of n matrices We can split sequence between \\(k^{th}\\) and \\((k+1)^{st}\\) matrices for any \\(k=1, 2, \\dots , n-1\\) , then parenthesize the two resulting sequences independently, i.e., \\[ (A_1 A_2 A_3 \\dots A_k \\overbrace{)(}^{break-point}A_{k+1} A_{k+2} \\dots A_n) \\] We obtain the recurrence \\[ P(1)=1 \\text{ and } P(n)=\\sum \\limits_{k=1}^{n-1}P(k)P(n-k) \\] Number of Parenthesizations: \u00b6 \\(P(1)=1\\) and \\(P(n)=\\sum \\limits_{k=1}^{n-1}P(k)P(n-k)\\) The recurrence generates the sequence of Catalan Numbers Solution is \\(P(n)=C(n-1)\\) where \\[ C(n)=\\frac{1}{n+1} {2n \\choose n} = \\Omega(4^n / n^{3/2}) \\] The number of solutions is exponential in \\(n\\) Therefore, brute force approach is a poor strategy The Structure of Optimal Parenthesization \u00b6 Notation: \\(A_{i..j}\\) : The matrix that results from evaluation of the product: \\(A_i A_{i+1} A_{i+2} \\dots A_j\\) Observation: Consider the last multiplication operation in any parenthesization: \\((A_1 A_2 \\dots A_k) \\cdot (A_{k+1} A_{k+2} \\dots A_n)\\) There is a \\(k\\) value \\((1 \\leq k < n)\\) such that: First, the product \\(A_1 \\dots k\\) is computed Then, the product \\(A_{k+1 \\dots n}\\) is computed Finally, the matrices \\(A_{1 \\dots k}\\) and \\(A_{k+1 \\dots n}\\) are multiplied Step 1: Characterize the Structure of an Optimal Solution \u00b6 An optimal parenthesization of product \\(A_1 A_2 \\dots A_n\\) will be: \\((A_1 A_2 \\dots A_k) \\cdot (A_{k+1} A_{k+2} \\dots A_n)\\) for some \\(k\\) value The cost of this optimal parenthesization will be: \\(=\\) Cost of computing \\(A_{1 \\dots k}\\) \\(+\\) Cost of computing \\(A_{k+1 \\dots n}\\) \\(+\\) Cost of multiplying \\(A_{1 \\dots k} \\cdot A_{k+1 \\dots n}\\) Step 1: Characterize the Structure of an Optimal Solution \u00b6 Key observation: Given optimal parenthesization \\((A_1 A_2 A_3 \\dots A_k) \\cdot (A_{k+1} A_{k+2} \\dots A_n)\\) Parenthesization of the subchain \\(A_1 A_2 A_3 \\dots A_k\\) Parenthesization of the subchain \\(A_{k+1} A_{k+2} \\dots A_n\\) should both be optimal Thus, optimal solution to an instance of the problem contains optimal solutions to subproblem instances i.e. , optimal substructure within an optimal solution exists. Step 2: A Recursive Solution \u00b6 Step 2: Define the value of an optimal solution recursively in terms of optimal solutions to the subproblems Assume we are trying to determine the min cost of computing \\(A_{i \\dots j}\\) \\(m_{i,j}\\) : min \\(\\#\\) of scalar multiply-add opns needed to compute \\(A_{i \\dots j}\\) Note: The optimal cost of the original problem: \\(m_{1,n}\\) How to compute \\(m_{i,j}\\) recursively? Step 2: A Recursive Solution \u00b6 Base case: \\(m_{i,i}=0\\) (single matrix, no multiplication) Let the size of matrix \\(A_i\\) be \\((p_{i-1} \\times p_i)\\) Consider an optimal parenthesization of chain \\(A_i \\dots A_j : (A_i \\dots A_k) \\cdot (A_{k+1} \\dots A_j)\\) The optimal cost: \\(m_{i,j} = m_{i,k} + m_{k+1,j} + p_{i-1} \\times p_k \\times p_j\\) where: \\(m_{i,k}\\) : Optimal cost of computing \\(A_{i \\dots k}\\) \\(m_{k+1,j}\\) : Optimal cost of computing \\(A_{k+1 \\dots j}\\) \\(p_{i-1} \\times p_k \\times p_j\\) : Cost of multiplying \\(A_{i \\dots k}\\) and \\(A_{k+1 \\dots j}\\) Step 2: A Recursive Solution \u00b6 In an optimal parenthesization: \\(k\\) must be chosen to minimize \\(m_{ij}\\) The recursive formulation for \\(m_{ij}\\) : \\[ \\begin{align*} m_{ij} = \\begin{cases} 0 & if & i=j \\\\ \\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} & if & i<j \\end{cases} \\end{align*} \\] Step 2: A Recursive Solution \u00b6 The \\(m_{ij}\\) values give the costs of optimal solutions to subproblems In order to keep track of how to construct an optimal solution Define \\(s_{ij}\\) to be the value of \\(k\\) which yields the optimal split of the subchain \\(A_{i \\dots j}\\) That is, \\(s_{ij}=k\\) such that \\(m_{ij} = m_{ik} + m_{k+1,j} +p_{i-1} p_k p_j\\) holds section{ font-size: 25px; } Direct Recursion: Inefficient! \u00b6 Recursive Matrix-Chain ( RMC ) Order RMC ( p , i , j ) if ( i == j ) then return 0 m [ i , j ] = INF for k = i to j -1 do q = RMC ( p , i , k ) + RMC ( p , k +1 , j ) + p_ { i -1 } p_k p_j if q < m [ i , j ] then m [ i , j ] = q endfor return m [ i , j ] Direct Recursion: Inefficient! \u00b6 Recursion tree for \\(RMC(p,1,4)\\) Nodes are labeled with \\(i\\) and \\(j\\) values Computing the Optimal Cost ( Matrix-Chain Multiplication ) \u00b6 An important observation: - We have relatively few subproblems - one problem for each choice of \\(i\\) and \\(j\\) satisfying \\(1 \\leq i \\leq j \\leq n\\) - total \\(n + (n-1) + \\dots + 2 + 1 = \\frac{1}{2}n(n+1) = \\Theta(n2)\\) subproblems - We can write a recursive algorithm based on recurrence. - However, a recursive algorithm may encounter each subproblem many times in different branches of the recursion tree - This property, overlapping subproblems , is the second important feature for applicability of dynamic programming Computing the Optimal Cost ( Matrix-Chain Multiplication ) \u00b6 Compute the value of an optimal solution in a bottom-up fashion matrix \\(A_i\\) has dimensions \\(p_{i-1} \\times p_i\\) for \\(i = 1, 2, \\dots , n\\) the input is a sequence \\(\\langle p_0, p_1, \\dots, p_n \\rangle\\) where \\(length[p] = n + 1\\) Procedure uses the following auxiliary tables: \\(m[1 \\dots n, 1 \\dots n]\\) : for storing the \\(m[i,j]\\) costs \\(s[1 \\dots n, 1 \\dots n]\\) : records which index of \\(k\\) achieved the optimal cost in computing \\(m[i,j]\\) Bottom-Up Computation \u00b6 How to choose the order in which we process \\(m_{ij}\\) values? Before computing \\(m_{ij}\\) , we have to make sure that the values for \\(m_{ik}\\) and \\(m_{k+1,j}\\) have been computed for all \\(k\\) . \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] Bottom-Up Computation \u00b6 \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] \\(m_{ij}\\) must be processed after \\(m_{ik}\\) and \\(m_{j,k+1}\\) Reminder: \\(m_{ij}\\) computed only for \\(j > i\\) Bottom-Up Computation \u00b6 \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] \\(m_{ij}\\) must be processed after \\(m_{ik}\\) and \\(m_{j,k+1}\\) How to set up the iterations over \\(i\\) and \\(j\\) to compute \\(m_{ij}\\) ? Bottom-Up Computation \u00b6 \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] If the entries \\(m_{ij}\\) are computed in the shown order, then \\(m_{ik}\\) and \\(m_{k+1,j}\\) values are guaranteed to be computed before \\(m_{ij}\\) . Bottom-Up Computation \u00b6 \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] Bottom-Up Computation \u00b6 \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] section{ font-size: 25px; } Algorithm for Computing the Optimal Costs \u00b6 Note : l \\(=\\ell\\) and p_{i-1} p_k p_j \\(=p_{i-1} p_k p_j\\) MATRIX - CHAIN - ORDER ( p ) n = length [ p ] -1 for i = 1 to n do m [ i , i ] = 0 endfor for l = 2 to n do for i = 1 to n n - l +1 do j = i + l -1 m [ i , j ] = INF for k = i to j -1 do q = m [ i , k ] + m [ k +1 , j ] + p_ { i -1 } p_k p_j if q < m [ i , j ] then m [ i , j ] = q s [ i , j ] = k endfor endfor endfor return m and s Algorithm for Computing the Optimal Costs \u00b6 The algorithm first computes \\(m[i, i] \\leftarrow 0\\) for \\(i=1,2, \\dots ,n\\) min costs for all chains of length 1 Then , for \\(\\ell = 2,3, \\dots,n\\) computes \\(m[i, i+\\ell-1]\\) for \\(i=1,\\dots,n-\\ell+1\\) min costs for all chains of length \\(\\ell\\) For each value of \\(\\ell = 2, 3, \\dots ,n\\) , \\(m[i, i+\\ell-1]\\) depends only on table entries \\(m[i,k] \\& m[k+1, i+\\ell-1]\\) for \\(i\\leq k < i+\\ell-1\\) , which are already computed Algorithm for Computing the Optimal Costs \u00b6 \\[ \\begin{align} \\begin{aligned} \\text{compute } m[i,i+1] \\\\ \\underbrace{ \\{ m[1,2],m[2,3], \\dots ,m[n-1,n]\\} }_{(n-1) \\text{ values}} \\end{aligned} & \\begin{cases} & \\ell=2 \\\\ & \\text{for } i=1 \\text{ to } n-1 \\text{ do } \\\\ & \\quad m[i,i+1]=\\infty \\\\ & \\quad \\quad \\text{for } k=i \\text{ to } i \\text{ do } \\\\ & \\quad \\quad \\quad \\vdots \\end{cases} \\\\ \\begin{aligned} \\text{compute } m[i,i+2] \\\\ \\underbrace{ \\{ m[1,3],m[2,4], \\dots ,m[n-2,n]\\} }_{(n-2) \\text{ values}} \\end{aligned} & \\begin{cases} & \\ell=3 \\\\ & \\text{for } i=1 \\text{ to } n-2 \\text{ do } \\\\ & \\quad m[i,i+2]=\\infty \\\\ & \\quad \\quad \\text{for } k=i \\text{ to } i+1 \\text{ do } \\\\ & \\quad \\quad \\quad \\vdots \\end{cases} \\\\ \\begin{aligned} \\text{compute } m[i,i+3] \\\\ \\underbrace{ \\{ m[1,4],m[2,5], \\dots ,m[n-3,n]\\} }_{(n-3) \\text{ values}} \\end{aligned} & \\begin{cases} & \\ell=4 \\\\ & \\text{for } i=1 \\text{ to } n-3 \\text{ do } \\\\ & \\quad m[i,i+3]=\\infty \\\\ & \\quad \\quad \\text{for } k=i \\text{ to } i+2 \\text{ do } \\\\ & \\quad \\quad \\quad \\vdots \\end{cases} \\end{align} \\] Table access pattern in computing \\(m[i, j]\\) s for \\(\\ell=j-i+1\\) \u00b6 \\[ \\begin{align*} & \\text{for} \\ k \\leftarrow i \\ \\text{to} \\ j-1 \\ \\text{do} \\\\ & \\quad q \\leftarrow m[i,k]+m[k+1,j]+p_{i-1}p_kp_j \\end{align*} \\] Table access pattern in computing \\(m[i, j]\\) s for \\(\\ell=j-i+1\\) \u00b6 \\[ \\begin{align*} & \\bigg( (A_i) \\overset{mult.}{ \\vdots } (A_{i+1}A_{i+2} \\dots A_j) \\bigg) \\end{align*} \\] Table access pattern in computing \\(m[i, j]\\) s for \\(\\ell=j-i+1\\) \u00b6 \\[ \\begin{align*} \\bigg( (A_iA_{i+1}) \\overset{mult.}{ \\vdots } (A_{i+2} \\dots A_j) \\bigg) \\end{align*} \\] Table access pattern in computing \\(m[i, j]\\) s for \\(\\ell=j-i+1\\) \u00b6 \\[ \\begin{align*} \\bigg( (A_iA_{i+1}A_{i+2}) \\overset{mult.}{ \\vdots } (A_{i+3} \\dots A_j) \\bigg) \\end{align*} \\] Table access pattern in computing \\(m[i, j]\\) s for \\(\\ell=j-i+1\\) \u00b6 \\[ \\begin{align*} \\bigg( (A_iA_{i+1} \\dots A_{j-1}) \\overset{mult.}{ \\vdots } (A_j) \\bigg) \\end{align*} \\] Table access pattern Example \u00b6 Compute \\(m_{25}\\) Choose the \\(k\\) value that leads to min cost \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\\\[10pt] \\begin{align*} \\begin{aligned} A_1 &: (30 \\times 35) \\\\ A_2 &: (35 \\times 15) \\\\ A_3 &: (15 \\times 5) \\\\ A_4 &: (5 \\times 10) \\\\ A_5 &: (10 \\times 20) \\\\ A_6 &: (20 \\times 25) \\end{aligned} \\begin{aligned} & ((A_2) \\overbrace{\\vdots}^{ (k=2) } (A_3 A_4 A_5)) \\\\[10 pt] \\quad cost &= m_{22} + m_{35} + p_1p_2p_5 \\\\ &= 0 + 2500 + 35 \\times 15 \\times 20 \\\\ &= 13000 \\end{aligned} \\end{align*} \\] Table access pattern Example \u00b6 Compute \\(m_{25}\\) Choose the \\(k\\) value that leads to min cost \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\\\[10pt] \\begin{align*} \\begin{aligned} A_1 &: (30 \\times 35) \\\\ A_2 &: (35 \\times 15) \\\\ A_3 &: (15 \\times 5) \\\\ A_4 &: (5 \\times 10) \\\\ A_5 &: (10 \\times 20) \\\\ A_6 &: (20 \\times 25) \\end{aligned} \\begin{aligned} & ((A_2 A_3) \\overbrace{\\vdots}^{ (k=3) } (A_4 A_5)) \\\\[10 pt] \\quad cost &= m_{23} + m_{45} + p_1p_3p_5 \\\\ &= 2625 + 1000 + 35 \\times 5 \\times 20 \\\\ &= 7125 \\end{aligned} \\end{align*} \\] Table access pattern Example \u00b6 Compute \\(m_{25}\\) Choose the \\(k\\) value that leads to min cost \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\\\[10pt] \\begin{align*} \\begin{aligned} A_1 &: (30 \\times 35) \\\\ A_2 &: (35 \\times 15) \\\\ A_3 &: (15 \\times 5) \\\\ A_4 &: (5 \\times 10) \\\\ A_5 &: (10 \\times 20) \\\\ A_6 &: (20 \\times 25) \\end{aligned} \\begin{aligned} & ((A_2 A_3 A_4)\\overbrace{\\vdots}^{ (k=4) }(A_5)) \\\\[10 pt] \\quad cost &= m_{24} + m_{55} + p_1p_4p_5 \\\\ &= 4375 + 0 + 35 \\times 10 \\times 20 \\\\ &= 11375 \\end{aligned} \\end{align*} \\] Table access pattern Example \u00b6 Compute \\(m_{25}\\) Choose the \\(k\\) value that leads to min cost \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\\\[10pt] \\begin{align*} \\begin{aligned} A_1 &: (30 \\times 35) \\\\ A_2 &: (35 \\times 15) \\\\ A_3 &: (15 \\times 5) \\\\ A_4 &: (5 \\times 10) \\\\ A_5 &: (10 \\times 20) \\\\ A_6 &: (20 \\times 25) \\end{aligned} \\quad \\begin{aligned} & ((A_2)\\overbrace{\\vdots}^{ (k=2) } (A_3 A_4 A_5)) \\rightarrow m_{22} + m_{35} + p_1p_2p_5 = 13000 \\\\ & ((A_2 A_3) \\overbrace{\\vdots}^{ (k=3) } (A_4 A_5)) \\rightarrow m_{23} + m_{45} + p_1p_3p_5 = \\overbrace{ \\boldsymbol{7125}}^{selected} \\Leftarrow \\text{min} \\\\ & ((A_2 A_3 A_4)\\overbrace{\\vdots}^{ (k=4) }(A_5)) \\rightarrow m_{24} + m_{55} + p_1p_4p_5 = 11375 \\\\[20 pt] & m_{25} = 7125 \\\\ & s_{25} = 3 \\end{aligned} \\end{align*} \\] Constructing an Optimal Solution \u00b6 MATRIX-CHAIN-ORDER determines the optimal \\(\\#\\) of scalar mults/adds needed to compute a matrix-chain product it does not directly show how to multiply the matrices That is, it determines the cost of the optimal solution(s) it does not show how to obtain an optimal solution Each entry \\(s[i, j]\\) records the value of \\(k\\) such that optimal parenthesization of \\(A_i \\dots A_j\\) splits the product between \\(A_k\\) & \\(A_{k+1}\\) We know that the final matrix multiplication in computing \\(A_{1 \\dots n}\\) optimally is \\(A_{1 \\dots s[1,n]} \\times A_{s[1,n]+1,n}\\) Example: Constructing an Optimal Solution \u00b6 Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) What is the optimal top-level split for: \\(A_1 A_2 A_3 A_4 A_5 A_6\\) \\(s_{16}=3\\) Example: Constructing an Optimal Solution \u00b6 Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) \\((A_1 A_2 A_3) \\overbrace{\\vdots}^{ (k=4) } (A_4 A_5 A_6)\\) What is the optimal split for \\(A_1 \\dots A_3\\) ? ( \\(s_{13}=1\\) ) What is the optimal split for \\(A_4 \\dots A_6\\) ? ( \\(s_{46}=5\\) ) Example: Constructing an Optimal Solution \u00b6 Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) \\(\\Big((A_1) \\overbrace{\\vdots}^{ (k=1) } (A_2 A_3) \\Big) \\Big( (A_4 A_5) \\overbrace{\\vdots}^{ (k=5) } (A_6) \\Big)\\) What is the optimal split for \\(A_1 \\dots A_3\\) ? ( \\(s_{13}=1\\) ) What is the optimal split for \\(A_4 \\dots A_6\\) ? ( \\(s_{46}=5\\) ) Example: Constructing an Optimal Solution \u00b6 Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) \\(\\Big((A_1) (A_2 A_3) \\Big) \\Big( (A_4 A_5) (A_6) \\Big)\\) What is the optimal split for \\(A_2 A_3\\) ? ( \\(s_{23}=2\\) ) What is the optimal split for \\(A_4 A_5\\) ? ( \\(s_{45}=4\\) ) Example: Constructing an Optimal Solution \u00b6 Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) \\(\\bigg(\\Big(A_1\\Big)\\Big((A_2)\\overbrace{\\vdots}^{ (k=2) }(A_3)\\Big) \\bigg) \\bigg( \\Big((A_4)\\overbrace{\\vdots}^{ (k=4) }(A_5)\\Big) \\Big(A_6\\Big) \\bigg)\\) What is the optimal split for \\(A_2 A_3\\) ? ( \\(s_{23}=2\\) ) What is the optimal split for \\(A_4 A_5\\) ? ( \\(s_{45}=4\\) ) Constructing an Optimal Solution \u00b6 section{ font-size: 20px; } Earlier optimal matrix multiplications can be computed recursively Given: the chain of matrices \\(A = \\langle A_1, A_2, \\dots A_n \\rangle\\) the s table computed by \\(\\text{MATRIX-CHAIN-ORDER}\\) The following recursive procedure computes the matrix-chain product \\(A_{i \\dots j}\\) \\[ \\begin{align*} & \\text{MATRIX-CHAIN-MULTIPLY}(A, s, i, j) \\\\ & \\quad \\text{if} \\ j > i \\ \\text{then} \\\\ & \\qquad X \\longleftarrow \\text{MATRIX-CHAIN-MULTIPLY}(A, s, i, s[i, j]) \\\\ & \\qquad Y \\longleftarrow \\text{MATRIX-CHAIN-MULTIPLY}(A, s, s[i, j]+1, j) \\\\ & \\qquad \\text{return} \\ \\text{MATRIX-MULTIPLY}(X, Y) \\\\ & \\quad \\text{else} \\\\ & \\qquad return A_i \\end{align*} \\] Invocation: \\(\\text{MATRIX-CHAIN-MULTIPLY}(A, s, 1, n)\\) Example: Recursive Construction of an Optimal Solution \u00b6 Example: Recursive Construction of an Optimal Solution \u00b6 Example: Recursive Construction of an Optimal Solution \u00b6 Table reference pattern for \\(m[i, j]\\) \\((1 \\leq i \\leq j \\leq n)\\) \u00b6 \\(m[i, j]\\) is referenced for the computation of \\(m[i, r] \\ \\text{for} \\ j < r \\leq n \\ (n - j )\\) times \\(m[r, j] \\ \\text{for} \\ 1 \\leq r < i \\ (i - 1 )\\) times Table reference pattern for \\(m[i, j]\\) \\((1 \\leq i \\leq j \\leq n)\\) \u00b6 \\(R(i, j)\\) = \\(\\#\\) of times that \\(m[i, j]\\) is referenced in computing other entries \\[ \\begin{align*} R(i, j) &= (n - j) + (i-1) \\\\ &=(n-1) - (j-i) \\end{align*} \\] The total \\(\\#\\) of references for the entire table is: \\(\\sum \\limits_{i=1}^{n}\\sum \\limits_{j=i}^{n}R(i,j)= \\frac{n^3-n}{3}\\) Summary \u00b6 Identification of the optimal substructure property Recursive formulation to compute the cost of the optimal solution Bottom-up computation of the table entries Constructing the optimal solution by backtracing the table entries References \u00b6 Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) \\(-End-Of-Week-5-Course-Module-\\)","title":"Week-5 (Dynamic Programming)"},{"location":"tr/week-5/ce100-week-5-dp/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"tr/week-5/ce100-week-5-dp/#week-5-dynamic-programming","text":"","title":"Week-5 (Dynamic Programming)"},{"location":"tr/week-5/ce100-week-5-dp/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"tr/week-5/ce100-week-5-dp/#quicksort-sort","text":"","title":"Quicksort Sort"},{"location":"tr/week-5/ce100-week-5-dp/#outline","text":"Convex Hull (Divide & Conquer) Dynamic Programming Introduction Divide-and-Conquer (DAC) vs Dynamic Programming (DP) Fibonacci Numbers Recursive Solution Bottom-Up Solution Optimization Problems Development of a DP Algorithms Matrix-Chain Multiplication Matrix Multiplication and Row Columns Definitions Cost of Multiplication Operations (pxqxr) Counting the Number of Parenthesizations The Structure of Optimal Parenthesization Characterize the structure of an optimal solution A Recursive Solution Direct Recursion Inefficiency. Computing the optimal Cost of Matrix-Chain Multiplication Bottom-up Computation Algorithm for Computing the Optimal Costs MATRIX-CHAIN-ORDER Construction and Optimal Solution MATRIX-CHAIN-MULTIPLY Summary","title":"Outline"},{"location":"tr/week-5/ce100-week-5-dp/#dynamic-programming-introduction","text":"An algorithm design paradigm like divide-and-conquer Programming: A tabular method (not writing computer code) Older sense of planning or scheduling, typically by filling in a table Divide-and-Conquer (DAC): subproblems are independent Dynamic Programming (DP): subproblems are not independent Overlapping subproblems: subproblems share sub-subproblems In solving problems with overlapping subproblems A DAC algorithm does redundant work Repeatedly solves common subproblems A DP algorithm solves each problem just once Saves its result in a table","title":"Dynamic Programming - Introduction"},{"location":"tr/week-5/ce100-week-5-dp/#problem-1-fibonacci-numbers-recursive-solution","text":"Reminder: \\[ \\begin{align*} & F(0)=0 \\text{ and } F(1)=1 \\\\ & F(n)=F(n-1)+F(n-2) \\\\[10 pt] &\\text{REC-FIBO}(n) \\{ \\\\ & \\quad \\text{if} \\ n < 2 \\\\ & \\qquad \\text{return} \\ n \\\\ & \\quad \\text{else} \\\\ & \\qquad \\text{return} \\ \\text{REC-FIBO}(n-1) + \\text{REC-FIBO}(n-2) \\ \\} \\end{align*} \\] Overlapping subproblems in different recursive calls. Repeated work!","title":"Problem 1: Fibonacci Numbers Recursive Solution"},{"location":"tr/week-5/ce100-week-5-dp/#problem-1-fibonacci-numbers-recursive-solution_1","text":"Recurrence: exponential runtime \\[ T(n) = T(n-1) + T(n-2) + 1 \\] Recursive algorithm inefficient because it recomputes the same \\(F(i)\\) repeatedly in different branches of the recursion tree.","title":"Problem 1: Fibonacci Numbers Recursive Solution"},{"location":"tr/week-5/ce100-week-5-dp/#problem-1-fibonacci-numbers-bottom-up-computation","text":"Reminder: \\[ \\begin{align*} & F(0)=0 \\text{ and } F(1)=1 \\\\ & F(n)=F(n-1)+F(n-2) \\end{align*} \\] Runtime \\(\\Theta(n)\\) ITER - FIBO ( n ) F [ 0 ] = 0 F [ 1 ] = 1 for i = 2 to n do F [ i ] = F [ i -1 ] + F [ i -2 ] return F [ n ]","title":"Problem 1: Fibonacci Numbers Bottom-up Computation"},{"location":"tr/week-5/ce100-week-5-dp/#optimization-problems","text":"DP typically applied to optimization problems In an optimization problem There are many possible solutions (feasible solutions) Each solution has a value Want to find an optimal solution to the problem A solution with the optimal value (min or max value) Wrong to say the optimal solution to the problem There may be several solutions with the same optimal value","title":"Optimization Problems"},{"location":"tr/week-5/ce100-week-5-dp/#development-of-a-dp-algorithm","text":"Step-1 . Characterize the structure of an optimal solution Step-2 . Recursively define the value of an optimal solution Step-3 . Compute the value of an optimal solution in a bottom-up fashion Step-4 . Construct an optimal solution from the information computed in Step 3 section{ font-size: 25px; }","title":"Development of a DP Algorithm"},{"location":"tr/week-5/ce100-week-5-dp/#problem-2-matric-chain-multiplication","text":"Input: a sequence (chain) \\(\\langle A_1,A_2, \\dots , A_n\\rangle\\) of \\(n\\) matrices Aim: compute the product \\(A_1 \\cdot A_2 \\cdot \\dots A_n\\) A product of matrices is fully parenthesized if It is either a single matrix Or, the product of two fully parenthesized matrix products surrounded by a pair of parentheses. \\(\\bigg(A_i(A_{i+1}A_{i+2} \\dots A_j) \\bigg)\\) \\(\\bigg((A_iA_{i+1}A_{i+2} \\dots A_{j-1})A_j \\bigg)\\) \\(\\bigg((A_iA_{i+1}A_{i+2} \\dots A_k)(A_{k+1}A_{k+2} \\dots A_j)\\bigg)\\) for \\(i \\leq k < j\\) All parenthesizations yield the same product; matrix product is associative section{ font-size: 25px; }","title":"Problem 2: Matric Chain Multiplication"},{"location":"tr/week-5/ce100-week-5-dp/#matrix-chain-multiplication-an-example-parenthesization","text":"Input: \\(\\langle A_1,A_2,A_3,A_4\\rangle\\) ( \\(5\\) distinct ways of full parenthesization) \\[ \\begin{align*} & \\bigg(A_1\\Big(A_2(A_3A_4)\\Big)\\bigg) \\\\ & \\bigg(A_1\\Big((A_2A_3)A_4\\Big)\\bigg) \\\\ & \\bigg((A_1A_2)(A_3A_4)\\bigg) \\\\ & \\bigg(\\Big(A_1(A_2A_3)A_4\\Big)\\bigg) \\\\ & \\bigg(\\Big((A_1A_2)A_3\\Big)A_4\\bigg) \\end{align*} \\] The way we parenthesize a chain of matrices can have a dramatic effect on the cost of computing the product","title":"Matrix-chain Multiplication: An Example Parenthesization"},{"location":"tr/week-5/ce100-week-5-dp/#matrix-chain-multiplication-reminder","text":"MATRIX - MULTIPLY ( A , B ) if cols [ A ] != rows [ B ] then error ( \u201c incompatible dimensions \u201d ) for i = 1 to rows [ A ] do for j = 1 to cols [ B ] do C [ i , j ] = 0 for k = 1 to cols [ A ] do C [ i , j ] = C [ i , j ] + A [ i , k ] \u00b7 B [ k , j ] return C","title":"Matrix-chain Multiplication: Reminder"},{"location":"tr/week-5/ce100-week-5-dp/#matrix-chain-multiplication-example","text":"\\(A1:10\\text{x}100\\) , \\(A2:100\\text{x}5\\) , \\(A3:5\\text{x}50\\) Which paranthesization is better? \\((A1A2)A3\\) or \\(A1(A2A3)\\) ?","title":"Matrix Chain Multiplication: Example"},{"location":"tr/week-5/ce100-week-5-dp/#matrix-chain-multiplication-example_1","text":"\\(A1:10 \\times 100\\) , \\(A2:100 \\times 5\\) , \\(A3:5 \\times 50\\) Which paranthesization is better? \\((A1A2)A3\\) or \\(A1(A2A3)\\) ?","title":"Matrix Chain Multiplication: Example"},{"location":"tr/week-5/ce100-week-5-dp/#matrix-chain-multiplication-example_2","text":"\\(A1:10 \\times 100\\) , \\(A2:100 \\times 5\\) , \\(A3:5 \\times 50\\) Which paranthesization is better? \\((A1A2)A3\\) or \\(A1(A2A3)\\) ? In summary: \\((A1A2)A3\\) = \\(\\#\\) of multiply-add ops: \\(7500\\) \\(A1(A2A3)\\) = \\(\\#\\) of multiple-add ops: \\(75000\\) First parenthesization yields 10x faster computation","title":"Matrix Chain Multiplication: Example"},{"location":"tr/week-5/ce100-week-5-dp/#matrix-chain-multiplication-problem","text":"Input: A chain \\(\\langle A_1,A_2, \\dots ,A_n\\rangle\\) of \\(n\\) matrices, where \\(A_i\\) is a \\(p_{i-1} \\times p_i\\) matrix Objective: Fully parenthesize the product \\(A_1 \\cdot A_2 \\dots A_n\\) such that the number of scalar mult-adds is minimized. section{ font-size: 25px; }","title":"Matrix-chain Multiplication Problem"},{"location":"tr/week-5/ce100-week-5-dp/#counting-the-number-of-parenthesizations","text":"Brute force approach: exhaustively check all parenthesizations \\(P(n)\\) : \\(\\#\\) of parenthesizations of a sequence of n matrices We can split sequence between \\(k^{th}\\) and \\((k+1)^{st}\\) matrices for any \\(k=1, 2, \\dots , n-1\\) , then parenthesize the two resulting sequences independently, i.e., \\[ (A_1 A_2 A_3 \\dots A_k \\overbrace{)(}^{break-point}A_{k+1} A_{k+2} \\dots A_n) \\] We obtain the recurrence \\[ P(1)=1 \\text{ and } P(n)=\\sum \\limits_{k=1}^{n-1}P(k)P(n-k) \\]","title":"Counting the Number of Parenthesizations"},{"location":"tr/week-5/ce100-week-5-dp/#number-of-parenthesizations","text":"\\(P(1)=1\\) and \\(P(n)=\\sum \\limits_{k=1}^{n-1}P(k)P(n-k)\\) The recurrence generates the sequence of Catalan Numbers Solution is \\(P(n)=C(n-1)\\) where \\[ C(n)=\\frac{1}{n+1} {2n \\choose n} = \\Omega(4^n / n^{3/2}) \\] The number of solutions is exponential in \\(n\\) Therefore, brute force approach is a poor strategy","title":"Number of Parenthesizations:"},{"location":"tr/week-5/ce100-week-5-dp/#the-structure-of-optimal-parenthesization","text":"Notation: \\(A_{i..j}\\) : The matrix that results from evaluation of the product: \\(A_i A_{i+1} A_{i+2} \\dots A_j\\) Observation: Consider the last multiplication operation in any parenthesization: \\((A_1 A_2 \\dots A_k) \\cdot (A_{k+1} A_{k+2} \\dots A_n)\\) There is a \\(k\\) value \\((1 \\leq k < n)\\) such that: First, the product \\(A_1 \\dots k\\) is computed Then, the product \\(A_{k+1 \\dots n}\\) is computed Finally, the matrices \\(A_{1 \\dots k}\\) and \\(A_{k+1 \\dots n}\\) are multiplied","title":"The Structure of Optimal Parenthesization"},{"location":"tr/week-5/ce100-week-5-dp/#step-1-characterize-the-structure-of-an-optimal-solution","text":"An optimal parenthesization of product \\(A_1 A_2 \\dots A_n\\) will be: \\((A_1 A_2 \\dots A_k) \\cdot (A_{k+1} A_{k+2} \\dots A_n)\\) for some \\(k\\) value The cost of this optimal parenthesization will be: \\(=\\) Cost of computing \\(A_{1 \\dots k}\\) \\(+\\) Cost of computing \\(A_{k+1 \\dots n}\\) \\(+\\) Cost of multiplying \\(A_{1 \\dots k} \\cdot A_{k+1 \\dots n}\\)","title":"Step 1: Characterize the Structure of an Optimal Solution"},{"location":"tr/week-5/ce100-week-5-dp/#step-1-characterize-the-structure-of-an-optimal-solution_1","text":"Key observation: Given optimal parenthesization \\((A_1 A_2 A_3 \\dots A_k) \\cdot (A_{k+1} A_{k+2} \\dots A_n)\\) Parenthesization of the subchain \\(A_1 A_2 A_3 \\dots A_k\\) Parenthesization of the subchain \\(A_{k+1} A_{k+2} \\dots A_n\\) should both be optimal Thus, optimal solution to an instance of the problem contains optimal solutions to subproblem instances i.e. , optimal substructure within an optimal solution exists.","title":"Step 1: Characterize the Structure of an Optimal Solution"},{"location":"tr/week-5/ce100-week-5-dp/#step-2-a-recursive-solution","text":"Step 2: Define the value of an optimal solution recursively in terms of optimal solutions to the subproblems Assume we are trying to determine the min cost of computing \\(A_{i \\dots j}\\) \\(m_{i,j}\\) : min \\(\\#\\) of scalar multiply-add opns needed to compute \\(A_{i \\dots j}\\) Note: The optimal cost of the original problem: \\(m_{1,n}\\) How to compute \\(m_{i,j}\\) recursively?","title":"Step 2: A Recursive Solution"},{"location":"tr/week-5/ce100-week-5-dp/#step-2-a-recursive-solution_1","text":"Base case: \\(m_{i,i}=0\\) (single matrix, no multiplication) Let the size of matrix \\(A_i\\) be \\((p_{i-1} \\times p_i)\\) Consider an optimal parenthesization of chain \\(A_i \\dots A_j : (A_i \\dots A_k) \\cdot (A_{k+1} \\dots A_j)\\) The optimal cost: \\(m_{i,j} = m_{i,k} + m_{k+1,j} + p_{i-1} \\times p_k \\times p_j\\) where: \\(m_{i,k}\\) : Optimal cost of computing \\(A_{i \\dots k}\\) \\(m_{k+1,j}\\) : Optimal cost of computing \\(A_{k+1 \\dots j}\\) \\(p_{i-1} \\times p_k \\times p_j\\) : Cost of multiplying \\(A_{i \\dots k}\\) and \\(A_{k+1 \\dots j}\\)","title":"Step 2: A Recursive Solution"},{"location":"tr/week-5/ce100-week-5-dp/#step-2-a-recursive-solution_2","text":"In an optimal parenthesization: \\(k\\) must be chosen to minimize \\(m_{ij}\\) The recursive formulation for \\(m_{ij}\\) : \\[ \\begin{align*} m_{ij} = \\begin{cases} 0 & if & i=j \\\\ \\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} & if & i<j \\end{cases} \\end{align*} \\]","title":"Step 2: A Recursive Solution"},{"location":"tr/week-5/ce100-week-5-dp/#step-2-a-recursive-solution_3","text":"The \\(m_{ij}\\) values give the costs of optimal solutions to subproblems In order to keep track of how to construct an optimal solution Define \\(s_{ij}\\) to be the value of \\(k\\) which yields the optimal split of the subchain \\(A_{i \\dots j}\\) That is, \\(s_{ij}=k\\) such that \\(m_{ij} = m_{ik} + m_{k+1,j} +p_{i-1} p_k p_j\\) holds section{ font-size: 25px; }","title":"Step 2: A Recursive Solution"},{"location":"tr/week-5/ce100-week-5-dp/#direct-recursion-inefficient","text":"Recursive Matrix-Chain ( RMC ) Order RMC ( p , i , j ) if ( i == j ) then return 0 m [ i , j ] = INF for k = i to j -1 do q = RMC ( p , i , k ) + RMC ( p , k +1 , j ) + p_ { i -1 } p_k p_j if q < m [ i , j ] then m [ i , j ] = q endfor return m [ i , j ]","title":"Direct Recursion: Inefficient!"},{"location":"tr/week-5/ce100-week-5-dp/#direct-recursion-inefficient_1","text":"Recursion tree for \\(RMC(p,1,4)\\) Nodes are labeled with \\(i\\) and \\(j\\) values","title":"Direct Recursion: Inefficient!"},{"location":"tr/week-5/ce100-week-5-dp/#computing-the-optimal-cost-matrix-chain-multiplication","text":"An important observation: - We have relatively few subproblems - one problem for each choice of \\(i\\) and \\(j\\) satisfying \\(1 \\leq i \\leq j \\leq n\\) - total \\(n + (n-1) + \\dots + 2 + 1 = \\frac{1}{2}n(n+1) = \\Theta(n2)\\) subproblems - We can write a recursive algorithm based on recurrence. - However, a recursive algorithm may encounter each subproblem many times in different branches of the recursion tree - This property, overlapping subproblems , is the second important feature for applicability of dynamic programming","title":"Computing the Optimal Cost (Matrix-Chain Multiplication)"},{"location":"tr/week-5/ce100-week-5-dp/#computing-the-optimal-cost-matrix-chain-multiplication_1","text":"Compute the value of an optimal solution in a bottom-up fashion matrix \\(A_i\\) has dimensions \\(p_{i-1} \\times p_i\\) for \\(i = 1, 2, \\dots , n\\) the input is a sequence \\(\\langle p_0, p_1, \\dots, p_n \\rangle\\) where \\(length[p] = n + 1\\) Procedure uses the following auxiliary tables: \\(m[1 \\dots n, 1 \\dots n]\\) : for storing the \\(m[i,j]\\) costs \\(s[1 \\dots n, 1 \\dots n]\\) : records which index of \\(k\\) achieved the optimal cost in computing \\(m[i,j]\\)","title":"Computing the Optimal Cost (Matrix-Chain Multiplication)"},{"location":"tr/week-5/ce100-week-5-dp/#bottom-up-computation","text":"How to choose the order in which we process \\(m_{ij}\\) values? Before computing \\(m_{ij}\\) , we have to make sure that the values for \\(m_{ik}\\) and \\(m_{k+1,j}\\) have been computed for all \\(k\\) . \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\]","title":"Bottom-Up Computation"},{"location":"tr/week-5/ce100-week-5-dp/#bottom-up-computation_1","text":"\\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] \\(m_{ij}\\) must be processed after \\(m_{ik}\\) and \\(m_{j,k+1}\\) Reminder: \\(m_{ij}\\) computed only for \\(j > i\\)","title":"Bottom-Up Computation"},{"location":"tr/week-5/ce100-week-5-dp/#bottom-up-computation_2","text":"\\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] \\(m_{ij}\\) must be processed after \\(m_{ik}\\) and \\(m_{j,k+1}\\) How to set up the iterations over \\(i\\) and \\(j\\) to compute \\(m_{ij}\\) ?","title":"Bottom-Up Computation"},{"location":"tr/week-5/ce100-week-5-dp/#bottom-up-computation_3","text":"\\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] If the entries \\(m_{ij}\\) are computed in the shown order, then \\(m_{ik}\\) and \\(m_{k+1,j}\\) values are guaranteed to be computed before \\(m_{ij}\\) .","title":"Bottom-Up Computation"},{"location":"tr/week-5/ce100-week-5-dp/#bottom-up-computation_4","text":"\\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\]","title":"Bottom-Up Computation"},{"location":"tr/week-5/ce100-week-5-dp/#bottom-up-computation_5","text":"\\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\] section{ font-size: 25px; }","title":"Bottom-Up Computation"},{"location":"tr/week-5/ce100-week-5-dp/#algorithm-for-computing-the-optimal-costs","text":"Note : l \\(=\\ell\\) and p_{i-1} p_k p_j \\(=p_{i-1} p_k p_j\\) MATRIX - CHAIN - ORDER ( p ) n = length [ p ] -1 for i = 1 to n do m [ i , i ] = 0 endfor for l = 2 to n do for i = 1 to n n - l +1 do j = i + l -1 m [ i , j ] = INF for k = i to j -1 do q = m [ i , k ] + m [ k +1 , j ] + p_ { i -1 } p_k p_j if q < m [ i , j ] then m [ i , j ] = q s [ i , j ] = k endfor endfor endfor return m and s","title":"Algorithm for Computing the Optimal Costs"},{"location":"tr/week-5/ce100-week-5-dp/#algorithm-for-computing-the-optimal-costs_1","text":"The algorithm first computes \\(m[i, i] \\leftarrow 0\\) for \\(i=1,2, \\dots ,n\\) min costs for all chains of length 1 Then , for \\(\\ell = 2,3, \\dots,n\\) computes \\(m[i, i+\\ell-1]\\) for \\(i=1,\\dots,n-\\ell+1\\) min costs for all chains of length \\(\\ell\\) For each value of \\(\\ell = 2, 3, \\dots ,n\\) , \\(m[i, i+\\ell-1]\\) depends only on table entries \\(m[i,k] \\& m[k+1, i+\\ell-1]\\) for \\(i\\leq k < i+\\ell-1\\) , which are already computed","title":"Algorithm for Computing the Optimal Costs"},{"location":"tr/week-5/ce100-week-5-dp/#algorithm-for-computing-the-optimal-costs_2","text":"\\[ \\begin{align} \\begin{aligned} \\text{compute } m[i,i+1] \\\\ \\underbrace{ \\{ m[1,2],m[2,3], \\dots ,m[n-1,n]\\} }_{(n-1) \\text{ values}} \\end{aligned} & \\begin{cases} & \\ell=2 \\\\ & \\text{for } i=1 \\text{ to } n-1 \\text{ do } \\\\ & \\quad m[i,i+1]=\\infty \\\\ & \\quad \\quad \\text{for } k=i \\text{ to } i \\text{ do } \\\\ & \\quad \\quad \\quad \\vdots \\end{cases} \\\\ \\begin{aligned} \\text{compute } m[i,i+2] \\\\ \\underbrace{ \\{ m[1,3],m[2,4], \\dots ,m[n-2,n]\\} }_{(n-2) \\text{ values}} \\end{aligned} & \\begin{cases} & \\ell=3 \\\\ & \\text{for } i=1 \\text{ to } n-2 \\text{ do } \\\\ & \\quad m[i,i+2]=\\infty \\\\ & \\quad \\quad \\text{for } k=i \\text{ to } i+1 \\text{ do } \\\\ & \\quad \\quad \\quad \\vdots \\end{cases} \\\\ \\begin{aligned} \\text{compute } m[i,i+3] \\\\ \\underbrace{ \\{ m[1,4],m[2,5], \\dots ,m[n-3,n]\\} }_{(n-3) \\text{ values}} \\end{aligned} & \\begin{cases} & \\ell=4 \\\\ & \\text{for } i=1 \\text{ to } n-3 \\text{ do } \\\\ & \\quad m[i,i+3]=\\infty \\\\ & \\quad \\quad \\text{for } k=i \\text{ to } i+2 \\text{ do } \\\\ & \\quad \\quad \\quad \\vdots \\end{cases} \\end{align} \\]","title":"Algorithm for Computing the Optimal Costs"},{"location":"tr/week-5/ce100-week-5-dp/#table-access-pattern-in-computing-mi-js-for-ellj-i1","text":"\\[ \\begin{align*} & \\text{for} \\ k \\leftarrow i \\ \\text{to} \\ j-1 \\ \\text{do} \\\\ & \\quad q \\leftarrow m[i,k]+m[k+1,j]+p_{i-1}p_kp_j \\end{align*} \\]","title":"Table access pattern in computing \\(m[i, j]\\)s for \\(\\ell=j-i+1\\)"},{"location":"tr/week-5/ce100-week-5-dp/#table-access-pattern-in-computing-mi-js-for-ellj-i1_1","text":"\\[ \\begin{align*} & \\bigg( (A_i) \\overset{mult.}{ \\vdots } (A_{i+1}A_{i+2} \\dots A_j) \\bigg) \\end{align*} \\]","title":"Table access pattern in computing \\(m[i, j]\\)s for \\(\\ell=j-i+1\\)"},{"location":"tr/week-5/ce100-week-5-dp/#table-access-pattern-in-computing-mi-js-for-ellj-i1_2","text":"\\[ \\begin{align*} \\bigg( (A_iA_{i+1}) \\overset{mult.}{ \\vdots } (A_{i+2} \\dots A_j) \\bigg) \\end{align*} \\]","title":"Table access pattern in computing \\(m[i, j]\\)s for \\(\\ell=j-i+1\\)"},{"location":"tr/week-5/ce100-week-5-dp/#table-access-pattern-in-computing-mi-js-for-ellj-i1_3","text":"\\[ \\begin{align*} \\bigg( (A_iA_{i+1}A_{i+2}) \\overset{mult.}{ \\vdots } (A_{i+3} \\dots A_j) \\bigg) \\end{align*} \\]","title":"Table access pattern in computing \\(m[i, j]\\)s for \\(\\ell=j-i+1\\)"},{"location":"tr/week-5/ce100-week-5-dp/#table-access-pattern-in-computing-mi-js-for-ellj-i1_4","text":"\\[ \\begin{align*} \\bigg( (A_iA_{i+1} \\dots A_{j-1}) \\overset{mult.}{ \\vdots } (A_j) \\bigg) \\end{align*} \\]","title":"Table access pattern in computing \\(m[i, j]\\)s for \\(\\ell=j-i+1\\)"},{"location":"tr/week-5/ce100-week-5-dp/#table-access-pattern-example","text":"Compute \\(m_{25}\\) Choose the \\(k\\) value that leads to min cost \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\\\[10pt] \\begin{align*} \\begin{aligned} A_1 &: (30 \\times 35) \\\\ A_2 &: (35 \\times 15) \\\\ A_3 &: (15 \\times 5) \\\\ A_4 &: (5 \\times 10) \\\\ A_5 &: (10 \\times 20) \\\\ A_6 &: (20 \\times 25) \\end{aligned} \\begin{aligned} & ((A_2) \\overbrace{\\vdots}^{ (k=2) } (A_3 A_4 A_5)) \\\\[10 pt] \\quad cost &= m_{22} + m_{35} + p_1p_2p_5 \\\\ &= 0 + 2500 + 35 \\times 15 \\times 20 \\\\ &= 13000 \\end{aligned} \\end{align*} \\]","title":"Table access pattern Example"},{"location":"tr/week-5/ce100-week-5-dp/#table-access-pattern-example_1","text":"Compute \\(m_{25}\\) Choose the \\(k\\) value that leads to min cost \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\\\[10pt] \\begin{align*} \\begin{aligned} A_1 &: (30 \\times 35) \\\\ A_2 &: (35 \\times 15) \\\\ A_3 &: (15 \\times 5) \\\\ A_4 &: (5 \\times 10) \\\\ A_5 &: (10 \\times 20) \\\\ A_6 &: (20 \\times 25) \\end{aligned} \\begin{aligned} & ((A_2 A_3) \\overbrace{\\vdots}^{ (k=3) } (A_4 A_5)) \\\\[10 pt] \\quad cost &= m_{23} + m_{45} + p_1p_3p_5 \\\\ &= 2625 + 1000 + 35 \\times 5 \\times 20 \\\\ &= 7125 \\end{aligned} \\end{align*} \\]","title":"Table access pattern Example"},{"location":"tr/week-5/ce100-week-5-dp/#table-access-pattern-example_2","text":"Compute \\(m_{25}\\) Choose the \\(k\\) value that leads to min cost \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\\\[10pt] \\begin{align*} \\begin{aligned} A_1 &: (30 \\times 35) \\\\ A_2 &: (35 \\times 15) \\\\ A_3 &: (15 \\times 5) \\\\ A_4 &: (5 \\times 10) \\\\ A_5 &: (10 \\times 20) \\\\ A_6 &: (20 \\times 25) \\end{aligned} \\begin{aligned} & ((A_2 A_3 A_4)\\overbrace{\\vdots}^{ (k=4) }(A_5)) \\\\[10 pt] \\quad cost &= m_{24} + m_{55} + p_1p_4p_5 \\\\ &= 4375 + 0 + 35 \\times 10 \\times 20 \\\\ &= 11375 \\end{aligned} \\end{align*} \\]","title":"Table access pattern Example"},{"location":"tr/week-5/ce100-week-5-dp/#table-access-pattern-example_3","text":"Compute \\(m_{25}\\) Choose the \\(k\\) value that leads to min cost \\[ m_{ij}=\\underset{i \\leq k < j}{MIN} \\{ m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j \\} \\\\[10pt] \\begin{align*} \\begin{aligned} A_1 &: (30 \\times 35) \\\\ A_2 &: (35 \\times 15) \\\\ A_3 &: (15 \\times 5) \\\\ A_4 &: (5 \\times 10) \\\\ A_5 &: (10 \\times 20) \\\\ A_6 &: (20 \\times 25) \\end{aligned} \\quad \\begin{aligned} & ((A_2)\\overbrace{\\vdots}^{ (k=2) } (A_3 A_4 A_5)) \\rightarrow m_{22} + m_{35} + p_1p_2p_5 = 13000 \\\\ & ((A_2 A_3) \\overbrace{\\vdots}^{ (k=3) } (A_4 A_5)) \\rightarrow m_{23} + m_{45} + p_1p_3p_5 = \\overbrace{ \\boldsymbol{7125}}^{selected} \\Leftarrow \\text{min} \\\\ & ((A_2 A_3 A_4)\\overbrace{\\vdots}^{ (k=4) }(A_5)) \\rightarrow m_{24} + m_{55} + p_1p_4p_5 = 11375 \\\\[20 pt] & m_{25} = 7125 \\\\ & s_{25} = 3 \\end{aligned} \\end{align*} \\]","title":"Table access pattern Example"},{"location":"tr/week-5/ce100-week-5-dp/#constructing-an-optimal-solution","text":"MATRIX-CHAIN-ORDER determines the optimal \\(\\#\\) of scalar mults/adds needed to compute a matrix-chain product it does not directly show how to multiply the matrices That is, it determines the cost of the optimal solution(s) it does not show how to obtain an optimal solution Each entry \\(s[i, j]\\) records the value of \\(k\\) such that optimal parenthesization of \\(A_i \\dots A_j\\) splits the product between \\(A_k\\) & \\(A_{k+1}\\) We know that the final matrix multiplication in computing \\(A_{1 \\dots n}\\) optimally is \\(A_{1 \\dots s[1,n]} \\times A_{s[1,n]+1,n}\\)","title":"Constructing an Optimal Solution"},{"location":"tr/week-5/ce100-week-5-dp/#example-constructing-an-optimal-solution","text":"Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) What is the optimal top-level split for: \\(A_1 A_2 A_3 A_4 A_5 A_6\\) \\(s_{16}=3\\)","title":"Example: Constructing an Optimal Solution"},{"location":"tr/week-5/ce100-week-5-dp/#example-constructing-an-optimal-solution_1","text":"Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) \\((A_1 A_2 A_3) \\overbrace{\\vdots}^{ (k=4) } (A_4 A_5 A_6)\\) What is the optimal split for \\(A_1 \\dots A_3\\) ? ( \\(s_{13}=1\\) ) What is the optimal split for \\(A_4 \\dots A_6\\) ? ( \\(s_{46}=5\\) )","title":"Example: Constructing an Optimal Solution"},{"location":"tr/week-5/ce100-week-5-dp/#example-constructing-an-optimal-solution_2","text":"Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) \\(\\Big((A_1) \\overbrace{\\vdots}^{ (k=1) } (A_2 A_3) \\Big) \\Big( (A_4 A_5) \\overbrace{\\vdots}^{ (k=5) } (A_6) \\Big)\\) What is the optimal split for \\(A_1 \\dots A_3\\) ? ( \\(s_{13}=1\\) ) What is the optimal split for \\(A_4 \\dots A_6\\) ? ( \\(s_{46}=5\\) )","title":"Example: Constructing an Optimal Solution"},{"location":"tr/week-5/ce100-week-5-dp/#example-constructing-an-optimal-solution_3","text":"Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) \\(\\Big((A_1) (A_2 A_3) \\Big) \\Big( (A_4 A_5) (A_6) \\Big)\\) What is the optimal split for \\(A_2 A_3\\) ? ( \\(s_{23}=2\\) ) What is the optimal split for \\(A_4 A_5\\) ? ( \\(s_{45}=4\\) )","title":"Example: Constructing an Optimal Solution"},{"location":"tr/week-5/ce100-week-5-dp/#example-constructing-an-optimal-solution_4","text":"Reminder: \\(s_{ij}\\) is the optimal top-level split of \\(A_i \\dots A_j\\) \\(\\bigg(\\Big(A_1\\Big)\\Big((A_2)\\overbrace{\\vdots}^{ (k=2) }(A_3)\\Big) \\bigg) \\bigg( \\Big((A_4)\\overbrace{\\vdots}^{ (k=4) }(A_5)\\Big) \\Big(A_6\\Big) \\bigg)\\) What is the optimal split for \\(A_2 A_3\\) ? ( \\(s_{23}=2\\) ) What is the optimal split for \\(A_4 A_5\\) ? ( \\(s_{45}=4\\) )","title":"Example: Constructing an Optimal Solution"},{"location":"tr/week-5/ce100-week-5-dp/#constructing-an-optimal-solution_1","text":"section{ font-size: 20px; } Earlier optimal matrix multiplications can be computed recursively Given: the chain of matrices \\(A = \\langle A_1, A_2, \\dots A_n \\rangle\\) the s table computed by \\(\\text{MATRIX-CHAIN-ORDER}\\) The following recursive procedure computes the matrix-chain product \\(A_{i \\dots j}\\) \\[ \\begin{align*} & \\text{MATRIX-CHAIN-MULTIPLY}(A, s, i, j) \\\\ & \\quad \\text{if} \\ j > i \\ \\text{then} \\\\ & \\qquad X \\longleftarrow \\text{MATRIX-CHAIN-MULTIPLY}(A, s, i, s[i, j]) \\\\ & \\qquad Y \\longleftarrow \\text{MATRIX-CHAIN-MULTIPLY}(A, s, s[i, j]+1, j) \\\\ & \\qquad \\text{return} \\ \\text{MATRIX-MULTIPLY}(X, Y) \\\\ & \\quad \\text{else} \\\\ & \\qquad return A_i \\end{align*} \\] Invocation: \\(\\text{MATRIX-CHAIN-MULTIPLY}(A, s, 1, n)\\)","title":"Constructing an Optimal Solution"},{"location":"tr/week-5/ce100-week-5-dp/#example-recursive-construction-of-an-optimal-solution","text":"","title":"Example: Recursive Construction of an Optimal Solution"},{"location":"tr/week-5/ce100-week-5-dp/#example-recursive-construction-of-an-optimal-solution_1","text":"","title":"Example: Recursive Construction of an Optimal Solution"},{"location":"tr/week-5/ce100-week-5-dp/#example-recursive-construction-of-an-optimal-solution_2","text":"","title":"Example: Recursive Construction of an Optimal Solution"},{"location":"tr/week-5/ce100-week-5-dp/#table-reference-pattern-for-mi-j-1-leq-i-leq-j-leq-n","text":"\\(m[i, j]\\) is referenced for the computation of \\(m[i, r] \\ \\text{for} \\ j < r \\leq n \\ (n - j )\\) times \\(m[r, j] \\ \\text{for} \\ 1 \\leq r < i \\ (i - 1 )\\) times","title":"Table reference pattern for \\(m[i, j]\\) \\((1 \\leq i \\leq j \\leq n)\\)"},{"location":"tr/week-5/ce100-week-5-dp/#table-reference-pattern-for-mi-j-1-leq-i-leq-j-leq-n_1","text":"\\(R(i, j)\\) = \\(\\#\\) of times that \\(m[i, j]\\) is referenced in computing other entries \\[ \\begin{align*} R(i, j) &= (n - j) + (i-1) \\\\ &=(n-1) - (j-i) \\end{align*} \\] The total \\(\\#\\) of references for the entire table is: \\(\\sum \\limits_{i=1}^{n}\\sum \\limits_{j=i}^{n}R(i,j)= \\frac{n^3-n}{3}\\)","title":"Table reference pattern for \\(m[i, j]\\) \\((1 \\leq i \\leq j \\leq n)\\)"},{"location":"tr/week-5/ce100-week-5-dp/#summary","text":"Identification of the optimal substructure property Recursive formulation to compute the cost of the optimal solution Bottom-up computation of the table entries Constructing the optimal solution by backtracing the table entries","title":"Summary"},{"location":"tr/week-5/ce100-week-5-dp/#references","text":"Introduction to Algorithms, Third Edition | The MIT Press Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) \\(-End-Of-Week-5-Course-Module-\\)","title":"References"},{"location":"tr/week-6/ce100-week-6-lcs/","text":"CE100 Algorithms and Programming II \u00b6 Week-6 (Matrix Chain Order / LCS) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Matrix Chain Order / Longest Common Subsequence \u00b6 Outline \u00b6 Elements of Dynamic Programming Optimal Substructure Overlapping Subproblems Recursive Matrix Chain Order Memoization Top-Down Approach RMC MemoizedMatrixChain LookupC Dynamic Programming vs Memoization Summary Dynamic Programming Problem-2 : Longest Common Subsequence Definitions LCS Problem Notations Optimal Substructure of LCS Proof Case-1 Proof Case-2 Proof Case-3 A recursive solution to subproblems (inefficient) Computing the length of and LCS LCS Data Structure for DP Bottom-Up Computation Constructing and LCS PRINT-LCS Back-pointer space optimization for LCS length Most Common Dynamic Programming Interview Questions Elements of Dynamic Programming \u00b6 When should we look for a DP solution to an optimization problem? Two key ingredients for the problem Optimal substructure Overlapping subproblems DP Hallmark #1 \u00b6 Optimal Substructure A problem exhibits optimal substructure if an optimal solution to a problem contains within it optimal solutions to subproblems Example: matrix-chain-multiplication Optimal parenthesization of \\(A_1 A_2 \\dots A_n\\) that splits the product between \\(A_k\\) and \\(A_{k+1}\\) , contains within it optimal soln\u2019s to the problems of parenthesizing \\(A_1A_2 \\dots A_k\\) and \\(A_{k+1} A_{k+2} \\dots A_n\\) Optimal Substructure \u00b6 Finding a suitable space of subproblems Iterate on subproblem instances Example: matrix-chain-multiplication Iterate and look at the structure of optimal soln\u2019s to subproblems, sub-subproblems, and so forth Discover that all subproblems consists of subchains of \\(\\langle A_1, A_2, \\dots , A_n \\rangle\\) Thus, the set of chains of the form \\(\\langle A_i,A_{i+1}, \\dots , A_j \\rangle\\) for \\(1 \\leq i \\leq j \\leq n\\) Makes a natural and reasonable space of subproblems DP Hallmark #2 \u00b6 Overlapping Subproblems Total number of distinct subproblems should be polynomial in the input size When a recursive algorithm revisits the same problem over and over again , We say that the optimization problem has overlapping subproblems Overlapping Subproblems \u00b6 DP algorithms typically take advantage of overlapping subproblems by solving each problem once then storing the solutions in a table where it can be looked up when needed using constant time per lookup section{ font-size: 25px; } Overlapping Subproblems \u00b6 Recursive matrix-chain order \\[ \\begin{align*} & \\text{RMC}(p, i, j) \\{ \\\\[5 pt] & \\quad \\text{if} \\ i = j \\ \\text{then} \\\\ & \\qquad \\text{return} \\ 0 \\\\[5 pt] & \\quad m[i, j] \\leftarrow \\infty \\\\[5 pt] & \\quad \\text{for} \\ k \\leftarrow i \\text{to} \\ j - 1 \\ \\text{do} \\\\ & \\qquad q \\leftarrow \\text{RMC}(p, i, k) + \\text{RMC}(p, k+1, j) + p_{i-1} p_k p_j \\\\[5 pt] & \\quad if \\ q < m[i, j] \\ \\text{then} \\\\ & \\qquad m[i, j] \\leftarrow q \\\\[5 pt] & \\quad \\text{return} \\ m[i, j] \\ \\} \\end{align*} \\] Direct Recursion: Inefficient! \u00b6 Recursion tree for \\(RMC(p,1,4)\\) Nodes are labeled with \\(i\\) and \\(j\\) values section{ font-size: 25px; } Running Time of RMC \u00b6 \\(T(1) \\geq 1\\) \\(T(n) \\geq 1 + \\sum \\limits_{k=1}^{n-1} (T(k)+T(n-k)+1)\\ \\text{for} \\ n>1\\) For \\(i =1,2, \\dots ,n\\) each term \\(T(i)\\) appears twice Once as \\(T(k)\\) , and once as \\(T(n-k)\\) Collect \\(n-1,\\) \\(1\\) \u2019s in the summation together with the front \\(1\\) \\[ \\begin{align*} T(n) \\geq 2 \\sum \\limits_{i=1}^{n-1}T(i)+n \\end{align*} \\] Prove that \\(T(n)= \\Omega(2n)\\) using the substitution method section{ font-size: 25px; } Running Time of RMC: Prove that \\(T(n)= \\Omega(2n)\\) \u00b6 Try to show that \\(T(n) \\geq 2^{n-1}\\) ( by substitution ) Base case: \\(T(1) \\geq 1 = 2^0 = 2^{1-1}\\) for \\(n=1\\) Ind. Hyp.: \\[ \\begin{align*} T(i) & \\geq 2^{i-1} \\ \\text{for all} \\ i=1, 2, \\dots, n-1 \\ \\text{and} \\ n \\geq 2 \\\\ T(n) & \\geq 2 \\sum \\limits_{i=1}^{n-1}2^{i-1} + n \\\\[15 pt] & = 2 \\sum \\limits_{i=1}^{n-1} 2^{i-1} + n \\\\ & = 2(2^{n-1}-1) + n \\\\ & = 2^{n-1} + (2^{n-1} - 2 + n) \\\\ & \\Rightarrow T(n) \\geq 2^{n-1} \\ \\text{ Q.E.D.} \\end{align*} \\] section{ font-size: 25px; } Running Time of RMC: \\(T(n) \\geq 2^{n-1}\\) \u00b6 Whenever a recursion tree for the natural recursive solution to a problem contains the same subproblem repeatedly the total number of different subproblems is small it is a good idea to see if \\(DP (Dynamic \\ Programming)\\) can be applied Memoization \u00b6 Offers the efficiency of the usual \\(DP\\) approach while maintaining top-down strategy Idea is to memoize the natural, but inefficient, recursive algorithm Memoized Recursive Algorithm \u00b6 Maintains an entry in a table for the soln to each subproblem Each table entry contains a special value to indicate that the entry has yet to be filled in When the subproblem is first encountered its solution is computed and then stored in the table Each subsequent time that the subproblem encountered the value stored in the table is simply looked up and returned Memoized Recursive Matrix-chain Order \u00b6 Shaded subtrees are looked-up rather than recomputing \\[ \\begin{align*} \\begin{aligned} & \\text{MemoizedMatrixChain(p)} \\\\ & \\quad n \\leftarrow length[p] - 1 \\\\ & \\quad \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad \\text{for} \\ j \\leftarrow 1 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad \\quad m[i, j] \\leftarrow \\infty \\\\ & \\quad \\text{return} \\ \\text{LookupC}(p, 1, n) \\Longrightarrow \\end{aligned} \\begin{aligned} & \\Longrightarrow \\text{LookupC}(p, i, j) \\\\ & \\quad \\text{if} \\ m[i, j] = \\infty \\ \\text{then} \\\\ & \\qquad \\text{if} \\ i = j \\ \\text{then} \\\\ & \\qquad \\quad m[i, j] \\leftarrow 0 \\\\ & \\qquad \\text{else} \\\\ & \\qquad \\quad \\text{for} \\ k \\leftarrow i \\ \\text{to} \\ j-1 \\ \\text{do} \\\\ & \\qquad \\quad \\quad q \\leftarrow \\text{LookupC}(p, i, k) + \\text{LookupC}(p, k+1, j) + p_{i-1} p_k p_j \\\\ & \\qquad \\quad \\quad \\text{if} \\ q < m[i, j] \\ \\text{then} \\\\ & \\qquad \\quad \\quad \\quad m[i, j] \\leftarrow q \\\\ & \\quad \\text{return} \\ m[i, j] \\end{aligned} \\end{align*} \\] Memoized Recursive Algorithm \u00b6 The approach assumes that The set of all possible subproblem parameters are known The relation between the table positions and subproblems is established Another approach is to memoize by using hashing with subproblem parameters as key section{ font-size: 25px; } Dynamic Programming vs Memoization Summary (1) \u00b6 Matrix-chain multiplication can be solved in \\(O(n^3)\\) time by either a top-down memoized recursive algorithm or a bottom-up dynamic programming algorithm Both methods exploit the overlapping subproblems property There are only \\(\\Theta(n^2)\\) different subproblems in total Both methods compute the soln to each problem once Without memoization the natural recursive algorithm runs in exponential time since subproblems are solved repeatedly section{ font-size: 25px; } Dynamic Programming vs Memoization Summary (2) \u00b6 In general practice If all subproblems must be solved at once a bottom-up DP algorithm always outperforms a top-down memoized algorithm by a constant factor because, bottom-up DP algorithm Has no overhead for recursion Less overhead for maintaining the table DP: Regular pattern of table accesses can be exploited to reduce the time and/or space requirements even further Memoized: If some problems need not be solved at all, it has the advantage of avoiding solutions to those subproblems Problem 3: Longest Common Subsequence \u00b6 Definitions A subsequence of a given sequence is just the given sequence with some elements (possibly none) left out Example: \\(X = \\langle A, B, C, B, D, A, B \\rangle\\) \\(Z = \\langle B, C, D, B \\rangle\\) \\(Z\\) is a subsequence of \\(X\\) Problem 3: Longest Common Subsequence \u00b6 Definitions Formal definition: Given a sequence \\(X = \\langle x_1, x_2, \\dots , x_m \\rangle\\) , sequence \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) is a subsequence of \\(X\\) if \\(\\exists\\) a strictly increasing sequence \\(\\langle i_1, i_2,\\dots, i_k \\rangle\\) of indices of \\(X\\) such that \\(x_{i_j} = z_j\\) for all \\(j = 1, 2, \u2026, k\\) , where \\(1 \\leq k \\leq m\\) Example: \\(Z = \\langle B,C,D,B \\rangle\\) is a subsequence of \\(X = \\langle A,B,C,B,D,A,B \\rangle\\) with the index sequence \\(\\langle i_1, i_2, i_3, i_4 \\rangle = \\langle 2, 3, 5, 7 \\rangle\\) section{ font-size: 25px; } Problem 3: Longest Common Subsequence \u00b6 Definitions If \\(Z\\) is a subsequence of both \\(X\\) and \\(Y\\) , we denote \\(Z\\) as a common subsequence of \\(X\\) and \\(Y\\) . Example: \\[ \\begin{align*} X &= \\langle A,B^*,C^*,B,D,A^*,B \\rangle \\\\ Y &= \\langle B^*,D,C^*,A^*,B,A \\rangle \\end{align*} \\] \\(Z_1 = \\langle B^*, C^*, A^* \\rangle\\) is a common subsequence ( of length 3 ) of \\(X\\) and \\(Y\\) . Two longest common subsequence (LCSs) of \\(X\\) and \\(Y\\) ? \\(Z2 = \\langle B, C, B, A \\rangle\\) of length \\(4\\) \\(Z3 = \\langle B, D, A, B \\rangle\\) of length \\(4\\) The optimal solution value = 4 Longest Common Subsequence (LCS) Problem \u00b6 LCS problem: Given two sequences \\(X = \\langle x_1, x_2, \\dots, x_m \\rangle\\) and \\(Y = \\langle y_1, y_2, \\dots , y_n \\rangle\\) , find the LCS of \\(X \\& Y\\) Brute force approach: Enumerate all subsequences of \\(X\\) Check if each subsequence is also a subsequence of \\(Y\\) Keep track of the LCS What is the complexity? There are \\(2^m\\) subsequences of \\(X\\) Exponential runtime Notation \u00b6 Notation: Let \\(X_i\\) denote the \\(i^{th}\\) prefix of \\(X\\) i.e. \\(X_i = \\langle x_1, x_2, \\dots, x_i \\rangle\\) Example: \\[ \\begin{align*} X &= \\langle A, B, C, B, D, A, B \\rangle \\\\[10 pt] X_4 &= \\langle A, B, C, B \\rangle \\\\ X_0 &= \\langle \\rangle \\end{align*} \\] Optimal Substructure of an LCS \u00b6 Let \\(X = <x1, x2, \u2026, xm>\\) and \\(Y = \\langle y_1, y_2, \\dots, y_n \\rangle\\) are given Let \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\) Question 1: If \\(x_m = y_n\\) , how to define the optimal substructure? We must have \\(z_k = x_m = y_n\\) and \\(Z_{k-1} = \\text{LCS}(X_{m-1}, Y_{n-1})\\) Optimal Substructure of an LCS \u00b6 Let \\(X = <x1, x2, \u2026, xm>\\) and \\(Y = \\langle y_1, y_2, \\dots, y_n \\rangle\\) are given Let \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\) Question 2: If \\(x_m \\neq y_n \\ \\text{and} \\ z_k \\neq x_m\\) , how to define the optimal substructure? We must have \\(Z = \\text{LCS}(X_{m-1}, Y)\\) Optimal Substructure of an LCS \u00b6 Let \\(X = <x1, x2, \u2026, xm>\\) and \\(Y = \\langle y_1, y_2, \\dots, y_n \\rangle\\) are given Let \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\) Question 3: If \\(x_m \\neq y_n \\ \\text{and} \\ z_k \\neq y_n\\) , how to define the optimal substructure? We must have \\(Z = \\text{LCS}(X, Y_{n-1})\\) Theorem: Optimal Substructure of an LCS \u00b6 Let \\(X = \\langle x_1, x_2, \\dots, x_m \\rangle\\) and Y = are given Let \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\) Theorem: Optimal substructure of an LCS: If \\(x_m = y_n\\) then \\(z_k = x_m = y_n\\) and \\(Z_{k-1}\\) is an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) If \\(x_m \\neq y_n\\) and \\(z_k \\neq x_m\\) then \\(Z\\) is an LCS of \\(X_{m-1}\\) and \\(Y\\) If \\(x_m \\neq y_n\\) and \\(z_k \\neq y_n\\) then \\(Z\\) is an LCS of \\(X\\) and \\(Y_{n-1}\\) Optimal Substructure Theorem (case 1) \u00b6 If \\(x_m = y_n\\) then \\(z_k = x_m = y_n\\) and \\(Z_{k-1}\\) is an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) Optimal Substructure Theorem (case 2) \u00b6 If \\(x_m \\neq y_n\\) and \\(z_k \\neq x_m\\) then \\(Z\\) is an LCS of \\(X_{m-1}\\) and \\(Y\\) Optimal Substructure Theorem (case 3) \u00b6 If \\(x_m \\neq y_n\\) and \\(z_k \\neq y_n\\) then \\(Z\\) is an LCS of \\(X\\) and \\(Y_{n-1}\\) Proof of Optimal Substructure Theorem (case 1) \u00b6 If \\(x_m = y_n\\) then \\(z_k = x_m = y_n\\) and \\(Z_{k-1}\\) is an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) Proof: If \\(z_k \\neq x_m = y_n\\) then we can append \\(x_m = y_n\\) to \\(Z\\) to obtain a common subsequence of length \\(k+1 \\Longrightarrow\\) contradiction Thus, we must have \\(z_k = x_m = y_n\\) Hence, the prefix \\(Z_{k-1}\\) is a length-( \\(k-1\\) ) CS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) We have to show that \\(Z_{k-1}\\) is in fact an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) Proof by contradiction: Assume that \\(\\exists\\) a CS \\(W\\) of \\(X_{m-1}\\) and \\(Y_{n-1}\\) with \\(|W| = k\\) Then appending \\(x_m = y_n\\) to \\(W\\) produces a CS of length \\(k+1\\) Proof of Optimal Substructure Theorem (case 2) \u00b6 If \\(x_m \\neq y_n\\) and \\(z_k \\neq x_m\\) then \\(Z\\) is an LCS of \\(X_{m-1}\\) and \\(Y\\) Proof : If \\(z_k \\neq x_m\\) then \\(Z\\) is a CS of \\(X_{m-1}\\) and \\(Y_n\\) We have to show that \\(Z\\) is in fact an LCS of \\(X_{m-1}\\) and \\(Y_n\\) (Proof by contradiction) Assume that \\(\\exists\\) a CS \\(W\\) of \\(X_{m-1}\\) and \\(Y_n\\) with \\(|W| > k\\) Then \\(W\\) would also be a CS of \\(X\\) and \\(Y\\) Contradiction to the assumption that \\(Z\\) is an LCS of \\(X\\) and \\(Y\\) with \\(|Z| = k\\) Case 3: Dual of the proof for (case 2) A Recursive Solution to Subproblems \u00b6 Theorem implies that there are one or two subproblems to examine if \\(x_m = y_n\\) then we must solve the subproblem of finding an LCS of \\(X_{m-1} \\& Y_{n-1}\\) appending \\(x_m = y_n\\) to this LCS yields an LCS of \\(X \\& Y\\) else we must solve two subproblems finding an LCS of \\(X_{m-1} \\& Y\\) finding an LCS of \\(X \\& Y_{n-1}\\) longer of these two LCS s is an LCS of \\(X \\& Y\\) endif section{ font-size: 25px; } Recursive Algorithm (Inefficient) \u00b6 \\[ \\begin{align*} & \\text{LCS}(X, Y) \\ \\{ \\\\ & \\quad m \\leftarrow length[X] \\\\ & \\quad n \\leftarrow length[Y] \\\\ & \\quad \\text{if} \\ x_m = y_n \\ \\text{then} \\\\ & \\qquad Z \\leftarrow \\text{LCS}(X_{m-1}, Y_{n-1}) \\triangleright \\text{solve one subproblem} \\\\ & \\qquad \\text{return} \\ \\langle Z, x_m = y_n \\rangle \\triangleright \\text{append} \\ x_m = y_n \\ \\text{to} \\ Z \\\\ & \\quad else \\\\ & \\qquad Z^{'} \\leftarrow \\text{LCS}(X_{m-1}, Y) \\triangleright \\text{solve two subproblems} \\\\ & \\qquad Z^{''} \\leftarrow \\text{LCS}(X, Y_{n-1}) \\\\ & \\qquad \\text{return longer of} \\ Z^{'} \\ \\text{and} \\ Z^{''} \\\\ & \\} \\end{align*} \\] A Recursive Solution \u00b6 \\(c[i, j]:\\) length of an LCS of \\(X_i\\) and \\(Y_j\\) \\[ \\begin{align*} c[i,j] = \\begin{cases} & 0 & \\text{if}& \\ i=0 \\ \\text{or} \\ j=0 \\\\ & c[i-1,j-1]+1 & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i=y_j \\\\ & \\text{max}\\{c[i,j-1],c[i-1,j]\\} & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i \\neq y_j \\\\ \\end{cases} \\end{align*} \\] Computing the Length of an LCS \u00b6 We can easily write an exponential-time recursive algorithm based on the given recurrence. \\(\\Longrightarrow\\) Inefficient! How many distinct subproblems to solve? \\(\\Theta(mn)\\) Overlapping subproblems property: Many subproblems share the same sub-subproblems. e.g. Finding an LCS to \\(X_{m-1} \\& Y\\) and an LCS to \\(X \\& Y_{n-1}\\) has the sub-subproblem of finding an LCS to \\(X_{m-1} \\& Y_{n-1}\\) Therefore, we can use dynamic programming . Data Structures \u00b6 Let: \\(c[i, j]:\\) length of an LCS of \\(X_i\\) and \\(Y_j\\) \\(b[i, j]:\\) direction towards the table entry corresponding to the optimal subproblem solution chosen when computing \\(c[i, j]\\) . Used to simplify the construction of an optimal solution at the end. Maintain the following tables: \\(c[0 \\dots m, 0 \\dots n]\\) \\(b[1 \\dots m, 1 \\dots n]\\) Bottom-up Computation \u00b6 Reminder: \\[ \\begin{align*} c[i,j] = \\begin{cases} & 0 & \\text{if}& \\ i=0 \\ \\text{or} \\ j=0 \\\\ & c[i-1,j-1]+1 & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i=y_j \\\\ & \\text{max}\\{c[i,j-1],c[i-1,j]\\} & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i \\neq y_j \\\\ \\end{cases} \\end{align*} \\] How to choose the order in which we process \\(c[i, j]\\) values? The values for \\(c[i-1, j-1]\\) , \\(c[i, j-1]\\) , and \\(c[i-1,j]\\) must be computed before computing \\(c[i, j]\\) . Bottom-up Computation \u00b6 section{ font-size: 25px; } \\[ \\begin{align*} c[i,j] = \\begin{cases} & 0 & \\text{if}& \\ i=0 \\ \\text{or} \\ j=0 \\\\ & c[i-1,j-1]+1 & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i=y_j \\\\ & \\text{max}\\{c[i,j-1],c[i-1,j]\\} & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i \\neq y_j \\\\ \\end{cases} \\end{align*} \\] Need to process: \\(c[i, j]\\) after computing: \\(c[i-1, j-1]\\) , \\(c[i, j-1]\\) , \\(c[i-1,j]\\) Bottom-up Computation \u00b6 section{ font-size: 25px; } \\[ \\begin{align*} c[i,j] = \\begin{cases} & 0 & \\text{if}& \\ i=0 \\ \\text{or} \\ j=0 \\\\ & c[i-1,j-1]+1 & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i=y_j \\\\ & \\text{max}\\{c[i,j-1],c[i-1,j]\\} & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i \\neq y_j \\\\ \\end{cases} \\end{align*} \\] \\[ \\Downarrow \\] \\[ \\begin{align*} & \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ m \\\\ & \\quad \\text{for} \\ j \\leftarrow 1 \\ \\text{to} \\ n \\\\ & \\qquad \\dots \\\\ & \\qquad \\dots \\\\ & \\qquad c[i, j] = \\cdots \\end{align*} \\] section{ font-size: 25px; } Computing the Length of an LCS \u00b6 \\[ \\begin{align*} \\frac{\\text{Total Runtime} = \\Theta(mn)}{\\text{Total Space} = \\Theta(mn)} \\begin{cases} & LCS-LENGTH(X,Y) \\\\ & \\quad m \\leftarrow length[X]; n \\leftarrow length[Y] \\\\ & \\quad \\text{for} \\ i \\leftarrow 0 \\ \\text{to} \\ m \\ \\text{do} \\ c[i, 0] \\leftarrow 0 \\\\ & \\quad \\text{for} \\ j \\leftarrow 0 \\ \\text{to} \\ n \\ \\text{do} \\ c[0, j] \\leftarrow 0 \\\\ & \\quad \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ m \\ \\text{do} \\\\ & \\qquad \\text{for} \\ j \\leftarrow 1 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad \\quad \\text{if} \\ x_i = y_j \\ \\text{then} \\\\ & \\qquad \\quad \\quad c[i, j] \\leftarrow c[i-1, j-1]+1 \\\\ & \\qquad \\quad \\quad b[i, j] \\leftarrow \" \\nwarrow \" \\\\ & \\qquad \\quad \\text{else if} \\ c[i - 1, j] \\geq c[i, j-1] \\\\ & \\qquad \\quad \\quad c[i, j] \\leftarrow c[i-1, j] \\\\ & \\qquad \\quad \\quad b[i, j] \\leftarrow \"\\uparrow \" \\\\ & \\qquad \\quad \\text{else} \\\\ & \\qquad \\quad \\quad c[i, j] \\leftarrow c[i, j-1] \\\\ & \\qquad \\quad \\quad b[i, j] \\leftarrow \" \\leftarrow \" \\\\ \\end{cases} \\end{align*} \\] section{ font-size: 25px; } Computing the Length of an LCS-1 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-2 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-3 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-4 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-5 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-6 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-7 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-8 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-9 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-10 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-11 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-12 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; } Computing the Length of an LCS-13 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ Running-time = \\(O(mn)\\) since each table entry takes \\(O(1)\\) time to compute section{ font-size: 25px; } Computing the Length of an LCS-14 \u00b6 Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ Running-time = \\(O(mn)\\) since each table entry takes \\(O(1)\\) time to compute LCS of \\(X \\& Y = \\langle B, C, B, A \\rangle\\) Constructing an LCS \u00b6 The \\(b\\) table returned by LCS-LENGTH can be used to quickly construct an LCS of \\(X \\& Y\\) Begin at \\(b[m, n]\\) and trace through the table following arrows Whenever you encounter a \" \\(\\nwarrow\\) \" in entry \\(b[i, j]\\) it implies that \\(x_i = y_j\\) is an element of LCS The elements of LCS are encountered in reverse order Constructing an LCS \u00b6 section{ font-size: 21px; } The recursive procedure \\(\\text{PRINT-LCS}\\) prints out \\(\\text{LCS}\\) in proper order This procedure takes \\(O(m+n)\\) time since at least one of \\(i\\) and \\(j\\) is decremented in each stage of the recursion \\[ \\begin{align*} & \\text{PRINT-LCS}(b, X, i, j) \\\\ & \\quad \\text{if} \\ i = 0 \\ \\text{or} j = 0 \\ \\text{then} \\\\ & \\quad \\text{return} \\\\ & \\quad \\text{if} \\ b[i, j] = \" \\nwarrow \" \\ \\text{then} \\\\ & \\qquad \\text{PRINT-LCS}(b, X, i-1, j-1) \\\\ & \\qquad \\text{print} \\ x_i \\\\ & \\quad \\text{else if} \\ b[i, j] = \" \\uparrow \" \\ \\text{then} \\\\ & \\qquad \\text{PRINT-LCS}(b, X, i-1, j) \\\\ & \\quad \\text{else} \\\\ & \\qquad \\text{PRINT-LCS}(b, X, i, j-1) \\end{align*} \\] The initial invocation: \\(\\text{PRINT-LCS}(b, X, length[X], length[Y])\\) section{ font-size: 25px; } Do we really need the b table (back-pointers)? \u00b6 Question: From which neighbor did we expand to the highlighted cell? Answer: Upper-left neighbor,because \\(X[i] = Y[j]\\) . section{ font-size: 25px; } Do we really need the b table (back-pointers)? \u00b6 Question: From which neighbor did we expand to the highlighted cell? Answer: Left neighbor, because \\(X[i] \\neq Y[j]\\) and \\(LCS[i, j-1] > LCS[i-1, j]\\) . section{ font-size: 25px; } Do we really need the b table (back-pointers)? \u00b6 Question: From which neighbor did we expand to the highlighted cell? Answer: Upper neighbor,because \\(X[i] \\neq Y[j]\\) and \\(LCS[i, j-1] = LCS[i-1, j]\\) . (See pseudo-code to see how ties are handled.) section{ font-size: 25px; } Improving the Space Requirements \u00b6 We can eliminate the b table altogether each \\(c[i, j]\\) entry depends only on \\(3\\) other \\(c\\) table entries: \\(c[i-1, j-1]\\) , \\(c[i-1, j]\\) and \\(c[i, j-1]\\) Given the value of \\(c[i, j]\\) : We can determine in \\(O(1)\\) time which of these \\(3\\) values was used to compute \\(c[i, j]\\) without inspecting table \\(b\\) We save \\(\\Theta(mn)\\) space by this method However, space requirement is still \\(\\Theta(mn)\\) since we need \\(\\Theta(mn)\\) space for the \\(c\\) table anyway section{ font-size: 25px; } What if we store the last 2 rows only? \u00b6 To compute \\(c[i, j]\\) , we only need \\(c[i-1, j-1]\\) , \\(c[i-1, j]\\) ,and \\(c[i-1, j-1]\\) So, we can store only the last two rows. section{ font-size: 25px; } What if we store the last 2 rows only? \u00b6 To compute \\(c[i, j]\\) , we only need \\(c[i-1, j-1]\\) , \\(c[i-1, j]\\) , and \\(c[i-1, j-1]\\) So, we can store only the last two rows. section{ font-size: 25px; } What if we store the last 2 rows only? \u00b6 To compute \\(c[i, j]\\) , we only need \\(c[i-1, j-1]\\) , \\(c[i-1, j]\\) , and \\(c[i-1, j-1]\\) So, we can store only the last two rows. This reduces space complexity from \\(\\Theta(mn)\\) to \\(\\Theta(n)\\) . Is there a problem with this approach? section{ font-size: 25px; } What if we store the last 2 rows only? \u00b6 Is there a problem with this approach? We cannot construct the optimal solution because we cannot backtrace anymore. This approach works if we only need the length of an LCS, not the actual LCS. Problem 4 Optimal Binary Search Tree \u00b6 Reminder: Binary Search Tree (BST) \u00b6 Binary Search Tree Example \u00b6 Example: English-to-French translation Organize (English, French) word pairs in a BST Keyword: English word Satellite Data: French word We can search for an English word (node key) efficiently, and return the corresponding French word (satellite data). ASCII Table \u00b6 section{ font-size: 25px; } Binary Search Tree Example \u00b6 Suppose we know the frequency of each keyword in texts: $$ \\underset{5\\%}{\\underline{begin}}, \\underset{40\\%}{\\underline{do}}, \\underset{8\\%}{\\underline{else}}, \\underset{4\\%}{\\underline{end}}, \\underset{10\\%}{\\underline{if}}, \\underset{10\\%}{\\underline{then}}, \\underset{23\\%}{\\underline{while}}, $$ section{ font-size: 25px; } Cost of a Binary Search Tree \u00b6 Example: If we search for keyword \"while\" , we need to access \\(3\\) nodes. So, \\(23%\\) of the queries will have cost of \\(3\\) . \\[ \\begin{align*} \\text{Total Cost} &= \\sum \\limits_{i}^{}(\\text{depth}(i)+1)\\text{freq}(i) \\\\ &= 1 \\times 0.04 + 2 \\times 0.4 + \\\\ & 2 \\times 0.1 + 3 \\times 0.05 + \\\\ & 3 \\times 0.08 + 3 \\times 0.1 + \\\\ & 3 \\times 0.23 \\\\ &= 2.42 \\end{align*} \\] section{ font-size: 25px; } Cost of a Binary Search Tree \u00b6 Example: If we search for keyword \"while\" , we need to access \\(3\\) nodes. So, \\(23%\\) of the queries will have cost of \\(3\\) . \\[ \\begin{align*} \\text{Total Cost} &= \\sum \\limits_{i}^{}(\\text{depth}(i)+1)\\text{freq}(i) \\\\ &= 1 \\times 0.4 + 2 \\times 0.05 + 2 \\times 0.23 + \\\\ & 3 \\times 0.1 + 4 \\times 0.08 + \\\\ & 4 \\times 0.1 + 5 \\times 0.04 \\\\ &= 2.18 \\end{align*} \\] This is in fact an optimal BST. section{ font-size: 25px; } Optimal Binary Search Tree Problem \u00b6 Given: A collection of \\(n\\) keys \\(K_1 < K_2 < \\dots K_n\\) to be stored in a BST . The corresponding \\(p_i\\) values for \\(1 \\leq i \\leq n\\) \\(p_i\\) : probability of searching for key \\(K_i\\) Find: An optimal BST with minimum total cost: \\[ \\begin{align*} \\text{Total Cost} &= \\sum \\limits_{i}^{}(\\text{depth}(i)+1)\\text{freq}(i) \\end{align*} \\] Note: The BST will be static. Only search operations will be performed. No insert, no delete, etc. section{ font-size: 25px; } Cost of a Binary Search Tree \u00b6 Lemma 1 : Let \\(Tij\\) be a BST containing keys \\(K_i < K_{i+1} < \\dots < K_j\\) . Let \\(T_L\\) and \\(T_R\\) be the left and right subtrees of \\(T\\) . Then we have: \\[ \\begin{align*} \\text{cost}(T_{ij})=\\text{cost}(T_{L})+\\text{cost}(T_{R})+\\sum \\limits_{h=i}^{j}p_h \\end{align*} \\] Intuition: When we add the root node, the depth of each node in \\(T_L\\) and \\(T_R\\) increases by \\(1\\) . So, the cost of node \\(h\\) increases by \\(p_h\\) . In addition, the cost of root node \\(r\\) is \\(p_r\\) . That\u2019s why, we have the last term at the end of the formula above. section{ font-size: 25px; } Optimal Substructure Property \u00b6 Lemma 2: Optimal substructure property Consider an optimal BST \\(T_{ij}\\) for keys \\(K_i < K_{i+1} < \\dots < K_j\\) Let \\(K_m\\) be the key at the root of \\(T_{ij}\\) Then: \\(T_{i,m-1}\\) is an optimal BST for subproblem containing keys: \\(K_i < \\dots < K_{m-1}\\) \\(T_{m+1,j}\\) is an optimal BST for subproblem containing keys: \\(K_{m+1} < \\dots < K_j\\) \\[ \\begin{align*} \\text{cost}(T_{ij})=\\text{cost}(T_{i,m-1})+\\text{cost}(T_{m+1,j})+\\sum \\limits_{h=i}^{j}p_h \\end{align*} \\] section{ font-size: 25px; } Recursive Formulation \u00b6 Note: We don\u2019t know which root vertex leads to the minimum total cost. So, we need to try each vertex \\(m\\) , and choose the one with minimum total cost. \\(c[i, j]\\) : cost of an optimal BST \\(T_{ij}\\) for the subproblem \\(K_i < \\dots < K_j\\) \\[ \\begin{align*} & c[i,j] = \\begin{cases} & 0 & \\text{if} \\ i>j \\\\ & \\underset{i \\leq r \\leq j}{\\text{min}}\\{ c[i,r-1]+c[r+1,j]+P_{ij} \\} & \\text{otherwise} \\\\ \\end{cases} \\\\ & \\text{where} \\ P_{ij}= \\sum \\limits_{h=i}^{j}p_h \\end{align*} \\] section{ font-size: 25px; } Bottom-up computation \u00b6 \\[ \\begin{align*} & c[i,j] = \\begin{cases} & 0 & \\text{if} \\ i>j \\\\ & \\underset{i \\leq r \\leq j}{\\text{min}}\\{ c[i,r-1]+c[r+1,j]+P_{ij} \\} & \\text{otherwise} \\\\ \\end{cases} \\end{align*} \\] How to choose the order in which we process \\(c[i, j]\\) values? Before computing \\(c[i, j]\\) , we have to make sure that the values for \\(c[i, r-1]\\) and \\(c[r+1,j]\\) have been computed for all \\(r\\) . section{ font-size: 25px; } Bottom-up computation \u00b6 \\[ \\begin{align*} & c[i,j] = \\begin{cases} & 0 & \\text{if} \\ i>j \\\\ & \\underset{i \\leq r \\leq j}{\\text{min}}\\{ c[i,r-1]+c[r+1,j]+P_{ij} \\} & \\text{otherwise} \\\\ \\end{cases} \\end{align*} \\] \\(c[i,j]\\) must be processed after \\(c[i,r-1]\\) and \\(c[r+1,j]\\) section{ font-size: 25px; } Bottom-up computation \u00b6 \\[ \\begin{align*} & c[i,j] = \\begin{cases} & 0 & \\text{if} \\ i>j \\\\ & \\underset{i \\leq r \\leq j}{\\text{min}}\\{ c[i,r-1]+c[r+1,j]+P_{ij} \\} & \\text{otherwise} \\\\ \\end{cases} \\end{align*} \\] If the entries \\(c[i,j]\\) are computed in the shown order, then \\(c[i,r-1]\\) and \\(c[r+1,j]\\) values are guaranteed to be computed before \\(c[i,j]\\) . section{ font-size: 25px; } Computing the Optimal BST Cost \u00b6 \\[ \\begin{align*} & \\text{OPTIMAL-BST-COST} (p, n) \\\\ & \\quad \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad c[i, i-1] \\leftarrow 0 \\\\ & \\qquad c[i, i] \\leftarrow p[i] \\\\ & \\qquad R[i, j] \\leftarrow i \\\\ & \\quad PS[1] \\leftarrow p[1] \\Longleftarrow PS[i] \\rightarrow \\text{ prefix-sum } (i): \\text{Sum of all} \\ p[j] \\ \\text{values for} \\ j \\leq i \\\\ & \\quad \\text{for} \\ i \\leftarrow 2 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad PS[i] \\leftarrow p[i] + PS[i-1] \\Longleftarrow \\text{compute the prefix sum} \\\\ & \\quad \\text{for} \\ d \\leftarrow 1 \\ \\text{to} \\ n\u22121 \\ \\text{do} \\Longleftarrow \\text{BSTs with} \\ d+1 \\ \\text{consecutive keys} \\\\ & \\qquad \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ n \u2013 d \\ \\text{do} \\\\ & \\qquad \\quad j \\leftarrow i + d \\\\ & \\qquad \\quad c[i, j] \\leftarrow \\infty \\\\ & \\qquad \\quad \\text{for} \\ r \\leftarrow i \\ \\text{to} \\ j \\ \\text{do} \\\\ & \\qquad \\qquad q \\leftarrow min\\{c[i,r-1] + c[r+1, j]\\} + PS[j] \u2013 PS[i-1]\\} \\\\ & \\qquad \\qquad \\text{if} \\ q < c[i, j] \\ \\text{then} \\\\ & \\qquad \\qquad \\quad c[i, j] \\leftarrow q \\\\ & \\qquad \\qquad \\quad R[i, j] \\leftarrow r \\\\ & \\quad \\text{return} \\ c[1, n], R \\end{align*} \\] section{ font-size: 25px; } Note on Prefix Sum \u00b6 We need \\(P_{ij}\\) values for each \\(i, j (1 \u2264 i \u2264 n \\ \\text{and} \\ 1 \u2264 j \u2264 n)\\) , where: \\[ \\begin{align*} P_{ij} = \\sum \\limits_{h=i}^{j}p_h \\end{align*} \\] If we compute the summation directly for every \\((i, j)\\) pair, the runtime would be \\(\\Theta(n^3)\\) . Instead, we spend \\(O(n)\\) time in preprocessing to compute the prefix sum array PS . Then we can compute each \\(P_{ij}\\) in \\(O(1)\\) time using PS . section{ font-size: 25px; } Note on Prefix Sum \u00b6 In preprocessing, compute for each \\(i\\) : \\(PS[i]\\) : the sum of \\(p[j]\\) values for \\(1 \\leq j \\leq i\\) Then, we can compute \\(P_{ij}\\) in \\(O(1)\\) time as follows: \\(P_{ij} = PS[i] \u2013 PS[j-1]\\) Example: \\[ \\begin{align*} p &: \\overset{1}{0.05} \\ \\overset{2}{0.02} \\ \\overset{3}{0.06} \\ \\overset{4}{0.07} \\ \\overset{5}{0.20} \\ \\overset{6}{0.05} \\ \\overset{7}{0.08} \\ \\overset{8}{0.02} \\\\ PS &: \\overset{1}{0.05} \\ \\overset{2}{0.07} \\ \\overset{3}{0.13} \\ \\overset{4}{0.20} \\ \\overset{5}{0.40} \\ \\overset{6}{0.45} \\ \\overset{7}{0.53} \\ \\overset{8}{0.55} \\\\[10 pt] P_{27} &= PS[7] \u2013 PS[1] = 0.53 \u2013 0.05 = 0.48 \\\\ P_{36} &= PS[6] \u2013 PS[2] = 0.45 \u2013 0.07 = 0.38 \\end{align*} \\] REVIEW \u00b6 Overlapping Subproblems Property in Dynamic Programming \u00b6 Dynamic Programming is an algorithmic paradigm that solves a given complex problem by breaking it into subproblems and stores the results of subproblems to avoid computing the same results again. Overlapping Subproblems Property in Dynamic Programming \u00b6 Following are the two main properties of a problem that suggests that the given problem can be solved using Dynamic programming. Overlapping Subproblems Optimal Substructure Overlapping Subproblems \u00b6 Like Divide and Conquer, Dynamic Programming combines solutions to sub-problems. Dynamic Programming is mainly used when solutions of the same subproblems are needed again and again. In dynamic programming, computed solutions to subproblems are stored in a table so that these don\u2019t have to be recomputed. So Dynamic Programming is not useful when there are no common (overlapping) subproblems because there is no point storing the solutions if they are not needed again. Overlapping Subproblems \u00b6 For example, Binary Search doesn\u2019t have common subproblems. If we take an example of following recursive program for Fibonacci Numbers, there are many subproblems that are solved again and again. Simple Recursion \u00b6 \\(f(n) = f(n-1) + f(n-2)\\) C sample code: #include <stdio.h> // a simple recursive program to compute fibonacci numbers int fib ( int n ) { if ( n <= 1 ) return n ; else return fib ( n -1 ) + fib ( n -2 ); } int main () { int n = 5 ; printf ( \"Fibonacci number is %d \" , fib ( n )); return 0 ; } Simple Recursion \u00b6 Output Fibonacci number is 5 Simple Recursion \u00b6 \\(f(n) = f(n-1) + f(n-2)\\) /* a simple recursive program for Fibonacci numbers */ public class Fibonacci { public static void main ( String [] args ) { int n = Integer . parseInt ( args [ 0 ] ); System . out . println ( fib ( n )); } public static int fib ( int n ) { if ( n <= 1 ) return n ; return fib ( n - 1 ) + fib ( n - 2 ); } } Simple Recursion \u00b6 \\(f(n) = f(n-1) + f(n-2)\\) public class Fibonacci { public static void Main ( string [] args ) { int n = int . Parse ( args [ 0 ]); Console . WriteLine ( fib ( n )); } public static int fib ( int n ) { if ( n <= 1 ) return n ; return fib ( n - 1 ) + fib ( n - 2 ); } } Recursion tree for execution of fib(5) \u00b6 fib(5) / \\ fib(4) fib(3) / \\ / \\ fib(3) fib(2) fib(2) fib(1) / \\ / \\ / \\ fib(2) fib(1) fib(1) fib(0) fib(1) fib(0) / \\ fib(1) fib(0) We can see that the function fib(3) is being called 2 times. If we would have stored the value of fib(3) , then instead of computing it again, we could have reused the old stored value. Recursion tree for execution of fib(5) \u00b6 There are following two different ways to store the values so that these values can be reused: Memoization (Top Down) Tabulation (Bottom Up) Memoization (Top Down) \u00b6 The memoized program for a problem is similar to the recursive version with a small modification that looks into a lookup table before computing solutions. We initialize a lookup array with all initial values as NIL . Whenever we need the solution to a subproblem, we first look into the lookup table. If the precomputed value is there then we return that value, otherwise, we calculate the value and put the result in the lookup table so that it can be reused later. Memoization (Top Down) \u00b6 Following is the memoized version for the nth Fibonacci Number. C++ Version: /* C++ program for Memoized version for nth Fibonacci number */ #include <bits/stdc++.h> using namespace std ; #define NIL -1 #define MAX 100 int lookup [ MAX ]; Memoization (Top Down) \u00b6 C++ Version: /* Function to initialize NIL values in lookup table */ void _initialize () { int i ; for ( i = 0 ; i < MAX ; i ++ ) lookup [ i ] = NIL ; } Memoization (Top Down) \u00b6 C++ Version: /* function for nth Fibonacci number */ int fib ( int n ) { if ( lookup [ n ] == NIL ) { if ( n <= 1 ) lookup [ n ] = n ; else lookup [ n ] = fib ( n - 1 ) + fib ( n - 2 ); } return lookup [ n ]; } Memoization (Top Down) \u00b6 C++ Version: // Driver code int main () { int n = 40 ; _initialize (); cout << \"Fibonacci number is \" << fib ( n ); return 0 ; } Memoization (Top Down) \u00b6 Java Version: /* Java program for Memoized version */ public class Fibonacci { final int MAX = 100 ; final int NIL = - 1 ; int lookup [] = new int [ MAX ] ; /* Function to initialize NIL values in lookup table */ void _initialize () { for ( int i = 0 ; i < MAX ; i ++ ) lookup [ i ] = NIL ; } Memoization (Top Down) \u00b6 Java Version: /* function for nth Fibonacci number */ int fib ( int n ) { if ( lookup [ n ] == NIL ) { if ( n <= 1 ) lookup [ n ] = n ; else lookup [ n ] = fib ( n - 1 ) + fib ( n - 2 ); } return lookup [ n ] ; } Memoization (Top Down) \u00b6 Java Version: public static void main ( String [] args ) { Fibonacci f = new Fibonacci (); int n = 40 ; f . _initialize (); System . out . println ( \"Fibonacci number is\" + \" \" + f . fib ( n )); } } Memoization (Top Down) \u00b6 C# Version: // C# program for Memoized versionof nth Fibonacci number using System ; class FiboCalcMemoized { static int MAX = 100 ; static int NIL = - 1 ; static int [] lookup = new int [ MAX ]; /* Function to initialize NIL values in lookup table */ static void initialize () { for ( int i = 0 ; i < MAX ; i ++) lookup [ i ] = NIL ; } Memoization (Top Down) \u00b6 C# Version: /* function for nth Fibonacci number */ static int fib ( int n ) { if ( lookup [ n ] == NIL ) { if ( n <= 1 ) lookup [ n ] = n ; else lookup [ n ] = fib ( n - 1 ) + fib ( n - 2 ); } return lookup [ n ]; } Memoization (Top Down) \u00b6 C# Version: // Driver code public static void Main () { int n = 40 ; initialize (); Console . Write ( \"Fibonacci number is\" + \" \" + fib ( n )); } } Tabulation (Bottom Up) \u00b6 The tabulated program for a given problem builds a table in bottom-up fashion and returns the last entry from the table. For example, for the same Fibonacci number, we first calculate fib(0) then fib(1) then fib(2) then fib(3) , and so on. So literally, we are building the solutions of subproblems bottom-up. Tabulation (Bottom Up) \u00b6 C++ Version: /* C program for Tabulated version */ #include <stdio.h> int fib ( int n ) { int f [ n + 1 ]; int i ; f [ 0 ] = 0 ; f [ 1 ] = 1 ; for ( i = 2 ; i <= n ; i ++ ) f [ i ] = f [ i - 1 ] + f [ i - 2 ]; return f [ n ]; } Tabulation (Bottom Up) \u00b6 C++ Version: ... int main () { int n = 9 ; printf ( \"Fibonacci number is %d \" , fib ( n )); return 0 ; } Output: Fibonacci number is 34 Tabulation (Bottom Up) \u00b6 Java Version: /* Java program for Tabulated version */ public class Fibonacci { public static void main ( String [] args ) { int n = 9 ; System . out . println ( \"Fibonacci number is \" + fib ( n )); } Tabulation (Bottom Up) \u00b6 Java Version: /* Function to calculate nth Fibonacci number */ static int fib ( int n ) { int f [] = new int [ n + 1 ] ; f [ 0 ] = 0 ; f [ 1 ] = 1 ; for ( int i = 2 ; i <= n ; i ++ ) f [ i ] = f [ i - 1 ] + f [ i - 2 ] ; return f [ n ] ; } } Tabulation (Bottom Up) \u00b6 C# Version: // C# program for Tabulated version using System ; class Fibonacci { static int fib ( int n ) { int [] f = new int [ n + 1 ]; f [ 0 ] = 0 ; f [ 1 ] = 1 ; for ( int i = 2 ; i <= n ; i ++) f [ i ] = f [ i - 1 ] + f [ i - 2 ]; return f [ n ]; } public static void Main () { int n = 9 ; Console . Write ( \"Fibonacci number is\" + \" \" + fib ( n )); } } Both Tabulated and Memoized store the solutions of subproblems. In Memoized version, the table is filled on demand while in the Tabulated version, starting from the first entry, all entries are filled one by one. Unlike the Tabulated version, all entries of the lookup table are not necessarily filled in Memoized version. To see the optimization achieved by Memoized and Tabulated solutions over the basic Recursive solution, see the time taken by following runs for calculating the 40 th Fibonacci number: Recursive Solution: https://ide.geeksforgeeks.org/vHt6ly Memoized Solution: https://ide.geeksforgeeks.org/Z94jYR Tabulated Solution: https://ide.geeksforgeeks.org/12C5bP Optimal Substructure Property in Dynamic Programming \u00b6 A given problems has Optimal Substructure Property if optimal solution of the given problem can be obtained by using optimal solutions of its subproblems. For example, the Shortest Path problem has following optimal substructure property: If a node x lies in the shortest path from a source node u to destination node v then the shortest path from u to v is combination of shortest path from u to x and shortest path from x to v. The standard All Pair Shortest Path algorithm like Floyd\u2013Warshall and Single Source Shortest path algorithm for negative weight edges like Bellman\u2013Ford are typical examples of Dynamic Programming. Optimal Substructure Property in Dynamic Programming \u00b6 On the other hand, the Longest Path problem doesn\u2019t have the Optimal Substructure property. Here by Longest Path we mean longest simple path (path without cycle) between two nodes Optimal Substructure Property in Dynamic Programming \u00b6 There are two longest paths from q to t: q\u2192r\u2192t and q\u2192s\u2192t. Unlike shortest paths, these longest paths do not have the optimal substructure property. For example, the longest path q\u2192r\u2192t is not a combination of longest path from q to r and longest path from r to t, because the longest path from q to r is q\u2192s\u2192t\u2192r and the longest path from r to t is r\u2192q\u2192s\u2192t. Most Common Dynamic Programming Interview Questions \u00b6 Problem-1: Longest Increasing Subsequence \u00b6 Problem-1: Longest Increasing Subsequence Problem-1: Longest Increasing Subsequence \u00b6 Problem-2: Edit Distance \u00b6 Problem-2: Edit Distance Problem-2: Edit Distance (Recursive) \u00b6 Problem-2: Edit Distance (DP) \u00b6 https://www.coursera.org/learn/dna-sequencing Problem-2: Edit Distance (DP) \u00b6 Problem-2: Edit Distance (Other) \u00b6 Problem-3: Partition a set into two subsets such that the difference of subset sums is minimum \u00b6 Problem-3: Partition a set into two subsets such that the difference of subset sums is minimum Problem-4: Count number of ways to cover a distance \u00b6 Problem-4: Count number of ways to cover a distance Problem-5: Find the longest path in a matrix with given constraints \u00b6 Problem-5: Find the longest path in a matrix with given constraints Problem-6: Subset Sum Problem \u00b6 Problem-6: Subset Sum Problem Problem-7: Optimal Strategy for a Game \u00b6 Problem-7: Optimal Strategy for a Game Problem-8: 0-1 Knapsack Problem \u00b6 Problem-8: 0-1 Knapsack Problem Problem-9: Boolean Parenthesization Problem \u00b6 Problem-9: Boolean Parenthesization Problem Problem-10: Shortest Common Supersequence \u00b6 Problem-10: Shortest Common Supersequence Problem-11: Partition Problem \u00b6 Problem-11: Partition Problem Problem-12: Cutting a Rod \u00b6 Problem-12: Cutting a Rod Problem-13: Coin Change \u00b6 Problem-13: Coin Change Problem-14: Word Break Problem \u00b6 Problem-14: Word Break Problem Problem-15: Maximum Product Cutting \u00b6 Problem-15: Maximum Product Cutting Problem-16: Dice Throw \u00b6 Problem-16: Dice Throw Problem-16: Dice Throw \u00b6 Problem-17: Box Stacking Problem \u00b6 Problem-17: Box Stacking Problem Problem-18: Egg Dropping Puzzle \u00b6 Problem-18: Egg Dropping Puzzle References \u00b6 Introduction to Algorithms, Third Edition | The MIT Press CLRS Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) \\(-End-Of-Week-6-Course-Module-\\)","title":"Week-6 (Matrix Chain Order / LCS)"},{"location":"tr/week-6/ce100-week-6-lcs/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"tr/week-6/ce100-week-6-lcs/#week-6-matrix-chain-order-lcs","text":"","title":"Week-6 (Matrix Chain Order / LCS)"},{"location":"tr/week-6/ce100-week-6-lcs/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"tr/week-6/ce100-week-6-lcs/#matrix-chain-order-longest-common-subsequence","text":"","title":"Matrix Chain Order / Longest Common Subsequence"},{"location":"tr/week-6/ce100-week-6-lcs/#outline","text":"Elements of Dynamic Programming Optimal Substructure Overlapping Subproblems Recursive Matrix Chain Order Memoization Top-Down Approach RMC MemoizedMatrixChain LookupC Dynamic Programming vs Memoization Summary Dynamic Programming Problem-2 : Longest Common Subsequence Definitions LCS Problem Notations Optimal Substructure of LCS Proof Case-1 Proof Case-2 Proof Case-3 A recursive solution to subproblems (inefficient) Computing the length of and LCS LCS Data Structure for DP Bottom-Up Computation Constructing and LCS PRINT-LCS Back-pointer space optimization for LCS length Most Common Dynamic Programming Interview Questions","title":"Outline"},{"location":"tr/week-6/ce100-week-6-lcs/#elements-of-dynamic-programming","text":"When should we look for a DP solution to an optimization problem? Two key ingredients for the problem Optimal substructure Overlapping subproblems","title":"Elements of Dynamic Programming"},{"location":"tr/week-6/ce100-week-6-lcs/#dp-hallmark-1","text":"Optimal Substructure A problem exhibits optimal substructure if an optimal solution to a problem contains within it optimal solutions to subproblems Example: matrix-chain-multiplication Optimal parenthesization of \\(A_1 A_2 \\dots A_n\\) that splits the product between \\(A_k\\) and \\(A_{k+1}\\) , contains within it optimal soln\u2019s to the problems of parenthesizing \\(A_1A_2 \\dots A_k\\) and \\(A_{k+1} A_{k+2} \\dots A_n\\)","title":"DP Hallmark #1"},{"location":"tr/week-6/ce100-week-6-lcs/#optimal-substructure","text":"Finding a suitable space of subproblems Iterate on subproblem instances Example: matrix-chain-multiplication Iterate and look at the structure of optimal soln\u2019s to subproblems, sub-subproblems, and so forth Discover that all subproblems consists of subchains of \\(\\langle A_1, A_2, \\dots , A_n \\rangle\\) Thus, the set of chains of the form \\(\\langle A_i,A_{i+1}, \\dots , A_j \\rangle\\) for \\(1 \\leq i \\leq j \\leq n\\) Makes a natural and reasonable space of subproblems","title":"Optimal Substructure"},{"location":"tr/week-6/ce100-week-6-lcs/#dp-hallmark-2","text":"Overlapping Subproblems Total number of distinct subproblems should be polynomial in the input size When a recursive algorithm revisits the same problem over and over again , We say that the optimization problem has overlapping subproblems","title":"DP Hallmark #2"},{"location":"tr/week-6/ce100-week-6-lcs/#overlapping-subproblems","text":"DP algorithms typically take advantage of overlapping subproblems by solving each problem once then storing the solutions in a table where it can be looked up when needed using constant time per lookup section{ font-size: 25px; }","title":"Overlapping Subproblems"},{"location":"tr/week-6/ce100-week-6-lcs/#overlapping-subproblems_1","text":"Recursive matrix-chain order \\[ \\begin{align*} & \\text{RMC}(p, i, j) \\{ \\\\[5 pt] & \\quad \\text{if} \\ i = j \\ \\text{then} \\\\ & \\qquad \\text{return} \\ 0 \\\\[5 pt] & \\quad m[i, j] \\leftarrow \\infty \\\\[5 pt] & \\quad \\text{for} \\ k \\leftarrow i \\text{to} \\ j - 1 \\ \\text{do} \\\\ & \\qquad q \\leftarrow \\text{RMC}(p, i, k) + \\text{RMC}(p, k+1, j) + p_{i-1} p_k p_j \\\\[5 pt] & \\quad if \\ q < m[i, j] \\ \\text{then} \\\\ & \\qquad m[i, j] \\leftarrow q \\\\[5 pt] & \\quad \\text{return} \\ m[i, j] \\ \\} \\end{align*} \\]","title":"Overlapping Subproblems"},{"location":"tr/week-6/ce100-week-6-lcs/#direct-recursion-inefficient","text":"Recursion tree for \\(RMC(p,1,4)\\) Nodes are labeled with \\(i\\) and \\(j\\) values section{ font-size: 25px; }","title":"Direct Recursion: Inefficient!"},{"location":"tr/week-6/ce100-week-6-lcs/#running-time-of-rmc","text":"\\(T(1) \\geq 1\\) \\(T(n) \\geq 1 + \\sum \\limits_{k=1}^{n-1} (T(k)+T(n-k)+1)\\ \\text{for} \\ n>1\\) For \\(i =1,2, \\dots ,n\\) each term \\(T(i)\\) appears twice Once as \\(T(k)\\) , and once as \\(T(n-k)\\) Collect \\(n-1,\\) \\(1\\) \u2019s in the summation together with the front \\(1\\) \\[ \\begin{align*} T(n) \\geq 2 \\sum \\limits_{i=1}^{n-1}T(i)+n \\end{align*} \\] Prove that \\(T(n)= \\Omega(2n)\\) using the substitution method section{ font-size: 25px; }","title":"Running Time of RMC"},{"location":"tr/week-6/ce100-week-6-lcs/#running-time-of-rmc-prove-that-tn-omega2n","text":"Try to show that \\(T(n) \\geq 2^{n-1}\\) ( by substitution ) Base case: \\(T(1) \\geq 1 = 2^0 = 2^{1-1}\\) for \\(n=1\\) Ind. Hyp.: \\[ \\begin{align*} T(i) & \\geq 2^{i-1} \\ \\text{for all} \\ i=1, 2, \\dots, n-1 \\ \\text{and} \\ n \\geq 2 \\\\ T(n) & \\geq 2 \\sum \\limits_{i=1}^{n-1}2^{i-1} + n \\\\[15 pt] & = 2 \\sum \\limits_{i=1}^{n-1} 2^{i-1} + n \\\\ & = 2(2^{n-1}-1) + n \\\\ & = 2^{n-1} + (2^{n-1} - 2 + n) \\\\ & \\Rightarrow T(n) \\geq 2^{n-1} \\ \\text{ Q.E.D.} \\end{align*} \\] section{ font-size: 25px; }","title":"Running Time of RMC: Prove that \\(T(n)= \\Omega(2n)\\)"},{"location":"tr/week-6/ce100-week-6-lcs/#running-time-of-rmc-tn-geq-2n-1","text":"Whenever a recursion tree for the natural recursive solution to a problem contains the same subproblem repeatedly the total number of different subproblems is small it is a good idea to see if \\(DP (Dynamic \\ Programming)\\) can be applied","title":"Running Time of RMC: \\(T(n) \\geq 2^{n-1}\\)"},{"location":"tr/week-6/ce100-week-6-lcs/#memoization","text":"Offers the efficiency of the usual \\(DP\\) approach while maintaining top-down strategy Idea is to memoize the natural, but inefficient, recursive algorithm","title":"Memoization"},{"location":"tr/week-6/ce100-week-6-lcs/#memoized-recursive-algorithm","text":"Maintains an entry in a table for the soln to each subproblem Each table entry contains a special value to indicate that the entry has yet to be filled in When the subproblem is first encountered its solution is computed and then stored in the table Each subsequent time that the subproblem encountered the value stored in the table is simply looked up and returned","title":"Memoized Recursive Algorithm"},{"location":"tr/week-6/ce100-week-6-lcs/#memoized-recursive-matrix-chain-order","text":"Shaded subtrees are looked-up rather than recomputing \\[ \\begin{align*} \\begin{aligned} & \\text{MemoizedMatrixChain(p)} \\\\ & \\quad n \\leftarrow length[p] - 1 \\\\ & \\quad \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad \\text{for} \\ j \\leftarrow 1 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad \\quad m[i, j] \\leftarrow \\infty \\\\ & \\quad \\text{return} \\ \\text{LookupC}(p, 1, n) \\Longrightarrow \\end{aligned} \\begin{aligned} & \\Longrightarrow \\text{LookupC}(p, i, j) \\\\ & \\quad \\text{if} \\ m[i, j] = \\infty \\ \\text{then} \\\\ & \\qquad \\text{if} \\ i = j \\ \\text{then} \\\\ & \\qquad \\quad m[i, j] \\leftarrow 0 \\\\ & \\qquad \\text{else} \\\\ & \\qquad \\quad \\text{for} \\ k \\leftarrow i \\ \\text{to} \\ j-1 \\ \\text{do} \\\\ & \\qquad \\quad \\quad q \\leftarrow \\text{LookupC}(p, i, k) + \\text{LookupC}(p, k+1, j) + p_{i-1} p_k p_j \\\\ & \\qquad \\quad \\quad \\text{if} \\ q < m[i, j] \\ \\text{then} \\\\ & \\qquad \\quad \\quad \\quad m[i, j] \\leftarrow q \\\\ & \\quad \\text{return} \\ m[i, j] \\end{aligned} \\end{align*} \\]","title":"Memoized Recursive Matrix-chain Order"},{"location":"tr/week-6/ce100-week-6-lcs/#memoized-recursive-algorithm_1","text":"The approach assumes that The set of all possible subproblem parameters are known The relation between the table positions and subproblems is established Another approach is to memoize by using hashing with subproblem parameters as key section{ font-size: 25px; }","title":"Memoized Recursive Algorithm"},{"location":"tr/week-6/ce100-week-6-lcs/#dynamic-programming-vs-memoization-summary-1","text":"Matrix-chain multiplication can be solved in \\(O(n^3)\\) time by either a top-down memoized recursive algorithm or a bottom-up dynamic programming algorithm Both methods exploit the overlapping subproblems property There are only \\(\\Theta(n^2)\\) different subproblems in total Both methods compute the soln to each problem once Without memoization the natural recursive algorithm runs in exponential time since subproblems are solved repeatedly section{ font-size: 25px; }","title":"Dynamic Programming vs Memoization Summary (1)"},{"location":"tr/week-6/ce100-week-6-lcs/#dynamic-programming-vs-memoization-summary-2","text":"In general practice If all subproblems must be solved at once a bottom-up DP algorithm always outperforms a top-down memoized algorithm by a constant factor because, bottom-up DP algorithm Has no overhead for recursion Less overhead for maintaining the table DP: Regular pattern of table accesses can be exploited to reduce the time and/or space requirements even further Memoized: If some problems need not be solved at all, it has the advantage of avoiding solutions to those subproblems","title":"Dynamic Programming vs Memoization Summary (2)"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-3-longest-common-subsequence","text":"Definitions A subsequence of a given sequence is just the given sequence with some elements (possibly none) left out Example: \\(X = \\langle A, B, C, B, D, A, B \\rangle\\) \\(Z = \\langle B, C, D, B \\rangle\\) \\(Z\\) is a subsequence of \\(X\\)","title":"Problem 3: Longest Common Subsequence"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-3-longest-common-subsequence_1","text":"Definitions Formal definition: Given a sequence \\(X = \\langle x_1, x_2, \\dots , x_m \\rangle\\) , sequence \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) is a subsequence of \\(X\\) if \\(\\exists\\) a strictly increasing sequence \\(\\langle i_1, i_2,\\dots, i_k \\rangle\\) of indices of \\(X\\) such that \\(x_{i_j} = z_j\\) for all \\(j = 1, 2, \u2026, k\\) , where \\(1 \\leq k \\leq m\\) Example: \\(Z = \\langle B,C,D,B \\rangle\\) is a subsequence of \\(X = \\langle A,B,C,B,D,A,B \\rangle\\) with the index sequence \\(\\langle i_1, i_2, i_3, i_4 \\rangle = \\langle 2, 3, 5, 7 \\rangle\\) section{ font-size: 25px; }","title":"Problem 3: Longest Common Subsequence"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-3-longest-common-subsequence_2","text":"Definitions If \\(Z\\) is a subsequence of both \\(X\\) and \\(Y\\) , we denote \\(Z\\) as a common subsequence of \\(X\\) and \\(Y\\) . Example: \\[ \\begin{align*} X &= \\langle A,B^*,C^*,B,D,A^*,B \\rangle \\\\ Y &= \\langle B^*,D,C^*,A^*,B,A \\rangle \\end{align*} \\] \\(Z_1 = \\langle B^*, C^*, A^* \\rangle\\) is a common subsequence ( of length 3 ) of \\(X\\) and \\(Y\\) . Two longest common subsequence (LCSs) of \\(X\\) and \\(Y\\) ? \\(Z2 = \\langle B, C, B, A \\rangle\\) of length \\(4\\) \\(Z3 = \\langle B, D, A, B \\rangle\\) of length \\(4\\) The optimal solution value = 4","title":"Problem 3: Longest Common Subsequence"},{"location":"tr/week-6/ce100-week-6-lcs/#longest-common-subsequence-lcs-problem","text":"LCS problem: Given two sequences \\(X = \\langle x_1, x_2, \\dots, x_m \\rangle\\) and \\(Y = \\langle y_1, y_2, \\dots , y_n \\rangle\\) , find the LCS of \\(X \\& Y\\) Brute force approach: Enumerate all subsequences of \\(X\\) Check if each subsequence is also a subsequence of \\(Y\\) Keep track of the LCS What is the complexity? There are \\(2^m\\) subsequences of \\(X\\) Exponential runtime","title":"Longest Common Subsequence (LCS) Problem"},{"location":"tr/week-6/ce100-week-6-lcs/#notation","text":"Notation: Let \\(X_i\\) denote the \\(i^{th}\\) prefix of \\(X\\) i.e. \\(X_i = \\langle x_1, x_2, \\dots, x_i \\rangle\\) Example: \\[ \\begin{align*} X &= \\langle A, B, C, B, D, A, B \\rangle \\\\[10 pt] X_4 &= \\langle A, B, C, B \\rangle \\\\ X_0 &= \\langle \\rangle \\end{align*} \\]","title":"Notation"},{"location":"tr/week-6/ce100-week-6-lcs/#optimal-substructure-of-an-lcs","text":"Let \\(X = <x1, x2, \u2026, xm>\\) and \\(Y = \\langle y_1, y_2, \\dots, y_n \\rangle\\) are given Let \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\) Question 1: If \\(x_m = y_n\\) , how to define the optimal substructure? We must have \\(z_k = x_m = y_n\\) and \\(Z_{k-1} = \\text{LCS}(X_{m-1}, Y_{n-1})\\)","title":"Optimal Substructure of an LCS"},{"location":"tr/week-6/ce100-week-6-lcs/#optimal-substructure-of-an-lcs_1","text":"Let \\(X = <x1, x2, \u2026, xm>\\) and \\(Y = \\langle y_1, y_2, \\dots, y_n \\rangle\\) are given Let \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\) Question 2: If \\(x_m \\neq y_n \\ \\text{and} \\ z_k \\neq x_m\\) , how to define the optimal substructure? We must have \\(Z = \\text{LCS}(X_{m-1}, Y)\\)","title":"Optimal Substructure of an LCS"},{"location":"tr/week-6/ce100-week-6-lcs/#optimal-substructure-of-an-lcs_2","text":"Let \\(X = <x1, x2, \u2026, xm>\\) and \\(Y = \\langle y_1, y_2, \\dots, y_n \\rangle\\) are given Let \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\) Question 3: If \\(x_m \\neq y_n \\ \\text{and} \\ z_k \\neq y_n\\) , how to define the optimal substructure? We must have \\(Z = \\text{LCS}(X, Y_{n-1})\\)","title":"Optimal Substructure of an LCS"},{"location":"tr/week-6/ce100-week-6-lcs/#theorem-optimal-substructure-of-an-lcs","text":"Let \\(X = \\langle x_1, x_2, \\dots, x_m \\rangle\\) and Y = are given Let \\(Z = \\langle z_1, z_2, \\dots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\) Theorem: Optimal substructure of an LCS: If \\(x_m = y_n\\) then \\(z_k = x_m = y_n\\) and \\(Z_{k-1}\\) is an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) If \\(x_m \\neq y_n\\) and \\(z_k \\neq x_m\\) then \\(Z\\) is an LCS of \\(X_{m-1}\\) and \\(Y\\) If \\(x_m \\neq y_n\\) and \\(z_k \\neq y_n\\) then \\(Z\\) is an LCS of \\(X\\) and \\(Y_{n-1}\\)","title":"Theorem: Optimal Substructure of an LCS"},{"location":"tr/week-6/ce100-week-6-lcs/#optimal-substructure-theorem-case-1","text":"If \\(x_m = y_n\\) then \\(z_k = x_m = y_n\\) and \\(Z_{k-1}\\) is an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\)","title":"Optimal Substructure Theorem (case 1)"},{"location":"tr/week-6/ce100-week-6-lcs/#optimal-substructure-theorem-case-2","text":"If \\(x_m \\neq y_n\\) and \\(z_k \\neq x_m\\) then \\(Z\\) is an LCS of \\(X_{m-1}\\) and \\(Y\\)","title":"Optimal Substructure Theorem (case 2)"},{"location":"tr/week-6/ce100-week-6-lcs/#optimal-substructure-theorem-case-3","text":"If \\(x_m \\neq y_n\\) and \\(z_k \\neq y_n\\) then \\(Z\\) is an LCS of \\(X\\) and \\(Y_{n-1}\\)","title":"Optimal Substructure Theorem (case 3)"},{"location":"tr/week-6/ce100-week-6-lcs/#proof-of-optimal-substructure-theorem-case-1","text":"If \\(x_m = y_n\\) then \\(z_k = x_m = y_n\\) and \\(Z_{k-1}\\) is an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) Proof: If \\(z_k \\neq x_m = y_n\\) then we can append \\(x_m = y_n\\) to \\(Z\\) to obtain a common subsequence of length \\(k+1 \\Longrightarrow\\) contradiction Thus, we must have \\(z_k = x_m = y_n\\) Hence, the prefix \\(Z_{k-1}\\) is a length-( \\(k-1\\) ) CS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) We have to show that \\(Z_{k-1}\\) is in fact an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\) Proof by contradiction: Assume that \\(\\exists\\) a CS \\(W\\) of \\(X_{m-1}\\) and \\(Y_{n-1}\\) with \\(|W| = k\\) Then appending \\(x_m = y_n\\) to \\(W\\) produces a CS of length \\(k+1\\)","title":"Proof of Optimal Substructure Theorem (case 1)"},{"location":"tr/week-6/ce100-week-6-lcs/#proof-of-optimal-substructure-theorem-case-2","text":"If \\(x_m \\neq y_n\\) and \\(z_k \\neq x_m\\) then \\(Z\\) is an LCS of \\(X_{m-1}\\) and \\(Y\\) Proof : If \\(z_k \\neq x_m\\) then \\(Z\\) is a CS of \\(X_{m-1}\\) and \\(Y_n\\) We have to show that \\(Z\\) is in fact an LCS of \\(X_{m-1}\\) and \\(Y_n\\) (Proof by contradiction) Assume that \\(\\exists\\) a CS \\(W\\) of \\(X_{m-1}\\) and \\(Y_n\\) with \\(|W| > k\\) Then \\(W\\) would also be a CS of \\(X\\) and \\(Y\\) Contradiction to the assumption that \\(Z\\) is an LCS of \\(X\\) and \\(Y\\) with \\(|Z| = k\\) Case 3: Dual of the proof for (case 2)","title":"Proof of Optimal Substructure Theorem (case 2)"},{"location":"tr/week-6/ce100-week-6-lcs/#a-recursive-solution-to-subproblems","text":"Theorem implies that there are one or two subproblems to examine if \\(x_m = y_n\\) then we must solve the subproblem of finding an LCS of \\(X_{m-1} \\& Y_{n-1}\\) appending \\(x_m = y_n\\) to this LCS yields an LCS of \\(X \\& Y\\) else we must solve two subproblems finding an LCS of \\(X_{m-1} \\& Y\\) finding an LCS of \\(X \\& Y_{n-1}\\) longer of these two LCS s is an LCS of \\(X \\& Y\\) endif section{ font-size: 25px; }","title":"A Recursive Solution to Subproblems"},{"location":"tr/week-6/ce100-week-6-lcs/#recursive-algorithm-inefficient","text":"\\[ \\begin{align*} & \\text{LCS}(X, Y) \\ \\{ \\\\ & \\quad m \\leftarrow length[X] \\\\ & \\quad n \\leftarrow length[Y] \\\\ & \\quad \\text{if} \\ x_m = y_n \\ \\text{then} \\\\ & \\qquad Z \\leftarrow \\text{LCS}(X_{m-1}, Y_{n-1}) \\triangleright \\text{solve one subproblem} \\\\ & \\qquad \\text{return} \\ \\langle Z, x_m = y_n \\rangle \\triangleright \\text{append} \\ x_m = y_n \\ \\text{to} \\ Z \\\\ & \\quad else \\\\ & \\qquad Z^{'} \\leftarrow \\text{LCS}(X_{m-1}, Y) \\triangleright \\text{solve two subproblems} \\\\ & \\qquad Z^{''} \\leftarrow \\text{LCS}(X, Y_{n-1}) \\\\ & \\qquad \\text{return longer of} \\ Z^{'} \\ \\text{and} \\ Z^{''} \\\\ & \\} \\end{align*} \\]","title":"Recursive Algorithm (Inefficient)"},{"location":"tr/week-6/ce100-week-6-lcs/#a-recursive-solution","text":"\\(c[i, j]:\\) length of an LCS of \\(X_i\\) and \\(Y_j\\) \\[ \\begin{align*} c[i,j] = \\begin{cases} & 0 & \\text{if}& \\ i=0 \\ \\text{or} \\ j=0 \\\\ & c[i-1,j-1]+1 & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i=y_j \\\\ & \\text{max}\\{c[i,j-1],c[i-1,j]\\} & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i \\neq y_j \\\\ \\end{cases} \\end{align*} \\]","title":"A Recursive Solution"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs","text":"We can easily write an exponential-time recursive algorithm based on the given recurrence. \\(\\Longrightarrow\\) Inefficient! How many distinct subproblems to solve? \\(\\Theta(mn)\\) Overlapping subproblems property: Many subproblems share the same sub-subproblems. e.g. Finding an LCS to \\(X_{m-1} \\& Y\\) and an LCS to \\(X \\& Y_{n-1}\\) has the sub-subproblem of finding an LCS to \\(X_{m-1} \\& Y_{n-1}\\) Therefore, we can use dynamic programming .","title":"Computing the Length of an LCS"},{"location":"tr/week-6/ce100-week-6-lcs/#data-structures","text":"Let: \\(c[i, j]:\\) length of an LCS of \\(X_i\\) and \\(Y_j\\) \\(b[i, j]:\\) direction towards the table entry corresponding to the optimal subproblem solution chosen when computing \\(c[i, j]\\) . Used to simplify the construction of an optimal solution at the end. Maintain the following tables: \\(c[0 \\dots m, 0 \\dots n]\\) \\(b[1 \\dots m, 1 \\dots n]\\)","title":"Data Structures"},{"location":"tr/week-6/ce100-week-6-lcs/#bottom-up-computation","text":"Reminder: \\[ \\begin{align*} c[i,j] = \\begin{cases} & 0 & \\text{if}& \\ i=0 \\ \\text{or} \\ j=0 \\\\ & c[i-1,j-1]+1 & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i=y_j \\\\ & \\text{max}\\{c[i,j-1],c[i-1,j]\\} & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i \\neq y_j \\\\ \\end{cases} \\end{align*} \\] How to choose the order in which we process \\(c[i, j]\\) values? The values for \\(c[i-1, j-1]\\) , \\(c[i, j-1]\\) , and \\(c[i-1,j]\\) must be computed before computing \\(c[i, j]\\) .","title":"Bottom-up Computation"},{"location":"tr/week-6/ce100-week-6-lcs/#bottom-up-computation_1","text":"section{ font-size: 25px; } \\[ \\begin{align*} c[i,j] = \\begin{cases} & 0 & \\text{if}& \\ i=0 \\ \\text{or} \\ j=0 \\\\ & c[i-1,j-1]+1 & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i=y_j \\\\ & \\text{max}\\{c[i,j-1],c[i-1,j]\\} & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i \\neq y_j \\\\ \\end{cases} \\end{align*} \\] Need to process: \\(c[i, j]\\) after computing: \\(c[i-1, j-1]\\) , \\(c[i, j-1]\\) , \\(c[i-1,j]\\)","title":"Bottom-up Computation"},{"location":"tr/week-6/ce100-week-6-lcs/#bottom-up-computation_2","text":"section{ font-size: 25px; } \\[ \\begin{align*} c[i,j] = \\begin{cases} & 0 & \\text{if}& \\ i=0 \\ \\text{or} \\ j=0 \\\\ & c[i-1,j-1]+1 & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i=y_j \\\\ & \\text{max}\\{c[i,j-1],c[i-1,j]\\} & \\text{if}& \\ i,j>0 \\ \\text{and} \\ x_i \\neq y_j \\\\ \\end{cases} \\end{align*} \\] \\[ \\Downarrow \\] \\[ \\begin{align*} & \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ m \\\\ & \\quad \\text{for} \\ j \\leftarrow 1 \\ \\text{to} \\ n \\\\ & \\qquad \\dots \\\\ & \\qquad \\dots \\\\ & \\qquad c[i, j] = \\cdots \\end{align*} \\] section{ font-size: 25px; }","title":"Bottom-up Computation"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs_1","text":"\\[ \\begin{align*} \\frac{\\text{Total Runtime} = \\Theta(mn)}{\\text{Total Space} = \\Theta(mn)} \\begin{cases} & LCS-LENGTH(X,Y) \\\\ & \\quad m \\leftarrow length[X]; n \\leftarrow length[Y] \\\\ & \\quad \\text{for} \\ i \\leftarrow 0 \\ \\text{to} \\ m \\ \\text{do} \\ c[i, 0] \\leftarrow 0 \\\\ & \\quad \\text{for} \\ j \\leftarrow 0 \\ \\text{to} \\ n \\ \\text{do} \\ c[0, j] \\leftarrow 0 \\\\ & \\quad \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ m \\ \\text{do} \\\\ & \\qquad \\text{for} \\ j \\leftarrow 1 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad \\quad \\text{if} \\ x_i = y_j \\ \\text{then} \\\\ & \\qquad \\quad \\quad c[i, j] \\leftarrow c[i-1, j-1]+1 \\\\ & \\qquad \\quad \\quad b[i, j] \\leftarrow \" \\nwarrow \" \\\\ & \\qquad \\quad \\text{else if} \\ c[i - 1, j] \\geq c[i, j-1] \\\\ & \\qquad \\quad \\quad c[i, j] \\leftarrow c[i-1, j] \\\\ & \\qquad \\quad \\quad b[i, j] \\leftarrow \"\\uparrow \" \\\\ & \\qquad \\quad \\text{else} \\\\ & \\qquad \\quad \\quad c[i, j] \\leftarrow c[i, j-1] \\\\ & \\qquad \\quad \\quad b[i, j] \\leftarrow \" \\leftarrow \" \\\\ \\end{cases} \\end{align*} \\] section{ font-size: 25px; }","title":"Computing the Length of an LCS"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-1","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-1"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-2","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-2"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-3","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-3"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-4","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-4"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-5","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-5"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-6","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-6"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-7","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-7"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-8","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-8"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-9","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-9"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-10","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-10"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-11","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-11"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-12","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ section{ font-size: 25px; }","title":"Computing the Length of an LCS-12"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-13","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ Running-time = \\(O(mn)\\) since each table entry takes \\(O(1)\\) time to compute section{ font-size: 25px; }","title":"Computing the Length of an LCS-13"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-length-of-an-lcs-14","text":"Operation of LCS-LENGTH on the sequences $$ \\begin{align*} X &= \\langle \\overset{1}{A}, \\overset{2}{B}, \\overset{3}{C}, \\overset{4}{B}, \\overset{5}{D}, \\overset{6}{A}, \\overset{7}{B} \\rangle \\ Y &= \\langle \\overset{1}{B}, \\overset{2}{D}, \\overset{3}{C}, \\overset{4}{A}, \\overset{5}{B}, \\overset{6}{A} \\rangle \\end{align*} $$ Running-time = \\(O(mn)\\) since each table entry takes \\(O(1)\\) time to compute LCS of \\(X \\& Y = \\langle B, C, B, A \\rangle\\)","title":"Computing the Length of an LCS-14"},{"location":"tr/week-6/ce100-week-6-lcs/#constructing-an-lcs","text":"The \\(b\\) table returned by LCS-LENGTH can be used to quickly construct an LCS of \\(X \\& Y\\) Begin at \\(b[m, n]\\) and trace through the table following arrows Whenever you encounter a \" \\(\\nwarrow\\) \" in entry \\(b[i, j]\\) it implies that \\(x_i = y_j\\) is an element of LCS The elements of LCS are encountered in reverse order","title":"Constructing an LCS"},{"location":"tr/week-6/ce100-week-6-lcs/#constructing-an-lcs_1","text":"section{ font-size: 21px; } The recursive procedure \\(\\text{PRINT-LCS}\\) prints out \\(\\text{LCS}\\) in proper order This procedure takes \\(O(m+n)\\) time since at least one of \\(i\\) and \\(j\\) is decremented in each stage of the recursion \\[ \\begin{align*} & \\text{PRINT-LCS}(b, X, i, j) \\\\ & \\quad \\text{if} \\ i = 0 \\ \\text{or} j = 0 \\ \\text{then} \\\\ & \\quad \\text{return} \\\\ & \\quad \\text{if} \\ b[i, j] = \" \\nwarrow \" \\ \\text{then} \\\\ & \\qquad \\text{PRINT-LCS}(b, X, i-1, j-1) \\\\ & \\qquad \\text{print} \\ x_i \\\\ & \\quad \\text{else if} \\ b[i, j] = \" \\uparrow \" \\ \\text{then} \\\\ & \\qquad \\text{PRINT-LCS}(b, X, i-1, j) \\\\ & \\quad \\text{else} \\\\ & \\qquad \\text{PRINT-LCS}(b, X, i, j-1) \\end{align*} \\] The initial invocation: \\(\\text{PRINT-LCS}(b, X, length[X], length[Y])\\) section{ font-size: 25px; }","title":"Constructing an LCS"},{"location":"tr/week-6/ce100-week-6-lcs/#do-we-really-need-the-b-table-back-pointers","text":"Question: From which neighbor did we expand to the highlighted cell? Answer: Upper-left neighbor,because \\(X[i] = Y[j]\\) . section{ font-size: 25px; }","title":"Do we really need the b table (back-pointers)?"},{"location":"tr/week-6/ce100-week-6-lcs/#do-we-really-need-the-b-table-back-pointers_1","text":"Question: From which neighbor did we expand to the highlighted cell? Answer: Left neighbor, because \\(X[i] \\neq Y[j]\\) and \\(LCS[i, j-1] > LCS[i-1, j]\\) . section{ font-size: 25px; }","title":"Do we really need the b table (back-pointers)?"},{"location":"tr/week-6/ce100-week-6-lcs/#do-we-really-need-the-b-table-back-pointers_2","text":"Question: From which neighbor did we expand to the highlighted cell? Answer: Upper neighbor,because \\(X[i] \\neq Y[j]\\) and \\(LCS[i, j-1] = LCS[i-1, j]\\) . (See pseudo-code to see how ties are handled.) section{ font-size: 25px; }","title":"Do we really need the b table (back-pointers)?"},{"location":"tr/week-6/ce100-week-6-lcs/#improving-the-space-requirements","text":"We can eliminate the b table altogether each \\(c[i, j]\\) entry depends only on \\(3\\) other \\(c\\) table entries: \\(c[i-1, j-1]\\) , \\(c[i-1, j]\\) and \\(c[i, j-1]\\) Given the value of \\(c[i, j]\\) : We can determine in \\(O(1)\\) time which of these \\(3\\) values was used to compute \\(c[i, j]\\) without inspecting table \\(b\\) We save \\(\\Theta(mn)\\) space by this method However, space requirement is still \\(\\Theta(mn)\\) since we need \\(\\Theta(mn)\\) space for the \\(c\\) table anyway section{ font-size: 25px; }","title":"Improving the Space Requirements"},{"location":"tr/week-6/ce100-week-6-lcs/#what-if-we-store-the-last-2-rows-only","text":"To compute \\(c[i, j]\\) , we only need \\(c[i-1, j-1]\\) , \\(c[i-1, j]\\) ,and \\(c[i-1, j-1]\\) So, we can store only the last two rows. section{ font-size: 25px; }","title":"What if we store the last 2 rows only?"},{"location":"tr/week-6/ce100-week-6-lcs/#what-if-we-store-the-last-2-rows-only_1","text":"To compute \\(c[i, j]\\) , we only need \\(c[i-1, j-1]\\) , \\(c[i-1, j]\\) , and \\(c[i-1, j-1]\\) So, we can store only the last two rows. section{ font-size: 25px; }","title":"What if we store the last 2 rows only?"},{"location":"tr/week-6/ce100-week-6-lcs/#what-if-we-store-the-last-2-rows-only_2","text":"To compute \\(c[i, j]\\) , we only need \\(c[i-1, j-1]\\) , \\(c[i-1, j]\\) , and \\(c[i-1, j-1]\\) So, we can store only the last two rows. This reduces space complexity from \\(\\Theta(mn)\\) to \\(\\Theta(n)\\) . Is there a problem with this approach? section{ font-size: 25px; }","title":"What if we store the last 2 rows only?"},{"location":"tr/week-6/ce100-week-6-lcs/#what-if-we-store-the-last-2-rows-only_3","text":"Is there a problem with this approach? We cannot construct the optimal solution because we cannot backtrace anymore. This approach works if we only need the length of an LCS, not the actual LCS.","title":"What if we store the last 2 rows only?"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-4-optimal-binary-search-tree","text":"","title":"Problem 4 Optimal Binary Search Tree"},{"location":"tr/week-6/ce100-week-6-lcs/#reminder-binary-search-tree-bst","text":"","title":"Reminder: Binary Search Tree (BST)"},{"location":"tr/week-6/ce100-week-6-lcs/#binary-search-tree-example","text":"Example: English-to-French translation Organize (English, French) word pairs in a BST Keyword: English word Satellite Data: French word We can search for an English word (node key) efficiently, and return the corresponding French word (satellite data).","title":"Binary Search Tree Example"},{"location":"tr/week-6/ce100-week-6-lcs/#ascii-table","text":"section{ font-size: 25px; }","title":"ASCII Table"},{"location":"tr/week-6/ce100-week-6-lcs/#binary-search-tree-example_1","text":"Suppose we know the frequency of each keyword in texts: $$ \\underset{5\\%}{\\underline{begin}}, \\underset{40\\%}{\\underline{do}}, \\underset{8\\%}{\\underline{else}}, \\underset{4\\%}{\\underline{end}}, \\underset{10\\%}{\\underline{if}}, \\underset{10\\%}{\\underline{then}}, \\underset{23\\%}{\\underline{while}}, $$ section{ font-size: 25px; }","title":"Binary Search Tree Example"},{"location":"tr/week-6/ce100-week-6-lcs/#cost-of-a-binary-search-tree","text":"Example: If we search for keyword \"while\" , we need to access \\(3\\) nodes. So, \\(23%\\) of the queries will have cost of \\(3\\) . \\[ \\begin{align*} \\text{Total Cost} &= \\sum \\limits_{i}^{}(\\text{depth}(i)+1)\\text{freq}(i) \\\\ &= 1 \\times 0.04 + 2 \\times 0.4 + \\\\ & 2 \\times 0.1 + 3 \\times 0.05 + \\\\ & 3 \\times 0.08 + 3 \\times 0.1 + \\\\ & 3 \\times 0.23 \\\\ &= 2.42 \\end{align*} \\] section{ font-size: 25px; }","title":"Cost of a Binary Search Tree"},{"location":"tr/week-6/ce100-week-6-lcs/#cost-of-a-binary-search-tree_1","text":"Example: If we search for keyword \"while\" , we need to access \\(3\\) nodes. So, \\(23%\\) of the queries will have cost of \\(3\\) . \\[ \\begin{align*} \\text{Total Cost} &= \\sum \\limits_{i}^{}(\\text{depth}(i)+1)\\text{freq}(i) \\\\ &= 1 \\times 0.4 + 2 \\times 0.05 + 2 \\times 0.23 + \\\\ & 3 \\times 0.1 + 4 \\times 0.08 + \\\\ & 4 \\times 0.1 + 5 \\times 0.04 \\\\ &= 2.18 \\end{align*} \\] This is in fact an optimal BST. section{ font-size: 25px; }","title":"Cost of a Binary Search Tree"},{"location":"tr/week-6/ce100-week-6-lcs/#optimal-binary-search-tree-problem","text":"Given: A collection of \\(n\\) keys \\(K_1 < K_2 < \\dots K_n\\) to be stored in a BST . The corresponding \\(p_i\\) values for \\(1 \\leq i \\leq n\\) \\(p_i\\) : probability of searching for key \\(K_i\\) Find: An optimal BST with minimum total cost: \\[ \\begin{align*} \\text{Total Cost} &= \\sum \\limits_{i}^{}(\\text{depth}(i)+1)\\text{freq}(i) \\end{align*} \\] Note: The BST will be static. Only search operations will be performed. No insert, no delete, etc. section{ font-size: 25px; }","title":"Optimal Binary Search Tree Problem"},{"location":"tr/week-6/ce100-week-6-lcs/#cost-of-a-binary-search-tree_2","text":"Lemma 1 : Let \\(Tij\\) be a BST containing keys \\(K_i < K_{i+1} < \\dots < K_j\\) . Let \\(T_L\\) and \\(T_R\\) be the left and right subtrees of \\(T\\) . Then we have: \\[ \\begin{align*} \\text{cost}(T_{ij})=\\text{cost}(T_{L})+\\text{cost}(T_{R})+\\sum \\limits_{h=i}^{j}p_h \\end{align*} \\] Intuition: When we add the root node, the depth of each node in \\(T_L\\) and \\(T_R\\) increases by \\(1\\) . So, the cost of node \\(h\\) increases by \\(p_h\\) . In addition, the cost of root node \\(r\\) is \\(p_r\\) . That\u2019s why, we have the last term at the end of the formula above. section{ font-size: 25px; }","title":"Cost of a Binary Search Tree"},{"location":"tr/week-6/ce100-week-6-lcs/#optimal-substructure-property","text":"Lemma 2: Optimal substructure property Consider an optimal BST \\(T_{ij}\\) for keys \\(K_i < K_{i+1} < \\dots < K_j\\) Let \\(K_m\\) be the key at the root of \\(T_{ij}\\) Then: \\(T_{i,m-1}\\) is an optimal BST for subproblem containing keys: \\(K_i < \\dots < K_{m-1}\\) \\(T_{m+1,j}\\) is an optimal BST for subproblem containing keys: \\(K_{m+1} < \\dots < K_j\\) \\[ \\begin{align*} \\text{cost}(T_{ij})=\\text{cost}(T_{i,m-1})+\\text{cost}(T_{m+1,j})+\\sum \\limits_{h=i}^{j}p_h \\end{align*} \\] section{ font-size: 25px; }","title":"Optimal Substructure Property"},{"location":"tr/week-6/ce100-week-6-lcs/#recursive-formulation","text":"Note: We don\u2019t know which root vertex leads to the minimum total cost. So, we need to try each vertex \\(m\\) , and choose the one with minimum total cost. \\(c[i, j]\\) : cost of an optimal BST \\(T_{ij}\\) for the subproblem \\(K_i < \\dots < K_j\\) \\[ \\begin{align*} & c[i,j] = \\begin{cases} & 0 & \\text{if} \\ i>j \\\\ & \\underset{i \\leq r \\leq j}{\\text{min}}\\{ c[i,r-1]+c[r+1,j]+P_{ij} \\} & \\text{otherwise} \\\\ \\end{cases} \\\\ & \\text{where} \\ P_{ij}= \\sum \\limits_{h=i}^{j}p_h \\end{align*} \\] section{ font-size: 25px; }","title":"Recursive Formulation"},{"location":"tr/week-6/ce100-week-6-lcs/#bottom-up-computation_3","text":"\\[ \\begin{align*} & c[i,j] = \\begin{cases} & 0 & \\text{if} \\ i>j \\\\ & \\underset{i \\leq r \\leq j}{\\text{min}}\\{ c[i,r-1]+c[r+1,j]+P_{ij} \\} & \\text{otherwise} \\\\ \\end{cases} \\end{align*} \\] How to choose the order in which we process \\(c[i, j]\\) values? Before computing \\(c[i, j]\\) , we have to make sure that the values for \\(c[i, r-1]\\) and \\(c[r+1,j]\\) have been computed for all \\(r\\) . section{ font-size: 25px; }","title":"Bottom-up computation"},{"location":"tr/week-6/ce100-week-6-lcs/#bottom-up-computation_4","text":"\\[ \\begin{align*} & c[i,j] = \\begin{cases} & 0 & \\text{if} \\ i>j \\\\ & \\underset{i \\leq r \\leq j}{\\text{min}}\\{ c[i,r-1]+c[r+1,j]+P_{ij} \\} & \\text{otherwise} \\\\ \\end{cases} \\end{align*} \\] \\(c[i,j]\\) must be processed after \\(c[i,r-1]\\) and \\(c[r+1,j]\\) section{ font-size: 25px; }","title":"Bottom-up computation"},{"location":"tr/week-6/ce100-week-6-lcs/#bottom-up-computation_5","text":"\\[ \\begin{align*} & c[i,j] = \\begin{cases} & 0 & \\text{if} \\ i>j \\\\ & \\underset{i \\leq r \\leq j}{\\text{min}}\\{ c[i,r-1]+c[r+1,j]+P_{ij} \\} & \\text{otherwise} \\\\ \\end{cases} \\end{align*} \\] If the entries \\(c[i,j]\\) are computed in the shown order, then \\(c[i,r-1]\\) and \\(c[r+1,j]\\) values are guaranteed to be computed before \\(c[i,j]\\) . section{ font-size: 25px; }","title":"Bottom-up computation"},{"location":"tr/week-6/ce100-week-6-lcs/#computing-the-optimal-bst-cost","text":"\\[ \\begin{align*} & \\text{OPTIMAL-BST-COST} (p, n) \\\\ & \\quad \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad c[i, i-1] \\leftarrow 0 \\\\ & \\qquad c[i, i] \\leftarrow p[i] \\\\ & \\qquad R[i, j] \\leftarrow i \\\\ & \\quad PS[1] \\leftarrow p[1] \\Longleftarrow PS[i] \\rightarrow \\text{ prefix-sum } (i): \\text{Sum of all} \\ p[j] \\ \\text{values for} \\ j \\leq i \\\\ & \\quad \\text{for} \\ i \\leftarrow 2 \\ \\text{to} \\ n \\ \\text{do} \\\\ & \\qquad PS[i] \\leftarrow p[i] + PS[i-1] \\Longleftarrow \\text{compute the prefix sum} \\\\ & \\quad \\text{for} \\ d \\leftarrow 1 \\ \\text{to} \\ n\u22121 \\ \\text{do} \\Longleftarrow \\text{BSTs with} \\ d+1 \\ \\text{consecutive keys} \\\\ & \\qquad \\text{for} \\ i \\leftarrow 1 \\ \\text{to} \\ n \u2013 d \\ \\text{do} \\\\ & \\qquad \\quad j \\leftarrow i + d \\\\ & \\qquad \\quad c[i, j] \\leftarrow \\infty \\\\ & \\qquad \\quad \\text{for} \\ r \\leftarrow i \\ \\text{to} \\ j \\ \\text{do} \\\\ & \\qquad \\qquad q \\leftarrow min\\{c[i,r-1] + c[r+1, j]\\} + PS[j] \u2013 PS[i-1]\\} \\\\ & \\qquad \\qquad \\text{if} \\ q < c[i, j] \\ \\text{then} \\\\ & \\qquad \\qquad \\quad c[i, j] \\leftarrow q \\\\ & \\qquad \\qquad \\quad R[i, j] \\leftarrow r \\\\ & \\quad \\text{return} \\ c[1, n], R \\end{align*} \\] section{ font-size: 25px; }","title":"Computing the Optimal BST Cost"},{"location":"tr/week-6/ce100-week-6-lcs/#note-on-prefix-sum","text":"We need \\(P_{ij}\\) values for each \\(i, j (1 \u2264 i \u2264 n \\ \\text{and} \\ 1 \u2264 j \u2264 n)\\) , where: \\[ \\begin{align*} P_{ij} = \\sum \\limits_{h=i}^{j}p_h \\end{align*} \\] If we compute the summation directly for every \\((i, j)\\) pair, the runtime would be \\(\\Theta(n^3)\\) . Instead, we spend \\(O(n)\\) time in preprocessing to compute the prefix sum array PS . Then we can compute each \\(P_{ij}\\) in \\(O(1)\\) time using PS . section{ font-size: 25px; }","title":"Note on Prefix Sum"},{"location":"tr/week-6/ce100-week-6-lcs/#note-on-prefix-sum_1","text":"In preprocessing, compute for each \\(i\\) : \\(PS[i]\\) : the sum of \\(p[j]\\) values for \\(1 \\leq j \\leq i\\) Then, we can compute \\(P_{ij}\\) in \\(O(1)\\) time as follows: \\(P_{ij} = PS[i] \u2013 PS[j-1]\\) Example: \\[ \\begin{align*} p &: \\overset{1}{0.05} \\ \\overset{2}{0.02} \\ \\overset{3}{0.06} \\ \\overset{4}{0.07} \\ \\overset{5}{0.20} \\ \\overset{6}{0.05} \\ \\overset{7}{0.08} \\ \\overset{8}{0.02} \\\\ PS &: \\overset{1}{0.05} \\ \\overset{2}{0.07} \\ \\overset{3}{0.13} \\ \\overset{4}{0.20} \\ \\overset{5}{0.40} \\ \\overset{6}{0.45} \\ \\overset{7}{0.53} \\ \\overset{8}{0.55} \\\\[10 pt] P_{27} &= PS[7] \u2013 PS[1] = 0.53 \u2013 0.05 = 0.48 \\\\ P_{36} &= PS[6] \u2013 PS[2] = 0.45 \u2013 0.07 = 0.38 \\end{align*} \\]","title":"Note on Prefix Sum"},{"location":"tr/week-6/ce100-week-6-lcs/#review","text":"","title":"REVIEW"},{"location":"tr/week-6/ce100-week-6-lcs/#overlapping-subproblems-property-in-dynamic-programming","text":"Dynamic Programming is an algorithmic paradigm that solves a given complex problem by breaking it into subproblems and stores the results of subproblems to avoid computing the same results again.","title":"Overlapping Subproblems Property in Dynamic Programming"},{"location":"tr/week-6/ce100-week-6-lcs/#overlapping-subproblems-property-in-dynamic-programming_1","text":"Following are the two main properties of a problem that suggests that the given problem can be solved using Dynamic programming. Overlapping Subproblems Optimal Substructure","title":"Overlapping Subproblems Property in Dynamic Programming"},{"location":"tr/week-6/ce100-week-6-lcs/#overlapping-subproblems_2","text":"Like Divide and Conquer, Dynamic Programming combines solutions to sub-problems. Dynamic Programming is mainly used when solutions of the same subproblems are needed again and again. In dynamic programming, computed solutions to subproblems are stored in a table so that these don\u2019t have to be recomputed. So Dynamic Programming is not useful when there are no common (overlapping) subproblems because there is no point storing the solutions if they are not needed again.","title":"Overlapping Subproblems"},{"location":"tr/week-6/ce100-week-6-lcs/#overlapping-subproblems_3","text":"For example, Binary Search doesn\u2019t have common subproblems. If we take an example of following recursive program for Fibonacci Numbers, there are many subproblems that are solved again and again.","title":"Overlapping Subproblems"},{"location":"tr/week-6/ce100-week-6-lcs/#simple-recursion","text":"\\(f(n) = f(n-1) + f(n-2)\\) C sample code: #include <stdio.h> // a simple recursive program to compute fibonacci numbers int fib ( int n ) { if ( n <= 1 ) return n ; else return fib ( n -1 ) + fib ( n -2 ); } int main () { int n = 5 ; printf ( \"Fibonacci number is %d \" , fib ( n )); return 0 ; }","title":"Simple Recursion"},{"location":"tr/week-6/ce100-week-6-lcs/#simple-recursion_1","text":"Output Fibonacci number is 5","title":"Simple Recursion"},{"location":"tr/week-6/ce100-week-6-lcs/#simple-recursion_2","text":"\\(f(n) = f(n-1) + f(n-2)\\) /* a simple recursive program for Fibonacci numbers */ public class Fibonacci { public static void main ( String [] args ) { int n = Integer . parseInt ( args [ 0 ] ); System . out . println ( fib ( n )); } public static int fib ( int n ) { if ( n <= 1 ) return n ; return fib ( n - 1 ) + fib ( n - 2 ); } }","title":"Simple Recursion"},{"location":"tr/week-6/ce100-week-6-lcs/#simple-recursion_3","text":"\\(f(n) = f(n-1) + f(n-2)\\) public class Fibonacci { public static void Main ( string [] args ) { int n = int . Parse ( args [ 0 ]); Console . WriteLine ( fib ( n )); } public static int fib ( int n ) { if ( n <= 1 ) return n ; return fib ( n - 1 ) + fib ( n - 2 ); } }","title":"Simple Recursion"},{"location":"tr/week-6/ce100-week-6-lcs/#recursion-tree-for-execution-of-fib5","text":"fib(5) / \\ fib(4) fib(3) / \\ / \\ fib(3) fib(2) fib(2) fib(1) / \\ / \\ / \\ fib(2) fib(1) fib(1) fib(0) fib(1) fib(0) / \\ fib(1) fib(0) We can see that the function fib(3) is being called 2 times. If we would have stored the value of fib(3) , then instead of computing it again, we could have reused the old stored value.","title":"Recursion tree for execution of fib(5)"},{"location":"tr/week-6/ce100-week-6-lcs/#recursion-tree-for-execution-of-fib5_1","text":"There are following two different ways to store the values so that these values can be reused: Memoization (Top Down) Tabulation (Bottom Up)","title":"Recursion tree for execution of fib(5)"},{"location":"tr/week-6/ce100-week-6-lcs/#memoization-top-down","text":"The memoized program for a problem is similar to the recursive version with a small modification that looks into a lookup table before computing solutions. We initialize a lookup array with all initial values as NIL . Whenever we need the solution to a subproblem, we first look into the lookup table. If the precomputed value is there then we return that value, otherwise, we calculate the value and put the result in the lookup table so that it can be reused later.","title":"Memoization (Top Down)"},{"location":"tr/week-6/ce100-week-6-lcs/#memoization-top-down_1","text":"Following is the memoized version for the nth Fibonacci Number. C++ Version: /* C++ program for Memoized version for nth Fibonacci number */ #include <bits/stdc++.h> using namespace std ; #define NIL -1 #define MAX 100 int lookup [ MAX ];","title":"Memoization (Top Down)"},{"location":"tr/week-6/ce100-week-6-lcs/#memoization-top-down_2","text":"C++ Version: /* Function to initialize NIL values in lookup table */ void _initialize () { int i ; for ( i = 0 ; i < MAX ; i ++ ) lookup [ i ] = NIL ; }","title":"Memoization (Top Down)"},{"location":"tr/week-6/ce100-week-6-lcs/#memoization-top-down_3","text":"C++ Version: /* function for nth Fibonacci number */ int fib ( int n ) { if ( lookup [ n ] == NIL ) { if ( n <= 1 ) lookup [ n ] = n ; else lookup [ n ] = fib ( n - 1 ) + fib ( n - 2 ); } return lookup [ n ]; }","title":"Memoization (Top Down)"},{"location":"tr/week-6/ce100-week-6-lcs/#memoization-top-down_4","text":"C++ Version: // Driver code int main () { int n = 40 ; _initialize (); cout << \"Fibonacci number is \" << fib ( n ); return 0 ; }","title":"Memoization (Top Down)"},{"location":"tr/week-6/ce100-week-6-lcs/#memoization-top-down_5","text":"Java Version: /* Java program for Memoized version */ public class Fibonacci { final int MAX = 100 ; final int NIL = - 1 ; int lookup [] = new int [ MAX ] ; /* Function to initialize NIL values in lookup table */ void _initialize () { for ( int i = 0 ; i < MAX ; i ++ ) lookup [ i ] = NIL ; }","title":"Memoization (Top Down)"},{"location":"tr/week-6/ce100-week-6-lcs/#memoization-top-down_6","text":"Java Version: /* function for nth Fibonacci number */ int fib ( int n ) { if ( lookup [ n ] == NIL ) { if ( n <= 1 ) lookup [ n ] = n ; else lookup [ n ] = fib ( n - 1 ) + fib ( n - 2 ); } return lookup [ n ] ; }","title":"Memoization (Top Down)"},{"location":"tr/week-6/ce100-week-6-lcs/#memoization-top-down_7","text":"Java Version: public static void main ( String [] args ) { Fibonacci f = new Fibonacci (); int n = 40 ; f . _initialize (); System . out . println ( \"Fibonacci number is\" + \" \" + f . fib ( n )); } }","title":"Memoization (Top Down)"},{"location":"tr/week-6/ce100-week-6-lcs/#memoization-top-down_8","text":"C# Version: // C# program for Memoized versionof nth Fibonacci number using System ; class FiboCalcMemoized { static int MAX = 100 ; static int NIL = - 1 ; static int [] lookup = new int [ MAX ]; /* Function to initialize NIL values in lookup table */ static void initialize () { for ( int i = 0 ; i < MAX ; i ++) lookup [ i ] = NIL ; }","title":"Memoization (Top Down)"},{"location":"tr/week-6/ce100-week-6-lcs/#memoization-top-down_9","text":"C# Version: /* function for nth Fibonacci number */ static int fib ( int n ) { if ( lookup [ n ] == NIL ) { if ( n <= 1 ) lookup [ n ] = n ; else lookup [ n ] = fib ( n - 1 ) + fib ( n - 2 ); } return lookup [ n ]; }","title":"Memoization (Top Down)"},{"location":"tr/week-6/ce100-week-6-lcs/#memoization-top-down_10","text":"C# Version: // Driver code public static void Main () { int n = 40 ; initialize (); Console . Write ( \"Fibonacci number is\" + \" \" + fib ( n )); } }","title":"Memoization (Top Down)"},{"location":"tr/week-6/ce100-week-6-lcs/#tabulation-bottom-up","text":"The tabulated program for a given problem builds a table in bottom-up fashion and returns the last entry from the table. For example, for the same Fibonacci number, we first calculate fib(0) then fib(1) then fib(2) then fib(3) , and so on. So literally, we are building the solutions of subproblems bottom-up.","title":"Tabulation (Bottom Up)"},{"location":"tr/week-6/ce100-week-6-lcs/#tabulation-bottom-up_1","text":"C++ Version: /* C program for Tabulated version */ #include <stdio.h> int fib ( int n ) { int f [ n + 1 ]; int i ; f [ 0 ] = 0 ; f [ 1 ] = 1 ; for ( i = 2 ; i <= n ; i ++ ) f [ i ] = f [ i - 1 ] + f [ i - 2 ]; return f [ n ]; }","title":"Tabulation (Bottom Up)"},{"location":"tr/week-6/ce100-week-6-lcs/#tabulation-bottom-up_2","text":"C++ Version: ... int main () { int n = 9 ; printf ( \"Fibonacci number is %d \" , fib ( n )); return 0 ; } Output: Fibonacci number is 34","title":"Tabulation (Bottom Up)"},{"location":"tr/week-6/ce100-week-6-lcs/#tabulation-bottom-up_3","text":"Java Version: /* Java program for Tabulated version */ public class Fibonacci { public static void main ( String [] args ) { int n = 9 ; System . out . println ( \"Fibonacci number is \" + fib ( n )); }","title":"Tabulation (Bottom Up)"},{"location":"tr/week-6/ce100-week-6-lcs/#tabulation-bottom-up_4","text":"Java Version: /* Function to calculate nth Fibonacci number */ static int fib ( int n ) { int f [] = new int [ n + 1 ] ; f [ 0 ] = 0 ; f [ 1 ] = 1 ; for ( int i = 2 ; i <= n ; i ++ ) f [ i ] = f [ i - 1 ] + f [ i - 2 ] ; return f [ n ] ; } }","title":"Tabulation (Bottom Up)"},{"location":"tr/week-6/ce100-week-6-lcs/#tabulation-bottom-up_5","text":"C# Version: // C# program for Tabulated version using System ; class Fibonacci { static int fib ( int n ) { int [] f = new int [ n + 1 ]; f [ 0 ] = 0 ; f [ 1 ] = 1 ; for ( int i = 2 ; i <= n ; i ++) f [ i ] = f [ i - 1 ] + f [ i - 2 ]; return f [ n ]; } public static void Main () { int n = 9 ; Console . Write ( \"Fibonacci number is\" + \" \" + fib ( n )); } } Both Tabulated and Memoized store the solutions of subproblems. In Memoized version, the table is filled on demand while in the Tabulated version, starting from the first entry, all entries are filled one by one. Unlike the Tabulated version, all entries of the lookup table are not necessarily filled in Memoized version. To see the optimization achieved by Memoized and Tabulated solutions over the basic Recursive solution, see the time taken by following runs for calculating the 40 th Fibonacci number: Recursive Solution: https://ide.geeksforgeeks.org/vHt6ly Memoized Solution: https://ide.geeksforgeeks.org/Z94jYR Tabulated Solution: https://ide.geeksforgeeks.org/12C5bP","title":"Tabulation (Bottom Up)"},{"location":"tr/week-6/ce100-week-6-lcs/#optimal-substructure-property-in-dynamic-programming","text":"A given problems has Optimal Substructure Property if optimal solution of the given problem can be obtained by using optimal solutions of its subproblems. For example, the Shortest Path problem has following optimal substructure property: If a node x lies in the shortest path from a source node u to destination node v then the shortest path from u to v is combination of shortest path from u to x and shortest path from x to v. The standard All Pair Shortest Path algorithm like Floyd\u2013Warshall and Single Source Shortest path algorithm for negative weight edges like Bellman\u2013Ford are typical examples of Dynamic Programming.","title":"Optimal Substructure Property in Dynamic Programming"},{"location":"tr/week-6/ce100-week-6-lcs/#optimal-substructure-property-in-dynamic-programming_1","text":"On the other hand, the Longest Path problem doesn\u2019t have the Optimal Substructure property. Here by Longest Path we mean longest simple path (path without cycle) between two nodes","title":"Optimal Substructure Property in Dynamic Programming"},{"location":"tr/week-6/ce100-week-6-lcs/#optimal-substructure-property-in-dynamic-programming_2","text":"There are two longest paths from q to t: q\u2192r\u2192t and q\u2192s\u2192t. Unlike shortest paths, these longest paths do not have the optimal substructure property. For example, the longest path q\u2192r\u2192t is not a combination of longest path from q to r and longest path from r to t, because the longest path from q to r is q\u2192s\u2192t\u2192r and the longest path from r to t is r\u2192q\u2192s\u2192t.","title":"Optimal Substructure Property in Dynamic Programming"},{"location":"tr/week-6/ce100-week-6-lcs/#most-common-dynamic-programming-interview-questions","text":"","title":"Most Common Dynamic Programming Interview Questions"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-1-longest-increasing-subsequence","text":"Problem-1: Longest Increasing Subsequence","title":"Problem-1: Longest Increasing Subsequence"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-1-longest-increasing-subsequence_1","text":"","title":"Problem-1: Longest Increasing Subsequence"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-2-edit-distance","text":"Problem-2: Edit Distance","title":"Problem-2: Edit Distance"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-2-edit-distance-recursive","text":"","title":"Problem-2: Edit Distance (Recursive)"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-2-edit-distance-dp","text":"https://www.coursera.org/learn/dna-sequencing","title":"Problem-2: Edit Distance (DP)"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-2-edit-distance-dp_1","text":"","title":"Problem-2: Edit Distance (DP)"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-2-edit-distance-other","text":"","title":"Problem-2: Edit Distance (Other)"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-3-partition-a-set-into-two-subsets-such-that-the-difference-of-subset-sums-is-minimum","text":"Problem-3: Partition a set into two subsets such that the difference of subset sums is minimum","title":"Problem-3: Partition a set into two subsets such that the difference of subset sums is minimum"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-4-count-number-of-ways-to-cover-a-distance","text":"Problem-4: Count number of ways to cover a distance","title":"Problem-4: Count number of ways to cover a distance"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-5-find-the-longest-path-in-a-matrix-with-given-constraints","text":"Problem-5: Find the longest path in a matrix with given constraints","title":"Problem-5: Find the longest path in a matrix with given constraints"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-6-subset-sum-problem","text":"Problem-6: Subset Sum Problem","title":"Problem-6: Subset Sum Problem"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-7-optimal-strategy-for-a-game","text":"Problem-7: Optimal Strategy for a Game","title":"Problem-7: Optimal Strategy for a Game"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-8-0-1-knapsack-problem","text":"Problem-8: 0-1 Knapsack Problem","title":"Problem-8: 0-1 Knapsack Problem"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-9-boolean-parenthesization-problem","text":"Problem-9: Boolean Parenthesization Problem","title":"Problem-9: Boolean Parenthesization Problem"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-10-shortest-common-supersequence","text":"Problem-10: Shortest Common Supersequence","title":"Problem-10: Shortest Common Supersequence"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-11-partition-problem","text":"Problem-11: Partition Problem","title":"Problem-11: Partition Problem"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-12-cutting-a-rod","text":"Problem-12: Cutting a Rod","title":"Problem-12: Cutting a Rod"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-13-coin-change","text":"Problem-13: Coin Change","title":"Problem-13: Coin Change"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-14-word-break-problem","text":"Problem-14: Word Break Problem","title":"Problem-14: Word Break Problem"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-15-maximum-product-cutting","text":"Problem-15: Maximum Product Cutting","title":"Problem-15: Maximum Product Cutting"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-16-dice-throw","text":"Problem-16: Dice Throw","title":"Problem-16: Dice Throw"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-16-dice-throw_1","text":"","title":"Problem-16: Dice Throw"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-17-box-stacking-problem","text":"Problem-17: Box Stacking Problem","title":"Problem-17: Box Stacking Problem"},{"location":"tr/week-6/ce100-week-6-lcs/#problem-18-egg-dropping-puzzle","text":"Problem-18: Egg Dropping Puzzle","title":"Problem-18: Egg Dropping Puzzle"},{"location":"tr/week-6/ce100-week-6-lcs/#references","text":"Introduction to Algorithms, Third Edition | The MIT Press CLRS Bilkent CS473 Course Notes (new) Bilkent CS473 Course Notes (old) \\(-End-Of-Week-6-Course-Module-\\)","title":"References"},{"location":"tr/week-7/ce100-week-7-knapsack/","text":"CE100 Algorithms and Programming II \u00b6 Week-7 (Greedy Algorithms, Knapsack) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Greedy Algorithms, Knapsack \u00b6 Outline \u00b6 Greedy Algorithms and Dynamic Programming Differences Greedy Algorithms Activity Selection Problem Knapsack Problems The 0-1 knapsack problem The fractional knapsack problem References \u00b6 TODO","title":"Week-7 (Greedy Algorithms, Knapsack)"},{"location":"tr/week-7/ce100-week-7-knapsack/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"tr/week-7/ce100-week-7-knapsack/#week-7-greedy-algorithms-knapsack","text":"","title":"Week-7 (Greedy Algorithms, Knapsack)"},{"location":"tr/week-7/ce100-week-7-knapsack/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"tr/week-7/ce100-week-7-knapsack/#greedy-algorithms-knapsack","text":"","title":"Greedy Algorithms, Knapsack"},{"location":"tr/week-7/ce100-week-7-knapsack/#outline","text":"Greedy Algorithms and Dynamic Programming Differences Greedy Algorithms Activity Selection Problem Knapsack Problems The 0-1 knapsack problem The fractional knapsack problem","title":"Outline"},{"location":"tr/week-7/ce100-week-7-knapsack/#references","text":"TODO","title":"References"},{"location":"tr/week-8/ce100-week-8-midterm/","text":"CE100 Algorithms and Programming II \u00b6 Week-8 (Midterm) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Midterm \u00b6 Outline \u00b6 References \u00b6 TODO","title":"Week-8 (Midterm)"},{"location":"tr/week-8/ce100-week-8-midterm/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"tr/week-8/ce100-week-8-midterm/#week-8-midterm","text":"","title":"Week-8 (Midterm)"},{"location":"tr/week-8/ce100-week-8-midterm/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"tr/week-8/ce100-week-8-midterm/#midterm","text":"","title":"Midterm"},{"location":"tr/week-8/ce100-week-8-midterm/#outline","text":"","title":"Outline"},{"location":"tr/week-8/ce100-week-8-midterm/#references","text":"TODO","title":"References"},{"location":"tr/week-9/ce100-week-9-huffman/","text":"CE100 Algorithms and Programming II \u00b6 Week-9 (Huffman Coding) \u00b6 Spring Semester, 2021-2022 \u00b6 Download DOC , SLIDE , PPTX Huffman Coding \u00b6 Outline \u00b6 Heap Data Structure Heap Sort Huffman Coding References \u00b6 TODO","title":"Week-9 (Huffman Coding)"},{"location":"tr/week-9/ce100-week-9-huffman/#ce100-algorithms-and-programming-ii","text":"","title":"CE100 Algorithms and Programming II"},{"location":"tr/week-9/ce100-week-9-huffman/#week-9-huffman-coding","text":"","title":"Week-9 (Huffman Coding)"},{"location":"tr/week-9/ce100-week-9-huffman/#spring-semester-2021-2022","text":"Download DOC , SLIDE , PPTX","title":"Spring Semester, 2021-2022"},{"location":"tr/week-9/ce100-week-9-huffman/#huffman-coding","text":"","title":"Huffman Coding"},{"location":"tr/week-9/ce100-week-9-huffman/#outline","text":"Heap Data Structure Heap Sort Huffman Coding","title":"Outline"},{"location":"tr/week-9/ce100-week-9-huffman/#references","text":"TODO","title":"References"}]}